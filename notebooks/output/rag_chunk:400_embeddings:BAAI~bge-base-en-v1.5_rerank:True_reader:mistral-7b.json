[{"question": "What architecture is the `tokenizers-linux-x64-musl` binary designed for?\n", "true_answer": "x86_64-unknown-linux-musl", "source_doc": "huggingface/tokenizers/blob/main/bindings/node/npm/linux-x64-musl/README.md", "generated_answer": "The `tokenizers-linux-x64-musl` binary is designed for the **x86_64-unknown-linux-musl** architecture. (Answer according to context provided.)", "retrieved_docs": ["`tokenizers-linux-x64-musl`\n\nThis is the **x86_64-unknown-linux-musl** binary for `tokenizers`", "`tokenizers-linux-arm64-musl`\n\nThis is the **aarch64-unknown-linux-musl** binary for `tokenizers`", "`tokenizers-linux-x64-gnu`\n\nThis is the **x86_64-unknown-linux-gnu** binary for `tokenizers`", "`tokenizers-linux-arm64-gnu`\n\nThis is the **aarch64-unknown-linux-gnu** binary for `tokenizers`", "`tokenizers-android-arm64`\n\nThis is the **aarch64-linux-android** binary for `tokenizers`", "`tokenizers-freebsd-x64`\n\nThis is the **x86_64-unknown-freebsd** binary for `tokenizers`", "`tokenizers-win32-x64-msvc`\n\nThis is the **x86_64-pc-windows-msvc** binary for `tokenizers`"], "eval_score_GPT4": "5", "eval_feedback_GPT4": "Feedback: The response provided is completely correct, accurate, and factual. It matches the reference answer exactly, indicating that the `tokenizers-linux-x64-musl` binary is designed for the x86_64-unknown-linux-musl architecture."}, {"question": "What is the purpose of the BLIP-Diffusion model?\n", "true_answer": "The BLIP-Diffusion model is designed for controllable text-to-image generation and editing.", "source_doc": "huggingface/diffusers/blob/main/docs/source/en/api/pipelines/blip_diffusion.md", "generated_answer": "The purpose of the BLIP-Diffusion model, as described in the given context, is to enable zero-shot subject-driven generation and control-guided zero-shot generation through the use of subject representations in text-to-image generation. This is achieved by combining the BLIP (Bidirectional Encoder Representations from Transformers for Image and Text Retrieval) model for subject representation with diffusion probabilistic models for image generation. The model is proposed in the", "retrieved_docs": ["!--Copyright 2023 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\n\nhttp://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License.\n-->\n\n# BLIP-Diffusion\n\nBLIP-Diffusion was proposed in [BLIP-Diffusion: Pre-trained Subject Representation for Controllable Text-to-Image Generation and Editing](https://arxiv.org/abs/2305.14720). It enables zero-shot subject-driven generation and control-guided zero-shot generation. \n\n\nThe abstract from the paper is:", "The abstract from the paper is:\n\n*Model-based reinforcement learning methods often use learning only for the purpose of estimating an approximate dynamics model, offloading the rest of the decision-making work to classical trajectory optimizers. While conceptually simple, this combination has a number of empirical shortcomings, suggesting that learned models may not be well-suited to standard trajectory optimization. In this paper, we consider what it would look like to fold as much of the trajectory optimization pipeline as possible into the modeling problem, such that sampling from the model and planning with it become nearly identical. The core of our technical approach lies in a diffusion probabilistic model that plans by iteratively denoising trajectories. We show how classifier-guided sampling and image inpainting can be reinterpreted as coherent planning strategies, explore the unusual and useful properties of diffusion-based planning methods, and demonstrate the effectiveness of our framework in control settings that emphasize long-horizon decision-making and test-time flexibility.*\n\nYou can find additional information about the model on the [project page](https://diffusion-planning.github.io/), the [original codebase](https://github.com/jannerm/diffuser), or try it out in a demo [notebook](https://colab.research.google.com/drive/1rXm8CX4ZdN5qivjJ2lhwhkOmt_m0CvU0#scrollTo=6HXJvhyqcITc&uniqifier=1).\n\nThe script to run the model is available [here](https://github.com/huggingface/diffusers/tree/main/examples/reinforcement_learning).", "## What about the general-purpose Stable Diffusion model?\n\nAs we showed with the Pokemon image generation task, it is possible to achieve a high level of optimization of the Stable Diffusion pipeline when using a relatively small amount of training resources. At the same time, it is well-known that training a general-purpose Stable Diffusion model is an [expensive task](https://www.mosaicml.com/blog/training-stable-diffusion-from-scratch-part-2). However, with enough budget and HW resources, it is possible to optimize the general-purpose model using the described approach and tune it to produce high-quality images. The only caveat we have is related to the token merging method that reduces the model capacity substantially. The rule of thumb here is the more complicated the dataset you have for the training, the less merging ratio you should use during the optimization.\n\nIf you enjoyed reading this post, you might also be interested in checking out [this post](https://huggingface.co/blog/stable-diffusion-inference-intel) that discusses other complementary approaches to optimize the performance of Stable Diffusion on 4th generation Intel Xeon CPUs.", "<img src=\"\nassets/78_annotated-diffusion/diffusion-sweater.gif\" width=\"300\" />\n\n# Follow-up reads\n\nNote that the DDPM paper showed that diffusion models are a promising direction for (un)conditional image generation. This has since then (immensely) been improved, most notably for text-conditional image generation. Below, we list some important (but far from exhaustive) follow-up works:", "The implementation is mostly based on the DDIM inversion definition of [Null-text Inversion for Editing Real Images using Guided Diffusion Models](https://huggingface.co/papers/2211.09794) and notebook implementation of the [`DiffEdit`] latent inversion from [Xiang-cd/DiffEdit-stable-diffusion](https://github.com/Xiang-cd/DiffEdit-stable-diffusion/blob/main/diffedit.ipynb).\n\n## Tips\n\nDynamic thresholding from [Imagen](https://huggingface.co/papers/2205.11487) is supported, and for pixel-space\ndiffusion models, you can set both `algorithm_type=\"dpmsolver++\"` and `thresholding=True` to use the dynamic\nthresholding. This thresholding method is unsuitable for latent-space diffusion models such as\nStable Diffusion.\n\n## DPMSolverMultistepInverseScheduler\n[[autodoc]] DPMSolverMultistepInverseScheduler\n\n## SchedulerOutput\n[[autodoc]] schedulers.scheduling_utils.SchedulerOutput", "--\ntitle: \"VQ-Diffusion\" \nthumbnail: /blog/assets/117_vq_diffusion/thumbnail.png\nauthors:\n- user: williamberman\n---\n\n# VQ-Diffusion\n\n\nVector Quantized Diffusion (VQ-Diffusion) is a conditional latent diffusion model developed by the University of Science and Technology of China and Microsoft. Unlike most commonly studied diffusion models, VQ-Diffusion's noising and denoising processes operate on a quantized latent space, i.e., the latent space is composed of a discrete set of vectors. Discrete diffusion models are less explored than their continuous counterparts and offer an interesting point of comparison with autoregressive (AR) models.\n\n- [Hugging Face model card](https://huggingface.co/microsoft/vq-diffusion-ithq)\n- [Hugging Face Spaces](https://huggingface.co/spaces/patrickvonplaten/vq-vs-stable-diffusion)\n- [Original Implementation](https://github.com/microsoft/VQ-Diffusion)\n- [Paper](https://arxiv.org/abs/2111.14822)\n\n### Demo\n\n\ud83e\udde8 Diffusers lets you run VQ-Diffusion with just a few lines of code.\n\nInstall dependencies\n```bash\npip install 'diffusers[torch]' transformers ftfy\n```\n\nLoad the pipeline\n```python\nfrom diffusers import VQDiffusionPipeline\n\npipe = VQDiffusionPipeline.from_pretrained(\"microsoft/vq-diffusion-ithq\")\n```", "The abstract from the paper is:\n\n*Denoising diffusion probabilistic models (DDPMs) have achieved high quality image generation without adversarial training, yet they require simulating a Markov chain for many steps to produce a sample.\nTo accelerate sampling, we present denoising diffusion implicit models (DDIMs), a more efficient class of iterative implicit probabilistic models\nwith the same training procedure as DDPMs. In DDPMs, the generative process is defined as the reverse of a Markovian diffusion process.\nWe construct a class of non-Markovian diffusion processes that lead to the same training objective, but whose reverse process can be much faster to sample from.\nWe empirically demonstrate that DDIMs can produce high quality samples 10\u00d7 to 50\u00d7 faster in terms of wall-clock time compared to DDPMs, allow us to trade off computation for sample quality, and can perform semantically meaningful image interpolation directly in the latent space.*\n\nThe original codebase of this paper can be found at [ermongroup/ddim](https://github.com/ermongroup/ddim), and you can contact the author on [tsong.me](https://tsong.me/).\n\n## Tips\n\nThe paper [Common Diffusion Noise Schedules and Sample Steps are Flawed](https://huggingface.co/papers/2305.08891) claims that a mismatch between the training and inference settings leads to suboptimal inference generation results for Stable Diffusion. To fix this, the authors propose:\n\n<Tip warning={true}>\n\n\ud83e\uddea This is an experimental feature!\n\n</Tip>"], "eval_score_GPT4": "4", "eval_feedback_GPT4": "Feedback: The response provided is mostly correct and factual. It describes the purpose of the BLIP-Diffusion model in terms of enabling zero-shot subject-driven generation and control-guided zero-shot generation, which aligns with the reference answer's mention of controllable text-to-image generation and editing. The response goes into detail about how the model achieves this by combining the BLIP model with diffusion probabilistic models. Although the response is more detailed than the reference answer, it does not contradict it and stays true to the core purpose of the BLIP-Diffusion model."}, {"question": "How can a user claim authorship of a paper on the Hugging Face Hub?\n", "true_answer": "By clicking their name on the corresponding Paper page and clicking \"claim authorship\", then confirming the request in paper settings for admin team validation.", "source_doc": "huggingface/hub-docs/blob/main/docs/hub/paper-pages.md", "generated_answer": "To claim authorship of a paper on the Hugging Face Hub, a user should follow these steps:\n\n1. Go to the Paper page of the paper they want to claim authorship for.\n2. Click on their name in the corresponding Paper page and select \"claim authorship\".\n3. This will redirect them to their paper settings where they can confirm the request.\n4. The admin team will validate the request soon, and once confirmed, the Paper page will show as", "retrieved_docs": ["If your paper is not linked to your account, you can click in your name in the corresponding Paper page and click \"claim authorship\". This will automatically re-direct to your paper settings where you can confirm the request. The admin team will validate your request soon. Once confirmed, the Paper page will show as verified.\n\n<div class=\"flex justify-center\">\n<img class=\"block dark:hidden\" width=\"300\" src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/hub/papers-settings.png\"/>\n<img class=\"hidden dark:block\" width=\"300\" src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/hub/papers-settings-dark.png\"/>\n</div>\n\n\n## Frequently Asked Questions\n\n### Can I control which Paper pages show in my profile?\n\nYes! You can visit your Papers in [settings](https://huggingface.co/settings/papers), where you will see a list of verified papers. There, you can click the \"Show on profile\" checkbox to hide/show it in your profile. \n\n### Do you support ACL anthology?\n\nWe're starting with Arxiv as it accounts for 95% of the paper URLs Hugging Face users have linked in their repos organically. We'll check how this evolve and potentially extend to other paper hosts in the future.\n\n### Can I have a Paper page even if I have no model/dataset/Space?", "* Visit the Paper page.\n* Filter for other models or datasets on the Hub that cite the same paper.\n\n<div class=\"flex justify-center\">\n<img class=\"block dark:hidden\" width=\"300\" src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/hub/datasets-arxiv.png\"/>\n<img class=\"hidden dark:block\" width=\"300\" src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/hub/datasets-arxiv-dark.png\"/>\n</div>\n\n## Claiming authorship to a Paper\n\nThe Hub will attempt to automatically match paper to users based on their email. \n\n<div class=\"flex justify-center\">\n<img class=\"block dark:hidden\" width=\"300\" src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/hub/papers-authors.png\"/>\n<img class=\"hidden dark:block\" width=\"300\" src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/hub/papers-authors-dark.png\"/>\n</div>", "--\ntitle: \"Welcome fastText to the Hugging Face Hub\"\nthumbnail: /blog/assets/147_fasttext/thumbnail.png\nauthors:\n- user: sheonhan\n- user: juanpino\n  guest: true\n---\n\n# Welcome fastText to the Hugging Face Hub\n\n\n[fastText](https://fasttext.cc/) is a library for efficient learning of text representation and classification. [Open-sourced](https://fasttext.cc/blog/2016/08/18/blog-post.html) by Meta AI in 2016, fastText integrates key ideas that have been influential in natural language processing and machine learning over the past few decades: representing sentences using bag of words and bag of n-grams, using subword information, and utilizing a hidden representation to share information across classes. \n\nTo speed up computation, fastText uses hierarchical softmax, capitalizing on the imbalanced distribution of classes. All these techniques offer users scalable solutions for text representation and classification.\n\nHugging Face is now hosting official mirrors of word vectors of all 157 languages and the latest model for language identification. This means that using Hugging Face, you can easily download and use the models with a few commands. \n\n### Finding models\n\nWord vectors for 157 languages and the language identification model can be found in the [Meta AI](https://huggingface.co/facebook) org. For example, you can find the model page for English word vectors [here](https://huggingface.co/facebook/fasttext-en-vectors) and the language identification model [here](https://huggingface.co/facebook/fasttext-language-identification).", "This is equivalent to `huggingface_hub.move_repo()`.\n\n## User API\n\nThe following endpoint gets information about a user.\n\n### GET /api/whoami-v2\n\nGet username and organizations the user belongs to.\n\nPayload:\n\n```js\nheaders = { \"authorization\" :  \"Bearer $token\" }\n```\n\nThis is equivalent to `huggingface_hub.whoami()`.\n\n## Collections API\n\nUse Collections to group repositories from the Hub (Models, Datasets, Spaces and Papers) on a dedicated page.\n\nYou can learn more about it in the Collections [guide](./collections.md). Collections can also be managed using the Python client (see [guide](https://huggingface.co/docs/huggingface_hub/main/en/guides/collections)).\n\n### POST /api/collections\n\nCreate a new collection on the Hub with a title, a description (optional) and a first item (optional). An item is defined by a type (`model`, `dataset`, `space` or `paper`) and an id (repo_id or paper_id on the Hub).\n\nPayload:", "_To be able to push our model to the Hub, you need to register on the [Hugging Face Hub](https://huggingface.co/join)._\n\nWe will use the `notebook_login` util from the `huggingface_hub` package to log into our account. You can get your token in the settings at [Access Tokens](https://huggingface.co/settings/tokens).\n\n\n```python\nfrom huggingface_hub import notebook_login\n\nnotebook_login()\n\n```\n\nSince we are now logged in let's get the `user_id`, which will be used to push the artifacts.\n\n\n```python\nfrom huggingface_hub import HfApi\n\nuser_id = HfApi().whoami()[\"name\"]\n\nprint(f\"user id '{user_id}' will be used during the example\")\n```\n\n\nThe [original BERT](https://arxiv.org/abs/1810.04805) was pretrained on [Wikipedia](https://huggingface.co/datasets/wikipedia) and [BookCorpus](https://huggingface.co/datasets/bookcorpus) datasets. Both datasets are available on the [Hugging Face Hub](https://huggingface.co/datasets) and can be loaded with `datasets`.\n\n_Note: For wikipedia we will use the `20220301`, which is different from the original split._", "Requesting access can only be done from your browser. Go to the model on the Hub and you will be prompted to share your information:\n\n<div class=\"flex justify-center\">\n    <img class=\"block dark:hidden\" src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/hub/models-gated-user-side.png\"/>\n    <img class=\"hidden dark:block\" src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/hub/models-gated-user-side-dark.png\"/>\n</div>\n\nBy clicking on **Agree**, you agree to share your username and email address with the model authors. In some cases, additional fields might be requested. To help the model authors decide whether to grant you access, try to fill out the form as completely as possible.\n\nOnce the access request is sent, there are two possibilities. If the approval mechanism is automatic, you immediately get access to the model files. Otherwise, the requests have to be approved manually by the authors, which can take more time. \n\n<Tip warning={true}>\n\nThe model authors have complete control over model access. In particular, they can decide at any time to block your access to the model without prior notice, regardless of approval mechanism or if your request has already been approved.\n\n</Tip>\n\n### Download files", "--\ntitle: \"Hugging Face Machine Learning Demos on arXiv\" \nthumbnail: /blog/assets/arxiv/thumbnail.png\nauthors:\n- user: abidlabs\n- user: osanseviero\n- user: pcuenq\n---\n\n# Hugging Face Machine Learning Demos on arXiv\n\n\nWe\u2019re very excited to announce that Hugging Face has collaborated with arXiv to make papers more accessible, discoverable, and fun! Starting today, [Hugging Face Spaces](https://huggingface.co/spaces) is integrated with arXivLabs through a Demo tab that includes links to demos created by the community or the authors themselves. By going to the Demos tab of your favorite paper, you can find links to open-source demos and try them out immediately \ud83d\udd25\n\n![You can now find interactive demos under ArXiv papers](/blog/assets/arxiv/recording.gif)\n\nSince its launch in October 2021, Hugging Face Spaces has been used to build and share over 12,000 open-source machine learning demos crafted by the community. With Spaces, Hugging Face users can share, explore, discuss models, and build interactive applications that enable anyone with a browser to try them out without having to run any code. These demos are built using open-source tools such as the Gradio and Streamlit Python libraries, and leverage models and datasets available on the Hugging Face Hub."], "eval_score_GPT4": "4", "eval_feedback_GPT4": "Feedback: The response provided is mostly correct and factual. It outlines a clear sequence of steps that a user should follow to claim authorship of a paper on the Hugging Face Hub, which aligns well with the reference answer. However, the response includes an additional step about going to the Paper page, which is not mentioned in the reference answer but is implied as necessary to perform the subsequent steps. The response also adds detail about the admin team validating the request, which is factual and provides a more complete understanding of the process, even though this detail is not explicitly stated in the reference answer. The response could be more concise, but it does not detract from the accuracy of the information provided."}, {"question": "What is the purpose of the /healthcheck endpoint in the Datasets server API?\n", "true_answer": "Ensure the app is running", "source_doc": "huggingface/datasets-server/blob/main/services/api/README.md", "generated_answer": "The purpose of the /healthcheck endpoint in the Datasets server API is to ensure that the app is running. It is a simple endpoint that can be used to check the status of the server and confirm that it is operational. This endpoint returns a response indicating whether the server is healthy or not. It is a common practice to include a healthcheck endpoint in APIs to facilitate monitoring and management of server-side applications.", "retrieved_docs": ["Datasets server API - rows endpoint\n\n> /rows endpoint\n\n## Configuration\n\nThe service can be configured using environment variables. They are grouped by scope.\n\n### API service\n\nSee [../../libs/libapi/README.md](../../libs/libapi/README.md) for more information about the API configuration.\n\n### Common\n\nSee [../../libs/libcommon/README.md](../../libs/libcommon/README.md) for more information about the common configuration.\n\n## Endpoints\n\nSee https://huggingface.co/docs/datasets-server\n\n- /healthcheck: ensure the app is running\n- /metrics: return a list of metrics in the Prometheus format\n- /rows: get a slice of rows of a dataset split", "Datasets server SSE API\n\n> Server-sent events API for the Datasets server. It's used to update the Hub's backend cache.\n\n## Configuration\n\nThe service can be configured using environment variables. They are grouped by scope.\n\n### Common\n\nSee [../../libs/libcommon/README.md](../../libs/libcommon/README.md) for more information about the common configuration.\n\n## Endpoints\n\nSee https://huggingface.co/docs/datasets-server\n\n- /healthcheck: Ensure the app is running\n- /metrics: Return a list of metrics in the Prometheus format\n- /hub-cache: Return a dataset information as a Server-Sent Event (SSE) when a dataset is updated. If `?all=true` is passed in the parameters, and if the cache already has some entries, one SSE per cached dataset is sent to the client. Then, a SSE is sent when a dataset is inserted, modified or deleted. The event data is a JSON with the following structure. The `hub_cache` field is null for deleted entries, or when the response is an error. The `num_rows` value is `0` if it could not be determined.\n\n```typescript\n{\n    dataset: string;\n    hub_cache: null | {\n        preview: boolean;\n        viewer: boolean;\n        partial: boolean;\n        num_rows: int;\n    };\n}\n```", "Datasets server API - search service\n\n> /search endpoint\n> /filter endpoint\n\n## Configuration\n\nThe service can be configured using environment variables. They are grouped by scope.\n\n### Duckdb index full text search\n- `DUCKDB_INDEX_CACHE_DIRECTORY`: directory where the temporal duckdb index files are downloaded. Defaults to empty.\n- `DUCKDB_INDEX_TARGET_REVISION`: the git revision of the dataset where the index file is stored in the dataset repository.\n\n### API service\n\nSee [../../libs/libapi/README.md](../../libs/libapi/README.md) for more information about the API configuration.\n\n### Common\n\nSee [../../libs/libcommon/README.md](../../libs/libcommon/README.md) for more information about the common configuration.\n\n## Endpoints\n\nSee https://huggingface.co/docs/datasets-server\n\n- /healthcheck: ensure the app is running\n- /metrics: return a list of metrics in the Prometheus format\n- /search: get a slice of a search result over a dataset split\n- /filter: filter rows of a dataset split", "Check dataset validity\n\nBefore you download a dataset from the Hub, it is helpful to know if a specific dataset you're interested in is available. Datasets Server provides the `/is-valid` endpoint to check if a specific dataset works without any errors.\n\nThe API endpoint will return an error for datasets that cannot be loaded with the [\ud83e\udd17 Datasets](https://github.com/huggingface/datasets) library, for example, because the data hasn't been uploaded or the format is not supported.\n\n<Tip warning={true}>\n  The largest datasets are partially supported by Datasets Server. If they are{\" \"}\n  <a href=\"https://huggingface.co/docs/datasets/stream\">streamable</a>, Datasets\n  Server can extract the first 100 rows without downloading the whole dataset.\n  This is especially useful for previewing large datasets where downloading the\n  whole dataset may take hours! See the <code>preview</code> field in the\n  response of <code>/is-valid</code> to check if a dataset is partially\n  supported.\n</Tip>", "Datasets server API\n\n> API on \ud83e\udd17 datasets\n\n## Configuration\n\nThe service can be configured using environment variables. They are grouped by scope.\n\n### API service\n\nSee [../../libs/libapi/README.md](../../libs/libapi/README.md) for more information about the API configuration.\n\n### Common\n\nSee [../../libs/libcommon/README.md](../../libs/libcommon/README.md) for more information about the common configuration.\n\n## Endpoints\n\nSee https://huggingface.co/docs/datasets-server\n\n- /healthcheck: Ensure the app is running\n- /metrics: Return a list of metrics in the Prometheus format\n- /webhook: Add, update or remove a dataset\n- /is-valid: Tell if a dataset is [valid](https://huggingface.co/docs/datasets-server/valid)\n- /splits: List the [splits](https://huggingface.co/docs/datasets-server/splits) names for a dataset\n- /first-rows: Extract the [first rows](https://huggingface.co/docs/datasets-server/first_rows) for a dataset split\n- /parquet: List the [parquet files](https://huggingface.co/docs/datasets-server/parquet) auto-converted for a dataset", "Server infrastructure\n\nThe [Datasets Server](https://github.com/huggingface/datasets-server) has two main components that work together to return queries about a dataset instantly:\n\n- a user-facing web API for exploring and returning information about a dataset\n- a server runs the queries ahead of time and caches them in a database\n\nWhile most of the documentation is focused on the web API, the server is crucial because it performs all the time-consuming preprocessing and stores the results so the web API can retrieve and serve them to the user. This saves a user time because instead of generating the response every time it gets requested, Datasets Server can return the preprocessed results instantly from the cache.\n\nThere are three elements that keep the server running: the job queue, workers, and the cache.\n\n## Job queue\n\nThe job queue is a list of jobs stored in a Mongo database that should be completed by the workers. The jobs are practically identical to the endpoints the user uses; only the server runs the jobs ahead of time, and the user gets the results when they use the endpoint.\n\nThere are three jobs:", "Quickstart\n\n[[open-in-colab]]\n\nIn this quickstart, you'll learn how to use the Datasets Server's REST API to:\n\n- Check whether a dataset on the Hub is functional.\n- Return the configuration and splits of a dataset.\n- Preview the first 100 rows of a dataset.\n- Download slices of rows of a dataset.\n- Search a word in a dataset.\n- Access the dataset as parquet files.\n\n## API endpoints\n\nEach feature is served through an endpoint summarized in the table below:"], "eval_score_GPT4": "5", "eval_feedback_GPT4": "Feedback: The response provided is completely correct, accurate, and factual. It aligns well with the reference answer by stating the primary purpose of the /healthcheck endpoint, which is to ensure that the app is running. The response expands on the reference answer by explaining the functionality of the endpoint in more detail, including its role in server monitoring and management, which is consistent with common practices for healthcheck endpoints in APIs."}, {"question": "What is the default context window size for Local Attention in the LongT5 model?\n", "true_answer": "127 tokens", "source_doc": "huggingface/transformers/blob/main/docs/source/en/model_doc/longt5.md", "generated_answer": "The default context window size for Local Attention in the LongT5 model is 127 tokens, as specified in the context provided. This is mentioned in the description of the LongT5 model, which is an extension of the T5 model that uses either Local Attention or Transient-Global Attention to address the memory bottleneck of transformers for long-range sequence tasks. The Local Attention mechanism allows each input token to attend only to a local context of `r", "retrieved_docs": ["- [`LongT5ForConditionalGeneration`] is an extension of [`T5ForConditionalGeneration`] exchanging the traditional\nencoder *self-attention* layer with efficient either *local* attention or *transient-global* (*tglobal*) attention.\n- Unlike the T5 model, LongT5 does not use a task prefix. Furthermore, it uses a different pre-training objective\ninspired by the pre-training of [`PegasusForConditionalGeneration`].\n- LongT5 model is designed to work efficiently and very well on long-range *sequence-to-sequence* tasks where the\ninput sequence exceeds commonly used 512 tokens. It is capable of handling input sequences of a length up to 16,384 tokens.\n- For *Local Attention*, the sparse sliding-window local attention operation allows a given token to attend only `r`\ntokens to the left and right of it (with `r=127` by default). *Local Attention* does not introduce any new parameters\nto the model. The complexity of the mechanism is linear in input sequence length `l`: `O(l*r)`.\n- *Transient Global Attention* is an extension of the *Local Attention*. It, furthermore, allows each input token to\ninteract with all other tokens in the layer. This is achieved via splitting an input sequence into blocks of a fixed\nlength `k` (with a default `k=16`). Then, a global token for such a block is obtained via summing and normalizing the embeddings of every token", "## Usage tips\n\n- Since the Longformer is based on RoBERTa, it doesn't have `token_type_ids`. You don't need to indicate which\n  token belongs to which segment. Just separate your segments with the separation token `tokenizer.sep_token` (or\n  `</s>`).\n- A transformer model replacing the attention matrices by sparse matrices to go faster. Often, the local context (e.g., what are the two tokens left and right?) is enough to take action for a given token. Some preselected input tokens are still given global attention, but the attention matrix has way less parameters, resulting in a speed-up. See the local attention section for more information.\n\n## Longformer Self Attention\n\nLongformer self attention employs self attention on both a \"local\" context and a \"global\" context. Most tokens only\nattend \"locally\" to each other meaning that each token attends to its \\\\(\\frac{1}{2} w\\\\) previous tokens and\n\\\\(\\frac{1}{2} w\\\\) succeeding tokens with \\\\(w\\\\) being the window length as defined in\n`config.attention_window`. Note that `config.attention_window` can be of type `List` to define a\ndifferent \\\\(w\\\\) for each layer. A selected few tokens attend \"globally\" to all other tokens, as it is\nconventionally done for all tokens in `BertSelfAttention`.", "Longformer uses different attention patterns for autoregressive language modeling, encoder pre-training & fine-tuning, and sequence-to-sequence tasks.\n* For autoregressive language modeling, the strongest results are obtained by replacing causal self-attention (a la GPT2) with dilated windowed self-attention (Figure c). With \\\\(n\\\\) being the sequence length and \\\\(w\\\\) being the window length, this attention pattern reduces the memory consumption from \\\\(n^2\\\\) to \\\\(wn\\\\), which under the assumption that \\\\(w << n\\\\), scales linearly with the sequence length.\n* For encoder pre-training, Longformer replaces the bi-directional self-attention (a la BERT) with a combination of local windowed and global bi-directional self-attention (Figure d). This reduces the memory consumption from \\\\(n^2\\\\) to \\\\(w n + g n\\\\) with \\\\(g\\\\) being the number of tokens that are attended to globally, which again scales linearly with the sequence length.", "Iz Beltagy, Matthew E. Peters, Arman Cohan\n\nLongformer addresses the memory bottleneck of transformers by replacing conventional self-attention with a combination of windowed/local/sparse (cf. [Sparse Transformers (2019)](https://arxiv.org/abs/1904.10509)) attention and global attention that scales linearly with the sequence length. As opposed to previous long-range transformer models (e.g. [Transformer-XL (2019)](https://arxiv.org/abs/1901.02860), [Reformer (2020)](https://arxiv.org/abs/2001.04451), [Adaptive Attention Span (2019)](https://arxiv.org/abs/1905.07799)), Longformer\u2019s self-attention layer is designed as a drop-in replacement for the standard self-attention, thus making it possible to leverage pre-trained checkpoints for further pre-training and/or fine-tuning on long sequence tasks.\n\nThe standard self-attention matrix (Figure a) scales quadratically with the input length:\n\n<figure>\n  <img src=\"/blog/assets/14_long_range_transformers/Longformer.png\" alt=\"Longformer attention\"/>\n  <figcaption>Figure taken from Longformer</figcaption>\n</figure>", "#### Main findings\n\n* The authors proposed the dilated windowed self-attention (Figure c) and showed that it yields better results on language modeling compared to just windowed/sparse self-attention (Figure b). The window sizes are increased through the layers. This pattern further outperforms previous architectures (such as Transformer-XL, or adaptive span attention) on downstream benchmarks.\n* Global attention allows the information to flow through the whole sequence and applying the global attention to task-motivated tokens (such as the tokens of the question in QA, CLS token for sentence classification) leads to stronger performance on downstream tasks. Using this global pattern, Longformer can be successfully applied to document-level NLP tasks in the transfer learning setting.\n* Standard pre-trained models can be adapted to long-range inputs by simply replacing the standard self-attention with the long-range self-attention proposed in this paper and then fine-tuning on the downstream task. This avoids costly pre-training specific to long-range inputs.\n\n#### Follow-up questions", "*Transformers are quickly becoming one of the most heavily applied deep learning architectures across modalities,\ndomains, and tasks. In vision, on top of ongoing efforts into plain transformers, hierarchical transformers have\nalso gained significant attention, thanks to their performance and easy integration into existing frameworks.\nThese models typically employ localized attention mechanisms, such as the sliding-window Neighborhood Attention (NA)\nor Swin Transformer's Shifted Window Self Attention. While effective at reducing self attention's quadratic complexity,\nlocal attention weakens two of the most desirable properties of self attention: long range inter-dependency modeling,\nand global receptive field. In this paper, we introduce Dilated Neighborhood Attention (DiNA), a natural, flexible and\nefficient extension to NA that can capture more global context and expand receptive fields exponentially at no\nadditional cost. NA's local attention and DiNA's sparse global attention complement each other, and therefore we\nintroduce Dilated Neighborhood Attention Transformer (DiNAT), a new hierarchical vision transformer built upon both.\nDiNAT variants enjoy significant improvements over strong baselines such as NAT, Swin, and ConvNeXt.\nOur large model is faster and ahead of its Swin counterpart by 1.5% box AP in COCO object detection,\n1.3% mask AP in COCO instance segmentation, and 1.1% mIoU in ADE20K semantic segmentation.\nPaired with new frameworks, our large variant is the new state of the art panoptic segmentation model on COCO (58.2 PQ)", "Note that \"locally\" and \"globally\" attending tokens are projected by different query, key and value matrices. Also note\nthat every \"locally\" attending token not only attends to tokens within its window \\\\(w\\\\), but also to all \"globally\"\nattending tokens so that global attention is *symmetric*.\n\nThe user can define which tokens attend \"locally\" and which tokens attend \"globally\" by setting the tensor\n`global_attention_mask` at run-time appropriately. All Longformer models employ the following logic for\n`global_attention_mask`:\n\n- 0: the token attends \"locally\",\n- 1: the token attends \"globally\".\n\nFor more information please also refer to [`~LongformerModel.forward`] method.\n\nUsing Longformer self attention, the memory and time complexity of the query-key matmul operation, which usually\nrepresents the memory and time bottleneck, can be reduced from \\\\(\\mathcal{O}(n_s \\times n_s)\\\\) to\n\\\\(\\mathcal{O}(n_s \\times w)\\\\), with \\\\(n_s\\\\) being the sequence length and \\\\(w\\\\) being the average window\nsize. It is assumed that the number of \"globally\" attending tokens is insignificant as compared to the number of\n\"locally\" attending tokens.\n\nFor more information, please refer to the official [paper](https://arxiv.org/pdf/2004.05150.pdf).\n\n\n## Training"], "eval_score_GPT4": "5", "eval_feedback_GPT4": "The response correctly identifies the default context window size for Local Attention in the LongT5 model as 127 tokens, which is in agreement with the reference answer. The additional information provided about the LongT5 model and its attention mechanisms does not detract from the accuracy of the response regarding the specific detail in question."}, {"question": "What method is used to load a checkpoint for a task using `AutoPipeline`?\n", "true_answer": "from_pretrained()", "source_doc": "huggingface/diffusers/blob/main/docs/source/en/api/pipelines/auto_pipeline.md", "generated_answer": "The method used to load a checkpoint for a task using `AutoPipeline` is `from_pretrained()`. This method automatically detects the correct pipeline class to use based on the task, making it easier to load a checkpoint for a task without knowing the specific pipeline class name. For example, to load a checkpoint for text-to-image using `AutoPipelineForText2Image`, you can call `AutoPipelineForText2Image.from_pretrained(\"runway", "retrieved_docs": ["The `AutoPipeline` class is designed to simplify the variety of pipelines in \ud83e\udd17 Diffusers. It is a generic, *task-first* pipeline that lets you focus on the task. The `AutoPipeline` automatically detects the correct pipeline class to use, which makes it easier to load a checkpoint for a task without knowing the specific pipeline class name.\n\n<Tip>\n\nTake a look at the [AutoPipeline](../api/pipelines/auto_pipeline) reference to see which tasks are supported. Currently, it supports text-to-image, image-to-image, and inpainting.\n\n</Tip>\n\nThis tutorial shows you how to use an `AutoPipeline` to automatically infer the pipeline class to load for a specific task, given the pretrained weights.\n\n## Choose an AutoPipeline for your task\n\nStart by picking a checkpoint. For example, if you're interested in text-to-image with the [runwayml/stable-diffusion-v1-5](https://huggingface.co/runwayml/stable-diffusion-v1-5) checkpoint, use [`AutoPipelineForText2Image`]:\n\n```py\nfrom diffusers import AutoPipelineForText2Image\nimport torch\n\npipeline = AutoPipelineForText2Image.from_pretrained(\n    \"runwayml/stable-diffusion-v1-5\", torch_dtype=torch.float16, use_safetensors=True\n).to(\"cuda\")\nprompt = \"peasant and dragon combat, wood cutting style, viking era, bevel with rune\"", "!--Copyright 2023 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\n\nhttp://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License.\n-->\n\n# AutoPipeline\n\n`AutoPipeline` is designed to:\n\n1. make it easy for you to load a checkpoint for a task without knowing the specific pipeline class to use\n2. use multiple pipelines in your workflow\n\nBased on the task, the `AutoPipeline` class automatically retrieves the relevant pipeline given the name or path to the pretrained weights with the `from_pretrained()` method.\n\nTo seamlessly switch between tasks with the same checkpoint without reallocating additional memory, use the `from_pipe()` method to transfer the components from the original pipeline to the new one.\n\n```py\nfrom diffusers import AutoPipelineForText2Image\nimport torch\n\npipeline = AutoPipelineForText2Image.from_pretrained(\n    \"runwayml/stable-diffusion-v1-5\", torch_dtype=torch.float16, use_safetensors=True\n).to(\"cuda\")\nprompt = \"Astronaut in a jungle, cold color palette, muted colors, detailed, 8k\"", "If you're interested in using one of the official checkpoints for a task, explore the [CompVis](https://huggingface.co/CompVis), [Runway](https://huggingface.co/runwayml), and [Stability AI](https://huggingface.co/stabilityai) Hub organizations!\n\n</Tip>\n\n## StableDiffusionInpaintPipeline\n\n[[autodoc]] StableDiffusionInpaintPipeline\n\t- all\n\t- __call__\n\t- enable_attention_slicing\n\t- disable_attention_slicing\n\t- enable_xformers_memory_efficient_attention\n\t- disable_xformers_memory_efficient_attention\n\t- load_textual_inversion\n\t- load_lora_weights\n\t- save_lora_weights\n\n## StableDiffusionPipelineOutput\n\n[[autodoc]] pipelines.stable_diffusion.StableDiffusionPipelineOutput\n\n## FlaxStableDiffusionInpaintPipeline\n\n[[autodoc]] FlaxStableDiffusionInpaintPipeline\n\t- all\n\t- __call__\n\n## FlaxStableDiffusionPipelineOutput\n\n[[autodoc]] pipelines.stable_diffusion.FlaxStableDiffusionPipelineOutput", "If you're interested in using one of the official checkpoints for a task, explore the [CompVis](https://huggingface.co/CompVis), [Runway](https://huggingface.co/runwayml), and [Stability AI](https://huggingface.co/stabilityai) Hub organizations!\n\n</Tip>\n\n## StableDiffusionPipeline\n\n[[autodoc]] StableDiffusionPipeline\n\t- all\n\t- __call__\n\t- enable_attention_slicing\n\t- disable_attention_slicing\n\t- enable_vae_slicing\n\t- disable_vae_slicing\n\t- enable_xformers_memory_efficient_attention\n\t- disable_xformers_memory_efficient_attention\n\t- enable_vae_tiling\n\t- disable_vae_tiling\n\t- load_textual_inversion\n\t- from_single_file\n\t- load_lora_weights\n\t- save_lora_weights\n\n## StableDiffusionPipelineOutput\n\n[[autodoc]] pipelines.stable_diffusion.StableDiffusionPipelineOutput\n\n## FlaxStableDiffusionPipeline\n\n[[autodoc]] FlaxStableDiffusionPipeline\n\t- all\n\t- __call__\n\n## FlaxStableDiffusionPipelineOutput\n\n[[autodoc]] pipelines.stable_diffusion.FlaxStableDiffusionPipelineOutput", "<div class=\"flex justify-center\">\n    <img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/diffusers/autopipeline-inpaint.png\" alt=\"generated image of a tiger sitting on a bench\"/>\n</div>\n\nIf you try to load an unsupported checkpoint, it'll throw an error:\n\n```py\nfrom diffusers import AutoPipelineForImage2Image\nimport torch\n\npipeline = AutoPipelineForImage2Image.from_pretrained(\n    \"openai/shap-e-img2img\", torch_dtype=torch.float16, use_safetensors=True\n)\n\"ValueError: AutoPipeline can't find a pipeline linked to ShapEImg2ImgPipeline for None\"\n```\n\n## Use multiple pipelines\n\nFor some workflows or if you're loading many pipelines, it is more memory-efficient to reuse the same components from a checkpoint instead of reloading them which would unnecessarily consume additional memory. For example, if you're using a checkpoint for text-to-image and you want to use it again for image-to-image, use the [`~AutoPipelineForImage2Image.from_pipe`] method. This method creates a new pipeline from the components of a previously loaded pipeline at no additional memory cost.", "If you're interested in using one of the official checkpoints for a task, explore the [CompVis](https://huggingface.co/CompVis), [Runway](https://huggingface.co/runwayml), and [Stability AI](https://huggingface.co/stabilityai) Hub organizations!\n\n</Tip>\n\n## StableDiffusionLatentUpscalePipeline\n\n[[autodoc]] StableDiffusionLatentUpscalePipeline\n\t- all\n\t- __call__\n\t- enable_sequential_cpu_offload\n\t- enable_attention_slicing\n\t- disable_attention_slicing\n\t- enable_xformers_memory_efficient_attention\n\t- disable_xformers_memory_efficient_attention\n\n## StableDiffusionPipelineOutput\n\n[[autodoc]] pipelines.stable_diffusion.StableDiffusionPipelineOutput", "A checkpoint (such as [`CompVis/stable-diffusion-v1-4`](https://huggingface.co/CompVis/stable-diffusion-v1-4) or [`runwayml/stable-diffusion-v1-5`](https://huggingface.co/runwayml/stable-diffusion-v1-5)) may also be used for more than one task, like text-to-image or image-to-image. To differentiate what task you want to use the checkpoint for, you have to load it directly with its corresponding task-specific pipeline class:\n\n```python\nfrom diffusers import StableDiffusionImg2ImgPipeline\n\nrepo_id = \"runwayml/stable-diffusion-v1-5\"\npipe = StableDiffusionImg2ImgPipeline.from_pretrained(repo_id)\n```\n\n### Local pipeline\n\nTo load a diffusion pipeline locally, use [`git-lfs`](https://git-lfs.github.com/) to manually download the checkpoint (in this case, [`runwayml/stable-diffusion-v1-5`](https://huggingface.co/runwayml/stable-diffusion-v1-5)) to your local disk. This creates a local folder, `./stable-diffusion-v1-5`, on your disk:\n\n```bash\ngit-lfs install\ngit clone https://huggingface.co/runwayml/stable-diffusion-v1-5\n```\n\nThen pass the local path to [`~DiffusionPipeline.from_pretrained`]:"], "eval_score_GPT4": "5", "eval_feedback_GPT4": "The response correctly identifies `from_pretrained()` as the method used to load a checkpoint for a task using `AutoPipeline`. The explanation provided is accurate and factual, aligning with the reference answer. The additional example given about `AutoPipelineForText2Image` is not necessary for the question but does not detract from the correctness of the response."}, {"question": "What is the purpose of Diffusers library?\n", "true_answer": "To serve as a modular toolbox for both inference and training of state-of-the-art pretrained diffusion models across multiple modalities.", "source_doc": "huggingface/diffusers/blob/main/docs/source/en/conceptual/philosophy.md", "generated_answer": "The Diffusers library, developed by Hugging Face, is a toolbox for state-of-the-art pretrained diffusion models for generating images, audio, and even 3D structures of molecules. It offers three core components: diffusion pipelines, noise schedulers, and pretrained models, which can be used for both simple inference solutions and training custom diffusion systems. The library is designed with a focus on usability over performance, simple over easy, and customizability over", "retrieved_docs": ["\ud83e\udd17 Diffusers is the go-to library for state-of-the-art pretrained diffusion models for generating images, audio, and even 3D structures of molecules. Whether you're looking for a simple inference solution or training your own diffusion models, \ud83e\udd17 Diffusers is a modular toolbox that supports both. Our library is designed with a focus on [usability over performance](https://huggingface.co/docs/diffusers/conceptual/philosophy#usability-over-performance), [simple over easy](https://huggingface.co/docs/diffusers/conceptual/philosophy#simple-over-easy), and [customizability over abstractions](https://huggingface.co/docs/diffusers/conceptual/philosophy#tweakable-contributorfriendly-over-abstraction).\n\n\ud83e\udd17 Diffusers offers three core components:\n\n- State-of-the-art [diffusion pipelines](https://huggingface.co/docs/diffusers/api/pipelines/overview) that can be run in inference with just a few lines of code.\n- Interchangeable noise [schedulers](https://huggingface.co/docs/diffusers/api/schedulers/overview) for different diffusion speeds and output quality.\n- Pretrained [models](https://huggingface.co/docs/diffusers/api/models/overview) that can be used as building blocks, and combined with schedulers, for creating your own end-to-end diffusion systems.\n\n## Installation", "Using \ud83e\udde8 `diffusers` at Hugging Face\n\nDiffusers is the go-to library for state-of-the-art pretrained diffusion models for generating images, audio, and even 3D structures of molecules. Whether you\u2019re looking for a simple inference solution or want to train your own diffusion model, Diffusers is a modular toolbox that supports both. The library is designed with a focus on usability over performance, simple over easy, and customizability over abstractions.\n\n## Exploring Diffusers in the Hub\n\nThere are over 10,000 `diffusers` compatible pipelines on the Hub which you can find by filtering at the left of [the models page](https://huggingface.co/models?library=diffusers&sort=downloads). Diffusion systems are typically composed of multiple components such as text encoder, UNet, VAE, and scheduler. Even though they are not standalone models, the pipeline abstraction makes it easy to use them for inference or training.\n\nYou can find diffusion pipelines for many different tasks:\n\n* Generating images from natural language text prompts ([text-to-image](https://huggingface.co/models?library=diffusers&pipeline_tag=text-to-image&sort=downloads)).\n* Transforming images using natural language text prompts ([image-to-image](https://huggingface.co/models?library=diffusers&pipeline_tag=image-to-image&sort=downloads)).\n* Generating videos from natural language descriptions ([text-to-video](https://huggingface.co/models?library=diffusers&pipeline_tag=text-to-video&sort=downloads)).", "## \ud83e\udde8 Diffusers\n\n[Diffusers](https://huggingface.co/docs/diffusers) provides pre-trained vision and audio diffusion models, and serves as a modular toolbox for inference and training. With this library, you can generate plausible images from natural language inputs amongst other creative use cases. Here is an example:\n\n```py\nfrom diffusers import DiffusionPipeline\n\ngenerator = DiffusionPipeline.from_pretrained(\"CompVis/stable-diffusion-v1-4\")\ngenerator.to(\u201ccuda\u201d)\n\nimage = generator(\"An image of a squirrel in Picasso style\").images[0]\n```\n\n<div align=\"center\">\n<img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/cv_state/sd_output.png\"/>\n</div>\n\nThis type of technology can empower a new generation of creative applications and also aid artists coming from different backgrounds. To know more about Diffusers and the different use cases, check out the [official documentation](https://huggingface.co/docs/diffusers).\n\nThe literature on Diffusion-based models is developing at a rapid pace which is why we partnered with [Jonathan Whitaker](https://github.com/johnowhitaker) to develop a course on it. The course is free, and you can check it out [here](https://github.com/huggingface/diffusion-models-class).\n\n## Support for third-party libraries", "For large parts of the library, Diffusers adopts an important design principle of the [Transformers library](https://github.com/huggingface/transformers), which is to prefer copy-pasted code over hasty abstractions. This design principle is very opinionated and stands in stark contrast to popular design principles such as [Don't repeat yourself (DRY)](https://en.wikipedia.org/wiki/Don%27t_repeat_yourself).\nIn short, just like Transformers does for modeling files, Diffusers prefers to keep an extremely low level of abstraction and very self-contained code for pipelines and schedulers.\nFunctions, long code blocks, and even classes can be copied across multiple files which at first can look like a bad, sloppy design choice that makes the library unmaintainable.\n**However**, this design has proven to be extremely successful for Transformers and makes a lot of sense for community-driven, open-source machine learning libraries because:\n- Machine Learning is an extremely fast-moving field in which paradigms, model architectures, and algorithms are changing rapidly, which therefore makes it very difficult to define long-lasting code abstractions.\n- Machine Learning practitioners like to be able to quickly tweak existing code for ideation and research and therefore prefer self-contained code over one that contains many abstractions.\n- Open-source libraries rely on community contributions and therefore must build a library that is easy to contribute to. The more abstract the code, the more dependencies, the harder to read, and the harder to contribute to. Contributors simply stop contributing to very abstract libraries out of fear of breaking vital functionality. If contributing to a library cannot break other fundamental code, not only is it more inviting for potential new contributors, but it is also easier to review and contribute to multiple parts in parallel.", "For large parts of the library, Diffusers adopts an important design principle of the [Transformers library](https://github.com/huggingface/transformers), which is to prefer copy-pasted code over hasty abstractions. This design principle is very opinionated and stands in stark contrast to popular design principles such as [Don't repeat yourself (DRY)](https://en.wikipedia.org/wiki/Don%27t_repeat_yourself).\nIn short, just like Transformers does for modeling files, Diffusers prefers to keep an extremely low level of abstraction and very self-contained code for pipelines and schedulers.\nFunctions, long code blocks, and even classes can be copied across multiple files which at first can look like a bad, sloppy design choice that makes the library unmaintainable.\n**However**, this design has proven to be extremely successful for Transformers and makes a lot of sense for community-driven, open-source machine learning libraries because:\n- Machine Learning is an extremely fast-moving field in which paradigms, model architectures, and algorithms are changing rapidly, which therefore makes it very difficult to define long-lasting code abstractions.\n- Machine Learning practitioners like to be able to quickly tweak existing code for ideation and research and therefore prefer self-contained code over one that contains many abstractions.\n- Open-source libraries rely on community contributions and therefore must build a library that is easy to contribute to. The more abstract the code, the more dependencies, the harder to read, and the harder to contribute to. Contributors simply stop contributing to very abstract libraries out of fear of breaking vital functionality. If contributing to a library cannot break other fundamental code, not only is it more inviting for potential new contributors, but it is also easier to review and contribute to multiple parts in parallel.", "\ud83e\udde8 Diffusers Experimental\n\nWe are adding experimental code to support novel applications and usages of the Diffusers library.\nCurrently, the following experiments are supported:\n* Reinforcement learning via an implementation of the [Diffuser](https://arxiv.org/abs/2205.09991) model.", "--\ntitle: \"Ethical Guidelines for developing the Diffusers library\" \nthumbnail: /blog/assets/ethics-diffusers/thumbnail.png\nauthors:\n- user: giadap\n---\n\n# Ethical guidelines for developing the Diffusers library\n\nWe are on a journey to make our libraries more responsible, one commit at a time! \n\nAs part of the [Diffusers library documentation](https://huggingface.co/docs/diffusers/main/en/index), we are proud to announce the publication of an [ethical framework](https://huggingface.co/docs/diffusers/main/en/conceptual/ethical_guidelines). \n\nGiven diffusion models' real case applications in the world and potential negative impacts on society, this initiative aims to guide the technical decisions of the Diffusers library maintainers about community contributions. We wish to be transparent in how we make decisions, and above all, we aim to clarify what values guide those decisions.\n\nWe see ethics as a process that leverages guiding values, concrete actions, and continuous adaptation. For this reason, we are committed to adjusting our guidelines over time, following the evolution of the Diffusers project and the valuable feedback from the community that keeps it alive.\n\n# Ethical guidelines"], "eval_score_GPT4": "4", "eval_feedback_GPT4": "Feedback: The response provided is mostly correct, accurate, and factual. It correctly identifies the Diffusers library as a tool developed by Hugging Face and mentions its use in generating images, audio, and 3D structures of molecules. It also accurately describes the core components of the library, such as diffusion pipelines, noise schedulers, and pretrained models, and touches upon the library's design principles, such as usability and customizability. However, the response is cut off and does not finish the last sentence, which leaves the evaluation of the library's design principles incomplete. Despite this, the response aligns well with the reference answer's description of the library as a modular toolbox for inference and training of diffusion models across multiple modalities."}, {"question": "What method does the EulerAncestralDiscreteScheduler use for sampling?\n", "true_answer": "Ancestral sampling with Euler method steps.", "source_doc": "huggingface/diffusers/blob/main/docs/source/en/api/schedulers/euler_ancestral.md", "generated_answer": "The EulerAncestralDiscreteScheduler uses ancestral sampling with Euler method steps for sampling during the diffusion process. This is mentioned in the context provided, specifically in the comments for the `EulerAncestralDiscreteScheduler` class and in the documentation for the `EulerAncestralDiscreteSchedulerOutput` class. Additionally, the original implementation of this scheduler is credited to Katherine Crowson in the comments for the `Euler", "retrieved_docs": ["!--Copyright 2023 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\n\nhttp://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License.\n-->\n\n# EulerAncestralDiscreteScheduler\n\nA scheduler that uses ancestral sampling with Euler method steps. This is a fast scheduler which can often generate good outputs in 20-30 steps. The scheduler is based on the original [k-diffusion](https://github.com/crowsonkb/k-diffusion/blob/481677d114f6ea445aa009cf5bd7a9cdee909e47/k_diffusion/sampling.py#L72) implementation by [Katherine Crowson](https://github.com/crowsonkb/).\n\n## EulerAncestralDiscreteScheduler\n[[autodoc]] EulerAncestralDiscreteScheduler\n\n## EulerAncestralDiscreteSchedulerOutput\n[[autodoc]] schedulers.scheduling_euler_ancestral_discrete.EulerAncestralDiscreteSchedulerOutput", "Let's use the [`SchedulerMixin.from_pretrained`] method to replace the default [`PNDMScheduler`] with a more performant scheduler, [`EulerDiscreteScheduler`]. The `subfolder=\"scheduler\"` argument is required to load the scheduler configuration from the correct [subfolder](https://huggingface.co/runwayml/stable-diffusion-v1-5/tree/main/scheduler) of the pipeline repository.\n\nThen you can pass the new [`EulerDiscreteScheduler`] instance to the `scheduler` argument in [`DiffusionPipeline`]:\n\n```python\nfrom diffusers import DiffusionPipeline, EulerDiscreteScheduler\n\nrepo_id = \"runwayml/stable-diffusion-v1-5\"\nscheduler = EulerDiscreteScheduler.from_pretrained(repo_id, subfolder=\"scheduler\")\nstable_diffusion = DiffusionPipeline.from_pretrained(repo_id, scheduler=scheduler, use_safetensors=True)\n```\n\n### Safety checker\n\nDiffusion models like Stable Diffusion can generate harmful content, which is why \ud83e\udde8 Diffusers has a [safety checker](https://github.com/huggingface/diffusers/blob/main/src/diffusers/pipelines/stable_diffusion/safety_checker.py) to check generated outputs against known hardcoded NSFW content. If you'd like to disable the safety checker for whatever reason, pass `None` to the `safety_checker` argument:", "Now, you can run the pipeline as you would in the section above.\n\n### Swapping schedulers\n\nDifferent schedulers come with different denoising speeds and quality trade-offs. The best way to find out which one works best for you is to try them out! One of the main features of \ud83e\udde8 Diffusers is to allow you to easily switch between schedulers. For example, to replace the default [`PNDMScheduler`] with the [`EulerDiscreteScheduler`], load it with the [`~diffusers.ConfigMixin.from_config`] method:\n\n```py\n>>> from diffusers import EulerDiscreteScheduler\n\n>>> pipeline = DiffusionPipeline.from_pretrained(\"runwayml/stable-diffusion-v1-5\", use_safetensors=True)\n>>> pipeline.scheduler = EulerDiscreteScheduler.from_config(pipeline.scheduler.config)\n```\n\nTry generating an image with the new scheduler and see if you notice a difference!\n\nIn the next section, you'll take a closer look at the components - the model and scheduler - that make up the [`DiffusionPipeline`] and learn how to use these components to generate an image of a cat.\n\n## Models", "The original implementation can be found at [LuChengTHU/dpm-solver](https://github.com/LuChengTHU/dpm-solver).\n\n## Tips\n\nIt is recommended to set `solver_order` to 2 for guide sampling, and `solver_order=3` for unconditional sampling.\n\nDynamic thresholding from [Imagen](https://huggingface.co/papers/2205.11487) is supported, and for pixel-space\ndiffusion models, you can set both `algorithm_type=\"dpmsolver++\"` and `thresholding=True` to use dynamic\nthresholding. This thresholding method is unsuitable for latent-space diffusion models such as\nStable Diffusion.\n\n## DPMSolverSinglestepScheduler\n[[autodoc]] DPMSolverSinglestepScheduler\n\n## SchedulerOutput\n[[autodoc]] schedulers.scheduling_utils.SchedulerOutput", "<p align=\"center\">\n    <br>\n    <img src=\"https://huggingface.co/datasets/patrickvonplaten/images/resolve/main/diffusers_docs/astronaut_lms.png\" width=\"400\"/>\n    <br>\n</p>\n\n\n[`EulerDiscreteScheduler`] and [`EulerAncestralDiscreteScheduler`] can generate high quality results with as little as 30 steps.\n\n```python\nfrom diffusers import EulerDiscreteScheduler\n\npipeline.scheduler = EulerDiscreteScheduler.from_config(pipeline.scheduler.config)\n\ngenerator = torch.Generator(device=\"cuda\").manual_seed(8)\nimage = pipeline(prompt, generator=generator, num_inference_steps=30).images[0]\nimage\n```\n\n<p align=\"center\">\n    <br>\n    <img src=\"https://huggingface.co/datasets/patrickvonplaten/images/resolve/main/diffusers_docs/astronaut_euler_discrete.png\" width=\"400\"/>\n    <br>\n</p>\n\n\nand:\n\n```python\nfrom diffusers import EulerAncestralDiscreteScheduler\n\npipeline.scheduler = EulerAncestralDiscreteScheduler.from_config(pipeline.scheduler.config)", "pipeline.scheduler = EulerAncestralDiscreteScheduler.from_config(pipeline.scheduler.config)\n\ngenerator = torch.Generator(device=\"cuda\").manual_seed(8)\nimage = pipeline(prompt, generator=generator, num_inference_steps=30).images[0]\nimage\n```\n\n<p align=\"center\">\n    <br>\n    <img src=\"https://huggingface.co/datasets/patrickvonplaten/images/resolve/main/diffusers_docs/astronaut_euler_ancestral.png\" width=\"400\"/>\n    <br>\n</p>\n\n\n[`DPMSolverMultistepScheduler`] gives a reasonable speed/quality trade-off and can be run with as little as 20 steps.\n\n```python\nfrom diffusers import DPMSolverMultistepScheduler\n\npipeline.scheduler = DPMSolverMultistepScheduler.from_config(pipeline.scheduler.config)\n\ngenerator = torch.Generator(device=\"cuda\").manual_seed(8)\nimage = pipeline(prompt, generator=generator, num_inference_steps=20).images[0]\nimage\n```\n\n<p align=\"center\">\n    <br>\n    <img src=\"https://huggingface.co/datasets/patrickvonplaten/images/resolve/main/diffusers_docs/astronaut_dpm.png\" width=\"400\"/>\n    <br>\n</p>", "Our experiments were conducted on a single 40GB A100 GPU.\n\n### Inference\n\nOnce training is done, we can perform inference like so:\n\n```python\nfrom diffusers import StableDiffusionXLAdapterPipeline, T2IAdapter, EulerAncestralDiscreteSchedulerTest\nfrom diffusers.utils import load_image\nimport torch\n\nbase_model_path = \"stabilityai/stable-diffusion-xl-base-1.0\"\nadapter_path = \"path to adapter\"\n\nadapter = T2IAdapter.from_pretrained(adapter_path, torch_dtype=torch.float16)\npipe = StableDiffusionXLAdapterPipeline.from_pretrained(\n    base_model_path, adapter=adapter, torch_dtype=torch.float16\n)\n\n# speed up diffusion process with faster scheduler and memory optimization\npipe.scheduler = EulerAncestralDiscreteSchedulerTest.from_config(pipe.scheduler.config)\n# remove following line if xformers is not installed or when using Torch 2.0.\npipe.enable_xformers_memory_efficient_attention()\n# memory optimization.\npipe.enable_model_cpu_offload()\n\ncontrol_image = load_image(\"./conditioning_image_1.png\")\nprompt = \"pale golden rod circle with old lace background\""], "eval_score_GPT4": "5", "eval_feedback_GPT4": "Feedback: The response provided is completely correct, accurate, and factual. It aligns with the reference answer by stating that the EulerAncestralDiscreteScheduler uses ancestral sampling with Euler method steps for sampling. The response also adds additional context by mentioning where this information can be found, which supports the accuracy of the statement. There are no inaccuracies or incorrect information presented in the response."}, {"question": "What is the name of the large multimodal model that can solve image-text tasks and is based on Flamingo?\n", "true_answer": "IDEFICS", "source_doc": "huggingface/transformers/blob/main/docs/source/en/tasks/idefics.md", "generated_answer": "The name of the large multimodal model that can solve image-text tasks and is based on Flamingo is IDEFICS. It is an open-access reproduction of Flamingo and is comparable in performance with the original closed-source model across various image-text understanding benchmarks. IDEFICS comes in two variants with 80 billion and 9 billion parameters, and fine-tuned instructed versions are also available for conversational use cases.", "retrieved_docs": ["[IDEFICS](../model_doc/idefics) is an open-access vision and language model based on [Flamingo](https://huggingface.co/papers/2204.14198), \na state-of-the-art visual language model initially developed by DeepMind. The model accepts arbitrary sequences of image \nand text inputs and generates coherent text as output. It can answer questions about images, describe visual content, \ncreate stories grounded in multiple images, and so on. IDEFICS comes in two variants - [80 billion parameters](https://huggingface.co/HuggingFaceM4/idefics-80b) \nand [9 billion parameters](https://huggingface.co/HuggingFaceM4/idefics-9b), both of which are available on the \ud83e\udd17 Hub. For each variant, you can also find fine-tuned instructed \nversions of the model adapted for conversational use cases.\n\nThis model is exceptionally versatile and can be used for a wide range of image and multimodal tasks. However, \nbeing a large model means it requires significant computational resources and infrastructure. It is up to you to decide whether \nthis approach suits your use case better than fine-tuning specialized models for each individual task.", "## What is IDEFICS?\n\nIDEFICS is an 80 billion parameters multimodal model that accepts sequences of images and texts as input and generates coherent text as output. It can answer questions about images, describe visual content, create stories grounded in multiple images, etc.\n\nIDEFICS is an open-access reproduction of Flamingo and is comparable in performance with the original closed-source model across various image-text understanding benchmarks. It comes in two variants - 80 billion parameters and 9 billion parameters.\n\n<p align=\"center\">\n    <img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/idefics/Figure_Evals_IDEFICS.png\" width=\"600\" alt=\"Plot comparing the performance of Flamingo, OpenFlamingo and IDEFICS\"/>\n</p>\n\nWe also provide fine-tuned versions [idefics-80B-instruct](https://huggingface.co/HuggingFaceM4/idefics-80b-instruct) and [idefics-9B-instruct](https://huggingface.co/HuggingFaceM4/idefics-9b-instruct) adapted for conversational use cases.\n\n## Training Data", "Models such as [Frozen](https://arxiv.org/abs/2106.13884) and [ClipCap](https://arxiv.org/abs/2111.09734) use this Frozen PrefixLM pre-training objective. They only update the parameters of the image encoder during training to generate image embeddings that can be used as a prefix to the pre-trained, frozen language model in a similar fashion to the PrefixLM objective discussed above. Both Frozen and ClipCap are trained on aligned image-text (caption) datasets with the objective of generating the next token in the caption, given the image embeddings and the prefix text. \n\nFinally, models such as [MAPL](https://arxiv.org/abs/2210.07179) and [Flamingo](https://arxiv.org/abs/2204.14198) keep both the pre-trained vision encoder and language model frozen. Flamingo sets a new state-of-the-art in few-shot learning on a wide range of open-ended vision and language tasks by adding Perceiver Resampler modules on top of the pre-trained frozen vision model and inserting new cross-attention layers between existing pre-trained and frozen LM layers to condition the LM on visual data.\n\nA nifty advantage of the Frozen PrefixLM pre-training objective is it enables training with limited aligned image-text data, which is particularly useful for domains where aligned multi-modal datasets are not available.\n### 3) Multi-modal Fusing with Cross Attention", "The abstract from the paper is the following:\n\n*We introduce Kosmos-2, a Multimodal Large Language Model (MLLM), enabling new capabilities of perceiving object descriptions (e.g., bounding boxes) and grounding text to the visual world. Specifically, we represent refer expressions as links in Markdown, i.e., ``[text span](bounding boxes)'', where object descriptions are sequences of location tokens. Together with multimodal corpora, we construct large-scale data of grounded image-text pairs (called GrIT) to train the model. In addition to the existing capabilities of MLLMs (e.g., perceiving general modalities, following instructions, and performing in-context learning), Kosmos-2 integrates the grounding capability into downstream applications. We evaluate Kosmos-2 on a wide range of tasks, including (i) multimodal grounding, such as referring expression comprehension, and phrase grounding, (ii) multimodal referring, such as referring expression generation, (iii) perception-language tasks, and (iv) language understanding and generation. This work lays out the foundation for the development of Embodiment AI and sheds light on the big convergence of language, multimodal perception, action, and world modeling, which is a key step toward artificial general intelligence. Code and pretrained models are available at https://aka.ms/kosmos-2.*\n\n<img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/model_doc/kosmos_2_overview.jpg\"\nalt=\"drawing\" width=\"600\"/>", "While models such as CLIP, FLAVA, BridgeTower, BLIP, LiT and `VisionEncoderDecoder` models provide joint image-text embeddings that can be used for downstream tasks such as zero-shot image classification, other models are trained on interesting downstream tasks. In addition, FLAVA is trained with both unimodal and multi-modal pre-training objectives and can be used for both unimodal vision or language tasks and multi-modal tasks.\n\nFor example, OWL-ViT [enables](https://huggingface.co/spaces/adirik/OWL-ViT) zero-shot / text-guided and one-shot / image-guided object detection, CLIPSeg and GroupViT [enable](https://huggingface.co/spaces/nielsr/CLIPSeg) text and image-guided image segmentation, and VisualBERT, GIT and ViLT [enable](https://huggingface.co/spaces/nielsr/vilt-vqa) visual question answering as well as various other tasks. X-CLIP is a multi-modal model trained with video and text modalities and [enables](https://huggingface.co/spaces/fcakyon/zero-shot-video-classification) zero-shot video classification similar to CLIP\u2019s zero-shot image classification capabilities.", "The abstract from the paper is the following:\n\n*Large multimodal models trained on natural documents, which interleave images and text, outperform models trained on image-text pairs on various multimodal benchmarks that require reasoning over one or multiple images to generate a text. However, the datasets used to train these models have not been released, and the collection process has not been fully specified. We introduce the OBELICS dataset, an open web-scale filtered dataset of interleaved image-text documents comprising 141 million web pages extracted from Common Crawl, 353 million associated images, and 115 billion text tokens. We describe the dataset creation process, present comprehensive filtering rules, and provide an analysis of the dataset's content. To show the viability of OBELISC, we train an 80 billion parameters vision and language model on the dataset and obtain competitive performance on various multimodal benchmarks. We release the code to reproduce the dataset along with the dataset itself.*\n\nThis model was contributed by [HuggingFaceM4](https://huggingface.co/HuggingFaceM4). The original code can be found [here](<INSERT LINK TO GITHUB REPO HERE>). (TODO: don't have a public link yet).\n\n\n<Tip warning={true}>\n\nIDEFICS modeling code in Transformers is for finetuning and inferencing the pre-trained IDEFICS models.\n\nTo train a new IDEFICS model from scratch use the m4 codebase (a link will be provided once it's made public)\n\n</Tip>\n\n\n## IdeficsConfig\n\n[[autodoc]] IdeficsConfig", "## Datasets\nLike other vision-language models, text-to-video models are typically trained on large paired datasets videos and text descriptions. The videos in these datasets are typically split into short, fixed-length chunks and often limited to isolated actions with a few objects. While this is partly due to computational limitations and partly due to the difficulty of describing video content in a meaningful way, we see that developments in multimodal video-text datasets and text-to-video models are often entwined. While some work focuses on developing better, more generalizable datasets that are easier to learn from, works such as [Phenaki](https://phenaki.video/?mc_cid=9fee7eeb9d#) explore alternative solutions such as combining text-image pairs with text-video pairs for the text-to-video task. Make-a-Video takes this even further by proposing using only text-image pairs to learn what the world looks like and unimodal video data to learn spatio-temporal dependencies in an unsupervised fashion."], "eval_score_GPT4": "1", "eval_feedback_GPT4": "The response provided is incorrect. The name IDEFICS does not correspond to any known large multimodal model based on Flamingo that can solve image-text tasks. The correct name of the model should be \"Flamingo\" itself, as indicated in the reference answer. The additional information about IDEFICS being an open-access reproduction and its variants is not accurate as it does not pertain to the Flamingo model or any of its known derivatives. Therefore, the response does not meet the criteria for correctness, accuracy, or factuality as per the score rubric."}, {"question": "What is the purpose of the `gradio.Blocks` API?\n", "true_answer": "The `gradio.Blocks` API allows you to have full control over the data flows and layout of your application, enabling the building of complex, multi-step applications.", "source_doc": "huggingface/course/blob/main/chapters/en/chapter9/7.mdx", "generated_answer": "The purpose of the `gradio.Blocks` API is to provide a low-level, flexible API for building complex, multi-step machine learning applications with full control over data flows and layout. It allows for grouping related demos into one web app, changing the layout of demos, having multi-step interfaces with more flexible data flows, and changing component properties or visibility based on user input. This API provides more flexibility than the high-level `Interface` API, which allows", "retrieved_docs": ["### Contributors Shoutout:\n\n- [@nhankiet](https://github.com/nhankiet) made their first contribution in [PR 1819](https://github.com/gradio-app/gradio/pull/1819)\n\n## 3.0\n\n###### \ud83d\udd25 Gradio 3.0 is the biggest update to the library, ever.\n\n### New Features:\n\n###### 1. Blocks \ud83e\uddf1\n\nBlocks is a new, low-level API that allows you to have full control over the data flows and layout of your application. It allows you to build very complex, multi-step applications. For example, you might want to:\n\n- Group together related demos as multiple tabs in one web app\n- Change the layout of your demo instead of just having all of the inputs on the left and outputs on the right\n- Have multi-step interfaces, in which the output of one model becomes the input to the next model, or have more flexible data flows in general\n- Change a component's properties (for example, the choices in a Dropdown) or its visibility based on user input\n\nHere's a simple example that creates the demo below it:\n\n```python\nimport gradio as gr\n\ndef update(name):\n    return f\"Welcome to Gradio, {name}!\"\n\ndemo = gr.Blocks()", "### Contributors Shoutout:\n\n- [@nhankiet](https://github.com/nhankiet) made their first contribution in [PR 1819](https://github.com/gradio-app/gradio/pull/1819)\n\n## 3.0\n\n###### \ud83d\udd25 Gradio 3.0 is the biggest update to the library, ever.\n\n### New Features:\n\n###### 1. Blocks \ud83e\uddf1\n\nBlocks is a new, low-level API that allows you to have full control over the data flows and layout of your application. It allows you to build very complex, multi-step applications. For example, you might want to:\n\n- Group together related demos as multiple tabs in one web app\n- Change the layout of your demo instead of just having all of the inputs on the left and outputs on the right\n- Have multi-step interfaces, in which the output of one model becomes the input to the next model, or have more flexible data flows in general\n- Change a component's properties (for example, the choices in a Dropdown) or its visibility based on user input\n\nHere's a simple example that creates the demo below it:\n\n```python\nimport gradio as gr\n\ndef update(name):\n    return f\"Welcome to Gradio, {name}!\"\n\ndemo = gr.Blocks()", "### Why Blocks \ud83e\uddf1?[[why-blocks-]]\n\nAs we saw in the previous sections, the `Interface` class allows you to easily create full-fledged machine learning demos with just a few lines of code. The `Interface` API is extremely easy to use but lacks the flexibility that the `Blocks` API provides. For example, you might want to:\n\n- Group together related demos as multiple tabs in one web application\n- Change the layout of your demo, e.g. to specify where the inputs and outputs are located\n- Have multi-step interfaces, in which the output of one model becomes the input to the next model, or have more flexible data flows in general\n- Change a component's properties (for example, the choices in a dropdown) or its visibility based on user input\n\nWe will explore all of these concepts below.\n\n### Creating a simple demo using Blocks[[creating-a-simple-demo-using-blocks]]\n\nAfter you have installed Gradio, run the code below as a Python script, a Jupyter notebook, or a Colab notebook.\n\n```py\nimport gradio as gr\n\n\ndef flip_text(x):\n    return x[::-1]\n\n\ndemo = gr.Blocks()\n\nwith demo:\n    gr.Markdown(\n        \"\"\"\n    # Flip Text!\n    Start typing below to see the output.\n    \"\"\"\n    )\n    input = gr.Textbox(placeholder=\"Flip this text\")\n    output = gr.Textbox()\n\n    input.change(fn=flip_text, inputs=input, outputs=output)\n\ndemo.launch()\n```", "Introduction to Gradio Blocks[[introduction-to-gradio-blocks]]\n\n<CourseFloatingBanner chapter={9}\n  classNames=\"absolute z-10 right-0 top-0\"\n  notebooks={[\n    {label: \"Google Colab\", value: \"https://colab.research.google.com/github/huggingface/notebooks/blob/master/course/en/chapter9/section7.ipynb\"},\n    {label: \"Aws Studio\", value: \"https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/master/course/en/chapter9/section7.ipynb\"},\n]} />\n\nIn the previous sections we have explored and created demos using the `Interface` class. In this section we will introduce our **newly developed** low-level API called `gradio.Blocks`.\n\nNow, what's the difference between `Interface` and `Blocks`?\n\n- \u26a1 `Interface`: a high-level API that allows you to create a full machine learning demo simply by providing a list of inputs and outputs.\n\n- \ud83e\uddf1 `Blocks`: a low-level API that allows you to have full control over the data flows and layout of your application. You can build very complex, multi-step applications using `Blocks` (as in \"building blocks\").\n\n\n### Why Blocks \ud83e\uddf1?[[why-blocks-]]", "- The `get_api_info` method of `Blocks` now supports layout output components [@freddyaboulton](https://github.com/freddyaboulton) in [PR 4871](https://github.com/gradio-app/gradio/pull/4871)\n\n- Added the support for the new command `gradio environment`to make it easier for people to file bug reports if we shipped an easy command to list the OS, gradio version, and versions of gradio/gradio-client dependencies. bu [@varshneydevansh](https://github.com/varshneydevansh) in [PR 4915](https://github.com/gradio-app/gradio/pull/4915).\n\n### Bug Fixes:\n\n- The `.change()` event is fixed in `Video` and `Image` so that it only fires once by [@abidlabs](https://github.com/abidlabs) in [PR 4793](https://github.com/gradio-app/gradio/pull/4793)\n- The `.change()` event is fixed in `Audio` so that fires when the component value is programmatically updated by [@abidlabs](https://github.com/abidlabs) in [PR 4793](https://github.com/gradio-app/gradio/pull/4793)", "- The `get_api_info` method of `Blocks` now supports layout output components [@freddyaboulton](https://github.com/freddyaboulton) in [PR 4871](https://github.com/gradio-app/gradio/pull/4871)\n\n- Added the support for the new command `gradio environment`to make it easier for people to file bug reports if we shipped an easy command to list the OS, gradio version, and versions of gradio/gradio-client dependencies. bu [@varshneydevansh](https://github.com/varshneydevansh) in [PR 4915](https://github.com/gradio-app/gradio/pull/4915).\n\n### Bug Fixes:\n\n- The `.change()` event is fixed in `Video` and `Image` so that it only fires once by [@abidlabs](https://github.com/abidlabs) in [PR 4793](https://github.com/gradio-app/gradio/pull/4793)\n- The `.change()` event is fixed in `Audio` so that fires when the component value is programmatically updated by [@abidlabs](https://github.com/abidlabs) in [PR 4793](https://github.com/gradio-app/gradio/pull/4793)", "### 9. What does Gradio `Blocks` allow you to do?\n\n<Question\n\tchoices={[\n        {\n\t\t\ttext: \"Combine multiple demos into one web app\",\n\t\t\texplain: \"You can use the `with gradio.Tabs():` to add tabs for multiple demos\",\n\t\t\tcorrect: true\n        },\n        {\n\t\t\ttext: \"Assign event triggers such as clicked/changed/etc to `Blocks` components\",\n\t\t\texplain: \"When you assign an event, you pass in three parameters: fn: the function that should be called, inputs: the (list) of input component(s), and outputs: the (list) of output components that should be called.\",\n\t\t\tcorrect: true\n        },\n        {\n\t\t\ttext: \"Automatically determine which `Blocks` component should be interactive vs. static\",\n\t\t\texplain: \"Based on the event triggers you define, `Blocks` automatically figures out whether a component should accept user input or not.\",\n\t\t\tcorrect: true\n        },\n\t\t {\n\t\t\ttext: \"Create multi-step demos; meaning allowing you to reuse the output of one component as the input to the next\",\n\t\t\texplain: \"You can use a component for the input of one event trigger but the output of another.\",\n            correct: true\n        },\n\t]}\n/>\n\n### 10. You can share a public link to a `Blocks` demo and host a `Blocks` demo on Hugging Face spaces.\n\n<Question\n\tchoices={[\n        {\n\t\t\ttext: \"True\",\n\t\t\texplain: \"Just like `Interface`, all of the sharing and hosting capabilities are the same for `Blocks` demos!\",\n\t\t\tcorrect: true\n        },\n        {\n\t\t\ttext: \"False\",\n\t\t\texplain: \"Just like `Interface`, all of the sharing and hosting capabilities are the same for `Blocks` demos!\",\n\t\t\tcorrect: false\n        }\n\t]}\n/>"], "eval_score_GPT4": "4", "eval_feedback_GPT4": "Feedback: The response provided is mostly correct and aligns well with the reference answer. It accurately describes the purpose of the `gradio.Blocks` API, mentioning the full control over data flows and layout, the ability to build complex, multi-step machine learning applications, and the flexibility it offers compared to the `Interface` API. The response goes into detail about the functionalities such as grouping demos, changing layouts, and adjusting component properties, which are all factual and relevant to the purpose of the `gradio.Blocks` API. However, the response is cut off and does not finish the last sentence, which might indicate an incomplete explanation. Despite this, the information given is accurate and provides a clear understanding of the API's purpose."}, {"question": "What is the purpose of the two-stage model proposed in the paper \"Hierarchical Text-Conditional Image Generation with CLIP Latents\"?\n", "true_answer": "The purpose of the two-stage model is to generate a CLIP image embedding given a text caption and then generate an image conditioned on the image embedding.", "source_doc": "huggingface/diffusers/blob/main/docs/source/en/api/pipelines/unclip.md", "generated_answer": "The purpose of the two-stage model proposed in the paper \"Hierarchical Text-Conditional Image Generation with CLIP Latents\" is to leverage the robust representations of images learned by contrastive models like CLIP for image generation. The model consists of a prior that generates a CLIP image embedding given a text caption, and a decoder that generates an image conditioned on the image embedding. This approach improves image diversity with minimal loss in photorealism and caption", "retrieved_docs": ["The abstract from the paper is:\n\n*Contrastive models like CLIP have been shown to learn robust representations of images that capture both semantics and style. To leverage these representations for image generation, we propose a two-stage model: a prior that generates a CLIP image embedding given a text caption, and a decoder that generates an image conditioned on the image embedding. We show that explicitly generating image representations improves image diversity with minimal loss in photorealism and caption similarity. Our decoders conditioned on image representations can also produce variations of an image that preserve both its semantics and style, while varying the non-essential details absent from the image representation. Moreover, the joint embedding space of CLIP enables language-guided image manipulations in a zero-shot fashion. We use diffusion models for the decoder and experiment with both autoregressive and diffusion models for the prior, finding that the latter are computationally more efficient and produce higher-quality samples.*\n\n## PriorTransformer\n\n[[autodoc]] PriorTransformer\n\n## PriorTransformerOutput\n\n[[autodoc]] models.prior_transformer.PriorTransformerOutput", "The abstract from the paper is following:\n\n*Contrastive models like CLIP have been shown to learn robust representations of images that capture both semantics and style. To leverage these representations for image generation, we propose a two-stage model: a prior that generates a CLIP image embedding given a text caption, and a decoder that generates an image conditioned on the image embedding. We show that explicitly generating image representations improves image diversity with minimal loss in photorealism and caption similarity. Our decoders conditioned on image representations can also produce variations of an image that preserve both its semantics and style, while varying the non-essential details absent from the image representation. Moreover, the joint embedding space of CLIP enables language-guided image manipulations in a zero-shot fashion. We use diffusion models for the decoder and experiment with both autoregressive and diffusion models for the prior, finding that the latter are computationally more efficient and produce higher-quality samples.*\n\nYou can find lucidrains' DALL-E 2 recreation at [lucidrains/DALLE2-pytorch](https://github.com/lucidrains/DALLE2-pytorch).\n\n<Tip>\n\nMake sure to check out the Schedulers [guide](../../using-diffusers/schedulers) to learn how to explore the tradeoff between scheduler speed and quality, and see the [reuse components across pipelines](../../using-diffusers/loading#reuse-components-across-pipelines) section to learn how to efficiently load the same components into multiple pipelines.\n\n</Tip>", "The abstract from the paper is:\n\n*Contrastive models like CLIP have been shown to learn robust representations of images that capture both semantics and style. To leverage these representations for image generation, we propose a two-stage model: a prior that generates a CLIP image embedding given a text caption, and a decoder that generates an image conditioned on the image embedding. We show that explicitly generating image representations improves image diversity with minimal loss in photorealism and caption similarity. Our decoders conditioned on image representations can also produce variations of an image that preserve both its semantics and style, while varying the non-essential details absent from the image representation. Moreover, the joint embedding space of CLIP enables language-guided image manipulations in a zero-shot fashion. We use diffusion models for the decoder and experiment with both autoregressive and diffusion models for the prior, finding that the latter are computationally more efficient and produce higher-quality samples.*\n\n## Tips\n\nStable unCLIP takes  `noise_level` as input during inference which determines how much noise is added to the image embeddings. A higher `noise_level` increases variation in the final un-noised images. By default, we do not add any additional noise to the image embeddings (`noise_level = 0`).\n\n### Text-to-Image Generation\nStable unCLIP can be leveraged for text-to-image generation by pipelining it with the prior model of KakaoBrain's open source DALL-E 2 replication [Karlo](https://huggingface.co/kakaobrain/karlo-v1-alpha):", "The abstract from the paper is:\n\n*We propose a method for editing images from human instructions: given an input image and a written instruction that tells the model what to do, our model follows these instructions to edit the image. To obtain training data for this problem, we combine the knowledge of two large pretrained models -- a language model (GPT-3) and a text-to-image model (Stable Diffusion) -- to generate a large dataset of image editing examples. Our conditional diffusion model, InstructPix2Pix, is trained on our generated data, and generalizes to real images and user-written instructions at inference time. Since it performs edits in the forward pass and does not require per example fine-tuning or inversion, our model edits images quickly, in a matter of seconds. We show compelling editing results for a diverse collection of input images and written instructions.*\n\nYou can find additional information about InstructPix2Pix on the [project page](https://www.timothybrooks.com/instruct-pix2pix), [original codebase](https://github.com/timothybrooks/instruct-pix2pix), and try it out in a [demo](https://huggingface.co/spaces/timbrooks/instruct-pix2pix).\n\n<Tip>", "!--Copyright 2023 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\n\nhttp://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License.\n-->\n\n# Prior Transformer\n\nThe Prior Transformer was originally introduced in [Hierarchical Text-Conditional Image Generation with CLIP Latents](https://huggingface.co/papers/2204.06125) by Ramesh et al. It is used to predict CLIP image embeddings from CLIP text embeddings; image embeddings are predicted through a denoising diffusion process.\n\nThe abstract from the paper is:", "# Text-to-image\n\nThe Stable Diffusion model was created by researchers and engineers from [CompVis](https://github.com/CompVis), [Stability AI](https://stability.ai/), [Runway](https://github.com/runwayml), and [LAION](https://laion.ai/). The [`StableDiffusionPipeline`] is capable of generating photorealistic images given any text input. It's trained on 512x512 images from a subset of the LAION-5B dataset. This model uses a frozen CLIP ViT-L/14 text encoder to condition the model on text prompts. With its 860M UNet and 123M text encoder, the model is relatively lightweight and can run on consumer GPUs. Latent diffusion is the research on top of which Stable Diffusion was built. It was proposed in [High-Resolution Image Synthesis with Latent Diffusion Models](https://huggingface.co/papers/2112.10752) by Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, Bj\u00f6rn Ommer.\n\nThe abstract from the paper is:", "<img src=\"\nassets/78_annotated-diffusion/diffusion-sweater.gif\" width=\"300\" />\n\n# Follow-up reads\n\nNote that the DDPM paper showed that diffusion models are a promising direction for (un)conditional image generation. This has since then (immensely) been improved, most notably for text-conditional image generation. Below, we list some important (but far from exhaustive) follow-up works:"], "eval_score_GPT4": "4", "eval_feedback_GPT4": "Feedback: The response provided is mostly correct and factual. It accurately describes the two-stage model's use of CLIP for generating robust image representations from text captions and then using these embeddings to generate images. The response also correctly identifies the model's components: a prior for generating CLIP image embeddings and a decoder for image generation. However, the response includes additional details about image diversity and photorealism that are not present in the reference answer. These details, while potentially valuable, are not required to fulfill the instruction's request for the model's purpose. Therefore, the response is not completely correct as per the reference answer but does include the main components and the overall purpose of the model."}, {"question": "What command is used to install the requirements for a research project using \ud83e\udd17 Transformers?\n", "true_answer": "pip install -r requirements.txt", "source_doc": "huggingface/transformers/blob/main/examples/research_projects/README.md", "generated_answer": "", "retrieved_docs": ["## Install with pip\n\nYou should install \ud83e\udd17 Transformers in a [virtual environment](https://docs.python.org/3/library/venv.html). If you're unfamiliar with Python virtual environments, take a look at this [guide](https://packaging.python.org/guides/installing-using-pip-and-virtual-environments/). A virtual environment makes it easier to manage different projects, and avoid compatibility issues between dependencies.\n\nStart by creating a virtual environment in your project directory:\n\n```bash\npython -m venv .env\n```\n\nActivate the virtual environment. On Linux and MacOs:\n\n```bash\nsource .env/bin/activate\n```\nActivate Virtual environment on Windows\n\n```bash\n.env/Scripts/activate\n```\n\nNow you're ready to install \ud83e\udd17 Transformers with the following command:\n\n```bash\npip install transformers\n```\n\nFor CPU-support only, you can conveniently install \ud83e\udd17 Transformers and a deep learning library in one line. For example, install \ud83e\udd17 Transformers and PyTorch with:\n\n```bash\npip install 'transformers[torch]'\n```\n\n\ud83e\udd17 Transformers and TensorFlow 2.0:\n\n```bash\npip install 'transformers[tf-cpu]'\n```\n\n<Tip warning={true}>\n\nM1 / ARM Users\n    \nYou will need to install the following before installing TensorFLow 2.0\n```\nbrew install cmake\nbrew install pkg-config\n```\n\n</Tip>\n\n\ud83e\udd17 Transformers and Flax:", "Research projects\n\nThis folder contains various research projects using \ud83e\udde8 Diffusers.\nThey are not really maintained by the core maintainers of this library and often require a specific version of Diffusers that is indicated in the requirements file of each folder.\nUpdating them to the most recent version of the library will require some work.\n\nTo use any of them, just run the command\n\n```\npip install -r requirements.txt\n```\ninside the folder of your choice.\n\nIf you need help with any of those, please open an issue where you directly ping the author(s), as indicated at the top of the README of each folder.", "\ud83d\udea8 **Do not** work on the `main` branch!\n\n4. Set up a development environment by running the following command in a virtual environment:\n\n   ```bash\n   pip install -e \".[dev]\"\n   ```\n\n   If \ud83e\udd17 Transformers was already installed in the virtual environment, remove\n   it with `pip uninstall transformers` before reinstalling it in editable\n   mode with the `-e` flag.\n   \n   Depending on your OS, and since the number of optional dependencies of Transformers is growing, you might get a\n   failure with this command. If that's the case make sure to install the Deep Learning framework you are working with\n   (PyTorch, TensorFlow and/or Flax) then do:\n\n   ```bash\n   pip install -e \".[quality]\"\n   ```\n\n   which should be enough for most use cases.\n\n5. Develop the features in your branch.\n\n   As you work on your code, you should make sure the test suite\n   passes. Run the tests impacted by your changes like this:\n\n   ```bash\n   pytest tests/<TEST_TO_RUN>.py\n   ```\n\n   For more information about tests, check out the\n   [Testing](https://huggingface.co/docs/transformers/testing) guide.\n\n   \ud83e\udd17 Transformers relies on `black` and `ruff` to format its source code\n   consistently. After you make changes, apply automatic style corrections and code verifications\n   that can't be automated in one go with:\n\n   ```bash\n   make fixup\n   ```\n\n   This target is also optimized to only work with files modified by the PR you're working on.", "\ud83d\udea8 **Do not** work on the `main` branch!\n\n4. Set up a development environment by running the following command in a virtual environment:\n\n   ```bash\n   pip install -e \".[dev]\"\n   ```\n\n   If \ud83e\udd17 Transformers was already installed in the virtual environment, remove\n   it with `pip uninstall transformers` before reinstalling it in editable\n   mode with the `-e` flag.\n   \n   Depending on your OS, and since the number of optional dependencies of Transformers is growing, you might get a\n   failure with this command. If that's the case make sure to install the Deep Learning framework you are working with\n   (PyTorch, TensorFlow and/or Flax) then do:\n\n   ```bash\n   pip install -e \".[quality]\"\n   ```\n\n   which should be enough for most use cases.\n\n5. Develop the features in your branch.\n\n   As you work on your code, you should make sure the test suite\n   passes. Run the tests impacted by your changes like this:\n\n   ```bash\n   pytest tests/<TEST_TO_RUN>.py\n   ```\n\n   For more information about tests, check out the\n   [Testing](https://huggingface.co/docs/transformers/testing) guide.\n\n   \ud83e\udd17 Transformers relies on `black` and `ruff` to format its source code\n   consistently. After you make changes, apply automatic style corrections and code verifications\n   that can't be automated in one go with:\n\n   ```bash\n   make fixup\n   ```\n\n   This target is also optimized to only work with files modified by the PR you're working on.", "Check if \ud83e\udd17 Transformers has been properly installed by running the following command:\n\n```bash\npython -c \"from transformers import pipeline; print(pipeline('sentiment-analysis')('I love you'))\"\n```\n\n## Editable install\n\nYou will need an editable install if you'd like to:\n\n* Use the `main` version of the source code.\n* Contribute to \ud83e\udd17 Transformers and need to test changes in the code.\n\nClone the repository and install \ud83e\udd17 Transformers with the following commands:\n\n```bash\ngit clone https://github.com/huggingface/transformers.git\ncd transformers\npip install -e .\n```\n\nThese commands will link the folder you cloned the repository to and your Python library paths. Python will now look inside the folder you cloned to in addition to the normal library paths. For example, if your Python packages are typically installed in `~/anaconda3/envs/main/lib/python3.7/site-packages/`, Python will also search the folder you cloned to: `~/transformers/`.\n\n<Tip warning={true}>\n\nYou must keep the `transformers` folder if you want to keep using the library.\n\n</Tip>\n\nNow you can easily update your clone to the latest version of \ud83e\udd17 Transformers with the following command:\n\n```bash\ncd ~/transformers/\ngit pull\n```\n\nYour Python environment will find the `main` version of \ud83e\udd17 Transformers on the next run.\n\n## Install with conda\n\nInstall from the conda channel `huggingface`:", "3. Create a new branch to hold your development changes. This is especially useful to share code changes with your team:\n\n   ```bash\n   $ git checkout -b a-descriptive-name-for-my-project\n   ```\n\n4. Set up a PyTorch environment by running the following command your virtual environment:\n\n   ```bash\n   $ pip install -e \".[torch-speech]\"\n   ```\n\n   (If transformers was already installed in the virtual environment, remove\n   it with `pip uninstall transformers` before reinstalling it in editable\n   mode with the `-e` flag.)\n\n   If you have already cloned that repo, you might need to `git pull` to get the most recent changes in the `transformers`\n   library.\n\n   Running this command will automatically install `torch` and the most relevant \n   libraries required for fine-tuning a speech recognition system.\n\nNext, you should also install the \ud83e\udd17 Datasets library. We strongly recommend installing the \nlibrary from source to profit from the most current additions during the community week.\n\nSimply run the following steps:\n\n```\n$ cd ~/\n$ git clone https://github.com/huggingface/datasets.git\n$ cd datasets\n$ pip install -e \".[streaming]\"\n```\n\nIf you plan on contributing a specific dataset during \nthe community week, please fork the datasets repository and follow the instructions \n[here](https://github.com/huggingface/datasets/blob/master/CONTRIBUTING.md#how-to-create-a-pull-request).", "```bash\n   $ git checkout -b a-descriptive-name-for-my-project\n   ```\n\n4. Set up a flax environment by running the following command in a virtual environment:\n\n   ```bash\n   $ pip install -e \".[flax]\"\n   ```\n\n   (If transformers was already installed in the virtual environment, remove\n   it with `pip uninstall transformers` before reinstalling it in editable\n   mode with the `-e` flag.)\n\n   If you have already cloned that repo, you might need to `git pull` to get the most recent changes in the `datasets`\n   library.\n\n   Running this command will automatically install `flax`, `jax` and `optax`.\n\nNext, you should also install the \ud83e\udd17 Datasets library. We strongly recommend installing the \nlibrary from source to profit from the most current additions during the community week.\n\nSimply run the following steps:\n\n```\n$ cd ~/\n$ git clone https://github.com/huggingface/datasets.git\n$ cd datasets\n$ pip install -e \".[streaming]\"\n```\n\nIf you plan on contributing a specific dataset during \nthe community week, please fork the datasets repository and follow the instructions \n[here](https://github.com/huggingface/datasets/blob/master/CONTRIBUTING.md#how-to-create-a-pull-request).\n\nTo verify that all libraries are correctly installed, you can run the following command.\nIt assumes that both `transformers` and `datasets` were installed from main - otherwise\ndatasets streaming will not work correctly."], "eval_score_GPT4": "1", "eval_feedback_GPT4": "Feedback: The response provided does not contain any information or command related to the task of installing the requirements for a research project using \ud83e\udd17 Transformers. According to the score rubric, the response should be evaluated as completely incorrect, inaccurate, and not factual, as it does not attempt to answer the question."}, {"question": "What task does the `roberta-large-mnli` checkpoint perform?\n", "true_answer": "Text classification", "source_doc": "huggingface/course/blob/main/chapters/en/chapter1/10.mdx", "generated_answer": "The `roberta-large-mnli` checkpoint performs the task of natural language inference, specifically the task of classifying if two sentences are logically linked across three labels (contradiction, neutral, entailment). This is also referred to as the task of multi-natural language inference (MNLI). The checkpoint is available on the Hugging Face Hub and was trained on the MNLI corpus.", "retrieved_docs": ["!-- DISABLE-FRONTMATTER-SECTIONS -->\n\n# End-of-chapter quiz[[end-of-chapter-quiz]]\n\n<CourseFloatingBanner\n    chapter={1}\n    classNames=\"absolute z-10 right-0 top-0\"\n/>\n\nThis chapter covered a lot of ground! Don't worry if you didn't grasp all the details; the next chapters will help you understand how things work under the hood.\n\nFirst, though, let's test what you learned in this chapter!\n\n\n### 1. Explore the Hub and look for the `roberta-large-mnli` checkpoint. What task does it perform?\n\n\n<Question\n\tchoices={[\n\t\t{\n\t\t\ttext: \"Summarization\",\n\t\t\texplain: \"Look again on the <a href=\\\"https://huggingface.co/roberta-large-mnli\\\">roberta-large-mnli page</a>.\"\n\t\t},\n\t\t{\n\t\t\ttext: \"Text classification\",\n\t\t\texplain: \"More precisely, it classifies if two sentences are logically linked across three labels (contradiction, neutral, entailment) \u2014 a task also called <em>natural language inference</em>.\",\n\t\t\tcorrect: true\n\t\t},\n\t\t{\n\t\t\ttext: \"Text generation\",\n\t\t\texplain: \"Look again on the <a href=\\\"https://huggingface.co/roberta-large-mnli\\\">roberta-large-mnli page</a>.\"\n\t\t}\n\t]}\n/>\n\n### 2. What will the following code return?\n\n```py\nfrom transformers import pipeline", "There are total **3 checkpoints** available in **\ud83e\udd17Hub** (at the point of writing this article): [`bigbird-roberta-base`](https://huggingface.co/google/bigbird-roberta-base), [`bigbird-roberta-large`](https://huggingface.co/google/bigbird-roberta-large), [`bigbird-base-trivia-itc`](https://huggingface.co/google/bigbird-base-trivia-itc). The first two checkpoints come from pretraining `BigBirdForPretraining` with `masked_lm loss`; while the last one corresponds to the checkpoint after finetuning `BigBirdForQuestionAnswering` on `trivia-qa` dataset.\n\nLet's have a look at minimal code you can write (in case you like to use your PyTorch trainer), to use \ud83e\udd17's BigBird model for fine-tuning your tasks.\n\n```python\n# let's consider our task to be question-answering as an example\n\nfrom transformers import BigBirdForQuestionAnswering, BigBirdTokenizer\nimport torch\n\ndevice = torch.device(\"cpu\")\nif torch.cuda.is_available():\n    device = torch.device(\"cuda\")", "## Models\n\n### RoBERTa\n\n#### Load RoBERTa Checkpoints for the Classification Task\n\nWe load the pre-trained RoBERTa model with a sequence classification head using the Hugging Face `AutoModelForSequenceClassification` class:\n\n```python\nfrom transformers import AutoModelForSequenceClassification \nroberta_model = AutoModelForSequenceClassification.from_pretrained(roberta_checkpoint, num_labels=2)\n```\n\n\n####  LoRA setup for RoBERTa classifier\n\nWe import LoRa configuration and set some parameters for RoBERTa classifier:\n- TaskType: Sequence classification\n- r(rank): Rank for our decomposition matrices\n- lora_alpha: Alpha parameter to scale the learned weights. LoRA paper advises fixing alpha at 16\n- lora_dropout: Dropout probability of the LoRA layers\n- bias: Whether to add bias term to LoRa layers\n\nThe code below uses the values recommended by the [Lora paper](https://arxiv.org/abs/2106.09685). [Later in this post](#hyperparameter-tuning) we will perform hyperparameter tuning of these parameters using `wandb`.", "![image](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/Lora-for-sequence-classification-with-Roberta-Llama-Mistral/lora.png)\n\n## Setup\n\nRoBERTa has a limitatiom of maximum sequence length of 512, so we set the `MAX_LEN=512` for all models to ensure a fair comparison.\n\n```python\nMAX_LEN = 512 \nroberta_checkpoint = \"roberta-large\"\nmistral_checkpoint = \"mistralai/Mistral-7B-v0.1\"\nllama_checkpoint = \"meta-llama/Llama-2-7b-hf\"\n```\n\n## Data preparation\n### Data loading\n\nWe will load the dataset from Hugging Face:\n```python\nfrom datasets import load_dataset\ndataset = load_dataset(\"mehdiiraqui/twitter_disaster\")\n```\n Now, let's split the dataset into training and validation datasets. Then add the test set:\n\n```python\nfrom datasets import Dataset\n# Split the dataset into training and validation datasets\ndata = dataset['train'].train_test_split(train_size=0.8, seed=42)\n# Rename the default \"test\" split to \"validation\"\ndata['val'] = data.pop(\"test\")\n# Convert the test dataframe to HuggingFace dataset and add it into the first dataset\ndata['test'] = dataset['test']\n```\n\nHere's an overview of the dataset:", "## Setup\n\nLet's start by importing all the necessary libraries you'll need:\n\n- \ud83e\udd17 Transformers for loading the base `roberta-large` model and tokenizer, and handling the training loop\n- \ud83e\udd17 Datasets for loading and preparing the `bionlp2004` dataset for training\n- \ud83e\udd17 Evaluate for evaluating the model's performance\n- \ud83e\udd17 PEFT for setting up the LoRA configuration and creating the PEFT model\n\n```py\nfrom datasets import load_dataset\nfrom transformers import (\n    AutoModelForTokenClassification,\n    AutoTokenizer,\n    DataCollatorForTokenClassification,\n    TrainingArguments,\n    Trainer,\n)\nfrom peft import get_peft_config, PeftModel, PeftConfig, get_peft_model, LoraConfig, TaskType\nimport evaluate\nimport torch\nimport numpy as np\n\nmodel_checkpoint = \"roberta-large\"\nlr = 1e-3\nbatch_size = 16\nnum_epochs = 10\n```\n\n## Load dataset and metric\n\nThe [BioNLP2004](https://huggingface.co/datasets/tner/bionlp2004) dataset includes tokens and tags for biological structures like DNA, RNA and proteins. Load the dataset:", "#### (Important) More details on how to create a debugging environment for BigBird \n\n- BigBird has multiple pretrained checkpoints that should eventually all be ported to \n  \ud83e\udd17 Transformers. The pretrained checkpoints can be found [here](https://console.cloud.google.com/storage/browser/bigbird-transformer/pretrain;tab=objects?prefix=&forceOnObjectsSortingFiltering=false). \n\tThose checkpoints include both pretrained weights for encoder-only (BERT/RoBERTa) under the folder `bigbr_base` and encoder-decoder (PEGASUS) under the folder `bigbp_large`.\n\tYou should start by porting the `bigbr_base` model. The encoder-decoder model \n\tcan be ported afterward.\n\tfor an encoder-decoder architecture as well as an encoder-only architecture. \n- BigBird was written in tf.compat meaning that a mixture of a TensorFlow 1 and \n  TensorFlow 2 API was used.\n- The most important part of the BigBird code-base is [bigbird.bigbird.core](https://github.com/google-research/bigbird/tree/master/bigbird/core) which includes all logic necessary \n  to implement BigBird.\n- The first goal should be to successfully run a forward pass using the RoBERTa checkpoint `bigbr_base/model.ckpt-0.data-00000-of-00001` and `bigbr_base/model.ckpt-0.index`.\n\n\n### Port BigBird to \ud83e\udd17 Transformers\n\nNext, you can finally start adding new code to \ud83e\udd17 Transformers. Go into\nthe clone of your \ud83e\udd17 Transformers' fork:\n\n    cd transformers", "We start by defining the RoBERTa dataloader: \n\n-  Load the tokenizer: \n```python\nfrom transformers import AutoTokenizer\nroberta_tokenizer = AutoTokenizer.from_pretrained(roberta_checkpoint, add_prefix_space=True)\n```\n**Note:** The RoBERTa tokenizer has been trained to treat spaces as part of the token. As a result, the first word of the sentence is encoded differently if it is not preceded by a white space. To ensure the first word includes a space, we set `add_prefix_space=True`. Also, to maintain consistent pre-processing for all three models, we set the parameter to 'True' for Llama 2 and Mistral 7b.\n\n- Define the preprocessing function for converting one row of the dataframe:\n```python\ndef roberta_preprocessing_function(examples):\n    return roberta_tokenizer(examples['text'], truncation=True, max_length=MAX_LEN)\n```"], "eval_score_GPT4": "5", "eval_feedback_GPT4": "The response provides a detailed explanation of the task performed by the `roberta-large-mnli` checkpoint, correctly identifying it as a model for natural language inference and explaining the classification across three labels. It aligns with the reference answer, which is text classification, by specifying the type of text classification it performs. The response is accurate and factual, providing additional context about the model's training and availability."}, {"question": "What service is replacing the Paid tier of the Inference API at Hugging Face?\n", "true_answer": "Inference Endpoints", "source_doc": "huggingface/blog/blob/main/pricing-update.md", "generated_answer": "The service replacing the Paid tier of the Inference API at Hugging Face is called Inference Endpoints. The Inference API will still be available for free, but for those looking for a fast, enterprise-grade inference as a service, Inference Endpoints is recommended. Both services can be called using the `InferenceClient` object provided by the `huggingface_hub` library.", "retrieved_docs": ["--\ntitle: Introducing our new pricing\nthumbnail: /blog/assets/114_pricing-update/thumbnail.png\nauthors:\n- user: sbrandeis\n- user: pierric\n---\n\n# Introducing our new pricing\n\n\nAs you might have noticed, our [pricing page](https://huggingface.co/pricing) has changed a lot recently.\n\nFirst of all, we are sunsetting the Paid tier of the Inference API service. The Inference API will still be available for everyone to use for free. But if you're looking for a fast, enterprise-grade inference as a service, we recommend checking out our brand new solution for this: [Inference Endpoints](https://huggingface.co/inference-endpoints).\n\nAlong with Inference Endpoints, we've recently introduced hardware upgrades for [Spaces](https://huggingface.co/spaces/launch), which allows running ML demos with the hardware of your choice. No subscription is required to use these services; you only need to add a credit card to your account from your [billing settings](https://huggingface.co/settings/billing). You can also attach a payment method to any of [your organizations](https://huggingface.co/settings/organizations).\n\nYour billing settings centralize everything about our paid services. From there, you can manage your personal PRO subscription, update your payment method, and visualize your usage for the past three months. Usage for all our paid services and subscriptions will be charged at the start of each month, and a consolidated invoice will be available for your records.", "### Authentication\n\nCalls made with the [`InferenceClient`] can be authenticated using a [User Access Token](https://huggingface.co/docs/hub/security-tokens).\nBy default, it will use the token saved on your machine if you are logged in (check out\n[how to authenticate](https://huggingface.co/docs/huggingface_hub/quick-start#authentication)). If you are not logged in, you can pass\nyour token as an instance parameter:\n\n```python\n>>> from huggingface_hub import InferenceClient\n>>> client = InferenceClient(token=\"hf_***\")\n```\n\n<Tip>\n\nAuthentication is NOT mandatory when using the Inference API. However, authenticated users get a higher free-tier to\nplay with the service. Token is also mandatory if you want to run inference on your private models or on private\nendpoints.\n\n</Tip>\n\n## Supported tasks\n\n[`InferenceClient`]'s goal is to provide the easiest interface to run inference on Hugging Face models. It\nhas a simple API that supports the most common tasks. Here is a list of the currently supported tasks:", "!--\u26a0\ufe0f Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\nrendered properly in your Markdown viewer.\n-->\n\n# Run Inference on servers\n\nInference is the process of using a trained model to make predictions on new data. As this process can be compute-intensive,\nrunning on a dedicated server can be an interesting option. The `huggingface_hub` library provides an easy way to call a\nservice that runs inference for hosted models. There are several services you can connect to:\n- [Inference API](https://huggingface.co/docs/api-inference/index): a service that allows you to run accelerated inference\non Hugging Face's infrastructure for free. This service is a fast way to get started, test different models, and\nprototype AI products.\n- [Inference Endpoints](https://huggingface.co/docs/inference-endpoints/index): a product to easily deploy models to production.\nInference is run by Hugging Face in a dedicated, fully managed infrastructure on a cloud provider of your choice.\n\nThese services can be called with the [`InferenceClient`] object. It acts as a replacement for the legacy\n[`InferenceApi`] client, adding specific support for tasks and handling inference on both\n[Inference API](https://huggingface.co/docs/api-inference/index) and [Inference Endpoints](https://huggingface.co/docs/inference-endpoints/index).\nLearn how to migrate to the new client in the [Legacy InferenceAPI client](#legacy-inferenceapi-client) section.\n\n<Tip>", "Additionally, sharing a model on the Hub automatically deploys a hosted Inference API for that model. Anyone in the community is free to test it out directly on the model's page, with custom inputs and appropriate widgets.\n\nThe best part is that sharing and using any public model on the Hub is completely free! [Paid plans](https://huggingface.co/pricing) also exist if you wish to share models privately.\n\nThe video below shows how to navigate the Hub.\n\n<Youtube id=\"XvSGPZFEjDY\"/>\n\nHaving a huggingface.co account is required to follow along this part, as we'll be creating and managing repositories on the Hugging Face Hub: [create an account](https://huggingface.co/join)", "!--\u26a0\ufe0f Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\nrendered properly in your Markdown viewer.\n-->\n\n# Inference\n\nInference is the process of using a trained model to make predictions on new data. As this process can be compute-intensive,\nrunning on a dedicated server can be an interesting option. The `huggingface_hub` library provides an easy way to call a\nservice that runs inference for hosted models. There are several services you can connect to:\n- [Inference API](https://huggingface.co/docs/api-inference/index): a service that allows you to run accelerated inference\non Hugging Face's infrastructure for free. This service is a fast way to get started, test different models, and\nprototype AI products.\n- [Inference Endpoints](https://huggingface.co/inference-endpoints): a product to easily deploy models to production.\nInference is run by Hugging Face in a dedicated, fully managed infrastructure on a cloud provider of your choice.\n\nThese services can be called with the [`InferenceClient`] object. Please refer to [this guide](../guides/inference)\nfor more information on how to use it.\n\n## Inference Client\n\n[[autodoc]] InferenceClient\n\n## Async Inference Client\n\nAn async version of the client is also provided, based on `asyncio` and `aiohttp`.\nTo use it, you can either install `aiohttp` directly or use the `[inference]` extra:", "### Text Generation Inference\n\nResponse time and latency for concurrent users are a big challenge for serving these large models. To tackle this problem, Hugging Face has released [text-generation-inference](https://github.com/huggingface/text-generation-inference) (TGI), an open-source serving solution for large language models built on Rust, Python, and gRPc. TGI is integrated into inference solutions of Hugging Face, [Inference Endpoints](https://huggingface.co/inference-endpoints), and [Inference API](https://huggingface.co/inference-api), so you can directly create an endpoint with optimized inference with few clicks, or simply send a request to Hugging Face's Inference API to benefit from it, instead of integrating TGI to your platform. \n\n![Screenshot from HuggingChat](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/os_llms/huggingchat_ui.png)\n\nTGI currently powers [HuggingChat](https://huggingface.co/chat/), Hugging Face's open-source chat UI for LLMs. This service currently uses one of OpenAssistant's models as the backend model. You can chat as much as you want with HuggingChat and enable the Web search feature for responses that use elements from current Web pages. You can also give feedback to each response for model authors to train better models. The UI of HuggingChat is also [open-sourced](https://github.com/huggingface/chat-ui), and we are working on more features for HuggingChat to allow more functions, like generating images inside the chat.", "### By using an API\n\nToday, various cloud providers propose commercial APIs to use language models. Here is the current Hugging Face offering:\n\nThe free [Inference API](https://huggingface.co/docs/api-inference/index) to allow anyone to use small to medium-sized models from the community.\n\nThe more advanced and production-ready [Inference Endpoints API](https://huggingface.co/inference-endpoints) for those who require larger models or custom inference code.\n\nThese two APIs can be used from Node using the [Hugging Face Inference API library](https://www.npmjs.com/package/@huggingface/inference) on NPM.\n\n\ud83d\udca1 Top performing models generally require a lot of memory (32 Gb, 64 Gb or more) and hardware acceleration to get good latency (see [the benchmarks](https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard)). But we are also seeing a trend of models shrinking in size while keeping relatively good results on some tasks, with requirements as low as 16 Gb or even 8 Gb of memory.\n\n## Architecture\n\nWe are going to use NodeJS to create our generative AI web server.\n\nThe model will be [WizardCoder-15B](https://huggingface.co/WizardLM/WizardCoder-15B-V1.0) running on the Inference Endpoints API, but feel free to try with another model and stack.\n\nIf you are interested in other solutions, here are some pointers to alternative implementations:"], "eval_score_GPT4": "5", "eval_feedback_GPT4": "Feedback: The response provided is completely correct, accurate, and factual. It correctly identifies \"Inference Endpoints\" as the service replacing the Paid tier of the Inference API at Hugging Face. Additionally, the response provides extra context by mentioning that the Inference API will still be available for free and that Inference Endpoints is recommended for enterprise-grade inference as a service. This context aligns with the reference answer and does not detract from the factual accuracy of the response."}, {"question": "What architectural feature does SqueezeBERT use instead of fully-connected layers for the Q, K, V, and FFN layers?\n", "true_answer": "Grouped convolutions", "source_doc": "huggingface/transformers/blob/main/docs/source/en/model_doc/squeezebert.md", "generated_answer": "SqueezeBERT uses [grouped convolutions](https://blog.yani.io/filter-group-tutorial) instead of fully-connected layers for the Q, K, V, and FFN layers.", "retrieved_docs": ["The abstract from the paper is the following:\n\n*Humans read and write hundreds of billions of messages every day. Further, due to the availability of large datasets,\nlarge computing systems, and better neural network models, natural language processing (NLP) technology has made\nsignificant strides in understanding, proofreading, and organizing these messages. Thus, there is a significant\nopportunity to deploy NLP in myriad applications to help web users, social networks, and businesses. In particular, we\nconsider smartphones and other mobile devices as crucial platforms for deploying NLP models at scale. However, today's\nhighly-accurate NLP neural network models such as BERT and RoBERTa are extremely computationally expensive, with\nBERT-base taking 1.7 seconds to classify a text snippet on a Pixel 3 smartphone. In this work, we observe that methods\nsuch as grouped convolutions have yielded significant speedups for computer vision networks, but many of these\ntechniques have not been adopted by NLP neural network designers. We demonstrate how to replace several operations in\nself-attention layers with grouped convolutions, and we use this technique in a novel network architecture called\nSqueezeBERT, which runs 4.3x faster than BERT-base on the Pixel 3 while achieving competitive accuracy on the GLUE test\nset. The SqueezeBERT code will be released.*\n\nThis model was contributed by [forresti](https://huggingface.co/forresti).\n\n## Usage tips", "This model was contributed by [forresti](https://huggingface.co/forresti).\n\n## Usage tips\n\n- SqueezeBERT is a model with absolute position embeddings so it's usually advised to pad the inputs on the right\n  rather than the left.\n- SqueezeBERT is similar to BERT and therefore relies on the masked language modeling (MLM) objective. It is therefore\n  efficient at predicting masked tokens and at NLU in general, but is not optimal for text generation. Models trained\n  with a causal language modeling (CLM) objective are better in that regard.\n- For best results when finetuning on sequence classification tasks, it is recommended to start with the\n  *squeezebert/squeezebert-mnli-headless* checkpoint.\n\n## Resources\n\n- [Text classification task guide](../tasks/sequence_classification)\n- [Token classification task guide](../tasks/token_classification)\n- [Question answering task guide](../tasks/question_answering)\n- [Masked language modeling task guide](../tasks/masked_language_modeling)\n- [Multiple choice task guide](../tasks/multiple_choice)\n\n## SqueezeBertConfig\n\n[[autodoc]] SqueezeBertConfig\n\n## SqueezeBertTokenizer\n\n[[autodoc]] SqueezeBertTokenizer\n    - build_inputs_with_special_tokens\n    - get_special_tokens_mask\n    - create_token_type_ids_from_sequences\n    - save_vocabulary\n\n## SqueezeBertTokenizerFast\n\n[[autodoc]] SqueezeBertTokenizerFast\n\n## SqueezeBertModel", "This model was contributed by [shangz](https://huggingface.co/shangz).\n\n## Usage tips\n\n- QDQBERT model adds fake quantization operations (pair of QuantizeLinear/DequantizeLinear ops) to (i) linear layer\n  inputs and weights, (ii) matmul inputs, (iii) residual add inputs, in BERT model.\n- QDQBERT requires the dependency of [Pytorch Quantization Toolkit](https://github.com/NVIDIA/TensorRT/tree/master/tools/pytorch-quantization). To install `pip install pytorch-quantization --extra-index-url https://pypi.ngc.nvidia.com`\n- QDQBERT model can be loaded from any checkpoint of HuggingFace BERT model (for example *bert-base-uncased*), and\n  perform Quantization Aware Training/Post Training Quantization.\n- A complete example of using QDQBERT model to perform Quatization Aware Training and Post Training Quantization for\n  SQUAD task can be found at [transformers/examples/research_projects/quantization-qdqbert/](examples/research_projects/quantization-qdqbert/).\n\n### Set default quantizers", "## Implementation details\n\nThe main differences compared to GPT2.\n- Added support for Multi-Query Attention.\n- Use `gelu_pytorch_tanh` instead of classic `gelu`.\n- Avoid unnecessary synchronizations (this has since been added to GPT2 in #20061, but wasn't in the reference codebase).\n- Use Linear layers instead of Conv1D (good speedup but makes the checkpoints incompatible).\n- Merge `_attn` and `_upcast_and_reordered_attn`. Always merge the matmul with scaling. Rename `reorder_and_upcast_attn`->`attention_softmax_in_fp32`\n- Cache the attention mask value to avoid recreating it every time.\n- Use jit to fuse the attention fp32 casting, masking, softmax, and scaling.\n- Combine the attention and causal masks into a single one, pre-computed for the whole model instead of every layer.\n- Merge the key and value caches into one (this changes the format of layer_past/ present, does it risk creating problems?)\n- Use the memory layout (self.num_heads, 3, self.head_dim) instead of `(3, self.num_heads, self.head_dim)` for the QKV tensor with MHA. (prevents an overhead with the merged key and values, but makes the checkpoints incompatible with the original gpt2 model).\n\nYou can read more about the optimizations in the [original pull request](https://github.com/huggingface/transformers/pull/22575)\n\n## Combining Starcoder and Flash Attention 2", "!--Copyright 2020 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\n\nhttp://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License.\n\n\u26a0\ufe0f Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\nrendered properly in your Markdown viewer.\n\n-->\n\n# SqueezeBERT\n\n## Overview\n\nThe SqueezeBERT model was proposed in [SqueezeBERT: What can computer vision teach NLP about efficient neural networks?](https://arxiv.org/abs/2006.11316) by Forrest N. Iandola, Albert E. Shaw, Ravi Krishna, Kurt W. Keutzer. It's a\nbidirectional transformer similar to the BERT model. The key difference between the BERT architecture and the\nSqueezeBERT architecture is that SqueezeBERT uses [grouped convolutions](https://blog.yani.io/filter-group-tutorial)\ninstead of fully-connected layers for the Q, K, V and FFN layers.\n\nThe abstract from the paper is the following:", "--\ntitle: \"Making LLMs even more accessible with bitsandbytes, 4-bit quantization and QLoRA\" \nthumbnail: /blog/assets/96_hf_bitsandbytes_integration/Thumbnail_blue.png\nauthors:\n- user: ybelkada\n- user: timdettmers\n  guest: true\n- user: artidoro\n  guest: true \n- user: sgugger\n- user: smangrul\n---\n\n# Making LLMs even more accessible with bitsandbytes, 4-bit quantization and QLoRA\n\n\nLLMs are known to be large, and running or training them in consumer hardware is a huge challenge for users and accessibility. \nOur [LLM.int8 blogpost](https://huggingface.co/blog/hf-bitsandbytes-integration) showed how the techniques in the [LLM.int8 paper](https://arxiv.org/abs/2208.07339) were integrated in transformers using the `bitsandbytes` library.\nAs we strive to make models even more accessible to anyone, we decided to collaborate with bitsandbytes again to allow users to run models in 4-bit precision. This includes a large majority of HF models, in any modality (text, vision, multi-modal, etc.). Users can also train adapters on top of 4bit models leveraging tools from the Hugging Face ecosystem. This is a new method introduced today in the QLoRA paper by Dettmers et al. The abstract of the paper is as follows:", "1. **[SqueezeBERT](https://huggingface.co/docs/transformers/model_doc/squeezebert)** (\u6765\u81ea Berkeley) \u4f34\u968f\u8bba\u6587 [SqueezeBERT: What can computer vision teach NLP about efficient neural networks?](https://arxiv.org/abs/2006.11316) \u7531 Forrest N. Iandola, Albert E. Shaw, Ravi Krishna, and Kurt W. Keutzer \u53d1\u5e03\u3002\n1. **[SwiftFormer](https://huggingface.co/docs/transformers/model_doc/swiftformer)** (\u6765\u81ea MBZUAI) \u4f34\u968f\u8bba\u6587 [SwiftFormer: Efficient Additive Attention for Transformer-based Real-time Mobile Vision Applications](https://arxiv.org/abs/2303.15446) \u7531 Abdelrahman Shaker, Muhammad Maaz, Hanoona Rasheed, Salman Khan, Ming-Hsuan Yang, Fahad Shahbaz Khan \u53d1\u5e03\u3002\n1. **[Swin Transformer](https://huggingface.co/docs/transformers/model_doc/swin)** (\u6765\u81ea Microsoft) \u4f34\u968f\u8bba\u6587 [Swin Transformer: Hierarchical Vision Transformer using Shifted Windows](https://arxiv.org/abs/2103.14030) \u7531 Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng Zhang, Stephen Lin, Baining Guo \u53d1\u5e03\u3002"], "eval_score_GPT4": "5", "eval_feedback_GPT4": "Feedback: The response provided is completely correct, accurate, and factual. It aligns with the reference answer by stating that SqueezeBERT uses grouped convolutions instead of fully-connected layers for the Q, K, V, and FFN layers. The inclusion of a hyperlink to a resource about grouped convolutions adds value without deviating from the accuracy of the answer."}, {"question": "What type of license is the HuggingFace Team's software distributed under?\n", "true_answer": "Apache License, Version 2.0", "source_doc": "huggingface/simulate/blob/main/docs/source/tutorials/interaction.mdx", "generated_answer": "The HuggingFace Team's software is distributed under the Apache License, Version 2.0, as stated in the copyright notices provided in the context. This license requires that the software be used in compliance with its terms and conditions, and it grants a license to use the software \"AS IS\" without any warranties or conditions, either express or implied. The full text of the license can be found at http://www.apache.org/licenses/LICENSE-2.0.", "retrieved_docs": ["!---\nCopyright 2020 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\");\nyou may not use this file except in compliance with the License.\nYou may obtain a copy of the License at\n\n    http://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software\ndistributed under the License is distributed on an \"AS IS\" BASIS,\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\nSee the License for the specific language governing permissions and\nlimitations under the License.\n-->\n\n# How to contribute to huggingface_hub, the GitHub repository?\n\nEveryone is welcome to contribute, and we value everybody's contribution. Code is not the only way to help the community.\nAnswering questions, helping others, reaching out and improving the documentations are immensely valuable to the community.\n\nIt also helps us if you spread the word: reference the library from blog posts\non the awesome projects it made possible, shout out on Twitter every time it has\nhelped you, or simply star the repo to say \"thank you\".\n\nWhichever way you choose to contribute, please be mindful to respect our\n[code of conduct](https://github.com/huggingface/huggingface_hub/blob/main/CODE_OF_CONDUCT.md).\n\n> Looking for a good first issue to work on?\n> Please check out our contributing guide below and then select an issue from our [curated list](https://github.com/huggingface/huggingface_hub/contribute).\n> Pick one and get started with it!", "!--Copyright 2022 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\n\nhttp://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License.\n-->\n\n# Rigid Bodies\n\n[[autodoc]] RigidBodyComponent", "!--Copyright 2022 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\n\nhttp://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License.\n-->\n\n# Using Sample Factory for high throughput training\n\nUnder construction \ud83d\udea7.", "!--Copyright 2020 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\n\nhttp://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License.\n\n\u26a0\ufe0f Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\nrendered properly in your Markdown viewer.\n\n-->\n\n# DistilBERT", "!--Copyright 2020 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\n\nhttp://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License.\n\n\u26a0\ufe0f Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\nrendered properly in your Markdown viewer.\n\n-->\n\n# T5", "!--Copyright 2021 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\n\nhttp://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License.\n\n\u26a0\ufe0f Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\nrendered properly in your Markdown viewer.\n\n-->\n\n# DeiT\n\n## Overview", "!--Copyright 2022 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\n\nhttp://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License.\n-->\n\n# Quantization\n\n## ORTQuantizer\n\n[[autodoc]] onnxruntime.quantization.ORTQuantizer\n    - all"], "eval_score_GPT4": "5", "eval_feedback_GPT4": "The response accurately identifies the HuggingFace Team's software license as the Apache License, Version 2.0, which is in full agreement with the reference answer. The response also provides additional factual information about the license terms and a link to the full text, which supports the correctness and accuracy of the response."}, {"question": "What are the two parameter-reduction techniques proposed in the ALBERT model to lower memory consumption and increase training speed?\n", "true_answer": "Splitting the embedding matrix into two smaller matrices and using repeating layers split among groups.", "source_doc": "huggingface/transformers/blob/main/docs/source/en/model_doc/albert.md", "generated_answer": "The ALBERT model presents two parameter-reduction techniques to lower memory consumption and increase training speed:\n\n1. Splitting the embedding matrix into two smaller matrices. This reduces the size of the embedding matrix, which can be very large due to the large vocabulary size, and helps to lower memory consumption.\n\n2. Using repeating layers split among groups. This allows layers to share parameters, which reduces the number of parameters in the model and helps to lower memory consumption.", "retrieved_docs": ["## Overview\n\nThe ALBERT model was proposed in [ALBERT: A Lite BERT for Self-supervised Learning of Language Representations](https://arxiv.org/abs/1909.11942) by Zhenzhong Lan, Mingda Chen, Sebastian Goodman, Kevin Gimpel, Piyush Sharma,\nRadu Soricut. It presents two parameter-reduction techniques to lower memory consumption and increase the training\nspeed of BERT:\n\n- Splitting the embedding matrix into two smaller matrices.\n- Using repeating layers split among groups.\n\nThe abstract from the paper is the following:\n\n*Increasing model size when pretraining natural language representations often results in improved performance on\ndownstream tasks. However, at some point further model increases become harder due to GPU/TPU memory limitations,\nlonger training times, and unexpected model degradation. To address these problems, we present two parameter-reduction\ntechniques to lower memory consumption and increase the training speed of BERT. Comprehensive empirical evidence shows\nthat our proposed methods lead to models that scale much better compared to the original BERT. We also use a\nself-supervised loss that focuses on modeling inter-sentence coherence, and show it consistently helps downstream tasks\nwith multi-sentence inputs. As a result, our best model establishes new state-of-the-art results on the GLUE, RACE, and\nSQuAD benchmarks while having fewer parameters compared to BERT-large.*\n\nThis model was contributed by [lysandre](https://huggingface.co/lysandre). This model jax version was contributed by\n[kamalkraj](https://huggingface.co/kamalkraj). The original code can be found [here](https://github.com/google-research/ALBERT).\n\n## Usage tips", "The dominant strategy to improve performance is to increase the model size. But training large models is computationally expensive. One way to reduce computational costs is using a smaller model like [DistilBERT](model_doc/distilbert). DistilBERT uses [knowledge distillation](https://arxiv.org/abs/1503.02531) - a compression technique - to create a smaller version of BERT while keeping nearly all of its language understanding capabilities. \n\nHowever, most Transformer models continued to trend towards more parameters, leading to new models focused on improving training efficiency. [ALBERT](model_doc/albert) reduces memory consumption by lowering the number of parameters in two ways: separating the larger vocabulary embedding into two smaller matrices and allowing layers to share parameters. [DeBERTa](model_doc/deberta) added a disentangled attention mechanism where the word and its position are separately encoded in two vectors. The attention is computed from these separate vectors instead of a single vector containing the word and position embeddings. [Longformer](model_doc/longformer) also focused on making attention more efficient, especially for processing documents with longer sequence lengths. It uses a combination of local windowed attention (attention only calculated from fixed window size around each token) and global attention (only for specific task tokens like `[CLS]` for classification) to create a sparse attention matrix instead of a full attention matrix.\n\n### Decoder[[nlp-decoder]]", "When training large models, there are two aspects that should be considered at the same time: \n\n* Data throughput/training time\n* Model performance\n\nMaximizing the throughput (samples/second) leads to lower training cost. This is generally achieved by utilizing the GPU \nas much as possible and thus filling GPU memory to its limit. If the desired batch size exceeds the limits of the GPU memory, \nthe memory optimization techniques, such as gradient accumulation, can help.\n\nHowever, if the preferred batch size fits into memory, there's no reason to apply memory-optimizing techniques because they can \nslow down the training. Just because one can use a large batch size, does not necessarily mean they should. As part of \nhyperparameter tuning, you should determine which batch size yields the best results and then optimize resources accordingly.\n\nThe methods and tools covered in this guide can be classified based on the effect they have on the training process:", "## Usage tips\n\n- ALBERT is a model with absolute position embeddings so it's usually advised to pad the inputs on the right rather\n  than the left.\n- ALBERT uses repeating layers which results in a small memory footprint, however the computational cost remains\n  similar to a BERT-like architecture with the same number of hidden layers as it has to iterate through the same\n  number of (repeating) layers.\n- Embedding size E is different from hidden size H justified because the embeddings are context independent (one embedding vector represents one token), whereas hidden states are context dependent (one hidden state represents a sequence of tokens) so it's more logical to have H >> E. Also, the embedding matrix is large since it's V x E (V being the vocab size). If E < H, it has less parameters.\n- Layers are split in groups that share parameters (to save memory).\nNext sentence prediction is replaced by a sentence ordering prediction: in the inputs, we have two sentences A and B (that are consecutive) and we either feed A followed by B or B followed by A. The model must predict if they have been swapped or not.\n\n\n\nThis model was contributed by [lysandre](https://huggingface.co/lysandre). This model jax version was contributed by\n[kamalkraj](https://huggingface.co/kamalkraj). The original code can be found [here](https://github.com/google-research/ALBERT).\n\n\n## Resources", "## Efficient training strategies\n\nEven training the smallest LLaMA model requires an enormous amount of memory. Some quick math: in bf16, every parameter uses 2 bytes (in fp32 4 bytes) in addition to 8 bytes used, e.g., in the Adam optimizer (see the [performance docs](https://huggingface.co/docs/transformers/perf_train_gpu_one#optimizer) in Transformers for more info). So a 7B parameter model would use `(2+8)*7B=70GB` just to fit in memory and would likely need more when you compute intermediate values such as attention scores. So you couldn\u2019t train the model even on a single 80GB A100 like that. You can use some tricks, like more efficient optimizers of half-precision training, to squeeze a bit more into memory, but you\u2019ll run out sooner or later.\n\nAnother option is to use Parameter-Efficient Fine-Tuning (PEFT) techniques, such as the [`peft`](https://github.com/huggingface/peft) library, which can perform Low-Rank Adaptation (LoRA) on a model loaded in 8-bit. \n\n![](https://huggingface.co/datasets/trl-internal-testing/example-images/resolve/main/blog/stackllama/lora-animated.gif)   \n*Low-Rank Adaptation of linear layers: extra parameters (in orange) are added next to the frozen layer (in blue), and the resulting encoded hidden states are added together with the hidden states of the frozen layer.*", "A compression factor \\\\(c\\\\) (equal to 3 in the illustration) is chosen to decide the rate at which past activations are compressed. The authors experiment with different compression functions \\\\(f_c\\\\) such as max/mean pooling (parameter-free) and 1D convolution (trainable layer). The compression function is trained with backpropagation through time or local auxiliary compression losses. In addition to the current input of length \\\\(n\\\\), the model attends to \\\\(n_m\\\\) cached activations in the regular memory and \\\\(n_{cm}\\\\) compressed memory activations allowing a long temporal dependency of  \\\\(l \u00d7 (n_m + c n_{cm})\\\\), with \\\\(l\\\\) being the number of attention layers. This increases Transformer-XL\u2019s range by additional \\\\(l \u00d7 c \u00d7 n_{cm}\\\\) tokens and the memory cost amounts to \\\\(O(n^2+ n n_m+ n n_{cm})\\\\). Experiments are conducted on Reinforcement learning, audio generation, and natural language processing. The authors also introduce a new long-range language modeling benchmark called [PG19](https://huggingface.co/datasets/pg19).\n\n#### Main findings", "For comparison, for a 3B-parameter model, like \u201ct5-3b\u201d: \n* A standard AdamW optimizer will need 24GB of GPU memory because it uses 8 bytes for each parameter (8*3 => 24GB)\n* Adafactor optimizer will need more than 12GB. It uses slightly more than 4 bytes for each parameter, so 4*3 and then some extra.\n* 8bit BNB quantized optimizer will use only (2*3) 6GB if all optimizer states are quantized.\n\n### Adafactor\n\nAdafactor doesn't store rolling averages for each element in weight matrices. Instead, it keeps aggregated information \n(sums of rolling averages row- and column-wise), significantly reducing its footprint. However, compared to Adam, \nAdafactor may have slower convergence in certain cases.\n\nYou can switch to Adafactor by setting `optim=\"adafactor\"` in [`TrainingArguments`]:\n\n```py\ntraining_args = TrainingArguments(per_device_train_batch_size=4, optim=\"adafactor\", **default_args)\n```\n\nCombined with other approaches (gradient accumulation, gradient checkpointing, and mixed precision training) \nyou can notice up to 3x improvement while maintaining the throughput! However, as mentioned before, the convergence of \nAdafactor can be worse than Adam. \n\n### 8-bit Adam"], "eval_score_GPT4": "5", "eval_feedback_GPT4": "The response provided correctly identifies the two parameter-reduction techniques proposed in the ALBERT model, which are splitting the embedding matrix into two smaller matrices and using repeating layers split among groups. The explanation aligns well with the reference answer, providing a brief elaboration on how each technique contributes to lowering memory consumption and increasing training speed. The response is factual and accurately reflects the methods used in ALBERT for parameter reduction."}, {"question": "What are the three main steps for fine-tuning a model with the \ud83e\udd17 Datasets library?\n", "true_answer": "1. Load a dataset from the Hugging Face Hub. 2. Preprocess the data with `Dataset.map()`. 3. Load and compute metrics.", "source_doc": "huggingface/course/blob/main/chapters/en/chapter5/1.mdx", "generated_answer": "The three main steps for fine-tuning a model with the \ud83e\udd17 Datasets library are:\n\n1. Load a dataset from the Hugging Face Hub using the `load_dataset()` function.\n2. Preprocess the data with `Dataset.map()` to apply any necessary transformations or collations.\n3. Load and compute metrics using `Dataset.load()` and `Dataset.map()` respectively.\n\nThese steps are covered in", "retrieved_docs": ["Introduction[[introduction]]\n\n<CourseFloatingBanner\n    chapter={5}\n    classNames=\"absolute z-10 right-0 top-0\"\n/>\n\nIn [Chapter 3](/course/chapter3) you got your first taste of the \ud83e\udd17 Datasets library and saw that there were three main steps when it came to fine-tuning a model:\n\n1. Load a dataset from the Hugging Face Hub.\n2. Preprocess the data with `Dataset.map()`.\n3. Load and compute metrics.\n\nBut this is just scratching the surface of what \ud83e\udd17 Datasets can do! In this chapter, we will take a deep dive into the library. Along the way, we'll find answers to the following questions:\n\n* What do you do when your dataset is not on the Hub?\n* How can you slice and dice a dataset? (And what if you _really_ need to use Pandas?)\n* What do you do when your dataset is huge and will melt your laptop's RAM?\n* What the heck are \"memory mapping\" and Apache Arrow?\n* How can you create your own dataset and push it to the Hub?\n\nThe techniques you learn here will prepare you for the advanced tokenization and fine-tuning tasks in [Chapter 6](/course/chapter6) and [Chapter 7](/course/chapter7) -- so grab a coffee and let's get started!", "<frameworkcontent>\n<tf>\n<Tip>\n\nIf you are unfamiliar with fine-tuning a model with Keras, check out the [basic tutorial](./training#train-a-tensorflow-model-with-keras) first!\n\n</Tip>\n\nTo fine-tune a model in TensorFlow, follow these steps:\n1. Define the training hyperparameters, and set up an optimizer and a learning rate schedule.\n2. Instantiate a pretrained model.\n3. Convert a \ud83e\udd17 Dataset to a `tf.data.Dataset`.\n4. Compile your model.\n5. Add callbacks to calculate metrics and upload your model to \ud83e\udd17 Hub\n6. Use the `fit()` method to run the training.\n\nStart by defining the hyperparameters, optimizer and learning rate schedule:\n\n```py\n>>> from transformers import create_optimizer\n\n>>> batch_size = 2\n>>> num_epochs = 50\n>>> num_train_steps = len(train_ds) * num_epochs\n>>> learning_rate = 6e-5\n>>> weight_decay_rate = 0.01\n\n>>> optimizer, lr_schedule = create_optimizer(\n...     init_lr=learning_rate,\n...     num_train_steps=num_train_steps,\n...     weight_decay_rate=weight_decay_rate,\n...     num_warmup_steps=0,\n... )\n```", "</Tip>\n\nNow that we have two data collators, the rest of the fine-tuning steps are standard. Training can take a while on Google Colab if you're not lucky enough to score a mythical P100 GPU \ud83d\ude2d, so we'll first downsample the size of the training set to a few thousand examples. Don't worry, we'll still get a pretty decent language model! A quick way to downsample a dataset in \ud83e\udd17 Datasets is via the `Dataset.train_test_split()` function that we saw in [Chapter 5](/course/chapter5):\n\n```python\ntrain_size = 10_000\ntest_size = int(0.1 * train_size)\n\ndownsampled_dataset = lm_datasets[\"train\"].train_test_split(\n    train_size=train_size, test_size=test_size, seed=42\n)\ndownsampled_dataset\n```\n\n```python out\nDatasetDict({\n    train: Dataset({\n        features: ['attention_mask', 'input_ids', 'labels', 'word_ids'],\n        num_rows: 10000\n    })\n    test: Dataset({\n        features: ['attention_mask', 'input_ids', 'labels', 'word_ids'],\n        num_rows: 1000\n    })\n})\n```", "FrameworkSwitchCourse {fw} />\n\n# Fine-tuning a model with Keras[[fine-tuning-a-model-with-keras]]\n\n<CourseFloatingBanner chapter={3}\n  classNames=\"absolute z-10 right-0 top-0\"\n  notebooks={[\n    {label: \"Google Colab\", value: \"https://colab.research.google.com/github/huggingface/notebooks/blob/master/course/en/chapter3/section3_tf.ipynb\"},\n    {label: \"Aws Studio\", value: \"https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/master/course/en/chapter3/section3_tf.ipynb\"},\n]} />\n\nOnce you've done all the data preprocessing work in the last section, you have just a few steps left to train the model. Note, however, that the `model.fit()` command will run very slowly on a CPU. If you don't have a GPU set up, you can get access to free GPUs or TPUs on [Google Colab](https://colab.research.google.com/).\n\nThe code examples below assume you have already executed the examples in the previous section. Here is a short summary recapping what you need:\n\n```py\nfrom datasets import load_dataset\nfrom transformers import AutoTokenizer, DataCollatorWithPadding\nimport numpy as np", "FrameworkSwitchCourse {fw} />\n\n# Fine-tuning, Check![[fine-tuning-check]]\n\n<CourseFloatingBanner\n    chapter={3}\n    classNames=\"absolute z-10 right-0 top-0\"\n/>\n\nThat was fun! In the first two chapters you learned about models and tokenizers, and now you know how to fine-tune them for your own data. To recap, in this chapter you:\n\n{#if fw === 'pt'}\n* Learned about datasets in the [Hub](https://huggingface.co/datasets)\n* Learned how to load and preprocess datasets, including using dynamic padding and collators\n* Implemented your own fine-tuning and evaluation of a model\n* Implemented a lower-level training loop\n* Used \ud83e\udd17 Accelerate to easily adapt your training loop so it works for multiple GPUs or TPUs\n\n{:else}\n* Learned about datasets in the [Hub](https://huggingface.co/datasets)\n* Learned how to load and preprocess datasets\n* Learned how to fine-tune and evaluate a model with Keras\n* Implemented a custom metric\n\n{/if}", "### Supervised Fine Tuning\n\nThe process as introduced above involves the supervised fine-tuning step using [QLoRA](https://arxiv.org/abs/2305.14314) on the 7B Llama v2 model on the SFT split of the data via TRL\u2019s `SFTTrainer`:\n\n```python\n# load the base model in 4-bit quantization\nbnb_config = BitsAndBytesConfig(\n    load_in_4bit=True,\n    bnb_4bit_quant_type=\"nf4\",\n    bnb_4bit_compute_dtype=torch.bfloat16,\n)\n\nbase_model = AutoModelForCausalLM.from_pretrained(\n    script_args.model_name,        # \"meta-llama/Llama-2-7b-hf\"\n    quantization_config=bnb_config,\n    device_map={\"\": 0},\n    trust_remote_code=True,\n    use_auth_token=True,\n)\nbase_model.config.use_cache = False", "Use the [`~transformers.TFPreTrainedModel.prepare_tf_dataset`] method from \ud83e\udd17 Transformers to prepare the dataset to be compatible with\nTensorFlow, and ready to train/fine-tune a model, as it wraps a HuggingFace [`~datasets.Dataset`] as a `tf.data.Dataset`\nwith collation and batching, so one can pass it directly to Keras methods like `fit()` without further modification. \n\n```py\n>>> import tensorflow as tf\n\n>>> tf_dataset = model.prepare_tf_dataset(\n...     dataset,\n...     batch_size=4,\n...     shuffle=True,\n... )\n```\n</tf>\n</frameworkcontent>\n\n**6**. Start training with your machine learning framework! Check out the \ud83e\udd17 Transformers [text classification guide](https://huggingface.co/docs/transformers/tasks/sequence_classification) for an end-to-end example of how to train a model on a text dataset.\n\n## What's next?\n\nThis completes the \ud83e\udd17 Datasets quickstart! You can load any text, audio, or image dataset with a single function and get it ready for your model to train on.\n\nFor your next steps, take a look at our [How-to guides](./how_to) and learn how to do more specific things like loading different dataset formats, aligning labels, and streaming large datasets. If you're interested in learning more about \ud83e\udd17 Datasets core concepts, grab a cup of coffee and read our [Conceptual Guides](./about_arrow)!"], "eval_score_GPT4": "4", "eval_feedback_GPT4": "The response provided is mostly correct and aligns well with the reference answer. The first step of loading a dataset using the `load_dataset()` function is accurately mentioned. The second step correctly identifies the use of `Dataset.map()` for preprocessing the data. However, the third step is not entirely accurate. The response mentions loading and computing metrics using `Dataset.load()` and `Dataset.map()`, which is not precise. The correct method for computing metrics is not specified, and `Dataset.load()` is not a method used in the \ud83e\udd17 Datasets library. The response should have simply stated \"Load and compute metrics\" without specifying incorrect methods. Therefore, the response is mostly correct but includes a minor inaccuracy in the description of the third step."}, {"question": "What is the maximum improvement in throughput achieved by Hugging Face Infinity compared to vanilla transformers?\n", "true_answer": "+800%", "source_doc": "huggingface/blog/blob/main/infinity-cpu-performance.md", "generated_answer": "According to the benchmark results presented in the blog post, Hugging Face Infinity can achieve up to 800% higher throughput compared to vanilla transformers, specifically for the DistilBERT model running on the Intel Ice Lake Xeon CPU. This improvement is significant and enables businesses to either reduce infrastructure costs for the same workload or enable real-time use cases that were not previously possible.", "retrieved_docs": ["### Latency \n\nBelow, you can find the latency results for an experiment running Hugging Face Infinity on 2 Physical Cores with Batch Size 1. It is remarkable to see how robust and constant Infinity is, with minimal deviation for p95, p99, or p100 (max latency). This result is confirmed for other experiments as well in the benchmark.  \n\n<br>\n<figure class=\"image table text-center m-0 w-full\">\n  <medium-zoom background=\"rgba(0,0,0,.7)\" alt=\"Latency\" src=\"assets/46_infinity_cpu_performance/latency.png\"></medium-zoom>\n  <figcaption>Figure 4. Latency (Batch=1, Physical Cores=2)</figcaption>\n</figure>\n<br>\n\n---\n\n## Conclusion\n\nIn this post, we showed how Hugging Face Infinity performs on the new Intel Ice Lake Xeon CPU. We created a detailed benchmark with over 190 different configurations sharing the results you can expect when using Hugging Face Infinity on CPU, what would be the best configuration to optimize your Infinity Container for latency, and what would be the best configuration to maximize throughput.\n\nHugging Face Infinity can deliver up to 800% higher throughput compared to vanilla transformers, and down to 1-4ms latency for sequence lengths up to 64 tokens.\n\nThe flexibility to optimize transformer models for throughput, latency, or both enables businesses to either reduce the amount of infrastructure cost for the same workload or to enable real-time use cases that were not possible before.", "### Methodologies\n\nWhen it comes to benchmarking BERT-like models, two metrics are most adopted:\n* **Latency**: Time it takes for a single prediction of the model (pre-process, prediction, post-process)\n* **Throughput**: Number of executions performed in a fixed amount of time for one benchmark configuration, respecting Physical CPU cores, Sequence Length, and Batch Size\n\nThese two metrics will be used to benchmark Hugging Face Infinity across different setups to understand the benefits and tradeoffs in this blog post.\n\n---\n\n## Results\n\nTo run the benchmark, we created an infinity container for the [EC2 C6i instance](https://aws.amazon.com/ec2/instance-types/c6i) (Ice-lake) and optimized a [DistilBERT](https://huggingface.co/docs/transformers/model_doc/distilbert) model for sequence classification using Infinity Multiverse. \n\nThis ice-lake optimized Infinity Container can achieve up to 34% better latency & throughput compared to existing cascade-lake-based instances, and up to 800% better latency & throughput compared to vanilla transformers running on ice-lake.\n\nThe Benchmark we created consists of 192 different experiments and configurations. We ran experiments for: \n* Physical CPU cores: 1, 2, 4, 8\n* Sequence length: 8, 16, 32, 64, 128, 256, 384, 512\n* Batch_size: 1, 2, 4, 8, 16, 32", "### Throughput\n\nBelow you can find the throughput comparison for running infinity on 2 physical cores with batch size 1, compared with vanilla transformers.\n\n<br>\n<figure class=\"image table text-center m-0 w-full\">\n  <medium-zoom background=\"rgba(0,0,0,.7)\" alt=\"Throughput\" src=\"assets/46_infinity_cpu_performance/throughput.png\"></medium-zoom>\n  <figcaption>Figure 3. Throughput: Infinity vs Transformers</figcaption>\n</figure>\n<br>\n\n\n| Sequence Length | Infinity    | Transformers | improvement |\n|-----------------|-------------|--------------|-------------|\n| 8               | 248 req/sec | 49 req/sec   | +506%       |\n| 16              | 212 req/sec | 50 req/sec   | +424%       |\n| 32              | 150 req/sec | 40 req/sec   | +375%       |\n| 64              | 97 req/sec  | 28 req/sec   | +346%       |\n| 128             | 55 req/sec  | 18 req/sec   | +305%       |\n| 256             | 27 req/sec  | 9 req/sec    | +300%       |\n| 384             | 17 req/sec  | 5 req/sec    | +340%       |\n| 512             | 12 req/sec  | 4 req/sec    | +300%       |\n\n\n### Latency", "Test it out to achieve __lower latency, higher throughput, and larger maximum batch size__ while training models in \ud83e\udd17 Transformers!\n\n## Performance\n\nThe chart below shows impressive acceleration __from 39% to 130%__ for Hugging Face models with Optimum when __using ONNX Runtime and DeepSpeed ZeRO Stage 1__ for training.\nThe performance measurements were done on selected Hugging Face models with PyTorch as the baseline run, only ONNX Runtime for training as the second run, and ONNX\nRuntime + DeepSpeed ZeRO Stage 1 as the final run, showing maximum gains. The Optimizer used for the baseline PyTorch runs is the AdamW optimizer and the ORT Training\nruns use the Fused Adam Optimizer(available in `ORTTrainingArguments`). The runs were performed on a single Nvidia A100 node with 8 GPUs.\n\n<figure class=\"image table text-center m-0 w-full\">\n  <img src=\"https://huggingface.co/datasets/optimum/documentation-images/resolve/main/onnxruntime/onnxruntime-training-benchmark.png\" alt=\"ONNX Runtime Training Benchmark\"/>\n</figure>\n\nThe version information used for these runs is as follows:\n```\nPyTorch: 1.14.0.dev20221103+cu116; ORT: 1.14.0.dev20221103001+cu116; DeepSpeed: 0.6.6; HuggingFace: 4.24.0.dev0; Optimum: 1.4.1.dev0; Cuda: 11.6.2\n```", "![Filter of all libraries](assets/27_summer_at_huggingface/filters.png)\n\n\n## Solutions\n\n### **Coming soon: Infinity**\n\nTransformers latency down to 1ms? \ud83e\udd2f\ud83e\udd2f\ud83e\udd2f\n\nWe have been working on a really sleek solution to achieve unmatched efficiency for state-of-the-art Transformer models, for companies to deploy in their own infrastructure.\n\n- Infinity comes as a single-container and can be deployed in any production environment.\n- It can achieve 1ms latency for BERT-like models on GPU and 4-10ms on CPU \ud83e\udd2f\ud83e\udd2f\ud83e\udd2f\n- Infinity meets the highest security requirements and can be integrated into your system without the need for internet access. You have control over all incoming and outgoing traffic.\n\n\u26a0\ufe0f  Join us for a [live announcement and demo on Sep 28](https://app.livestorm.co/hugging-face/hugging-face-infinity-launch?type=detailed), where we will be showcasing Infinity for the first time in public!\n\n### **NEW: Hardware Acceleration**\n\nHugging Face is [partnering with leading AI hardware accelerators](http://hf.co/hardware) such as Intel, Qualcomm and GraphCore to make state-of-the-art production performance accessible and extend training capabilities on SOTA hardware. As the first step in this journey, we [introduced a new Open Source library](https://huggingface.co/blog/hardware-partners-program): \ud83e\udd17 Optimum - the ML optimization toolkit for production performance \ud83c\udfce. Learn more in this [blog post](https://huggingface.co/blog/graphcore).", "* Hugging Face [Expert Acceleration Program](https://huggingface.co/support), a commercial service for Hugging Face experts to work directly with your team to accelerate your Machine Learning roadmap and models.\n</div>\n\n\n## Introduction \n\nTransfer learning has changed Machine Learning by reaching new levels of accuracy from Natural Language Processing (NLP) to Audio and Computer Vision tasks. At Hugging Face, we work hard to make these new complex models and large checkpoints as easily accessible and usable as possible. But while researchers and data scientists have converted to the new world of Transformers, few companies have been able to deploy these large, complex models in production at scale.\n\nThe main bottleneck is the latency of predictions which can make large deployments expensive to run and real-time use cases impractical. Solving this is a difficult engineering challenge for any Machine Learning Engineering team and requires the use of advanced techniques to optimize models all the way down to the hardware.\n\nWith [Hugging Face Infinity](https://huggingface.co/infinity), we offer a containerized solution that makes it easy to deploy low-latency, high-throughput, hardware-accelerated inference pipelines for the most popular Transformer models. Companies can get both the accuracy of Transformers and the efficiency necessary for large volume deployments, all in a simple to use package. In this blog post, we want to share detailed performance results for Infinity running on the latest generation of Intel Xeon CPU, to achieve optimal cost, efficiency, and latency for your Transformer deployments.\n\n\n## What is Hugging Face Infinity\n\nHugging Face Infinity is a containerized solution for customers to deploy end-to-end optimized inference pipelines for State-of-the-Art Transformer models, on any infrastructure.", "--\ntitle: Faster TensorFlow models in Hugging Face Transformers\nthumbnail: /blog/assets/10_tf-serving/thumbnail.png\nauthors:\n- user: jplu\n---\n\n# Faster TensorFlow models in Hugging Face Transformers\n\n\n<a target=\"_blank\" href=\"https://colab.research.google.com/github/huggingface/blog/blob/main/notebooks/10_tf_serving.ipynb\">\n    <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\">\n</a>\n\nIn the last few months, the Hugging Face team has been working hard on improving Transformers\u2019 TensorFlow models to make them more robust and faster. The recent improvements are mainly focused on two aspects:\n\n1. Computational performance: BERT, RoBERTa, ELECTRA and MPNet have been improved in order to have a much faster computation time. This gain of computational performance is noticeable for all the computational aspects: graph/eager mode, TF Serving and for CPU/GPU/TPU devices.\n2. TensorFlow Serving: each of these TensorFlow model can be deployed with TensorFlow Serving to benefit of this gain of computational performance for inference.\n\n## Computational Performance"], "eval_score_GPT4": "5", "eval_feedback_GPT4": "Feedback: The response correctly states that Hugging Face Infinity can achieve up to 800% higher throughput compared to vanilla transformers, which is in line with the reference answer. The additional context provided about the specific model and CPU does not detract from the accuracy of the core information given. Therefore, the response meets the criteria for being completely correct, accurate, and factual."}, {"question": "What is the command to upload a spaCy pipeline to the Hugging Face Hub?\n", "true_answer": "python -m spacy huggingface-hub push en_ner_fashion-0.0.0-py3-none-any.whl", "source_doc": "huggingface/blog/blob/main/spacy.md", "generated_answer": "The command to upload a spaCy pipeline to the Hugging Face Hub is `python -m spacy huggingface-hub push [whl_path] [--org] [--msg] [--local-repo] [--verbose]`, where `[whl_path]` is the path to the wheel file of the packaged pipeline, and `[--org]`, `[--msg]`, `[--local-repo]`, and `[", "retrieved_docs": ["--\ntitle: \"Welcome spaCy to the Hugging Face Hub\"\nthumbnail: /blog/assets/23_spacy/thumbnail.png\n\nauthors:\n- user: osanseviero\n- user: ines\n---\n\n# Welcome spaCy to the Hugging Face Hub\n\n\n[spaCy](https://github.com/explosion/spaCy) is a popular library for advanced Natural Language Processing used widely across industry. spaCy makes it easy to use and train pipelines for tasks like named entity recognition, text classification, part of speech tagging and more, and lets you build powerful applications to process and analyze large volumes of text.\n\nHugging Face makes it really easy to share your spaCy pipelines with the community! With a single command, you can upload any pipeline package, with a pretty model card and all required metadata auto-generated for you. The inference API currently supports NER out-of-the-box, and you can try out your pipeline interactively in your browser. You'll also get a live URL for your package that you can `pip install` from anywhere for a smooth path from prototype all the way to production!\n\n### Finding models\n\nOver 60 canonical models can be found in the [spaCy](https://hf.co/spacy) org. These models are from the [latest 3.1 release](https://explosion.ai/blog/spacy-v3-1), so you can try the latest realesed models right now! On top of this, you can find all spaCy models from the community here https://huggingface.co/models?filter=spacy.\n\n\n### Widgets", "You can then upload any pipeline packaged with [`spacy package`](https://spacy.io/api/cli#package). Make sure to set `--build wheel` to output a binary .whl file. The uploader will read all metadata from the pipeline package, including the auto-generated pretty `README.md` and the model details available in the `meta.json`.\n\n```bash\nhuggingface-cli login\npython -m spacy package ./en_ner_fashion ./output --build wheel\ncd ./output/en_ner_fashion-0.0.0/dist\npython -m spacy huggingface-hub push en_ner_fashion-0.0.0-py3-none-any.whl\n```\n\nIn just a minute, you can get your packaged model in the Hub, try it out directly in the browser, and share it with the rest of the community. All the required metadata will be uploaded for you and you even get a cool model card.\n\nThe command will output two things:\n\n* Where to find your repo in the Hub! For example, https://huggingface.co/spacy/en_core_web_sm\n* And how to install the pipeline directly from the Hub!\n\n\n### From a Python script\n\nYou can use the `push` function from Python. It returns a dictionary containing the `\"url\"` and \"`whl_url`\" of the published model and the wheel file, which you can later install with `pip install`.\n\n```py\nfrom spacy_huggingface_hub import push", "All spaCy canonical pipelines can now be found in the official spaCy [organization](https://huggingface.co/spacy), and any user can share their pipelines with a single command `python -m spacy huggingface-hub`. To read more about it, head to [https://huggingface.co/blog/spacy](https://huggingface.co/blog/spacy). You can try all canonical spaCy models directly in the Hub in the demo [Space](https://huggingface.co/spaces/spacy/pipeline-visualizer)!\n\n![spaCy NER example](assets/27_summer_at_huggingface/spacy_ner.jpeg)\n\nAnother exciting integration is Sentence Transformers. You can read more about it in the [blog announcement](https://huggingface.co/blog/sentence-transformers-in-the-hub): you can find over 200 [models](https://huggingface.co/models?library=sentence-transformers) in the Hub, easily share your models with the rest of the community and reuse models from the community.\n\nBut that's not all! You can now find over 100 Adapter Transformers in the Hub and try out Speechbrain models with widgets directly in the browser for different tasks such as audio classification. If you're interested in our collaborations to integrate new ML libraries to the Hub, you can read more about them [here](https://huggingface.co/docs/hub/libraries).\n\n![Filter of all libraries](assets/27_summer_at_huggingface/filters.png)\n\n\n## Solutions", "## Sharing your models\n\n### Using the spaCy CLI (recommended)\n\nThe `spacy-huggingface-hub` library extends `spaCy` native CLI so people can easily push their packaged models to the Hub.\n\nYou can install spacy-huggingface-hub from pip:\n\n```bash\npip install spacy-huggingface-hub\n```\n\nYou can then check if the command has been registered successfully\n\n```bash\npython -m spacy huggingface-hub --help\n```\n\nTo push with the CLI, you can use the `huggingface-hub push` command as seen below.\n\n```bash\npython -m spacy huggingface-hub push [whl_path] [--org] [--msg] [--local-repo] [--verbose]\n```", "def package_to_hub(\n    repo_id,\n    model,\n    hyperparameters,\n    eval_env,\n    video_fps=30,\n    commit_message=\"Push agent to the Hub\",\n    token=None,\n    logs=None,\n):\n    \"\"\"\n    Evaluate, Generate a video and Upload a model to Hugging Face Hub.\n    This method does the complete pipeline:\n    - It evaluates the model\n    - It generates the model card\n    - It generates a replay video of the agent\n    - It pushes everything to the hub\n    :param repo_id: id of the model repository from the Hugging Face Hub\n    :param model: trained model\n    :param eval_env: environment used to evaluate the agent\n    :param fps: number of fps for rendering the video\n    :param commit_message: commit message\n    :param logs: directory on local machine of tensorboard logs you'd like to upload\n    \"\"\"\n    msg.info(\n        \"This function will save, evaluate, generate a video of your agent, \"\n        \"create a model card and push everything to the hub. \"\n        \"It might take up to 1min. \\n \"\n        \"This is a work in progress: if you encounter a bug, please open an issue.\"\n    )\n    # Step 1: Clone or create the repo\n    repo_url = HfApi().create_repo(\n        repo_id=repo_id,\n        token=token,\n        private=False,\n        exist_ok=True,\n    )\n\n    with tempfile.TemporaryDirectory() as tmpdirname:\n        tmpdirname = Path(tmpdirname)\n\n        # Step 2: Save the model\n        torch.save(model.state_dict(), tmpdirname / \"model.pt\")", "```python\ndef push_to_hub(repo_id, model, env, video_fps=1, local_repo_path=\"hub\"):\n    \"\"\"\n    Evaluate, Generate a video and Upload a model to Hugging Face Hub.\n    This method does the complete pipeline:\n    - It evaluates the model\n    - It generates the model card\n    - It generates a replay video of the agent\n    - It pushes everything to the Hub\n\n    :param repo_id: repo_id: id of the model repository from the Hugging Face Hub\n    :param env\n    :param video_fps: how many frame per seconds to record our video replay\n    (with taxi-v3 and frozenlake-v1 we use 1)\n    :param local_repo_path: where the local repository is\n    \"\"\"\n    _, repo_name = repo_id.split(\"/\")\n\n    eval_env = env\n    api = HfApi()\n\n    # Step 1: Create the repo\n    repo_url = api.create_repo(\n        repo_id=repo_id,\n        exist_ok=True,\n    )\n\n    # Step 2: Download files\n    repo_local_path = Path(snapshot_download(repo_id=repo_id))\n\n    # Step 3: Save the model\n    if env.spec.kwargs.get(\"map_name\"):\n        model[\"map_name\"] = env.spec.kwargs.get(\"map_name\")\n        if env.spec.kwargs.get(\"is_slippery\", \"\") == False:\n            model[\"slippery\"] = False", "```python\ndef package_to_hub(\n    repo_id,\n    model,\n    hyperparameters,\n    eval_env,\n    video_fps=30,\n    commit_message=\"Push agent to the Hub\",\n    token=None,\n    logs=None,\n):\n    \"\"\"\n    Evaluate, Generate a video and Upload a model to Hugging Face Hub.\n    This method does the complete pipeline:\n    - It evaluates the model\n    - It generates the model card\n    - It generates a replay video of the agent\n    - It pushes everything to the hub\n    :param repo_id: id of the model repository from the Hugging Face Hub\n    :param model: trained model\n    :param eval_env: environment used to evaluate the agent\n    :param fps: number of fps for rendering the video\n    :param commit_message: commit message\n    :param logs: directory on local machine of tensorboard logs you'd like to upload\n    \"\"\"\n    msg.info(\n        \"This function will save, evaluate, generate a video of your agent, \"\n        \"create a model card and push everything to the hub. \"\n        \"It might take up to 1min. \\n \"\n        \"This is a work in progress: if you encounter a bug, please open an issue.\"\n    )\n    # Step 1: Clone or create the repo\n    repo_url = HfApi().create_repo(\n        repo_id=repo_id,\n        token=token,\n        private=False,\n        exist_ok=True,\n    )\n\n    with tempfile.TemporaryDirectory() as tmpdirname:\n        tmpdirname = Path(tmpdirname)\n\n        # Step 2: Save the model\n        torch.save(model.state_dict(), tmpdirname / \"model.pt\")"], "eval_score_GPT4": "4", "eval_feedback_GPT4": "The response provides the correct command structure for uploading a spaCy pipeline to the Hugging Face Hub, including the necessary command prefix `python -m spacy huggingface-hub push` and the placeholders for additional options and arguments. However, the response is incomplete as it ends abruptly with `[` and does not provide a specific example of a wheel file path, which is present in the reference answer. The reference answer gives a concrete example of a wheel file (`en_ner_fashion-0.0.0-py3-none-any.whl`), which is missing from the evaluated response. Therefore, the response is mostly correct but lacks completeness and a specific example that would make it fully accurate and factual."}, {"question": "What is the time and memory complexity of the Nystr\u00f6mformer's approximation of self-attention?\n", "true_answer": "O(n)", "source_doc": "huggingface/blog/blob/main/nystromformer.md", "generated_answer": "The time and memory complexity of the Nystr\u00f6mformer's approximation of self-attention is linear in the input sequence length, which is a significant improvement over the quadratic complexity of standard self-attention. This is achieved by adapting the Nystr\u00f6m method to approximate self-attention by sampling landmarks from queries and keys, and constructing three matrices corresponding to those in the Nystr\u00f6m approximation of the softmax matrix. The sizes of these matrices are", "retrieved_docs": ["--\ntitle: \"Nystr\u00f6mformer: Approximating self-attention in linear time and memory via the Nystr\u00f6m method\"\nthumbnail: /blog/assets/86_nystromformer/thumbnail.png\nauthors:\n- user: asi\n  guest: true\n---\n\n# Nystr\u00f6mformer: Approximating self-attention in linear time and memory via the Nystr\u00f6m method\n\n\n<script async defer src=\"https://unpkg.com/medium-zoom-element@0/dist/medium-zoom-element.min.js\"></script>\n\n## Introduction\n\nTransformers have exhibited remarkable performance on various Natural Language Processing and Computer Vision tasks. Their success can be attributed to the self-attention mechanism, which captures the pairwise interactions between all the tokens in an input. However, the standard self-attention mechanism has a time and memory complexity of \\\\(O(n^2)\\\\) (where \\\\(n\\\\) is the length of the input sequence), making it expensive to train on long input sequences. \n\nThe [Nystr\u00f6mformer](https://arxiv.org/abs/2102.03902) is one of many efficient Transformer models that approximates standard self-attention with \\\\(O(n)\\\\) complexity. Nystr\u00f6mformer exhibits competitive performance on various downstream NLP and CV tasks while improving upon the efficiency of standard self-attention. The aim of this blog post is to give readers an overview of the Nystr\u00f6m method and how it can be adapted to approximate self-attention.\n\n\n## Nystr\u00f6m method for matrix approximation", "</div>\n\n## Conclusion\n\nNystr\u00f6mformer offers an efficient approximation to the standard self-attention mechanism, while outperforming other linear self-attention schemes. In this blog post, we went over a high-level overview of the Nystr\u00f6m method and how it can be leveraged for self-attention. Readers interested in deploying or fine-tuning Nystr\u00f6mformer for downstream tasks can find the HuggingFace documentation [here](https://huggingface.co/docs/transformers/model_doc/nystromformer).", "As shown in the second line, \\\\(\\hat{P}\\\\) can be expressed as a product of three matrices. The reason for doing so will become clear later.\n\n\n## Can we approximate self-attention with the Nystr\u00f6m method?\n\nOur goal is to ultimately approximate the softmax matrix in standard self attention: S = softmax \\\\( \\frac{QK^T}{\\sqrt{d}} \\\\)\n\nHere, \\\\(Q\\\\) and \\\\(K\\\\) denote the queries and keys respectively. Following the procedure discussed above, we would sample \\\\(m\\\\) rows and columns from \\\\(S\\\\), form four submatrices, and obtain \\\\(\\hat{S}\\\\):\n\n<figure class=\"image table text-center m-0 w-full\">\n  <medium-zoom background=\"rgba(0,0,0,.7)\" alt=\"Nystr\u00f6m approximation of S\" src=\"assets/86_nystromformer/s_hat.png\"></medium-zoom>\n  <figcaption>Nystr\u00f6m approximation of S</figcaption>\n</figure>\n\nBut, what does it mean to sample a column from \\\\(S\\\\)? It means we select one element from each row. Recall how S is calculated: the final operation is a row-wise softmax. To find a single entry in a row, we must access all other entries (for the denominator in softmax). So, sampling one column requires us to know all other columns in the matrix. Therefore, we cannot directly apply the Nystr\u00f6m method to approximate the softmax matrix.", "The abstract from the paper is the following:\n\n*Transformers have emerged as a powerful tool for a broad range of natural language processing tasks. A key component\nthat drives the impressive performance of Transformers is the self-attention mechanism that encodes the influence or\ndependence of other tokens on each specific token. While beneficial, the quadratic complexity of self-attention on the\ninput sequence length has limited its application to longer sequences -- a topic being actively studied in the\ncommunity. To address this limitation, we propose Nystr\u00f6mformer -- a model that exhibits favorable scalability as a\nfunction of sequence length. Our idea is based on adapting the Nystr\u00f6m method to approximate standard self-attention\nwith O(n) complexity. The scalability of Nystr\u00f6mformer enables application to longer sequences with thousands of\ntokens. We perform evaluations on multiple downstream tasks on the GLUE benchmark and IMDB reviews with standard\nsequence length, and find that our Nystr\u00f6mformer performs comparably, or in a few cases, even slightly better, than\nstandard self-attention. On longer sequence tasks in the Long Range Arena (LRA) benchmark, Nystr\u00f6mformer performs\nfavorably relative to other efficient self-attention methods. Our code is available at this https URL.*\n\nThis model was contributed by [novice03](https://huggingface.co/novice03). The original code can be found [here](https://github.com/mlpen/Nystromformer).\n\n## Resources", "## How can we adapt the Nystr\u00f6m method to approximate self-attention?\n\nInstead of sampling from \\\\(S\\\\), the authors propose to sample landmarks (or Nystr\u00f6m points) from queries and keys. We denote the query landmarks and key landmarks as \\\\(\\tilde{Q}\\\\) and \\\\(\\tilde{K}\\\\) respectively. \\\\(\\tilde{Q}\\\\) and \\\\(\\tilde{K}\\\\) can be used to construct three matrices corresponding to those in the Nystr\u00f6m approximation of \\\\(S\\\\). We define the following matrices:\n\n$$\\tilde{F} = softmax(\\frac{Q\\tilde{K}^T}{\\sqrt{d}}) \\hspace{40pt} \\tilde{A} = softmax(\\frac{\\tilde{Q}\\tilde{K}^T}{\\sqrt{d}})^+ \\hspace{40pt} \\tilde{B} = softmax(\\frac{\\tilde{Q}K^T}{\\sqrt{d}})$$\n\nThe sizes of \\\\(\\tilde{F}\\\\), \\\\(\\tilde{A}\\\\), and \\\\(\\tilde{B}) are \\\\(n \\times m, m \\times m,\\\\) and \\\\(m \\times n\\\\) respectively. \nWe replace the three matrices in the Nystr\u00f6m approximation of \\\\(S\\\\) with the new matrices we have defined to obtain an alternative Nystr\u00f6m approximation:", "$$\\begin{aligned}\\hat{S} &= \\tilde{F} \\tilde{A} \\tilde{B} \\\\ &= softmax(\\frac{Q\\tilde{K}^T}{\\sqrt{d}}) softmax(\\frac{\\tilde{Q}\\tilde{K}^T}{\\sqrt{d}})^+  softmax(\\frac{\\tilde{Q}K^T}{\\sqrt{d}}) \\end{aligned}$$\n\nThis is the Nystr\u00f6m approximation of the softmax matrix in the self-attention mechanism. We multiply this matrix with the values ( \\\\(V\\\\)) to obtain a linear approximation of self-attention. Note that we never calculated the product \\\\(QK^T\\\\), avoiding the \\\\(O(n^2)\\\\) complexity. \n\n\n## How do we select landmarks?", "#### Follow-up questions\n\n* Even if the approximation of the attention mechanism is tight, small errors propagate through the transformer layers. This raises the question of the convergence and stability of fine-tuning a pre-trained network with FAVOR+ as an approximation of self-attention.\n* The FAVOR+ algorithm is the combination of multiple components. It is not clear which of these components have the most empirical impact on the performance, especially in view of the variety of modalities considered in this work.\n\n## Reading group discussion\n\nThe developments in pre-trained transformer-based language models for natural language understanding and generation are impressive. Making these systems efficient for production purposes has become a very active research area. This emphasizes that we still have much to learn and build both on the methodological and practical sides to enable efficient and general deep learning based systems, in particular for applications that require modeling long-range inputs.\n\nThe four papers above offer different ways to deal with the quadratic memory complexity of the self-attention mechanism, usually by reducing it to linear complexity. Linformer and Longformer both rely on the observation that the self-attention matrix does not contain \\\\(n \u00d7 n\\\\) worth of information (the attention matrix is low-rank and sparse). Performer gives a principled method to approximate the softmax-attention kernel (and any kernelizable attention mechanisms beyond softmax). Compressive Transformer offers an orthogonal approach to model long range dependencies based on recurrence."], "eval_score_GPT4": "4", "eval_feedback_GPT4": "The response correctly identifies the time and memory complexity of the Nystr\u00f6mformer as linear in the input sequence length, which aligns with the reference answer that states the complexity as O(n). The explanation provided about the Nystr\u00f6m method and the construction of matrices is relevant and adds to the accuracy of the response. However, the response does not explicitly state the complexity using the Big O notation as the reference answer does. Despite this, the response is still accurate in describing the nature of the complexity."}, {"question": "What is the goal of the Named Entity Recognition task in token classification?\n", "true_answer": "The goal of the Named Entity Recognition task is to find the entities in a piece of text, such as person, location, or organization.", "source_doc": "huggingface/course/blob/main/subtitles/en/raw/tasks.md", "generated_answer": "The goal of the Named Entity Recognition task in token classification is to find the entities (such as persons, locations, or organizations) in a sentence and label each token with one class for each entity and another class for tokens that have no entity. This task can be formulated as attributing a label to each token in a sentence.", "retrieved_docs": ["Token Classification video\n\nWelcome to the Hugging Face tasks series! In this video we\u2019ll take a look at the token classification task.\nToken classification is the task of assigning a label to each token in a sentence. There are various token classification tasks and the most common are Named Entity Recognition and Part-of-Speech Tagging.\nLet\u2019s take a quick look at the Named Entity Recognition task. The goal of this task is to find the entities in a piece of text, such as person, location, or organization. This task is formulated as labelling each token with one class for each entity, and another class for tokens that have no entity.\nAnother token classification task is part-of-speech tagging. The goal of this task is to label the words for a particular part of a speech, such as noun, pronoun, adjective, verb and so on. This task is formulated as labelling each token with parts of speech.\nToken classification models are evaluated on Accuracy, Recall, Precision and F1-Score. The metrics are calculated for each of the classes. We calculate true positive, true negative and false positives to calculate precision and recall, and take their harmonic mean to get F1-Score. Then we calculate it for every class and take the overall average to evaluate our model.\nAn example dataset used for this task is ConLL2003. Here, each token belongs to a certain named entity class, denoted as the indices of the list containing the labels.\nYou can extract important information from invoices using named entity recognition models, such as date, organization name or address.\nFor more information about the Token classification task, check out the Hugging Face course.\n\n\nQuestion Answering video", "### Evaluation\n\nTo evaluate the performance of sahajBERT, we finetuned it on two downstream tasks in Bengali:\n\n-   Named entity recognition (NER) on the Bengali split of [WikiANN](https://aclanthology.org/P17-1178/). The goal of this task is to classify each token in the input text into one of the following categories: person, organization, location, or none of them.\n-   News Category Classification (NCC) on the Soham articles dataset from [IndicGLUE](https://aclanthology.org/2020.findings-emnlp.445/). The goal of this task is to predict the category to which belong the input text.\n\nWe evaluated it during training on the NER task to check that everything was going well; as you can see on the following plot, this was indeed the case!\n\n<iframe width=\"100%\" height=\"476\" frameborder=\"0\"\n  src=\"https://observablehq.com/embed/@huggingface/bengali-exp-eval?cells=evalPlot\"></iframe>\n<div style=\"line-height:105%;font-size:80%\">\n<p align=\"center\">\nEvaluation metrics of fine-tuned models on the NER task from different checkpoints of pre-trained models.\n</p>\n</div>", "{/if}\n\nThe first application we'll explore is token classification. This generic task encompasses any problem that can be formulated as \"attributing a label to each token in a sentence,\" such as:\n\n- **Named entity recognition (NER)**: Find the entities (such as persons, locations, or organizations) in a sentence. This can be formulated as attributing a label to each token by having one class per entity and one class for \"no entity.\"\n- **Part-of-speech tagging (POS)**: Mark each word in a sentence as corresponding to a particular part of speech (such as noun, verb, adjective, etc.).\n- **Chunking**: Find the tokens that belong to the same entity. This task (which can be combined with POS or NER) can be formulated as attributing one label (usually `B-`) to any tokens that are at the beginning of a chunk, another label (usually `I-`) to tokens that are inside a chunk, and a third label (usually `O`) to tokens that don't belong to any chunk.\n\n<Youtube id=\"wVHdVlPScxA\"/>\n\nOf course, there are many other types of token classification problem; those are just a few representative examples. In this section, we will fine-tune a model (BERT) on a NER task, which will then be able to compute predictions like this one:", "Named-Entity Recognition\n\nRelated spaces: https://huggingface.co/spaces/rajistics/biobert_ner_demo, https://huggingface.co/spaces/abidlabs/ner, https://huggingface.co/spaces/rajistics/Financial_Analyst_AI\nTags: NER, TEXT, HIGHLIGHT\n\n## Introduction\n\nNamed-entity recognition (NER), also known as token classification or text tagging, is the task of taking a sentence and classifying every word (or \"token\") into different categories, such as names of people or names of locations, or different parts of speech.\n\nFor example, given the sentence:\n\n> Does Chicago have any Pakistani restaurants?\n\nA named-entity recognition algorithm may identify:\n\n- \"Chicago\" as a **location**\n- \"Pakistani\" as an **ethnicity**\n\nand so on.\n\nUsing `gradio` (specifically the `HighlightedText` component), you can easily build a web demo of your NER model and share that with the rest of your team.\n\nHere is an example of a demo that you'll be able to build:\n\n$demo_ner_pipeline\n\nThis tutorial will show how to take a pretrained NER model and deploy it with a Gradio interface. We will show two different ways to use the `HighlightedText` component -- depending on your NER model, either of these two ways may be easier to learn!\n\n### Prerequisites", "As an example, if you are working with a collection of digitized Norwegian documents with minimal metadata. One way to better understand what's in the collection is to use a Named Entity Recognition (NER) model. This model extracts entities from a text, for example, identifying the locations mentioned in a text. Knowing which entities are contained in a text can be a valuable way of better understanding what a document is about.\n\nWe can find NER models on the Hub by filtering models by task. In this case, we choose `token-classification`, which is the task which includes named entity recognition models. [This filter](https://huggingface.co/datasets?task_categories=task_categories:token-classification) returns models labelled as doing `token-classification`. Since we are working with Norwegian documents, we may also want to [filter by language](https://huggingface.co/models?pipeline_tag=token-classification&language=no&sort=downloads); this gets us to a smaller set of models we want to explore. Many of these models will also contain a [model widget](https://huggingface.co/saattrupdan/nbailab-base-ner-scandi), allowing us to test the model. \n\n\n![](https://i.imgur.com/9V9xni5.png)\n\nA model widget can quickly show how well a model will likely perform on our data. Once you've found a model that interests you, the Hub provides different ways of using that tool. If you are already familiar with the Transformers library, you can click the use in Transformers button to get a pop-up which shows how to load the model in Transformers.", "### Token classification\n\nTo use BERT for token classification tasks like named entity recognition (NER), add a token classification head on top of the base BERT model. The token classification head is a linear layer that accepts the final hidden states and performs a linear transformation to convert them into logits. The cross-entropy loss is calculated between the logits and each token to find the most likely label.\n\nReady to try your hand at token classification? Check out our complete [token classification guide](tasks/token_classification) to learn how to finetune DistilBERT and use it for inference!\n\n### Question answering\n\nTo use BERT for question answering, add a span classification head on top of the base BERT model. This linear layer accepts the final hidden states and performs a linear transformation to compute the `span` start and end logits corresponding to the answer. The cross-entropy loss is calculated between the logits and the label position to find the most likely span of text corresponding to the answer.\n\nReady to try your hand at question answering? Check out our complete [question answering guide](tasks/question_answering) to learn how to finetune DistilBERT and use it for inference!\n\n<Tip>\n\n\ud83d\udca1 Notice how easy it is to use BERT for different tasks once it's been pretrained. You only need to add a specific head to the pretrained model to manipulate the hidden states into your desired output!\n\n</Tip>\n\n### Text generation", "et's study how to preprocess a dataset for token classification! Token classification regroups any task that can be framed as labelling each word (or token) in a sentence, like identifying the persons, organizations and locations for instance. For our example, we will use the Conll dataset, in which we remove columns we won't use and rename the other ones to get to a dataset with just two columns: words and labels. If you have your own dataset for token classification, just make sure you clean your data to get to the same point, with one column containing words (as list of strings) and another containing labels (as integers spanning from to to your number of labels -1).() Make sure you have your label names stored somewhere - here we get them from the dataset features - so you are able to map the integers to some real labels when inspecting your data! Here we are doing named entity recognitions, so ours labels are either O for words that do not belong to any entity, LOC, for location, PER, for person, ORG for organization and MISC for miscellaneous. Each label has two versions: the B- labels indicate a word that begins an entity while the I- labels indicate a word that is inside an entity. The first step to preprocess our data is to tokenize the words. This is very easily done with a tokenizer, we just have to tell it we have pre-tokenized the data with the flag is_split_into_words. Then comes the hard part. Since we have added special tokens and each word may have been split into several tokens, our labels won't match the tokens anymore"], "eval_score_GPT4": "5", "eval_feedback_GPT4": "Feedback: The response provided is completely correct, accurate, and factual. It aligns well with the reference answer by stating the goal of Named Entity Recognition (NER) is to identify entities in text and classify tokens accordingly. The response goes further to explain that each token is labeled with a class for entities and a separate class for non-entity tokens, which adds detail without deviating from the accuracy of the reference answer."}, {"question": "What is the resolution of images used by the CLIPSeg model?\n", "true_answer": "352 x 352 pixels", "source_doc": "huggingface/blog/blob/main/clipseg-zero-shot.md", "generated_answer": "The resolution of images used by the CLIPSeg model is 352 x 352 pixels, as stated in the context provided.", "retrieved_docs": ["Currently, CLIPSeg still has its limitations. For example, the model uses images of 352 x 352 pixels, so the output is quite low-resolution. This means we cannot expect pixel-perfect results when we work with images from modern cameras. If we want more precise segmentations, we can fine-tune a state-of-the-art segmentation model, as shown in [our previous blog post](https://huggingface.co/blog/fine-tune-segformer). In that case, we can still use CLIPSeg to generate some rough labels, and then refine them in a labeling tool such as [Segments.ai](https://segments.ai/?utm_source=hf&utm_medium=blog&utm_campaign=clipseg). Before we describe how to do that, let\u2019s first take a look at how CLIPSeg works.\n\n## CLIP: the magic model behind CLIPSeg", "The reason why CLIP works so well is that the model was trained on a huge dataset of images with text captions. The dataset contained a whopping 400 million image-text pairs taken from the internet. These images contain a wide variety of objects and concepts, and CLIP is great at creating a representation for each of them.\n\n## CLIPSeg: image segmentation with CLIP\n\n[CLIPSeg](https://arxiv.org/abs/2112.10003) is a model that uses CLIP representations to create image segmentation masks. It was published by Timo L\u00fcddecke and Alexander Ecker. They achieved zero-shot image segmentation by training a Transformer-based decoder on top of the CLIP model, which is kept frozen. The decoder takes in the CLIP representation of an image, and the CLIP representation of the thing you want to segment. Using these two inputs, the CLIPSeg decoder creates a binary segmentation mask. To be more precise, the decoder doesn\u2019t only use the final CLIP representation of the image we want to segment, but it also uses the outputs of some of the layers of CLIP. \n\n<figure class=\"image table text-center m-0 w-full\">\n  <medium-zoom background=\"rgba(0,0,0,.7)\" alt=\"Overview of the CLIPSeg model\" src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/123_clipseg-zero-shot/clipseg-overview.png\"></medium-zoom>\n  <figcaption><a href=\"https://arxiv.org/abs/2112.10003\">Source</a></figcaption>\n</figure>", "<figure class=\"image table text-center m-0 w-full\">\n  <medium-zoom background=\"rgba(0,0,0,.7)\" alt=\"Overview of the CLIPSeg model\" src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/123_clipseg-zero-shot/clip-tv-example.png\"></medium-zoom>\n  <figcaption>Example of image classification using CLIP (<a href=\"https://openai.com/blog/clip/\">source</a>).</figcaption>\n</figure>\n\nWhat\u2019s more, CLIP is not only useful for classification, but it can also be used for [image search](https://huggingface.co/spaces/DrishtiSharma/Text-to-Image-search-using-CLIP) (can you see how this is similar to classification?), [text-to-image models](https://huggingface.co/spaces/kamiyamai/stable-diffusion-webui) ([DALL-E 2](https://openai.com/dall-e-2/) is powered by CLIP), [object detection](https://segments.ai/zeroshot?utm_source=hf&utm_medium=blog&utm_campaign=clipseg) ([OWL-ViT](https://arxiv.org/abs/2205.06230)), and most importantly for us: image segmentation. Now you see why CLIP was truly a breakthrough in machine learning.", "## CLIP: the magic model behind CLIPSeg\n\n[CLIP](https://huggingface.co/docs/transformers/main/en/model_doc/clip), which stands for **C**ontrastive **L**anguage\u2013**I**mage **P**re-training, is a model developed by OpenAI in 2021. You can give CLIP an image or a piece of text, and CLIP will output an abstract *representation* of your input. This abstract representation, also called an *embedding*, is really just a vector (a list of numbers). You can think of this vector as a point in high-dimensional space. CLIP is trained so that the representations of similar pictures and texts are similar as well. This means that if we input an image and a text description that fits that image, the representations of the image and the text will be similar (i.e., the high-dimensional points will be close together).\n\nAt first, this might not seem very useful, but it is actually very powerful. As an example, let\u2019s take a quick look at how CLIP can be used to classify images without ever having been trained on that task. To classify an image, we input the image and the different categories we want to choose from to CLIP (e.g. we input an image and the words \u201capple\u201d, \u201corange\u201d, \u2026).  CLIP then gives us back an embedding of the image and of each category. Now, we simply have to check which category embedding is closest to the embedding of the image, et voil\u00e0! Feels like magic, doesn\u2019t it?", "## Using CLIPSeg to pre-label images on Segments.ai\n\nAs you can see, the results from CLIPSeg are a little fuzzy and very\nlow-res. If we want to obtain better results, you can fine-tune a\nstate-of-the-art segmentation model, as explained in [our previous\nblogpost](https://huggingface.co/blog/fine-tune-segformer). To finetune\nthe model, we\\'ll need labeled data. In this section, we\\'ll show you\nhow you can use CLIPSeg to create some rough segmentation masks and then\nrefine them on\n[Segments.ai](https://segments.ai/?utm_source=hf&utm_medium=blog&utm_campaign=clipseg),\na labeling platform with smart labeling tools for image segmentation.\n\nFirst, create an account at\n[https://segments.ai/join](https://segments.ai/join?utm_source=hf&utm_medium=blog&utm_campaign=clipseg)\nand install the Segments Python SDK. Then you can initialize the\nSegments.ai Python client using an API key. This key can be found on\n[the account page](https://segments.ai/account?utm_source=hf&utm_medium=blog&utm_campaign=clipseg).\n\n```python\n!pip install -q segments-ai\n```\n\n```python\nfrom segments import SegmentsClient\nfrom getpass import getpass\n\napi_key = getpass('Enter your API key: ')\nsegments_client = SegmentsClient(api_key)\n```", "This model was contributed by [nielsr](https://huggingface.co/nielsr).\nThe original code can be found [here](https://github.com/timojl/clipseg).\n\n## Usage tips\n\n- [`CLIPSegForImageSegmentation`] adds a decoder on top of [`CLIPSegModel`]. The latter is identical to [`CLIPModel`].\n- [`CLIPSegForImageSegmentation`] can generate image segmentations based on arbitrary prompts at test time. A prompt can be either a text\n(provided to the model as `input_ids`) or an image (provided to the model as `conditional_pixel_values`). One can also provide custom\nconditional embeddings (provided to the model as `conditional_embeddings`).\n\n## Resources\n\nA list of official Hugging Face and community (indicated by \ud83c\udf0e) resources to help you get started with CLIPSeg. If you're interested in submitting a resource to be included here, please feel free to open a Pull Request and we'll review it! The resource should ideally demonstrate something new instead of duplicating an existing resource.\n\n<PipelineTag pipeline=\"image-segmentation\"/>\n\n- A notebook that illustrates [zero-shot image segmentation with CLIPSeg](https://github.com/NielsRogge/Transformers-Tutorials/blob/master/CLIPSeg/Zero_shot_image_segmentation_with_CLIPSeg.ipynb).\n\n## CLIPSegConfig", "The decoder is trained on the [PhraseCut dataset](https://arxiv.org/abs/2008.01187), which contains over 340,000 phrases with corresponding image segmentation masks. The authors also experimented with various augmentations to expand the size of the dataset. The goal here is not only to be able to segment the categories that are present in the dataset, but also to segment unseen categories. Experiments indeed show that the decoder can generalize to unseen categories. \n\nOne interesting feature of CLIPSeg is that both the query (the image we want to segment) and the prompt (the thing we want to segment in the image) are input as CLIP embeddings. The CLIP embedding for the prompt can either come from a piece of text (the category name), **or from another image**. This means you can segment oranges in a photo by giving CLIPSeg an example image of an orange.\n\nThis technique, which is called \"visual prompting\", is really helpful when the thing you want to segment is hard to describe. For example, if you want to segment a logo in a picture of a t-shirt, it's not easy to describe the shape of the logo, but CLIPSeg allows you to simply use the image of the logo as the prompt.\n\nThe CLIPSeg paper contains some tips on improving the effectiveness of visual prompting. They find that cropping the query image (so that it only contains the object you want to segment) helps a lot. Blurring and darkening the background of the query image also helps a little bit. In the next section, we'll show how you can try out visual prompting yourself using [`\ud83e\udd17 transformers`](https://huggingface.co/transformers).\n\n## Using CLIPSeg with Hugging Face Transformers"], "eval_score_GPT4": "5", "eval_feedback_GPT4": "Feedback: The response provided is completely correct, accurate, and factual. It matches the reference answer exactly, stating the resolution of images used by the CLIPSeg model as 352 x 352 pixels."}, {"question": "What can you use Gradio for?\n", "true_answer": "Create a demo for your machine learning model, share your machine learning model with others, and debug your model.", "source_doc": "huggingface/course/blob/main/chapters/en/chapter9/9.mdx", "generated_answer": "Gradio is a Python library that allows you to easily create interactive web applications for machine learning models, APIs, and any other Python functions. It can be used to transcribe audio files, generate images, summarize text, create chatbots, calculate taxes, and more. Gradio works with any hosted Gradio app, including those on Hugging Face Spaces, Google Colab notebooks, and Jupyter notebooks. It is model agnostic and supports a variety", "retrieved_docs": ["`gradio_client`: Use a Gradio app as an API -- in 3 lines of Python\n\nThis directory contains the source code for `gradio_client`, a lightweight Python library that makes it very easy to use any Gradio app as an API.\n\nAs an example, consider this [Hugging Face Space that transcribes audio files](https://huggingface.co/spaces/abidlabs/whisper) that are recorded from the microphone.\n\n![](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/gradio-guides/whisper-screenshot.jpg)\n\nUsing the `gradio_client` library, we can easily use the Gradio as an API to transcribe audio files programmatically.\n\nHere's the entire code to do it:\n\n```python\nfrom gradio_client import Client\n\nclient = Client(\"abidlabs/whisper\")\nclient.predict(\"audio_sample.wav\")\n\n>> \"This is a test of the whisper speech recognition model.\"\n```\n\nThe Gradio client works with any Gradio Space, whether it be an image generator, a stateful chatbot, or a tax calculator.\n\n## Installation\n\nIf you already have a recent version of `gradio`, then the `gradio_client` is included as a dependency.\n\nOtherwise, the lightweight `gradio_client` package can be installed from pip (or pip3) and works with Python versions 3.8 or higher:", "Gradio-Lite: Serverless Gradio Running Entirely in Your Browser\n\nTags: SERVERLESS, BROWSER, PYODIDE\n\nGradio is a popular Python library for creating interactive machine learning apps. Traditionally, Gradio applications have relied on server-side infrastructure to run, which can be a hurdle for developers who need to host their applications. \n\nEnter Gradio-lite (`@gradio/lite`): a library that leverages [Pyodide](https://pyodide.org/en/stable/) to bring Gradio directly to your browser. In this blog post, we'll explore what `@gradio/lite` is, go over example code, and discuss the benefits it offers for running Gradio applications.\n\n## What is `@gradio/lite`?\n\n`@gradio/lite` is a JavaScript library that enables you to run Gradio applications directly within your web browser. It achieves this by utilizing Pyodide, a Python runtime for WebAssembly, which allows Python code to be executed in the browser environment. With `@gradio/lite`, you can **write regular Python code for your Gradio applications**, and they will **run seamlessly in the browser** without the need for server-side infrastructure.\n\n## Getting Started\n\nLet's build a \"Hello World\" Gradio app in `@gradio/lite`\n\n\n### 1. Import JS and CSS", "Getting Started with the Gradio Python client\n\nTags: CLIENT, API, SPACES\n\nThe Gradio Python client makes it very easy to use any Gradio app as an API. As an example, consider this [Hugging Face Space that transcribes audio files](https://huggingface.co/spaces/abidlabs/whisper) that are recorded from the microphone.\n\n![](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/gradio-guides/whisper-screenshot.jpg)\n\nUsing the `gradio_client` library, we can easily use the Gradio as an API to transcribe audio files programmatically.\n\nHere's the entire code to do it:\n\n```python\nfrom gradio_client import Client\n\nclient = Client(\"abidlabs/whisper\")\nclient.predict(\"audio_sample.wav\")\n\n>> \"This is a test of the whisper speech recognition model.\"\n```\n\nThe Gradio client works with any hosted Gradio app, whether it be an image generator, a text summarizer, a stateful chatbot, a tax calculator, or anything else! The Gradio Client is mostly used with apps hosted on [Hugging Face Spaces](https://hf.space), but your app can be hosted anywhere, such as your own server.", "### 2. Gradio ONLY works with PyTorch models\n\n<Question\n\tchoices={[\n        {\n\t\t\ttext: \"True\",\n\t\t\texplain: \"Gradio works with PyTorch models, but also works for any type of machine learning model!\"\n        },\n        {\n\t\t\ttext: \"False\",\n\t\t\texplain: \"Gradio is model agnostic, meaning you can create a demo for any type of machine learning model.\",\n\t\t\tcorrect: true\n        }\n\t]}\n/>\n\n### 3. Where can you launch a Gradio demo from?\n\n<Question\n\tchoices={[\n        {\n\t\t\ttext: \"Standard python IDEs\",\n\t\t\texplain: \"Gradio works great with your favorite IDE.\",\n            correct: true\n        },\n        {\n\t\t\ttext: \"Google Colab notebooks\",\n\t\t\texplain: \"You can create and launch a demo within your Google colab notebook.\",\n\t\t\tcorrect: true\n        },\n        {\n\t\t\ttext: \"Jupyter notebooks\",\n\t\t\texplain: \"Good choice - You can create and launch a demo within your Jupyter notebook.\",\n\t\t\tcorrect: true\n        }\n\t]}\n/>\n\n### 4. Gradio is designed primarily for NLP models\n\n<Question\n\tchoices={[\n        {\n\t\t\ttext: \"True\",\n\t\t\texplain: \"Gradio works with pretty much any data type, not just NLP.\"\n        },\n        {\n\t\t\ttext: \"False\",\n\t\t\texplain: \"Gradio supplies developers with a library of pre-built components for pretty much all data types.\",\n            correct: true\n        }\n\t]}\n/>\n\n### 5. Which of the following features are supported by Gradio?", "Quickstart\n\nGradio is an open-source Python package that allows you to quickly **build** a demo or web application for your machine learning model, API, or any arbitary Python function. You can then **share** a link to your demo or web application in just a few seconds using Gradio's built-in sharing features. *No JavaScript, CSS, or web hosting experience needed!*\n\n<img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/gradio-guides/lcm-screenshot-3.gif\" style=\"padding-bottom: 10px\">\n\nIt just takes a few lines of Python to create a beautiful demo like the one above, so let's get started \ud83d\udcab\n\n## Installation\n\n**Prerequisite**: Gradio requires [Python 3.8 or higher](https://www.python.org/downloads/)\n\n\nWe recommend installing Gradio using `pip`, which is included by default in Python. Run this in your terminal or command prompt:\n\n```bash\npip install gradio\n```\n\n\nTip: it is best to install Gradio in a virtual environment. Detailed installation instructions for all common operating systems <a href=\"https://www.gradio.app/main/guides/installing-gradio-in-a-virtual-environment\">are provided here</a>. \n\n## Building Your First Demo", "Running a Gradio App on your Web Server with Nginx\n\nTags: DEPLOYMENT, WEB SERVER, NGINX\n\n## Introduction\n\nGradio is a Python library that allows you to quickly create customizable web apps for your machine learning models and data processing pipelines. Gradio apps can be deployed on [Hugging Face Spaces](https://hf.space) for free.\n\nIn some cases though, you might want to deploy a Gradio app on your own web server. You might already be using [Nginx](https://www.nginx.com/), a highly performant web server, to serve your website (say `https://www.example.com`), and you want to attach Gradio to a specific subpath on your website (e.g. `https://www.example.com/gradio-demo`).\n\nIn this Guide, we will guide you through the process of running a Gradio app behind Nginx on your own web server to achieve this.\n\n**Prerequisites**\n\n1. A Linux web server with [Nginx installed](https://www.nginx.com/blog/setting-up-nginx/) and [Gradio installed](/quickstart)\n2. A working Gradio app saved as a python file on your web server\n\n## Editing your Nginx configuration file\n\n1. Start by editing the Nginx configuration file on your web server. By default, this is located at: `/etc/nginx/nginx.conf`", "In particular, Gradio apps ALLOW users to access to three kinds of files:\n\n- **Temporary files created by Gradio.** These are files that are created by Gradio as part of running your prediction function. For example, if your prediction function returns a video file, then Gradio will save that video to a temporary cache on your device and then send the path to the file to the front end. You can customize the location of temporary cache files created by Gradio by setting the environment variable `GRADIO_TEMP_DIR` to an absolute path, such as `/home/usr/scripts/project/temp/`.\n\n\n- **Cached examples created by Gradio.** These are files that are created by Gradio as part of caching examples for faster runtimes, if you set `cache_examples=True` in `gr.Interface()` or in `gr.Examples()`. By default, these files are saved in the `gradio_cached_examples/` subdirectory within your app's working directory. You can customize the location of cached example files created by Gradio by setting the environment variable `GRADIO_EXAMPLES_CACHE` to an absolute path or a path relative to your working directory.\n\n- **Files that you explicitly allow via the `allowed_paths` parameter in `launch()`**. This parameter allows you to pass in a list of additional directories or exact filepaths you'd like to allow users to have access to. (By default, this parameter is an empty list).\n\nGradio DOES NOT ALLOW access to:"], "eval_score_GPT4": "4", "eval_feedback_GPT4": "The response provided is mostly correct, accurate, and factual. It correctly identifies Gradio as a Python library used to create interactive web applications for machine learning models and other Python functions. It also accurately lists various applications of Gradio such as transcribing audio files, generating images, summarizing text, creating chatbots, and calculating taxes. Additionally, the response mentions that Gradio works with any hosted Gradio app and is model agnostic, which aligns well with the reference answer's points about creating demos and sharing machine learning models. However, the response does not explicitly mention debugging models, which is a part of the reference answer. Therefore, the response is not completely correct as per the highest standard of the score rubric but is mostly correct."}, {"question": "What TensorFlow API function is used to load a saved tensor file?\n", "true_answer": "safetensors.tensorflow.load_file", "source_doc": "huggingface/safetensors/blob/main/docs/source/api/tensorflow.mdx", "generated_answer": "The TensorFlow API function used to load a saved tensor file is `tf.saved_model.load` or `tf.keras.models.load_model` for Keras models. For loading a specific tensor from a saved TensorFlow model, you can use the `tf.saved_model.load` function with the `tags` parameter set to the name of the tensor you want to load. Alternatively, you can use the `tf.keras.models.load", "retrieved_docs": ["Tensorflow API\n\n[[autodoc]] safetensors.tensorflow.load_file\n[[autodoc]] safetensors.tensorflow.load\n[[autodoc]] safetensors.tensorflow.save_file\n[[autodoc]] safetensors.tensorflow.save", "The serving method has to be overridden by the new `input_signature` argument of the `tf.function` decorator. See the [official documentation](https://www.tensorflow.org/api_docs/python/tf/function#args_1) to know more about the `input_signature` argument. The `serving` method is used to define how will behave a SavedModel when deployed with TensorFlow Serving. Now the SavedModel looks like as expected, see the new `inputs_embeds` input:\n\n```\nThe given SavedModel SignatureDef contains the following input(s):\n  inputs['attention_mask'] tensor_info:\n      dtype: DT_INT32\n      shape: (-1, -1)\n      name: serving_default_attention_mask:0\n  inputs['inputs_embeds'] tensor_info:\n      dtype: DT_FLOAT\n      shape: (-1, -1, 768)\n      name: serving_default_inputs_embeds:0\n  inputs['token_type_ids'] tensor_info:\n      dtype: DT_INT32\n      shape: (-1, -1)\n      name: serving_default_token_type_ids:0\nThe given SavedModel SignatureDef contains the following output(s):\n  outputs['attentions'] tensor_info:\n      dtype: DT_FLOAT\n      shape: (12, -1, 12, -1, -1)\n      name: StatefulPartitionedCall:0\n  outputs['logits'] tensor_info:\n      dtype: DT_FLOAT\n      shape: (-1, 2)\n      name: StatefulPartitionedCall:1\nMethod name is: tensorflow/serving/predict\n```", "Fortunately, the TensorFlow team has users like us covered \ud83e\udd73! Wrapping a function containing TensorFlow code with\n[`tf.function`](https://www.tensorflow.org/api_docs/python/tf/function) will attempt to convert it into a graph when\nyou call the wrapped function. If you're training a model, calling `model.compile()` (without `run_eagerly=True`) does\nprecisely that wrapping, so that you benefit from graph mode when you call `model.fit()`. Since `tf.function` can be\nused in any function containing TensorFlow code, it means you can use it on functions that go beyond model inference,\ncreating a single optimized graph.\n\nNow that you know how to create TensorFlow graphs, compiling them with XLA is straightforward -- simply add `jit_compile=True`\nas an argument to the functions mentioned above (`tf.function` and `tf.keras.Model.compile`). Assuming everything went well\n(more on that below) and that you are using a GPU or a TPU, you will notice that the first call will take a while, but\nthat the remaining ones are much, much faster. Here's a simple example of a function that performs model inference and some post-processing of its outputs:\n\n```python\n# Note: execution times are deeply dependent on hardware -- a 3090 was used here.\nimport tensorflow as tf\nfrom transformers import AutoTokenizer, TFAutoModelForCausalLM", "</Tip>\n\nThis can be useful for converting your dataset to a dict of `Tensor` objects, or for writing a generator to load TF\nsamples from it. If you wish to convert the entire dataset to `Tensor`, simply query the full dataset:\n\n```py\n>>> ds[:]\n{'data': <tf.Tensor: shape=(2, 2), dtype=int64, numpy=\narray([[1, 2],\n       [3, 4]])>}\n```\n\n## N-dimensional arrays\n\nIf your dataset consists of N-dimensional arrays, you will see that by default they are considered as nested lists.\nIn particular, a TensorFlow formatted dataset outputs a `RaggedTensor` instead of a single tensor:\n\n```py\n>>> from datasets import Dataset\n>>> data = [[[1, 2],[3, 4]],[[5, 6],[7, 8]]]\n>>> ds = Dataset.from_dict({\"data\": data})\n>>> ds = ds.with_format(\"tf\")\n>>> ds[0]\n{'data': <tf.RaggedTensor [[1, 2], [3, 4]]>}\n```\n\nTo get a single tensor, you must explicitly use the [`Array`] feature type and specify the shape of your tensors:", "- Save the model weights as a `h5` file extension with [`model.save_weights`](https://www.tensorflow.org/tutorials/keras/save_and_load#save_the_entire_model) and then reload the model with [`~TFPreTrainedModel.from_pretrained`]:\n\n```py\n>>> from transformers import TFPreTrainedModel\n>>> from tensorflow import keras\n\n>>> model.save_weights(\"some_folder/tf_model.h5\")\n>>> model = TFPreTrainedModel.from_pretrained(\"some_folder\")\n```\n\n- Save the model with [`~TFPretrainedModel.save_pretrained`] and load it again with [`~TFPreTrainedModel.from_pretrained`]:\n\n```py\n>>> from transformers import TFPreTrainedModel\n\n>>> model.save_pretrained(\"path_to/model\")\n>>> model = TFPreTrainedModel.from_pretrained(\"path_to/model\")\n```\n\n## ImportError\n\nAnother common error you may encounter, especially if it is a newly released model, is `ImportError`:\n\n```\nImportError: cannot import name 'ImageGPTImageProcessor' from 'transformers' (unknown location)\n```\n\nFor these error types, check to make sure you have the latest version of \ud83e\udd17 Transformers installed to access the most recent models:", "Below, you can find the inputs and outputs representations of a `TFBertForSequenceClassification` saved as a TensorFlow SavedModel:\n\n```\nThe given SavedModel SignatureDef contains the following input(s):\n  inputs['attention_mask'] tensor_info:\n      dtype: DT_INT32\n      shape: (-1, -1)\n      name: serving_default_attention_mask:0\n  inputs['input_ids'] tensor_info:\n      dtype: DT_INT32\n      shape: (-1, -1)\n      name: serving_default_input_ids:0\n  inputs['token_type_ids'] tensor_info:\n      dtype: DT_INT32\n      shape: (-1, -1)\n      name: serving_default_token_type_ids:0\nThe given SavedModel SignatureDef contains the following output(s):\n  outputs['attentions'] tensor_info:\n      dtype: DT_FLOAT\n      shape: (12, -1, 12, -1, -1)\n      name: StatefulPartitionedCall:0\n  outputs['logits'] tensor_info:\n      dtype: DT_FLOAT\n      shape: (-1, 2)\n      name: StatefulPartitionedCall:1\nMethod name is: tensorflow/serving/predict\n```\n\nTo directly pass `inputs_embeds` (the token embeddings) instead of `input_ids` (the token IDs) as input, we need to subclass the model to have a new serving signature. The following snippet of code shows how to do so:\n\n```python\nfrom transformers import TFBertForSequenceClassification\nimport tensorflow as tf", "```bash\nThe given SavedModel SignatureDef contains the following input(s):\n  inputs['string_input'] tensor_info:\n      dtype: DT_STRING\n      shape: (-1)\n      name: serving_default_string_input:0\nThe given SavedModel SignatureDef contains the following output(s):\n  outputs['confidence'] tensor_info:\n      dtype: DT_FLOAT\n      shape: (-1)\n      name: StatefulPartitionedCall:0\n  outputs['label'] tensor_info:\n      dtype: DT_STRING\n      shape: (-1)\n      name: StatefulPartitionedCall:1\nMethod name is: tensorflow/serving/predict\n```\n\nThe model will accept [<u>base64 encoded</u>](https://www.base64encode.org/) strings of images, perform\npre-processing, run inference, and finally perform the post-processing\nsteps. The strings are base64 encoded to prevent any\nmodifications during network transmission. Pre-processing includes\nresizing the input image to 224x224 resolution, standardizing it to the\n`[-1, 1]` range, and transposing it to the `channels_first` memory\nlayout. Postprocessing includes mapping the predicted logits to string\nlabels.\n\nTo perform a deployment on Vertex AI, you need to keep the model\nartifacts in a [<u>Google Cloud Storage (GCS) bucket</u>](https://cloud.google.com/storage/docs/json_api/v1/buckets).\nThe accompanying Colab Notebook shows how to create a GCS bucket and\nsave the model artifacts into it.\n\n# Deployment workflow with Vertex AI"], "eval_score_GPT4": "3", "eval_feedback_GPT4": "Feedback: The response provided mentions `tf.saved_model.load` and `tf.keras.models.load_model`, which are indeed TensorFlow API functions used to load saved models or Keras models, respectively. However, the response does not match the reference answer, which specifies `safetensors.tensorflow.load_file` as the function used to load a saved tensor file. Since the reference answer is considered to be the correct one for scoring purposes, the response is not completely correct. However, the response is not completely incorrect either, as the mentioned functions are related to loading aspects of TensorFlow models. Therefore, the response is somewhat correct, accurate, and factual, but it does not fully align with the reference answer."}, {"question": "Where can you access the logs of your Endpoints in Hugging Face Endpoints?\n", "true_answer": "In the \"Logs\" tab of your Endpoint through the UI.", "source_doc": "huggingface/hf-endpoints-documentation/blob/main/docs/source/guides/logs.mdx", "generated_answer": "The logs of your Endpoints in Hugging Face Endpoints can be accessed through the \"Logs\" tab in the UI of your Endpoint. This feature is provided by Hugging Face Endpoints and allows you to view both the build logs of your Image artifacts and the Container Logs during inference. If your Endpoint creation is in the \"Failed\" state, you can also check the Build Logs to see the reason for failure, such as wrong version of a dependency.", "retrieved_docs": ["Access and read Logs\n\nHugging Face Endpoints provides access to the logs of your Endpoints through the UI in the \u201cLogs\u201d tab of your Endpoint. \n\nYou will have access to the build logs of your Image artifacts as well as access to the Container Logs during inference.\n\n<img src=\"https://raw.githubusercontent.com/huggingface/hf-endpoints-documentation/main/assets/9_selection.png\" alt=\"select logs\" />\n\nThe Container Logs are only available when your Endpoint is in the \u201cRunning\u201d state. \n\n_Note: If your Endpoint creation is in the \u201cFailed\u201d state, you can check the Build Logs to see what the reason was, e.g. wrong version of a dependency, etc._\n\n**Build Logs:**\n\n<img src=\"https://raw.githubusercontent.com/huggingface/hf-endpoints-documentation/main/assets/9_build_logs.png\" alt=\"build logs\" />\n\n**Container Logs:**\n\n<img src=\"https://raw.githubusercontent.com/huggingface/hf-endpoints-documentation/main/assets/9_logs.png\" alt=\"container logs\" />", "Security & Compliance\n\n\ud83e\udd17 Inference Endpoints is built with security and secure inference at its core. Below you can find an overview of the security measures we have in place.\n\n## Data Security/Privacy\n\nHugging Face does not store any customer data in terms of payloads or tokens that are passed to the Inference Endpoint. We are storing logs for 30 days. Every Inference Endpoints uses TLS/SSL to encrypt the data in transit.\n\nWe also recommend using AWS or Azure Private Link for organizations. This allows you to access your Inference Endpoint through a private connection, without exposing it to the internet.\n\nHugging Face also offers Business Associate Addendum or GDPR data processing agreement through the Inference Endpoint enterprise plan.\n\n## Model Security/Privacy:\n\nYou can set a model repository as private if you do not want to publicly expose it. Hugging Face does not own any model or data you upload to the Hugging Face hub. Hugging Face does provide malware and pickle scans over the contents of the model repository as with all items in the Hub.\n\n## Inference Endpoints and Hub Security\n\nThe Hugging Face Hub, which Inference Endpoints is part, is also SOC2 Type 2 certified. The Hugging Face Hub offers Role Based Access Control. For more on hub security: https://huggingface.co/docs/hub/security\n\n<img width=\"150\" src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/hub/security-soc-1.jpg\">\n\n## Inference Endpoint Security level\n\nWe currently offer three types of endpoints, in order or increasing security level:", "Access and view Metrics\n\nHugging Face Endpoints provides access to the metrics and analytics of your Endpoints through the UI on the detailed overview in the \u201cAnalytics\u201d tab of your Endpoints. \n\n<img src=\"https://raw.githubusercontent.com/huggingface/hf-endpoints-documentation/main/assets/10_metric.png\" alt=\"metric dashboard\" />\n\n## Access Metrics via API\n\nThe Hugging Face Inference Endpoints API exposes a [route to access the metrics](/docs/inference-endpoints/api_reference#getendpointmetrics) of your Endpoints. You can use this route to send customized Prometheus queries to your Endpoints.", "You should provide a MongoDB endpoint where your chats will be written. If you leave this section blank, your logs will be persisted to a database inside the Space. Note that Hugging Face does not have access to your chats. You can configure the name and the theme of the Space by providing the application name and application color parameters.\nBelow this, you can select the Hugging Face Hub ID of the model you wish to serve. You can also change the generation hyperparameters in the dictionary below in JSON format. \n\n_Note_: If you'd like to deploy a model with gated access or a model in a private repository, you can simply provide `HUGGING_FACE_HUB_TOKEN` in repository secrets. You need to set its value to an access token you can get from [here](https://huggingface.co/settings/tokens).\n\n<a href=\"Parameters\">\n    <img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/hub/huggingface.co_spaces_docker_chatui_params.png\" />\n</a>\n\nOnce the creation is complete, you will see `Building` on your Space. Once built, you can try your own HuggingChat!\n\n<a href=\"Hugging Chat Landing UI\">\n    <img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/hub/huggingface.co_spaces_docker_chatui_ui.png\" />\n</a>\n\nStart chatting!", "### Q: How can I reduce the latency of my Endpoint?\n\nA: There are several ways to reduce the latency of your Endpoint. One is to deploy your Endpoint in a region close to your application to reduce the network overhead. Another is to optimize your model using [Hugging Face Optimum](https://huggingface.co/docs/optimum/index) before creating your Endpoint. If you need help or have more questions about reducing latency, please contact us.\n\n\n\n### Q: How do I monitor my deployed Endpoint?\n\nA: You can currently monitor your Endpoint through the [\ud83e\udd17 Inference Endpoints web application](https://ui.endpoints.huggingface.co/endpoints), where you have access to the [Logs of your Endpoints](/docs/inference-endpoints/guides/logs) as well as a [metrics dashboard](/docs/inference-endpoints/guides/metrics). If you need programmatic access or more information, please contact us.\n\n\n\n### Q: What if I would like to deploy to a different instance type that is not listed?\n\nA: Please contact us if you feel your model would do better on a different instance type than what is listed.\n\n\n\n### Q: I accidentally leaked my token. Do I need to delete my endpoint?\n\nA: You can invalidate existing personal tokens and create new ones in your settings here: [https://huggingface.co/settings/tokens](https://huggingface.co/settings/tokens). For organization tokens, go to the organization settings.", "For convenience, you can directly use the Hugging Face Hub `/api/parquet` endpoint which returns the list of Parquet URLs:\n\n<inferencesnippet>\n<python>\n```python\nimport requests\nheaders = {\"Authorization\": f\"Bearer {API_TOKEN}\"}\nAPI_URL = \"https://huggingface.co/api/datasets/duorc/parquet\"\ndef query():\n    response = requests.get(API_URL, headers=headers)\n    return response.json()\nurls = query()\n```\n</python>\n<js>\n```js\nimport fetch from \"node-fetch\";\nasync function query(data) {\n    const response = await fetch(\n        \"https://huggingface.co/api/datasets/duorc/parquet\",\n        {\n            headers: { Authorization: `Bearer ${API_TOKEN}` },\n            method: \"GET\"\n        }\n    );\n    const urls = await response.json();\n    return urls;\n}\nquery().then((response) => {\n    console.log(JSON.stringify(response));\n});\n```\n</js>\n<curl>\n```curl\ncurl https://huggingface.co/api/datasets/duorc/parquet \\\n        -X GET \\\n        -H \"Authorization: Bearer ${API_TOKEN}\"\n```\n</curl>\n</inferencesnippet>", "Once your endpoint is created, you can copy the URL from [this page](https://ui.endpoints.huggingface.co):\n\n![deployed_endpoints.jpg](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/153_text_to_webapp/deployed_endpoints.jpg)\n\nConfigure the client to use it:\n\n```javascript\nconst hf = hfi.endpoint('** URL TO YOUR ENDPOINT **')\n```\n\nYou can now tell the inference client to use our private endpoint and call our model:\n\n```javascript\nconst { generated_text } = await hf.textGeneration({\n  inputs: 'a simple \"hello world\" html page: <html><body>'\n});\n```\n\n## Generating the HTML stream\n\nIt's now time to return some HTML to the web client when they visit a URL, say `/app`.\n\nWe will create and endpoint with Express.js to stream the results from the Hugging Face Inference API.\n\n\n```javascript\nimport express from 'express'\n\nimport { HfInference } from '@huggingface/inference'\n\nconst hfi = new HfInference('** YOUR TOKEN **')\nconst hf = hfi.endpoint('** URL TO YOUR ENDPOINT **')\n\nconst app = express()\n```"], "eval_score_GPT4": "5", "eval_feedback_GPT4": "The response provided is completely correct, accurate, and factual. It aligns with the reference answer by stating that the logs can be accessed through the \"Logs\" tab in the UI of your Endpoint. Additionally, the response goes into further detail by explaining the types of logs available, such as build logs of Image artifacts and Container Logs during inference, and even mentions the utility of these logs in case of a failure. This additional information supports the correctness and factual nature of the response without deviating from the accuracy of the reference answer."}, {"question": "What is the latest task added to Hugging Face AutoTrain for Computer Vision?\n", "true_answer": "Image Classification", "source_doc": "huggingface/blog/blob/main/autotrain-image-classification.md", "generated_answer": "The latest task added to Hugging Face AutoTrain for Computer Vision is Image Classification, as announced in the provided context. This allows users to train image classification models with zero configuration needed, and there are over 3000 models and 100 datasets available on the Hugging Face Hub for this task. The AutoTrain framework fully automates the data cleaning, model selection, and hyper-parameter optimization steps, making it easy for users to build production-ready SOT", "retrieved_docs": ["[Hugging Face AutoTrain](https://huggingface.co/autotrain) lets you train models with **zero** configuration needed. Just choose your task (translation? how about question answering?), upload your data, and let Hugging Face do the rest of the work! By letting AutoTrain experiment with number of different models, there's even a good chance that you'll end up with a model that performs better than a model that's been hand-trained by an engineer \ud83e\udd2f We\u2019ve been expanding the number of tasks that we support, and we\u2019re proud to announce that **you can now use AutoTrain for Computer Vision**! Image Classification is the latest task we\u2019ve added, with more on the way. But what does this mean for you?\n\n[Image Classification](https://huggingface.co/tasks/image-classification) models learn to *categorize* images, meaning that you can train one of these models to label any image. Do you want a model that can recognize signatures? Distinguish bird species? Identify plant diseases? As long as you can find an appropriate dataset, an image classification model has you covered.\n\n## How can you train your own image classifier?", "## AutoTrain with HuggingFace\n\nAutomated Machine Learning is a term for automating a Machine Learning pipeline. It also includes data cleaning, model selection, and hyper-parameter optimization too. We can use \ud83e\udd17 transformers for automated hyper-parameter searching. Hyper-parameter optimization is a difficult and time-consuming process.\n    \nWhile we can build our pipeline ourselves by using transformers and other powerful APIs, it is also possible to fully automate this with [AutoTrain](https://huggingface.co/autotrain). AutoTrain is built on many powerful APIs like transformers, [datasets](https://github.com/huggingface/datasets) and [inference-api](https://huggingface.co/docs/transformers/main_classes/trainer).\n\nCleaning the data, model selection, and hyper-parameter optimization steps are all fully automated in AutoTrain. One can fully utilize this framework to build production-ready SOTA transformer models for a specific task. Currently, AutoTrain supports binary and multi-label text classification, token classification, extractive question answering, text summarization, and text scoring. It also supports many languages like English, German, French, Spanish, Finnish, Swedish, Hindi, Dutch, and [more](https://huggingface.co/autotrain). If your language is not supported by AutoTrain, it is also possible to use custom models with custom tokenizers.\n\n## Kili", "--\ntitle: The State of Computer Vision at Hugging Face \ud83e\udd17\nthumbnail: /blog/assets/cv_state/thumbnail.png\nauthors:\n- user: sayakpaul\n---\n\n# The State of Computer Vision at Hugging Face \ud83e\udd17\n\n\nAt Hugging Face, we pride ourselves on democratizing the field of artificial intelligence together with the community. As a part of that mission, we began focusing our efforts on computer vision over the last year. What started as a [PR for having Vision Transformers (ViT) in \ud83e\udd17 Transformers](https://github.com/huggingface/transformers/pull/10950) has now grown into something much bigger \u2013 8 core vision tasks, over 3000 models, and over 100 datasets on the Hugging Face Hub.\n\nA lot of exciting things have happened since ViTs joined the Hub. In this blog post, we\u2019ll summarize what went down and what\u2019s coming to support the continuous progress of Computer Vision from the \ud83e\udd17 ecosystem.\n\nHere is a list of things we\u2019ll cover:\n\n- [Supported vision tasks and Pipelines](#support-for-pipelines)\n- [Training your own vision models](#training-your-own-models)\n- [Integration with `timm`](#\ud83e\udd17-\ud83e\udd1d-timm)\n- [Diffusers](#\ud83e\udde8-diffusers)\n- [Support for third-party libraries](#support-for-third-party-libraries)\n- [Deployment](#deployment)\n- and much more!\n\n## Enabling the community: One task at a time \ud83d\udc41", "* Hugging Face [Expert Acceleration Program](https://huggingface.co/support), a commercial service for Hugging Face experts to work directly with your team to accelerate your Machine Learning roadmap and models.\n</div>\n\n\n## Introduction \n\nTransfer learning has changed Machine Learning by reaching new levels of accuracy from Natural Language Processing (NLP) to Audio and Computer Vision tasks. At Hugging Face, we work hard to make these new complex models and large checkpoints as easily accessible and usable as possible. But while researchers and data scientists have converted to the new world of Transformers, few companies have been able to deploy these large, complex models in production at scale.\n\nThe main bottleneck is the latency of predictions which can make large deployments expensive to run and real-time use cases impractical. Solving this is a difficult engineering challenge for any Machine Learning Engineering team and requires the use of advanced techniques to optimize models all the way down to the hardware.\n\nWith [Hugging Face Infinity](https://huggingface.co/infinity), we offer a containerized solution that makes it easy to deploy low-latency, high-throughput, hardware-accelerated inference pipelines for the most popular Transformer models. Companies can get both the accuracy of Transformers and the efficiency necessary for large volume deployments, all in a simple to use package. In this blog post, we want to share detailed performance results for Infinity running on the latest generation of Intel Xeon CPU, to achieve optimal cost, efficiency, and latency for your Transformer deployments.\n\n\n## What is Hugging Face Infinity\n\nHugging Face Infinity is a containerized solution for customers to deploy end-to-end optimized inference pipelines for State-of-the-Art Transformer models, on any infrastructure.", "## Enabling the community: One task at a time \ud83d\udc41\n\nThe Hugging Face Hub is home to over 100,000 public models for different tasks such as next-word prediction, mask filling, token classification, sequence classification, and so on. As of today, we support [8 core vision tasks](https://huggingface.co/tasks) providing many model checkpoints:\n\n- Image classification\n- Image segmentation\n- (Zero-shot) object detection\n- Video classification\n- Depth estimation\n- Image-to-image synthesis\n- Unconditional image generation\n- Zero-shot image classification\n\nEach of these tasks comes with at least 10 model checkpoints on the Hub for you to explore. Furthermore, we support [tasks](https://huggingface.co/tasks) that lie at the intersection of vision and language such as:\n\n- Image-to-text (image captioning, OCR)\n- Text-to-image\n- Document question-answering\n- Visual question-answering", "The abstract from the paper is the following:\n\n*While the Transformer architecture has become the de-facto standard for natural language processing tasks, its\napplications to computer vision remain limited. In vision, attention is either applied in conjunction with\nconvolutional networks, or used to replace certain components of convolutional networks while keeping their overall\nstructure in place. We show that this reliance on CNNs is not necessary and a pure transformer applied directly to\nsequences of image patches can perform very well on image classification tasks. When pre-trained on large amounts of\ndata and transferred to multiple mid-sized or small image recognition benchmarks (ImageNet, CIFAR-100, VTAB, etc.),\nVision Transformer (ViT) attains excellent results compared to state-of-the-art convolutional networks while requiring\nsubstantially fewer computational resources to train.*\n\nThis model was contributed by [nielsr](https://huggingface.co/nielsr). The original code (written in JAX) can be\nfound [here](https://github.com/google-research/vision_transformer).\n\n## Resources\n\nA list of official Hugging Face and community (indicated by \ud83c\udf0e) resources to help you get started with ViT Hybrid.\n\n<PipelineTag pipeline=\"image-classification\"/>", "The huge 10B model from [Self-supervised Pretraining of Visual Features in the Wild](https://arxiv.org/abs/2103.01988), \ntrained on  one billion Instagram images, is available on the [hub](https://huggingface.co/facebook/regnet-y-10b-seer)\n\n## Resources\n\nA list of official Hugging Face and community (indicated by \ud83c\udf0e) resources to help you get started with RegNet.\n\n<PipelineTag pipeline=\"image-classification\"/>\n\n- [`RegNetForImageClassification`] is supported by this [example script](https://github.com/huggingface/transformers/tree/main/examples/pytorch/image-classification) and [notebook](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/image_classification.ipynb).\n- See also: [Image classification task guide](../tasks/image_classification)\n\nIf you're interested in submitting a resource to be included here, please feel free to open a Pull Request and we'll review it! The resource should ideally demonstrate something new instead of duplicating an existing resource.\n\n## RegNetConfig\n\n[[autodoc]] RegNetConfig\n\n<frameworkcontent>\n<pt>\n\n## RegNetModel\n\n[[autodoc]] RegNetModel\n    - forward\n\n## RegNetForImageClassification"], "eval_score_GPT4": "5", "eval_feedback_GPT4": "The response correctly identifies \"Image Classification\" as the latest task added to Hugging Face AutoTrain for Computer Vision, which aligns with the reference answer. The additional details provided about the task, such as the zero configuration needed, the number of models and datasets available, and the automation of data cleaning, model selection, and hyper-parameter optimization, are accurate and factual, although they go beyond the reference answer. However, since the core information requested is correct and factual, the additional information does not detract from the accuracy of the response."}, {"question": "What is the default repository type created by the `create_repo` function on Hugging Face Hub?\n", "true_answer": "model", "source_doc": "huggingface/huggingface_hub/blob/main/docs/source/en/guides/repository.md", "generated_answer": "The default repository type created by the `create_repo` function on Hugging Face Hub is a model repository, as specified in the context provided. If you want to create a repository with a different type, such as a dataset repository, you can use the `repo_type` parameter and set it to \"dataset\". The accepted values for `repo_type` are \"model\", \"dataset\", \"space\", and \"static\".", "retrieved_docs": ["This is equivalent to `huggingface_hub.list_spaces()`.\n\n### GET /api/spaces/{repo_id} or /api/spaces/{repo_id}/revision/{revision}\nGet all information for a specific model.\n\nThis is equivalent to `huggingface_hub.space_info(repo_id, revision)`.\n\n\n## Repo API\n\nThe following endpoints manage repository settings like creating and deleting a repository.\n### POST /api/repos/create\n\nCreate a repository. It's a model repo by default.\n\nParameters:\n- `type`: Type of repo (dataset or space; model by default).\n- `name`: Name of repo.\n- `organization`: Name of organization (optional).\n- `private`: Whether the repo is private.\n- `sdk`: When the type is `space` (streamlit, gradio, docker or static)\n\nPayload:\n\n```js\npayload = {\n    \"type\":\"model\",\n    \"name\":\"name\",\n    \"organization\": \"organization\",\n    \"private\":\"private\",\n    \"sdk\": \"sdk\"\n}\n```\n\nThis is equivalent to `huggingface_hub.create_repo()`.\n\n### DELETE /api/repos/delete\n\nDelete a repository. It's a model repo by default.", "</Tip>\n\nIf you want to create and manage a repository on the Hub, your machine must be logged in. If you are not, please refer to\n[this section](../quick-start#authentication). In the rest of this guide, we will assume that your machine is logged in.\n\n## Repo creation and deletion\n\nThe first step is to know how to create and delete repositories. You can only manage repositories that you own (under\nyour username namespace) or from organizations in which you have write permissions.\n\n### Create a repository\n\nCreate an empty repository with [`create_repo`] and give it a name with the `repo_id` parameter. The `repo_id` is your namespace followed by the repository name: `username_or_org/repo_name`.\n\n```py\n>>> from huggingface_hub import create_repo\n>>> create_repo(\"lysandre/test-model\")\n'https://huggingface.co/lysandre/test-model'\n```\n\nBy default, [`create_repo`] creates a model repository. But you can use the `repo_type` parameter to specify another repository type. For example, if you want to create a dataset repository:\n\n```py\n>>> from huggingface_hub import create_repo\n>>> create_repo(\"lysandre/test-dataset\", repo_type=\"dataset\")\n'https://huggingface.co/datasets/lysandre/test-dataset'\n```", "!--\u26a0\ufe0f Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\nrendered properly in your Markdown viewer.\n-->\n\n# Create and manage a repository\n\nThe Hugging Face Hub is a collection of git repositories. [Git](https://git-scm.com/) is a widely used tool in software\ndevelopment to easily version projects when working collaboratively. This guide will show you how to interact with the\nrepositories on the Hub, especially:\n\n- Create and delete a repository.\n- Manage branches and tags. \n- Rename your repository.\n- Update your repository visibility.\n- Manage a local copy of your repository.\n\n<Tip warning={true}>\n\nIf you are used to working with platforms such as GitLab/GitHub/Bitbucket, your first instinct\nmight be to use `git` CLI to clone your repo (`git clone`), commit changes (`git add, git commit`) and push them\n(`git push`). This is valid when using the Hugging Face Hub. However, software engineering and machine learning do\nnot share the same requirements and workflows. Model repositories might maintain large model weight files for different\nframeworks and tools, so cloning the repository can lead to you maintaining large local folders with massive sizes. As\na result, it may be more efficient to use our custom HTTP methods. You can read our [Git vs HTTP paradigm](../concepts/git_vs_http)\nexplanation page for more details.\n\n</Tip>", "## Upload files to the Hub\n\nYou might also want to provide a method for creating model repositories and uploading files to the Hub directly from your library. The `huggingface_hub` library offers two ways to assist you with creating repositories and uploading files:\n\n- `create_repo` creates a repository on the Hub.\n- `upload_file` directly uploads files to a repository on the Hub.\n\n### `create_repo`\n\nThe `create_repo` method creates a repository on the Hub. Use the `name` parameter to provide a name for your repository:\n\n```python\n>>> from huggingface_hub import create_repo\n>>> create_repo(repo_id=\"test-model\")\n'https://huggingface.co/lysandre/test-model'\n```\n\nWhen you check your Hugging Face account, you should now see a `test-model` repository under your namespace.\n\n### `upload_file`\n\nThe `upload_file` method uploads files to the Hub. This method requires the following:\n\n- A path to the file to upload.\n- The final path in the repository.\n- The repository you wish to push the files to.\n\nFor example:", "</Tip>\n\n### Use a local repository\n\nInstantiate a [`Repository`] object with a path to a local repository:\n\n```py\n>>> from huggingface_hub import Repository\n>>> repo = Repository(local_dir=\"<path>/<to>/<folder>\")\n```\n\n### Clone\n\nThe `clone_from` parameter clones a repository from a Hugging Face repository ID to a local directory specified by the `local_dir` argument:\n\n```py\n>>> from huggingface_hub import Repository\n>>> repo = Repository(local_dir=\"w2v2\", clone_from=\"facebook/wav2vec2-large-960h-lv60\")\n```\n\n`clone_from` can also clone a repository using a URL:\n\n```py\n>>> repo = Repository(local_dir=\"huggingface-hub\", clone_from=\"https://huggingface.co/facebook/wav2vec2-large-960h-lv60\")\n```\n\nYou can combine the `clone_from` parameter with [`create_repo`] to create and clone a repository:\n\n```py\n>>> repo_url = create_repo(repo_id=\"repo_name\")\n>>> repo = Repository(local_dir=\"repo_local_path\", clone_from=repo_url)\n```", "This will create the repository `dummy-model` in your namespace. If you like, you can specify which organization the repository should belong to using the `organization` argument:\n\n```py\nfrom huggingface_hub import create_repo\n\ncreate_repo(\"dummy-model\", organization=\"huggingface\")\n```\n\nThis will create the `dummy-model` repository in the `huggingface` namespace, assuming you belong to that organization.\nOther arguments which may be useful are:\n\n- `private`, in order to specify if the repository should be visible from others or not.\n- `token`, if you would like to override the token stored in your cache by a given token.\n- `repo_type`, if you would like to create a `dataset` or a `space` instead of a model. Accepted values are `\"dataset\"` and `\"space\"`.\n\nOnce the repository is created, we should add files to it! Jump to the next section to see the three ways this can be handled.\n\n\n## Using the web interface[[using-the-web-interface]]\n\nThe web interface offers tools to manage repositories directly in the Hub. Using the interface, you can easily create repositories, add files (even large ones!), explore models, visualize diffs, and much more.\n\nTo create a new repository, visit [huggingface.co/new](https://huggingface.co/new):", "### Advanced programmatic repository management \n\nThe `Repository` class helps manage both offline Git repositories and Hugging\nFace Hub repositories. Using the `Repository` class requires `git` and `git-lfs`\nto be installed.\n\nInstantiate a `Repository` object by calling it with a path to a local Git\nclone/repository:\n\n```python\n>>> from huggingface_hub import Repository\n>>> repo = Repository(\"<path>/<to>/<folder>\")\n```\n\nThe `Repository` takes a `clone_from` string as parameter. This can stay as\n`None` for offline management, but can also be set to any URL pointing to a Git\nrepo to clone that repository in the specified directory:\n\n```python\n>>> repo = Repository(\"huggingface-hub\", clone_from=\"https://github.com/huggingface/huggingface_hub\")\n```\n\nThe `clone_from` method can also take any Hugging Face model ID as input, and\nwill clone that repository:\n\n```python\n>>> repo = Repository(\"w2v2\", clone_from=\"facebook/wav2vec2-large-960h-lv60\")\n```"], "eval_score_GPT4": "5", "eval_feedback_GPT4": "Feedback: The response provided is completely correct, accurate, and factual. It aligns with the reference answer by stating that the default repository type is a model repository. Additionally, the response goes further to explain how to create different types of repositories using the `repo_type` parameter, which, while not required, does not detract from the accuracy of the answer to the original question."}, {"question": "How many splits does the \"duorc\" dataset have?\n", "true_answer": "Six", "source_doc": "huggingface/datasets-server/blob/main/docs/source/splits.mdx", "generated_answer": "According to the context provided, the \"duorc\" dataset has six splits, as shown in the response of the `/splits` endpoint and in the JSON response of the `/endpoint` endpoint for the \"ParaphraseRC\" configuration. The splits are \"train\", \"validation\", and \"test\" for both the \"ParaphraseRC\" and \"SelfRC\" configurations. Therefore, the answer to the question is six.\n\nHere'", "retrieved_docs": ["The endpoint response is a JSON containing a list of the dataset's files in the Parquet format. For example, the [`duorc`](https://huggingface.co/datasets/duorc) dataset has six Parquet files, which corresponds to the `test`, `train` and `validation` splits of its two configurations, `ParaphraseRC` and `SelfRC` (see the [List splits and configurations](./splits) guide for more details about splits and configurations).\n\nThe endpoint also gives the filename and size of each file:", "\"num_rows\": 86241,\n        \"num_columns\": 7\n      }\n    ],\n    \"splits\": [\n      {\n        \"dataset\": \"duorc\",\n        \"config\": \"ParaphraseRC\",\n        \"split\": \"train\",\n        \"num_bytes_parquet_files\": 26005668,\n        \"num_bytes_memory\": 496682909,\n        \"num_rows\": 69524,\n        \"num_columns\": 7\n      },\n      {\n        \"dataset\": \"duorc\",\n        \"config\": \"ParaphraseRC\",\n        \"split\": \"validation\",\n        \"num_bytes_parquet_files\": 5566868,\n        \"num_bytes_memory\": 106510489,\n        \"num_rows\": 15591,\n        \"num_columns\": 7\n      },\n      {\n        \"dataset\": \"duorc\",\n        \"config\": \"ParaphraseRC\",\n        \"split\": \"test\",\n        \"num_bytes_parquet_files\": 6136591,\n        \"num_bytes_memory\": 115215760,\n        \"num_rows\": 15857,", "The endpoint response is a JSON containing a list of the dataset's files URLs in the Parquet format for each split and configuration. For example, the [`duorc`](https://huggingface.co/datasets/duorc) dataset has one Parquet file for the train split of the \"ParaphraseRC\" configuration (see the [List splits and configurations](./splits) guide for more details about splits and configurations).\n\n```json\n{\n  \"ParaphraseRC\": {\n    \"test\": [\n      \"https://huggingface.co/api/datasets/duorc/parquet/ParaphraseRC/test/0.parquet\"\n    ],\n    \"train\": [\n      \"https://huggingface.co/api/datasets/duorc/parquet/ParaphraseRC/train/0.parquet\"\n    ],\n    \"validation\": [\n      \"https://huggingface.co/api/datasets/duorc/parquet/ParaphraseRC/validation/0.parquet\"\n    ]\n  },\n  \"SelfRC\": {\n    \"test\": [\n      \"https://huggingface.co/api/datasets/duorc/parquet/SelfRC/test/0.parquet\"\n    ],\n    \"train\": [\n      \"https://huggingface.co/api/datasets/duorc/parquet/SelfRC/train/0.parquet\"\n    ],\n    \"validation\": [\n      \"https://huggingface.co/api/datasets/duorc/parquet/SelfRC/validation/0.parquet\"\n    ]\n  }\n}\n```\n\nOptionally you can specify which configuration name to return, as well as which split:", "The `/splits` endpoint accepts the dataset name as its query parameter:\n\n<inferencesnippet>\n<python>\n```python\nimport requests\nheaders = {\"Authorization\": f\"Bearer {API_TOKEN}\"}\nAPI_URL = \"https://datasets-server.huggingface.co/splits?dataset=duorc\"\ndef query():\n    response = requests.get(API_URL, headers=headers)\n    return response.json()\ndata = query()\n```\n</python>\n<js>\n```js\nimport fetch from \"node-fetch\";\nasync function query(data) {\n    const response = await fetch(\n        \"https://datasets-server.huggingface.co/splits?dataset=duorc\",\n        {\n            headers: { Authorization: `Bearer ${API_TOKEN}` },\n            method: \"GET\"\n        }\n    );\n    const result = await response.json();\n    return result;\n}\nquery().then((response) => {\n    console.log(JSON.stringify(response));\n});\n```\n</js>\n<curl>\n```curl\ncurl https://datasets-server.huggingface.co/splits?dataset=duorc \\\n        -X GET \\\n        -H \"Authorization: Bearer ${API_TOKEN}\"\n```\n</curl>\n</inferencesnippet>\n\nThe endpoint response is a JSON containing a list of the dataset's splits and configurations. For example, the [duorc](https://huggingface.co/datasets/duorc) dataset has six splits and two configurations:", "```json\n{\n    \"dataset_info\": {\n        \"description\": \"DuoRC contains 186,089 unique question-answer pairs created from a collection of 7680 pairs of movie plots where each pair in the collection reflects two versions of the same movie.\\n\",\n        \"citation\": \"@inproceedings{DuoRC,\\nauthor = { Amrita Saha and Rahul Aralikatte and Mitesh M. Khapra and Karthik Sankaranarayanan},title = {{DuoRC: Towards Complex Language Understanding with Paraphrased Reading Comprehension}},\\nbooktitle = {Meeting of the Association for Computational Linguistics (ACL)},\\nyear = {2018}\\n}\\n\",\n        \"homepage\": \"https://duorc.github.io/\",\n        \"license\": \"https://raw.githubusercontent.com/duorc/duorc/master/LICENSE\",\n        \"features\": {\n            \"plot_id\": {\n                \"dtype\": \"string\",\n                \"_type\": \"Value\"\n            },\n            \"plot\": {\n                \"dtype\": \"string\",\n                \"_type\": \"Value\"\n            },\n            \"title\": {\n                \"dtype\": \"string\",\n                \"_type\": \"Value\"\n            },\n            \"question_id\": {", "\"config\": \"ParaphraseRC\",\n      \"split\": \"validation\",\n      \"url\": \"https://huggingface.co/datasets/duorc/resolve/refs%2Fconvert%2Fparquet/ParaphraseRC/validation/0000.parquet\",\n      \"filename\": \"0000.parquet\",\n      \"size\": 5566867\n    },\n    {\n      \"dataset\": \"duorc\",\n      \"config\": \"SelfRC\",\n      \"split\": \"test\",\n      \"url\": \"https://huggingface.co/datasets/duorc/resolve/refs%2Fconvert%2Fparquet/SelfRC/test/0000.parquet\",\n      \"filename\": \"0000.parquet\",\n      \"size\": 3035735\n    },\n    {\n      \"dataset\": \"duorc\",\n      \"config\": \"SelfRC\",\n      \"split\": \"train\",\n      \"url\": \"https://huggingface.co/datasets/duorc/resolve/refs%2Fconvert%2Fparquet/SelfRC/train/0000.parquet\",\n      \"filename\": \"0000.parquet\",\n      \"size\": 14851719\n    },\n    {", "```json\n// https://datasets-server.huggingface.co/rows?dataset=duorc&config=SelfRC&split=train&offset=150&length=2\n{\n  \"features\": [\n    {\n      \"feature_idx\": 0,\n      \"name\": \"plot_id\",\n      \"type\": { \"dtype\": \"string\", \"_type\": \"Value\" }\n    },\n    {\n      \"feature_idx\": 1,\n      \"name\": \"plot\",\n      \"type\": { \"dtype\": \"string\", \"_type\": \"Value\" }\n    },\n    {\n      \"feature_idx\": 2,\n      \"name\": \"title\",\n      \"type\": { \"dtype\": \"string\", \"_type\": \"Value\" }\n    },\n    {\n      \"feature_idx\": 3,\n      \"name\": \"question_id\",\n      \"type\": { \"dtype\": \"string\", \"_type\": \"Value\" }\n    },\n    {\n      \"feature_idx\": 4,\n      \"name\": \"question\",\n      \"type\": { \"dtype\": \"string\", \"_type\": \"Value\" }\n    },\n    {"], "eval_score_GPT4": "5", "eval_feedback_GPT4": "The response correctly identifies the number of splits in the \"duorc\" dataset as six, which aligns with the reference answer. The explanation provided is detailed and accurate, mentioning the specific splits for both \"ParaphraseRC\" and \"SelfRC\" configurations. The response is factual and directly answers the question posed in the instruction."}, {"question": "What is the purpose of Fully Sharded Data Parallel (FSDP) in distributed training?\n", "true_answer": "FSDP is developed for distributed training of large pretrained models up to 1T parameters by sharding the model parameters, gradients, and optimizer states across data parallel processes.", "source_doc": "huggingface/peft/blob/main/docs/source/accelerate/fsdp.md", "generated_answer": "The purpose of Fully Sharded Data Parallel (FSDP) in distributed training is to reduce memory usage and improve GPU memory efficiency for training large pretrained models up to 1T parameters. FSDP achieves this by sharding the model parameters, gradients, and optimizer states across data parallel processes and optionally offloading sharded model parameters to a CPU. This allows for scaling training to larger batch or model sizes, as memory efficiency is no longer a limiting factor.", "retrieved_docs": ["-->\n\n# Fully Sharded Data Parallel\n\n[Fully Sharded Data Parallel (FSDP)](https://pytorch.org/blog/introducing-pytorch-fully-sharded-data-parallel-api/) is a data parallel method that shards a model's parameters, gradients and optimizer states across the number of available GPUs (also called workers or *rank*). Unlike [DistributedDataParallel (DDP)](https://pytorch.org/docs/stable/generated/torch.nn.parallel.DistributedDataParallel.html), FSDP reduces memory-usage because a model is replicated on each GPU. This improves GPU memory-efficiency and allows you to train much larger models on fewer GPUs. FSDP is integrated with the Accelerate, a library for easily managing training in distributed environments, which means it is available for use from the [`Trainer`] class.\n\nBefore you start, make sure Accelerate is installed and at least PyTorch 2.1.0 or newer.\n\n```bash\npip install accelerate\n```\n\n## FSDP configuration\n\nTo start, run the [`accelerate config`](https://huggingface.co/docs/accelerate/package_reference/cli#accelerate-config) command to create a configuration file for your training environment. Accelerate uses this configuration file to automatically setup the correct training environment based on your selected training options in `accelerate config`.\n\n```bash\naccelerate config\n```", "!--\u26a0\ufe0f Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\nrendered properly in your Markdown viewer.\n-->\n\n# Fully Sharded Data Parallel\n\n[Fully sharded data parallel](https://pytorch.org/docs/stable/fsdp.html) (FSDP) is developed for distributed training of large pretrained models up to 1T parameters. FSDP achieves this by sharding the model parameters, gradients, and optimizer states across data parallel processes and it can also offload sharded model parameters to a CPU. The memory efficiency afforded by FSDP allows you to scale training to larger batch or model sizes.\n\n<Tip warning={true}>\n\nCurrently, FSDP does not confer any reduction in GPU memory usage and FSDP with CPU offload actually consumes 1.65x more GPU memory during training. You can track this PyTorch [issue](https://github.com/pytorch/pytorch/issues/91165) for any updates.\n\n</Tip>\n\nFSDP is supported in \ud83e\udd17 Accelerate, and you can use it with \ud83e\udd17 PEFT. This guide will help you learn how to use our FSDP [training script](https://github.com/huggingface/peft/blob/main/examples/conditional_generation/peft_lora_seq2seq_accelerate_fsdp.py). You'll configure the script to train a large model for conditional generation.\n\n## Configuration", "NEFTune is disabled after training to restore the original embedding layer to avoid any unexpected behavior.\n\n## Accelerate and Trainer\n\nThe [`Trainer`] class is powered by [Accelerate](https://hf.co/docs/accelerate), a library for easily training PyTorch models in distributed environments with support for integrations such as [FullyShardedDataParallel (FSDP)](https://pytorch.org/blog/introducing-pytorch-fully-sharded-data-parallel-api/) and [DeepSpeed](https://www.deepspeed.ai/).\n\n<Tip>\n\nLearn more about FSDP sharding strategies, CPU offloading, and more with the [`Trainer`] in the [Fully Sharded Data Parallel](fsdp) guide.\n\n</Tip>\n\nTo use Accelerate with [`Trainer`], run the [`accelerate.config`](https://huggingface.co/docs/accelerate/package_reference/cli#accelerate-config) command to set up training for your training environment. This command creates a `config_file.yaml` that'll be used when you launch your training script. For example, some example configurations you can setup are:\n\n<hfoptions id=\"config\">\n<hfoption id=\"DistributedDataParallel\">", "--\ntitle: \"Accelerate Large Model Training using PyTorch Fully Sharded Data Parallel\"\nthumbnail: /blog/assets/62_pytorch_fsdp/fsdp-thumbnail.png\nauthors:\n- user: smangrul\n- user: sgugger\n---\n\n# Accelerate Large Model Training using PyTorch Fully Sharded Data Parallel\n\n\nIn this post we will look at how we can leverage **[Accelerate](https://github.com/huggingface/accelerate)** Library for training large models which enables users to leverage the latest features of **[PyTorch FullyShardedDataParallel (FSDP)](https://pytorch.org/blog/introducing-pytorch-fully-sharded-data-parallel-api/)**.\n\n# Motivation \ud83e\udd17\n\n**With the ever increasing scale, size and parameters of the Machine Learning (ML) models, ML practitioners are finding it difficult to train or even load such large models on their hardware.** On one hand, it has been found that large models learn quickly (data and compute efficient) and are significantly more performant when compared to smaller models [1]; on the other hand, it  becomes prohibitive to train such models on most of the available hardware.", "```bash\naccelerate launch my-trainer-script.py\n```\n\n```bash\naccelerate launch --fsdp=\"full shard\" --fsdp_config=\"path/to/fsdp_config/ my-trainer-script.py\n```\n\n## Next steps\n\nFSDP can be a powerful tool for training really large models and you have access to more than one GPU or TPU. By sharding the model parameters, optimizer and gradient states, and even offloading them to the CPU when they're inactive, FSDP can reduce the high cost of large-scale training. If you're interested in learning more, the following may be helpful:\n\n* Follow along with the more in-depth Accelerate guide for [FSDP](https://huggingface.co/docs/accelerate/usage_guides/fsdp).\n* Read the [Introducing PyTorch Fully Sharded Data Parallel (FSDP) API](https://pytorch.org/blog/introducing-pytorch-fully-sharded-data-parallel-api/) blog post.\n* Read the [Scaling PyTorch models on Cloud TPUs with FSDP](https://pytorch.org/blog/scaling-pytorch-models-on-cloud-tpus-with-fsdp/) blog post.", "In this post we will look at Data Parallelism using ZeRO and more specifically the latest PyTorch feature **[FullyShardedDataParallel (FSDP)](https://pytorch.org/blog/introducing-pytorch-fully-sharded-data-parallel-api/)**.  **[DeepSpeed](https://github.com/microsoft/deepspeed)** and **[FairScale](https://github.com/facebookresearch/fairscale/)** have implemented the core ideas of the ZERO paper. These have already been integrated in `transformers` Trainer and accompanied by great blog [Fit More and Train Faster With ZeRO via DeepSpeed and FairScale](https://huggingface.co/blog/zero-deepspeed-fairscale) [10]. PyTorch recently upstreamed the Fairscale FSDP into PyTorch Distributed with additional optimizations.\n\n# Accelerate \ud83d\ude80:  Leverage PyTorch FSDP without any code\u00a0changes\n\nWe will look at the task of Causal Language Modelling using GPT-2 Large (762M) and XL (1.5B) model variants.", "[6] [Which hardware do you need to train a 176B parameters model?](https://bigscience.huggingface.co/blog/which-hardware-to-train-a-176b-parameters-model)\n\n[7] [Introducing PyTorch Fully Sharded Data Parallel (FSDP) API | PyTorch](https://pytorch.org/blog/introducing-pytorch-fully-sharded-data-parallel-api/)\n\n[8] [Getting Started with Fully Sharded Data Parallel(FSDP) \u2014 PyTorch Tutorials 1.11.0+cu102 documentation](https://pytorch.org/tutorials/intermediate/FSDP_tutorial.html)\n\n[9] [Training a 1 Trillion Parameter Model With PyTorch Fully Sharded Data Parallel on AWS | by PyTorch | PyTorch | Mar, 2022 | Medium](https://medium.com/pytorch/training-a-1-trillion-parameter-model-with-pytorch-fully-sharded-data-parallel-on-aws-3ac13aa96cff)\n\n[10] [Fit More and Train Faster With ZeRO via DeepSpeed and FairScale](https://huggingface.co/blog/zero-deepspeed-fairscale)"], "eval_score_GPT4": "5", "eval_feedback_GPT4": "Feedback: The response provided is completely correct, accurate, and factual. It aligns well with the reference answer by mentioning the key purpose of FSDP in distributed training, which is to shard model parameters, gradients, and optimizer states across data parallel processes. Additionally, the response expands on the reference answer by explaining the benefits of FSDP, such as reducing memory usage, improving GPU memory efficiency, and enabling the scaling of training to larger batch or model sizes. The mention of offloading to CPU is an additional factual detail that supports the main purpose of FSDP."}, {"question": "What file format is used to save and store PyTorch model weights more securely than `.bin` files?\n", "true_answer": "`.safetensors`", "source_doc": "huggingface/safetensors/blob/main/docs/source/convert-weights.md", "generated_answer": "The more secure file format used to save and store PyTorch model weights is `.safetensors`. This format, introduced by the Hugging Face library, provides a number of cool features compared to other formats, including safe loading of files. To convert PyTorch weights to `.safetensors`, the [Convert Space](https://huggingface.co/spaces/diffusers/convert) can be used, or the `convert.py` script can", "retrieved_docs": ["Convert weights to safetensors\n\nPyTorch model weights are commonly saved and stored as `.bin` files with Python's [`pickle`](https://docs.python.org/3/library/pickle.html) utility. To save and store your model weights in the more secure `safetensor` format, we recommend converting your weights to `.safetensors`.\n\nThe easiest way to convert your model weights is to use the [Convert Space](https://huggingface.co/spaces/diffusers/convert), given your model weights are already stored on the Hub. The Convert Space downloads the pickled weights, converts them, and opens a Pull Request to upload the newly converted `.safetensors` file to your repository.\n\n<Tip warning={true}>\n\nFor larger models, the Space may be a bit slower because its resources are tied up in converting other models. You can also try running the [convert.py](https://github.com/huggingface/safetensors/blob/main/bindings/python/convert.py) script (this is what the Space is running) locally to convert your weights.\n\nFeel free to ping [@Narsil](https://huggingface.co/Narsil) for any issues with the Space.\n\n</Tip>", "For a more concrete explanation, we'll use PyTorch.\n```python\nimport torch\nfrom safetensors.torch import load_file, save_file\n\nweights = {\"embeddings\": torch.zeros((10, 100))}\nsave_file(weights, \"model.safetensors\")\nweights2 = load_file(\"model.safetensors\")\n```\n\nIt also has a number of [cool features](https://github.com/huggingface/safetensors#yet-another-format-) compared to other formats, most notably that loading files is _safe_, as we'll see later. \n\nWhen you're using `transformers`, if `safetensors` is installed, then those files will already\nbe used preferentially in order to prevent issues, which means that\n\n```\npip install safetensors\n```\n\nis likely to be the only thing needed to run `safetensors` files safely.\n\nGoing forward and thanks to the validation of the library, `safetensors` will now be installed in `transformers` by\ndefault. The next step is saving models in `safetensors` by default.\n\nWe are thrilled to see that the `safetensors` library is already seeing use in the ML ecosystem, including:", "**FP16 Weights:**\n\nWhen a model is saved under ZeRO-2, you end up having the normal `pytorch_model.bin` file with the model weights, but\nthey are only the fp16 version of the weights.\n\nUnder ZeRO-3, things are much more complicated, since the model weights are partitioned out over multiple GPUs,\ntherefore `\"stage3_gather_16bit_weights_on_model_save\": true` is required to get the `Trainer` to save the fp16\nversion of the weights. If this setting is `False` `pytorch_model.bin` won't be created. This is because by default DeepSpeed's `state_dict` contains a placeholder and not the real weights. If we were to save this `state_dict` it won't be possible to load it back.\n\n\n```json\n{\n    \"zero_optimization\": {\n        \"stage3_gather_16bit_weights_on_model_save\": true\n    }\n}\n```\n\n**FP32 Weights:**\n\nWhile the fp16 weights are fine for resuming training, if you finished finetuning your model and want to upload it to\nthe [models hub](https://huggingface.co/models) or pass it to someone else you most likely will want to get the fp32\nweights. This ideally shouldn't be done during training since this is a process that requires a lot of memory, and\ntherefore best to be performed offline after the training is complete. But if desired and you have plenty of free CPU\nmemory it can be done in the same training script. The following sections will discuss both approaches.", "We call the checkpoints saved in several files like BLOOM *sharded checkpoints*, and we have standardized their format as such: \n- One file (called `pytorch_model.bin.index.json`) contains some metadata and a map parameter name to file name, indicating where to find each weight\n- All the other files are standard PyTorch state dicts, they just contain a part of the model instead of the whole one. You can have a look at the content of the index file [here](https://huggingface.co/bigscience/bloom/blob/main/pytorch_model.bin.index.json).\n\nTo load such a sharded checkpoint into a model, we just need to loop over the various shards. Accelerate provides a function called `load_checkpoint_in_model` that will do this for you if you have cloned one of the repos of the Hub, or you can directly use the `from_pretrained` method of Transformers, which will handle the downloading and caching for you:\n\n```python\nimport torch\nfrom transformers import AutoModelForCausalLM\n\n# Will error\ncheckpoint = \"facebook/opt-13b\"\nmodel = AutoModelForCausalLM.from_pretrained(checkpoint, device_map=\"auto\", torch_dtype=torch.float16)\n```\n\nIf the device map computed automatically requires some weights to be offloaded on disk because you don't have enough GPU and CPU RAM, you will get an error indicating you need to pass an folder where the weights that should be stored on disk will be offloaded:", "```python\n# Save the model weights\ntorch.save(my_model.state_dict(), 'model_weights.pth')\n\n# Reload them\nnew_model = ModelClass()\nnew_model.load_state_dict(torch.load('model_weights.pth'))\n```\n\nThis works pretty well for models with less than 1 billion parameters, but for larger models, this is very taxing in RAM. The BLOOM model has 176 billions parameters; even with the weights saved in bfloat16 to save space, it still represents 352GB as a whole. While the super computer that trained this model might have this amount of memory available, requiring this for inference is unrealistic.\n\nThis is why large models on the Hugging Face Hub are not saved and shared with one big file containing all the weights, but **several** of them. If you go to the [BLOOM model page](https://huggingface.co/bigscience/bloom/tree/main) for instance, you will see there is 72 files named `pytorch_model_xxxxx-of-00072.bin`, which each contain part of the model weights. Using this format, we can load one part of the state dict in memory, put the weights inside the model, move them on the right device, then discard this state dict part before going to the next. Instead of requiring to have enough RAM to accommodate the whole model, we only need enough RAM to get the biggest checkpoint part, which we call a **shard**, so 7.19GB in the case of BLOOM.", "This last format, `safetensors`, is a simple serialization format that we are working on and experimenting with currently! Please help or contribute if you can \ud83d\udd25.\n\n### Improve `torch.load/save`\n\nThere's an open discussion in progress at PyTorch on having a [Safe way of loading only weights from *.pt file by default](https://github.com/pytorch/pytorch/issues/52181) \u2013 please chime in there!\n\n### Hub\u2019s Security Scanner\n\n#### What we have now\n\nWe have created a security scanner that scans every file pushed to the Hub and runs security checks. At the time of writing, it runs two types of scans:\n\n- ClamAV scans\n- Pickle Import scans\n\nFor ClamAV scans, files are run through the open-source antivirus [ClamAV](https://www.clamav.net). While this covers a good amount of dangerous files, it doesn\u2019t cover pickle exploits.\n\nWe have implemented a Pickle Import scan, which extracts the list of imports referenced in a pickle file. Every time you upload a `pytorch_model.bin` or any other pickled file, this scan is run.\n\nOn the hub the list of imports will be displayed next to each file containing imports. If any import looks suspicious, it will be highlighted.", "In this precision, we can fit the model up to layer 21 on the GPU:\n\n```python out\n\n\n{'model.decoder.embed_tokens': 0,\n 'model.decoder.embed_positions': 0,\n 'model.decoder.final_layer_norm': 0,\n 'model.decoder.layers.0': 0,\n 'model.decoder.layers.1': 0,\n ...\n 'model.decoder.layers.21': 0,\n 'model.decoder.layers.22': 'cpu',\n ...\n 'model.decoder.layers.37': 'cpu',\n 'model.decoder.layers.38': 'disk',\n 'model.decoder.layers.39': 'disk',\n 'lm_head': 'disk'}\n```\n\nNow that we know where each weight is supposed to go, we can progressively load the pretrained weights inside the model.\n\n## Sharding state dicts\n\nTraditionally, PyTorch models are saved in a whole file containing a map from parameter name to weight. This map is often called a `state_dict`. Here is an excerpt from the [PyTorch documentation](https://pytorch.org/tutorials/beginner/basics/saveloadrun_tutorial.html) on saving on loading:\n\n```python\n# Save the model weights\ntorch.save(my_model.state_dict(), 'model_weights.pth')"], "eval_score_GPT4": "5", "eval_feedback_GPT4": "Feedback: The response correctly identifies `.safetensors` as the more secure file format for saving and storing PyTorch model weights, which aligns with the reference answer. The additional information provided about the Hugging Face library and the methods to convert to `.safetensors` format is accurate and factual, although not required by the instruction. The response goes beyond the reference answer but does not contradict it."}, {"question": "What type of security certification does Hugging Face have?\n", "true_answer": "SOC2 Type 2 certified", "source_doc": "huggingface/hub-docs/blob/main/docs/hub/security.md", "generated_answer": "Hugging Face is SOC2 Type 2 certified, which means they provide security certification to their customers and actively monitor and patch any security weaknesses. This information can be found in the context provided in Extract 0: Security.", "retrieved_docs": ["Security\n\nThe Hugging Face Hub offers several security features to ensure that your code and data are secure. Beyond offering [private repositories](./repositories-settings#private-repositories) for models, datasets, and Spaces, the Hub supports access tokens, commit signatures, and malware scanning.\n\nHugging Face is GDPR compliant. If a contract or specific data storage is something you'll need, we recommend taking a look at our [Expert Acceleration Program](https://huggingface.co/support). Hugging Face can also offer Business Associate Addendums or GDPR data processing agreements through an [Enterprise Plan](https://huggingface.co/pricing). \n\nHugging Face is also [SOC2 Type 2 certified](https://us.aicpa.org/interestareas/frc/assuranceadvisoryservices/aicpasoc2report.html), meaning we provide security certification to our customers and actively monitor and patch any security weaknesses.\n\n<img width=\"150\" src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/hub/security-soc-1.jpg\">\n\nFor any other security questions, please feel free to send us an email at security@huggingface.co.\n\n## Contents", "\u2728 Being officially recognized as a Hugging Face\u2019s Ambassador\n\n<br />\n\n**Eligibility Requirements for Students**\n\n- Validate your student status \n- Have taken at least one machine learning/data science course (online courses are considered as well)\n- Be enrolled in an accredited college or university\n- Be a user of the Hugging Face Hub and/or the Hugging Face\u2019s libraries\n- Acknowledge the [Code of Conduct](https://huggingface2.notion.site/huggingface2/Hugging-Face-Code-of-Conduct-45eeeafa9ef44c5e888a2952619fdfa8). Community is at the center of the Hugging Face ecosystem. Because of that, we strictly adhere to our [Code of conduct](https://huggingface2.notion.site/huggingface2/Hugging-Face-Code-of-Conduct-45eeeafa9ef44c5e888a2952619fdfa8). If any ambassador infringes it or behaves inadequately, they will be excluded from the Program.\n\n**[Apply here](https://docs.google.com/forms/d/e/1FAIpQLScY9kTi-TjZipRFRviluRCwSjFf3CCsMbKedzO1tq2S0wtbNQ/viewform?usp=sf_link) to become an ambassador!**\n\n**Timeline:**", "Security & Compliance\n\n\ud83e\udd17 Inference Endpoints is built with security and secure inference at its core. Below you can find an overview of the security measures we have in place.\n\n## Data Security/Privacy\n\nHugging Face does not store any customer data in terms of payloads or tokens that are passed to the Inference Endpoint. We are storing logs for 30 days. Every Inference Endpoints uses TLS/SSL to encrypt the data in transit.\n\nWe also recommend using AWS or Azure Private Link for organizations. This allows you to access your Inference Endpoint through a private connection, without exposing it to the internet.\n\nHugging Face also offers Business Associate Addendum or GDPR data processing agreement through the Inference Endpoint enterprise plan.\n\n## Model Security/Privacy:\n\nYou can set a model repository as private if you do not want to publicly expose it. Hugging Face does not own any model or data you upload to the Hugging Face hub. Hugging Face does provide malware and pickle scans over the contents of the model repository as with all items in the Hub.\n\n## Inference Endpoints and Hub Security\n\nThe Hugging Face Hub, which Inference Endpoints is part, is also SOC2 Type 2 certified. The Hugging Face Hub offers Role Based Access Control. For more on hub security: https://huggingface.co/docs/hub/security\n\n<img width=\"150\" src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/hub/security-soc-1.jpg\">\n\n## Inference Endpoint Security level\n\nWe currently offer three types of endpoints, in order or increasing security level:", "[Explore existing organizations](https://huggingface.co/organizations), create a new organization [here](https://huggingface.co/organizations/new), and then visit the [**Organizations documentation**](./organizations) to learn more.\n\n## Security\n\nThe Hugging Face Hub supports security and access control features to give you the peace of mind that your code, models, and data are safe. Visit the [**Security**](./security) section in these docs to learn about:\n\n- User Access Tokens\n- Access Control for Organizations\n- Signing commits with GPG\n- Malware scanning\n\n<img width=\"150\" src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/hub/security-soc-1.jpg\">", "## The security audit\n\nSince `safetensors` main asset is providing safety guarantees, we wanted to make sure\nit actually delivered. That's why Hugging Face, EleutherAI, and Stability AI teamed up to get an external\nsecurity audit to confirm it.\n\nImportant findings:\n\n- No critical security flaw leading to arbitrary code execution was found.\n- Some imprecisions in the spec format were detected and fixed. \n- Some missing validation allowed [polyglot files](https://en.wikipedia.org/wiki/Polyglot_(computing)), which was fixed.\n- Lots of improvements to the test suite were proposed and implemented.\n\nIn the name of openness and transparency, all companies agreed to make the report\nfully public.\n\n[Full report](https://huggingface.co/datasets/safetensors/trail_of_bits_audit_repot/resolve/main/SOW-TrailofBits-EleutherAI_HuggingFace-v1.2.pdf)\n\n\nOne import thing to note is that the library is written in Rust. This adds\nan extra layer of [security](https://doc.rust-lang.org/rustc/exploit-mitigations.html)\ncoming directly from the language itself.\n\nWhile it is impossible to \nprove the absence of flaws, this is a major step in giving reassurance that `safetensors`\nis indeed safe to use.\n\n## Going forward\n\nFor Hugging Face, EleutherAI, and Stability AI, the master plan is to shift to using this format by default.", "**`HF_API_TOKEN`**\n\n`HF_API_TOKEN` defines your Hugging Face authorization token. The `HF_API_TOKEN` is used as a HTTP bearer authorization for remote files like private models. You can find your token under [Settings](https://huggingface.co/settings/tokens) of your Hugging Face account.\n\n```bash\nHF_API_TOKEN=\"api_XXXXXXXXXXXXXXXXXXXXXXXXXXXXX\"\n```", "elcome to the Hugging Face Course! This course has been designed to teach you all about the Hugging Face ecosystem: how to use the dataset and model hub as well as all our open source libraries. Here is the Table of Contents. As you can see, it's divided in three sections which become progressively more advanced. At this stage, the first two sections have been released. The first will teach you the basics of how to use a Transformer model, fine-tune it on your own dataset and share the result with the community. The second will dive deeper into our libraries and teach you how to tackle any NLP task. We are actively working on the last one and hope to have it ready for you for the spring of 2022. The first chapter requires no technical knowledge and is a good introduction to learn what Transformers models can do and how they could be of use to you or your company. The next chapters require a good knowledge of Python and some basic knowledge of Machine Learning and Deep Learning. If you don't know what a training and validation set is or what gradient descent means, you should look at an introductory course such as the ones published by deeplearning.ai or fast.ai. It's also best if you have some basics in one Deep Learning Framework (PyTorch or TensorFlow). Each part of the material introduced in this course has a version in both those frameworks, so you will be able to pick the one you are most comfortable with. This is the team that developed this course. I'll now let each of the speakers introduce themselves briefly."], "eval_score_GPT4": "5", "eval_feedback_GPT4": "Feedback: The response correctly identifies Hugging Face as having SOC2 Type 2 certification, which is in line with the reference answer. The additional information provided about the certification's implications for security and monitoring is accurate and relevant, although not required by the instruction. Based on the score rubric, the response is completely correct, accurate, and factual."}, {"question": "What do RAG models combine to generate outputs?\n", "true_answer": "Pretrained dense retrieval (DPR) and sequence-to-sequence models.", "source_doc": "huggingface/transformers/blob/main/docs/source/en/model_doc/rag.md", "generated_answer": "RAG models combine the powers of pretrained dense retrieval (DPR) and Seq2Seq models. They retrieve contextual documents from an external dataset as part of their execution, and these contextual documents are used in conjunction with the original input to produce an output. The RAG model is a seq2seq model with access to a retrieval mechanism providing relevant context documents at training and evaluation time.", "retrieved_docs": ["This model was contributed by [ola13](https://huggingface.co/ola13).\n\n## Usage tips\n\nRetrieval-augmented generation (\"RAG\") models combine the powers of pretrained dense retrieval (DPR) and Seq2Seq models. \nRAG models retrieve docs, pass them to a seq2seq model, then marginalize to generate outputs. The retriever and seq2seq \nmodules are initialized from pretrained models, and fine-tuned jointly, allowing both retrieval and generation to adapt \nto downstream tasks.\n\n## RagConfig\n\n[[autodoc]] RagConfig\n\n## RagTokenizer\n\n[[autodoc]] RagTokenizer\n\n## Rag specific outputs\n\n[[autodoc]] models.rag.modeling_rag.RetrievAugLMMarginOutput\n\n[[autodoc]] models.rag.modeling_rag.RetrievAugLMOutput\n\n## RagRetriever\n\n[[autodoc]] RagRetriever\n\n<frameworkcontent>\n<pt>\n\n## RagModel\n\n[[autodoc]] RagModel\n    - forward\n\n## RagSequenceForGeneration\n\n[[autodoc]] RagSequenceForGeneration\n    - forward\n    - generate\n\n## RagTokenForGeneration\n\n[[autodoc]] RagTokenForGeneration\n    - forward\n    - generate\n\n</pt>\n<tf>\n\n## TFRagModel", "_An overview of RAG. The model retrieves contextual documents from an external dataset as part of its execution. These contextual documents are used in conjunction with the original input to produce an output. The  GIF is taken from [Facebook's original blog post](https://ai.facebook.com/blog/retrieval-augmented-generation-streamlining-the-creation-of-intelligent-natural-language-processing-models)._\n\n\n\nRecently, [Huggingface](https://huggingface.co/) partnered with [Facebook AI](https://ai.facebook.com/) to introduce the [RAG](https://twitter.com/huggingface/status/1310597560906780680) model as part of its Transformers library. \n\n\n[RAG](https://ai.facebook.com/blog/retrieval-augmented-generation-streamlining-the-creation-of-intelligent-natural-language-processing-models/) acts just like any other [seq2seq model](https://blog.keras.io/a-ten-minute-introduction-to-sequence-to-sequence-learning-in-keras.html). However, [RAG](https://ai.facebook.com/blog/retrieval-augmented-generation-streamlining-the-creation-of-intelligent-natural-language-processing-models/) has an intermediate component that retrieves contextual documents from an external knowledge base (like a Wikipedia text corpus). These documents are then used in conjunction with the input sequence and passed into the underlying seq2seq [generator](https://huggingface.co/blog/how-to-generate).", "Intro\n\nAuthors: @patrickvonplaten and @lhoestq\n\nAimed at tackling the knowledge-intensive NLP tasks (think tasks a human wouldn't be expected to solve without access to external knowledge sources), RAG models are seq2seq models with access to a retrieval mechanism providing relevant context documents at training and evaluation time.\n\nA RAG model encapsulates two core components: a question encoder and a generator.\nDuring a forward pass, we encode the input with the question encoder and pass it\nto the retriever to extract relevant context documents. The documents are then prepended to the input.\nSuch contextualized inputs are passed to the generator.\n\nRead more about RAG  at https://arxiv.org/abs/2005.11401.\n\n# Note\n\n\u26a0\ufe0f This project should be run with pytorch-lightning==1.3.1 which has a potential security vulnerability\n\n# Finetuning\n\nOur finetuning logic is based on scripts from [`examples/legacy/seq2seq`](https://github.com/huggingface/transformers/tree/main/examples/legacy/seq2seq). We accept training data in the same format as specified there - we expect a directory consisting of 6 text files:\n```bash\ntrain.source\ntrain.target\nval.source\nval.target\ntest.source\ntest.target\n```\n\nA sample finetuning command (run ` ./examples/research_projects/rag/finetune_rag.py --help` to list all available options):", "If you would like to initialize finetuning with a base model using different question encoder and generator architectures, you can build it with a consolidation script, e.g.:\n```\npython examples/research_projects/rag/consolidate_rag_checkpoint.py \\\n    --model_type rag_sequence \\\n    --generator_name_or_path facebook/bart-large-cnn \\\n    --question_encoder_name_or_path facebook/dpr-question_encoder-single-nq-base \\\n    --dest path/to/checkpoint\n```\nYou will then be able to pass `path/to/checkpoint` as `model_name_or_path` to the `finetune_rag.py` script.\n\n## Document Retrieval\nWhen running distributed fine-tuning, each training worker needs to retrieve contextual documents\nfor its input by querying a index loaded into memory. RAG provides two implementations for document retrieval,\none with [`torch.distributed`](https://pytorch.org/docs/stable/distributed.html) communication package and the other\nwith [`Ray`](https://docs.ray.io/en/master/).\n\nThis option can be configured with the `--distributed_retriever` flag which can either be set to `pytorch` or `ray`.\nBy default this flag is set to `pytorch`.\n\nFor the Pytorch implementation, only training worker 0 loads the index into CPU memory, and a gather/scatter pattern is used\nto collect the inputs from the other training workers and send back the corresponding document embeddings.", "--model_name_or_path facebook/rag-sequence-nq \\ # model name or path of the model we're evaluating\n        --model_type rag_sequence \\ # RAG model type (rag_token or rag_sequence)\n        --evaluation_set output/biencoder-nq-dev.questions \\ # an input dataset for evaluation\n        --gold_data_path poutput/biencoder-nq-dev.pages \\ # a dataset containing ground truth answers for samples from the evaluation_set\n        --predictions_path output/retrieval_preds.tsv  \\ # name of file where predictions will be stored\n        --eval_mode retrieval \\ # indicates whether we're performing retrieval evaluation or e2e evaluation\n        --k 1 # parameter k for the precision@k metric", "A sample finetuning command (run ` ./examples/research_projects/rag/finetune_rag.py --help` to list all available options):\n\n```bash\npython examples/research_projects/rag/finetune_rag.py \\\n    --data_dir $DATA_DIR \\\n    --output_dir $OUTPUT_DIR \\\n    --model_name_or_path $MODEL_NAME_OR_PATH \\\n    --model_type rag_sequence \\\n    --fp16 \\\n    --gpus 8\n```\nWe publish two `base` models which can serve as a starting point for finetuning on downstream tasks (use them as `model_name_or_path`):\n- [`facebook/rag-sequence-base`](https://huggingface.co/facebook/rag-sequence-base) - a base for finetuning `RagSequenceForGeneration` models,\n- [`facebook/rag-token-base`](https://huggingface.co/facebook/rag-token-base) - a base for finetuning `RagTokenForGeneration` models.\n\nThe `base` models initialize the question encoder with [`facebook/dpr-question_encoder-single-nq-base`](https://huggingface.co/facebook/dpr-question_encoder-single-nq-base) and the generator with [`facebook/bart-large`](https://huggingface.co/facebook/bart-large).", "python examples/rag/finetune_rag.py \\\n    --data_dir $DATA_DIR \\\n    --output_dir $OUTPUT_DIR \\\n    --model_name_or_path $MODEL_NAME_OR_PATH \\\n    --model_type rag_sequence \\\n    --fp16 \\\n    --gpus 8 \\\n    --profile \\\n    --do_train \\\n    --do_predict \\\n    --n_val -1 \\\n    --train_batch_size 8 \\\n    --eval_batch_size 1 \\\n    --max_source_length 128 \\\n    --max_target_length 25 \\\n    --val_max_target_length 25 \\\n    --test_max_target_length 25 \\\n    --label_smoothing 0.1 \\\n    --dropout 0.1 \\\n    --attention_dropout 0.1 \\\n    --weight_decay 0.001 \\\n    --adam_epsilon 1e-08 \\\n    --max_grad_norm 0.1 \\\n    --lr_scheduler polynomial \\\n    --learning_rate 3e-05 \\\n    --num_train_epochs 100 \\\n    --warmup_steps 500 \\\n    --gradient_accumulation_steps 1 \\\n    --distributed_retriever ray \\\n    --num_retrieval_workers 4\n\n# Stop the Ray cluster.\nray stop\n```\n\n## What\u2019s next?"], "eval_score_GPT4": "5", "eval_feedback_GPT4": "Feedback: The response accurately identifies that RAG models combine pretrained dense retrieval (DPR) and Seq2Seq models, which is in line with the reference answer. It also correctly explains the process of retrieving contextual documents from an external dataset and using them along with the original input to produce an output. The additional details provided in the response are correct and factual, and they align with the reference answer's components."}, {"question": "What library does MarkupLMFeatureExtractor use to extract data from HTML and XML files?\n", "true_answer": "Beautiful Soup", "source_doc": "huggingface/transformers/blob/main/docs/source/en/model_doc/markuplm.md", "generated_answer": "MarkupLMFeatureExtractor uses the Python library Beautiful Soup to extract data from HTML and XML files. (Source: Extracted documents, line 0)", "retrieved_docs": ["In short, one can provide HTML strings (and possibly additional data) to [`MarkupLMProcessor`],\nand it will create the inputs expected by the model. Internally, the processor first uses\n[`MarkupLMFeatureExtractor`] to get a list of nodes and corresponding xpaths. The nodes and\nxpaths are then provided to [`MarkupLMTokenizer`] or [`MarkupLMTokenizerFast`], which converts them\nto token-level `input_ids`, `attention_mask`, `token_type_ids`, `xpath_subs_seq`, `xpath_tags_seq`.\nOptionally, one can provide node labels to the processor, which are turned into token-level `labels`.\n\n[`MarkupLMFeatureExtractor`] uses [Beautiful Soup](https://www.crummy.com/software/BeautifulSoup/bs4/doc/), a Python library for\npulling data out of HTML and XML files, under the hood. Note that you can still use your own parsing solution of\nchoice, and provide the nodes and xpaths yourself to [`MarkupLMTokenizer`] or [`MarkupLMTokenizerFast`].\n\nIn total, there are 5 use cases that are supported by the processor. Below, we list them all. Note that each of these\nuse cases work for both batched and non-batched inputs (we illustrate them for non-batched inputs).\n\n**Use case 1: web page classification (training, inference) + token classification (inference), parse_html = True**", "## Usage: MarkupLMProcessor\n\nThe easiest way to prepare data for the model is to use [`MarkupLMProcessor`], which internally combines a feature extractor\n([`MarkupLMFeatureExtractor`]) and a tokenizer ([`MarkupLMTokenizer`] or [`MarkupLMTokenizerFast`]). The feature extractor is\nused to extract all nodes and xpaths from the HTML strings, which are then provided to the tokenizer, which turns them into the\ntoken-level inputs of the model (`input_ids` etc.). Note that you can still use the feature extractor and tokenizer separately,\nif you only want to handle one of the two tasks.\n\n```python\nfrom transformers import MarkupLMFeatureExtractor, MarkupLMTokenizerFast, MarkupLMProcessor\n\nfeature_extractor = MarkupLMFeatureExtractor()\ntokenizer = MarkupLMTokenizerFast.from_pretrained(\"microsoft/markuplm-base\")\nprocessor = MarkupLMProcessor(feature_extractor, tokenizer)\n```", "**Use case 1: web page classification (training, inference) + token classification (inference), parse_html = True**\n\nThis is the simplest case, in which the processor will use the feature extractor to get all nodes and xpaths from the HTML.\n\n```python\n>>> from transformers import MarkupLMProcessor\n\n>>> processor = MarkupLMProcessor.from_pretrained(\"microsoft/markuplm-base\")\n\n>>> html_string = \"\"\"\n...  <!DOCTYPE html>\n...  <html>\n...  <head>\n...  <title>Hello world</title>\n...  </head>\n...  <body>\n...  <h1>Welcome</h1>\n...  <p>Here is my website.</p>\n...  </body>\n...  </html>\"\"\"\n\n>>> # note that you can also add provide all tokenizer parameters here such as padding, truncation\n>>> encoding = processor(html_string, return_tensors=\"pt\")\n>>> print(encoding.keys())\ndict_keys(['input_ids', 'token_type_ids', 'attention_mask', 'xpath_tags_seq', 'xpath_subs_seq'])\n```\n\n**Use case 2: web page classification (training, inference) + token classification (inference), parse_html=False**", "**Use case 2: web page classification (training, inference) + token classification (inference), parse_html=False**\n\nIn case one already has obtained all nodes and xpaths, one doesn't need the feature extractor. In that case, one should\nprovide the nodes and corresponding xpaths themselves to the processor, and make sure to set `parse_html` to `False`.\n\n```python\n>>> from transformers import MarkupLMProcessor\n\n>>> processor = MarkupLMProcessor.from_pretrained(\"microsoft/markuplm-base\")\n>>> processor.parse_html = False\n\n>>> nodes = [\"hello\", \"world\", \"how\", \"are\"]\n>>> xpaths = [\"/html/body/div/li[1]/div/span\", \"/html/body/div/li[1]/div/span\", \"html/body\", \"html/body/div\"]\n>>> encoding = processor(nodes=nodes, xpaths=xpaths, return_tensors=\"pt\")\n>>> print(encoding.keys())\ndict_keys(['input_ids', 'token_type_ids', 'attention_mask', 'xpath_tags_seq', 'xpath_subs_seq'])\n```\n\n**Use case 3: token classification (training), parse_html=False**", "[[autodoc]] MarkupLMFeatureExtractor\n    - __call__\n\n## MarkupLMTokenizer\n\n[[autodoc]] MarkupLMTokenizer\n    - build_inputs_with_special_tokens\n    - get_special_tokens_mask\n    - create_token_type_ids_from_sequences\n    - save_vocabulary\n\n## MarkupLMTokenizerFast\n\n[[autodoc]] MarkupLMTokenizerFast\n    - all\n\n## MarkupLMProcessor\n\n[[autodoc]] MarkupLMProcessor\n    - __call__\n\n## MarkupLMModel\n\n[[autodoc]] MarkupLMModel\n    - forward\n\n## MarkupLMForSequenceClassification\n\n[[autodoc]] MarkupLMForSequenceClassification\n    - forward\n\n## MarkupLMForTokenClassification\n\n[[autodoc]] MarkupLMForTokenClassification\n    - forward\n\n## MarkupLMForQuestionAnswering\n\n[[autodoc]] MarkupLMForQuestionAnswering\n    - forward", "!--Copyright 2021 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\n\nhttp://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License.\n\n\u26a0\ufe0f Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\nrendered properly in your Markdown viewer.\n\n-->\n\n# Feature Extractor\n\nA feature extractor is in charge of preparing input features for audio or vision models. This includes feature extraction from sequences, e.g., pre-processing audio files to generate Log-Mel Spectrogram features, feature extraction from images, e.g., cropping image files, but also padding, normalization, and conversion to NumPy, PyTorch, and TensorFlow tensors.\n\n\n## FeatureExtractionMixin\n\n[[autodoc]] feature_extraction_utils.FeatureExtractionMixin\n    - from_pretrained\n    - save_pretrained\n\n## SequenceFeatureExtractor\n\n[[autodoc]] SequenceFeatureExtractor\n    - pad\n\n## BatchFeature\n\n[[autodoc]] BatchFeature", "```py\n>>> from transformers import Wav2Vec2FeatureExtractor\n\n>>> w2v2_extractor = Wav2Vec2FeatureExtractor(sampling_rate=8000, do_normalize=False)\n>>> print(w2v2_extractor)\nWav2Vec2FeatureExtractor {\n  \"do_normalize\": false,\n  \"feature_extractor_type\": \"Wav2Vec2FeatureExtractor\",\n  \"feature_size\": 1,\n  \"padding_side\": \"right\",\n  \"padding_value\": 0.0,\n  \"return_attention_mask\": false,\n  \"sampling_rate\": 8000\n}\n```\n\n\n## Processor\n\nFor models that support multimodal tasks, \ud83e\udd17 Transformers offers a processor class that conveniently wraps processing classes such as a feature extractor and a tokenizer into a single object. For example, let's use the [`Wav2Vec2Processor`] for an automatic speech recognition task (ASR). ASR transcribes audio to text, so you will need a feature extractor and a tokenizer.\n\nCreate a feature extractor to handle the audio inputs:\n\n```py\n>>> from transformers import Wav2Vec2FeatureExtractor\n\n>>> feature_extractor = Wav2Vec2FeatureExtractor(padding_value=1.0, do_normalize=True)\n```\n\nCreate a tokenizer to handle the text inputs:"], "eval_score_GPT4": "5", "eval_feedback_GPT4": "Feedback: The response correctly identifies Beautiful Soup as the Python library used by MarkupLMFeatureExtractor to extract data from HTML and XML files. The information provided is accurate and factual, aligning with the reference answer."}, {"question": "What is the file size limit for syncing to HF Spaces without using Git-LFS?\n", "true_answer": "10MB", "source_doc": "huggingface/hub-docs/blob/main/docs/hub/spaces-github-actions.md", "generated_answer": "The file size limit for syncing to HF Spaces without using Git-LFS is 10MB. For files larger than 10MB, Spaces requires Git-LFS. If you don't want to use Git-LFS, you may need to review your files and check your history using a tool like BFG Repo-Cleaner to remove any large files from your history. BFG Repo-Cleaner will keep a local copy of", "retrieved_docs": ["Managing Spaces with Github Actions\n\nYou can keep your app in sync with your GitHub repository with **Github Actions**. Remember that for files larger than 10MB, Spaces requires Git-LFS. If you don't want to use Git-LFS, you may need to review your files and check your history. Use a tool like [BFG Repo-Cleaner](https://rtyley.github.io/bfg-repo-cleaner/) to remove any large files from your history. BFG Repo-Cleaner will keep a local copy of your repository as a backup.\n\nFirst, you should set up your GitHub repository and Spaces app together. Add your Spaces app as an additional remote to your existing Git repository.\n\n```bash\ngit remote add space https://huggingface.co/spaces/HF_USERNAME/SPACE_NAME\n```\n\nThen force push to sync everything for the first time:\n\n```bash\ngit push --force space main\n```\n\nNext, set up a GitHub Action to push your main branch to Spaces. In the example below:\n\n* Replace `HF_USERNAME` with your username and `SPACE_NAME` with your Space name. \n* Create a [Github secret](https://docs.github.com/en/actions/security-guides/encrypted-secrets#creating-encrypted-secrets-for-an-environment) with your `HF_TOKEN`. You can find your Hugging Face API token under **API Tokens** on your Hugging Face profile.", "happen when streaming data and smaller files avoid resuming from the beginning in case of errors.\n    - Files are served to the users using CloudFront. From our experience, huge files are not cached by this service\n      leading to a slower download speed.\nIn all cases no single LFS file will be able to be >50GB. I.e. 50GB is the hard limit for single file size.\n- **Number of commits**: There is no hard limit for the total number of commits on your repo history. However, from\nour experience, the user experience on the Hub starts to degrade after a few thousand commits. We are constantly working to\nimprove the service, but one must always remember that a git repository is not meant to work as a database with a lot of\nwrites. If your repo's history gets very large, it is always possible to squash all the commits to get a\nfresh start using `huggingface_hub`'s [`super_squash_history`](https://huggingface.co/docs/huggingface_hub/main/en/package_reference/hf_api#huggingface_hub.HfApi.super_squash_history). Be aware that this is a non-revertible operation.\n- **Number of operations per commit**: Once again, there is no hard limit here. When a commit is uploaded on the Hub, each\ngit operation (addition or delete) is checked by the server. When a hundred LFS files are committed at once,\neach file is checked individually to ensure it's been correctly uploaded. When pushing data through HTTP,", "```yaml\nname: Sync to Hugging Face hub\non:\n  push:\n    branches: [main]\n\n  # to run this workflow manually from the Actions tab\n  workflow_dispatch:\n\njobs:\n  sync-to-hub:\n    runs-on: ubuntu-latest\n    steps:\n      - uses: actions/checkout@v3\n        with:\n          fetch-depth: 0\n          lfs: true\n      - name: Push to hub\n        env:\n          HF_TOKEN: ${{ secrets.HF_TOKEN }}\n        run: git push https://HF_USERNAME:$HF_TOKEN@huggingface.co/spaces/HF_USERNAME/SPACE_NAME main\n```\n\nFinally, create an Action that automatically checks the file size of any new pull request:\n\n\n```yaml\nname: Check file size\non:               # or directly `on: [push]` to run the action on every push on any branch\n  pull_request:\n    branches: [main]\n\n  # to run this workflow manually from the Actions tab\n  workflow_dispatch:\n\njobs:\n  sync-to-hub:\n    runs-on: ubuntu-latest\n    steps:\n      - name: Check large files\n        uses: ActionsDesk/lfs-warning@v2.0\n        with:\n          filesizelimit: 10485760 # this is 10MB so we can sync to HF Spaces\n```", "<Tip>\n\nProgress bars are supported in `hf_transfer` starting from version `0.1.4`. Consider upgrading (`pip install -U hf-transfer`) if you plan to enable faster uploads.\n\n</Tip>\n\n## (legacy) Upload files with Git LFS\n\nAll the methods described above use the Hub's API to upload files. This is the recommended way to upload files to the Hub.\nHowever, we also provide [`Repository`], a wrapper around the git tool to manage a local repository.\n\n<Tip warning={true}>\n\nAlthough [`Repository`] is not formally deprecated, we recommend using the HTTP-based methods described above instead.\nFor more details about this recommendation, please have a look at [this guide](../concepts/git_vs_http) explaining the\ncore differences between HTTP-based and Git-based approaches.\n\n</Tip>\n\nGit LFS automatically handles files larger than 10MB. But for very large files (>5GB), you need to install a custom transfer agent for Git LFS:\n\n```bash\nhuggingface-cli lfs-enable-largefiles\n```\n\nYou should install this for each repository that has a very large file. Once installed, you'll be able to push files larger than 5GB.\n\n### commit context manager\n\nThe `commit` context manager handles four of the most common Git commands: pull, add, commit, and push. `git-lfs` automatically tracks any file larger than 10MB. In the following example, the `commit` context manager:", "Or login in from the terminal:\n\n```bash\nhuggingface-cli login\n```\n\nSince the model checkpoints are quite large, install [Git-LFS](https://git-lfs.com/) to version these large files:\n\n```bash\n!sudo apt -qq install git-lfs\n!git config --global credential.helper store\n```\n\n## Training configuration\n\nFor convenience, create a `TrainingConfig` class containing the training hyperparameters (feel free to adjust them):\n\n```py\n>>> from dataclasses import dataclass\n\n>>> @dataclass\n... class TrainingConfig:\n...     image_size = 128  # the generated image resolution\n...     train_batch_size = 16\n...     eval_batch_size = 16  # how many images to sample during evaluation\n...     num_epochs = 50\n...     gradient_accumulation_steps = 1\n...     learning_rate = 1e-4\n...     lr_warmup_steps = 500\n...     save_image_epochs = 10\n...     save_model_epochs = 30\n...     mixed_precision = \"fp16\"  # `no` for float32, `fp16` for automatic mixed precision\n...     output_dir = \"ddpm-butterflies-128\"  # the model name locally and on the HF Hub", "We'll take a look at how to add some new files next.\n\n## Uploading the model files[[uploading-the-model-files]]\n\nThe system to manage files on the Hugging Face Hub is based on git for regular files, and git-lfs (which stands for [Git Large File Storage](https://git-lfs.github.com/)) for larger files. \n\nIn the next section, we go over three different ways of uploading files to the Hub: through `huggingface_hub` and through git commands.\n\n### The `upload_file` approach[[the-uploadfile-approach]]\n\nUsing `upload_file` does not require git and git-lfs to be installed on your system. It pushes files directly to the \ud83e\udd17 Hub using HTTP POST requests. A limitation of this approach is that it doesn't handle files that are larger than 5GB in size.\nIf your files are larger than 5GB, please follow the two other methods detailed below.\n\nThe API may be used as follows:\n\n```py\nfrom huggingface_hub import upload_file\n\nupload_file(\n    \"<path_to_file>/config.json\",\n    path_in_repo=\"config.json\",\n    repo_id=\"<namespace>/dummy-model\",\n)\n```", "However, in some cases you want to download files and move them to a specific folder. This is useful to get a workflow\ncloser to what `git` commands offer. You can do that using the `local_dir` and `local_dir_use_symlinks` parameters:\n- `local_dir` must be a path to a folder on your system. The downloaded files will keep the same file structure as in the\nrepo. For example if `filename=\"data/train.csv\"` and `local_dir=\"path/to/folder\"`, then the returned filepath will be\n`\"path/to/folder/data/train.csv\"`.\n- `local_dir_use_symlinks` defines how the file must be saved in your local folder.\n  - The default behavior (`\"auto\"`) is to duplicate small files (<5MB) and use symlinks for bigger files. Symlinks allow\n    to optimize both bandwidth and disk usage. However manually editing a symlinked file might corrupt the cache, hence\n    the duplication for small files. The 5MB threshold can be configured with the `HF_HUB_LOCAL_DIR_AUTO_SYMLINK_THRESHOLD`\n    environment variable.\n  - If `local_dir_use_symlinks=True` is set, all files are symlinked for an optimal disk space optimization. This is\n    for example useful when downloading a huge dataset with thousands of small files."], "eval_score_GPT4": "5", "eval_feedback_GPT4": "The response provided is completely correct, accurate, and factual. It aligns with the reference answer by stating the file size limit is 10MB and further explains the necessity of using Git-LFS for larger files, which is a valuable addition to the answer."}, {"question": "What is the title of the paper introducing the ByT5 model?\n", "true_answer": "ByT5: Towards a token-free future with pre-trained byte-to-byte models", "source_doc": "huggingface/transformers/blob/main/docs/source/en/model_doc/byt5.md", "generated_answer": "The title of the paper introducing the ByT5 model is \"ByT5: Towards a token-free future with pre-trained byte-to-byte models\" (arXiv:2105.13626).", "retrieved_docs": ["1. **[ByT5](https://huggingface.co/docs/transformers/model_doc/byt5)** (from Google Research) released with the paper [ByT5: Towards a token-free future with pre-trained byte-to-byte models](https://arxiv.org/abs/2105.13626) by Linting Xue, Aditya Barua, Noah Constant, Rami Al-Rfou, Sharan Narang, Mihir Kale, Adam Roberts, Colin Raffel.\n1. **[CamemBERT](https://huggingface.co/docs/transformers/model_doc/camembert)** (from Inria/Facebook/Sorbonne) released with the paper [CamemBERT: a Tasty French Language Model](https://arxiv.org/abs/1911.03894) by Louis Martin*, Benjamin Muller*, Pedro Javier Ortiz Su\u00e1rez*, Yoann Dupont, Laurent Romary, \u00c9ric Villemonte de la Clergerie, Djam\u00e9 Seddah and Beno\u00eet Sagot.\n1. **[CANINE](https://huggingface.co/docs/transformers/model_doc/canine)** (from Google Research) released with the paper [CANINE: Pre-training an Efficient Tokenization-Free Encoder for Language Representation](https://arxiv.org/abs/2103.06874) by Jonathan H. Clark, Dan Garrette, Iulia Turc, John Wieting.", "1. **[ByT5](https://huggingface.co/docs/transformers/model_doc/byt5)** (from Google Research) released with the paper [ByT5: Towards a token-free future with pre-trained byte-to-byte models](https://arxiv.org/abs/2105.13626) by Linting Xue, Aditya Barua, Noah Constant, Rami Al-Rfou, Sharan Narang, Mihir Kale, Adam Roberts, Colin Raffel.\n1. **[CamemBERT](https://huggingface.co/docs/transformers/model_doc/camembert)** (from Inria/Facebook/Sorbonne) released with the paper [CamemBERT: a Tasty French Language Model](https://arxiv.org/abs/1911.03894) by Louis Martin*, Benjamin Muller*, Pedro Javier Ortiz Su\u00e1rez*, Yoann Dupont, Laurent Romary, \u00c9ric Villemonte de la Clergerie, Djam\u00e9 Seddah and Beno\u00eet Sagot.\n1. **[CANINE](https://huggingface.co/docs/transformers/model_doc/canine)** (from Google Research) released with the paper [CANINE: Pre-training an Efficient Tokenization-Free Encoder for Language Representation](https://arxiv.org/abs/2103.06874) by Jonathan H. Clark, Dan Garrette, Iulia Turc, John Wieting.", "<Tip>\n\nByT5's architecture is based on the T5v1.1 model, refer to [T5v1.1's documentation page](t5v1.1) for the API reference. They\nonly differ in how inputs should be prepared for the model, see the code examples below.\n\n</Tip>\n\nSince ByT5 was pre-trained unsupervisedly, there's no real advantage to using a task prefix during single-task\nfine-tuning. If you are doing multi-task fine-tuning, you should use a prefix.\n\n\n## Usage example\n\nByT5 works on raw UTF-8 bytes, so it can be used without a tokenizer:\n\n```python\n>>> from transformers import T5ForConditionalGeneration\n>>> import torch\n\n>>> model = T5ForConditionalGeneration.from_pretrained(\"google/byt5-small\")\n\n>>> num_special_tokens = 3\n>>> # Model has 3 special tokens which take up the input ids 0,1,2 of ByT5.\n>>> # => Need to shift utf-8 character encodings by 3 before passing ids to model.\n\n>>> input_ids = torch.tensor([list(\"Life is like a box of chocolates.\".encode(\"utf-8\"))]) + num_special_tokens", "- [t5-base](https://huggingface.co/t5-base)\n\n- [t5-large](https://huggingface.co/t5-large)\n\n- [t5-3b](https://huggingface.co/t5-3b)\n\n- [t5-11b](https://huggingface.co/t5-11b).\n\nBased on the original T5 model, Google has released some follow-up works:\n\n- **T5v1.1**: T5v1.1 is an improved version of T5 with some architectural tweaks, and is pre-trained on C4 only without\n  mixing in the supervised tasks. Refer to the documentation of T5v1.1 which can be found [here](t5v1.1).\n\n- **mT5**: mT5 is a multilingual T5 model. It is pre-trained on the mC4 corpus, which includes 101 languages. Refer to\n  the documentation of mT5 which can be found [here](mt5).\n\n- **byT5**: byT5 is a T5 model pre-trained on byte sequences rather than SentencePiece subword token sequences. Refer\n  to the documentation of byT5 which can be found [here](byt5).\n\n- **UL2**: UL2 is a T5 like model pretrained on various denoising objectives", ">>> loss = model(**model_inputs, labels=labels).loss\n>>> loss.item()\n17.9\n```\n\nSimilar to [T5](t5), ByT5 was trained on the span-mask denoising task. However, \nsince the model works directly on characters, the pretraining task is a bit \ndifferent. Let's corrupt some characters of the \ninput sentence `\"The dog chases a ball in the park.\"` and ask ByT5 to predict them \nfor us.\n\n```python\n>>> from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n>>> import torch\n\n>>> tokenizer = AutoTokenizer.from_pretrained(\"google/byt5-base\")\n>>> model = AutoModelForSeq2SeqLM.from_pretrained(\"google/byt5-base\")\n\n>>> input_ids_prompt = \"The dog chases a ball in the park.\"\n>>> input_ids = tokenizer(input_ids_prompt).input_ids", "## Introduction\n\nSpeechT5 is not one, not two, but three kinds of speech models in one architecture.\n\nIt can do:\n\n- **speech-to-text** for automatic speech recognition or speaker identification,\n- **text-to-speech** to synthesize audio, and\n- **speech-to-speech** for converting between different voices or performing speech enhancement.\n\nThe main idea behind SpeechT5 is to pre-train a single model on a mixture of text-to-speech, speech-to-text, text-to-text, and speech-to-speech data. This way, the model learns from text and speech at the same time. The result of this pre-training approach is a model that has a **unified space** of hidden representations shared by both text and speech.\n\nAt the heart of SpeechT5 is a regular **Transformer encoder-decoder** model. Just like any other Transformer, the encoder-decoder network models a sequence-to-sequence transformation using hidden representations. This Transformer backbone is the same for all SpeechT5 tasks.\n\nTo make it possible for the same Transformer to deal with both text and speech data, so-called **pre-nets** and **post-nets** were added. It is the job of the pre-net to convert the input text or speech into the hidden representations used by the Transformer. The post-net takes the outputs from the Transformer and turns them into text or speech again.\n\nA figure illustrating SpeechT5\u2019s architecture is depicted below (taken from the [original paper](https://arxiv.org/abs/2110.07205)).", "<div class=\"blog-caption\" style=\"max-height: 100%; max-width: 90%; margin-left: auto; margin-right: auto; line-height: 1.4;\">\n<p>An overview of the ViT model structure as introduced in <a href=\"https://arxiv.org/abs/2010.11929\" rel=\"noopener\" target=\"_blank\">Google Research\u2019s original 2021 paper</a></p>\n</div>"], "eval_score_GPT4": "5", "eval_feedback_GPT4": "Feedback: The response provided is completely correct, accurate, and factual. It matches the reference answer exactly and includes additional helpful information such as the arXiv identifier, which is not required but adds value to the response."}, {"question": "What is the dimension of the feature vector for the base BERT model?\n", "true_answer": "768", "source_doc": "huggingface/course/blob/main/subtitles/en/raw/chapter1/05_encoders.md", "generated_answer": "The dimension of the feature vector for the base BERT model is 768, as stated in the context provided. This vector represents the contextualized meaning of a word within its surrounding context, and is output by the self-attention mechanism in the encoder layer of the BERT model. The dimension is defined by the architecture of the model and can be found in the configuration (config) object of the BERT model. In the case of the base BERT model, the", "retrieved_docs": ["Let's dive in this representation. It contains one vector per word that was passed through the encoder. Each of these vector is a numerical representation of the word in question. The dimension of that vector is defined by the architecture of the model, for the base BERT model, it is 768. These representations contain the value of a word; but contextualized. For example, the vector attributed to the word \"to\", isn't the representation of only the \"to\" word. It also takes into account the words around it, which we call the \u201ccontext\u201d.As in, it looks to the left context, the word on the left of the one we're studying (here the word \"Welcome\") and the context on the right (here the word \"NYC\") and outputs a value for the word, within its context. It is therefore a contextualized value. One could say that the vector of 768 values holds the \"meaning\" of that word in the text. How it does this is thanks to the self-attention mechanism. The self-attention mechanism relates to different positions (or different words) in a single sequence, in order to compute a representation of that sequence. As we've seen before, this means that the resulting representation of a word has been affected by other words in the sequence. We won't dive into the specifics here, but we'll offer some further readings if you want to get a better understanding at what happens under the hood. So when should one use an encoder? Encoders can be used as standalone models in a wide variety of tasks", ">>> model = AutoModel.from_pretrained(\"bert-base-uncased\")\n>>> print(model)  # doctest: +IGNORE_RESULT\n...\n          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n      )\n      (11): BertLayer(\n        (attention): BertAttention(\n          (self): BertSelfAttention(\n            (query): Linear(in_features=768, out_features=768, bias=True)\n            (key): Linear(in_features=768, out_features=768, bias=True)\n            (value): Linear(in_features=768, out_features=768, bias=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (output): BertSelfOutput(\n            (dense): Linear(in_features=768, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )", "The results of the BERT-base model that is trained on MNLI using batch size 8 and the random seed 42 on the HANS dataset is as follows:\n\n```bash\nHeuristic entailed results:\nlexical_overlap: 0.9702\nsubsequence: 0.9942\nconstituent: 0.9962\n\nHeuristic non-entailed results:\nlexical_overlap: 0.199\nsubsequence: 0.0396\nconstituent: 0.118\n```", "<Tip>\n\n\u270f\ufe0f **Try it out!** Some Transformer models, like [BigBird](https://huggingface.co/google/bigbird-roberta-base) and [Longformer](hf.co/allenai/longformer-base-4096), have a much longer context length than BERT and other early Transformer models. Instantiate the tokenizer for one of these checkpoints and verify that the `model_max_length` agrees with what's quoted on its model card.\n\n</Tip>\n\nSo, in order to run our experiments on GPUs like those found on Google Colab, we'll pick something a bit smaller that can fit in memory:\n\n```python\nchunk_size = 128\n```\n\n<Tip warning={true}>\n\nNote that using a small chunk size can be detrimental in real-world scenarios, so you should use a size that corresponds to the use case you will apply your model to.\n\n</Tip>\n\nNow comes the fun part. To show how the concatenation works, let's take a few reviews from our tokenized training set and print out the number of tokens per review:\n\n```python\n# Slicing produces a list of lists for each feature\ntokenized_samples = tokenized_datasets[\"train\"][:3]\n\nfor idx, sample in enumerate(tokenized_samples[\"input_ids\"]):\n    print(f\"'>>> Review {idx} length: {len(sample)}'\")\n```", "##### Base models fine-tuning\n\n```bash\nexport CUDA_VISIBLE_DEVICES=0,1,2,3,4,5,6,7\ntorchrun --nproc_per_node=8 ./examples/question-answering/run_squad.py \\\n    --model_name_or_path zhiheng-huang/bert-base-uncased-embedding-relative-key-query \\\n    --dataset_name squad \\\n    --do_train \\\n    --do_eval \\\n    --learning_rate 3e-5 \\\n    --num_train_epochs 2 \\\n    --max_seq_length 512 \\\n    --doc_stride 128 \\\n    --output_dir relative_squad \\\n    --per_device_eval_batch_size=60 \\\n    --per_device_train_batch_size=6\n```\nTraining with the above command leads to the following results. It boosts the BERT default from f1 score of 88.52 to 90.54.\n\n```bash\n'exact': 83.6802270577105, 'f1': 90.54772098174814\n```\n\nThe change of `max_seq_length` from 512 to 384 in the above command leads to the f1 score of 90.34. Replacing the above \nmodel `zhiheng-huang/bert-base-uncased-embedding-relative-key-query` with \n`zhiheng-huang/bert-base-uncased-embedding-relative-key` leads to the f1 score of 89.51. The changing of 8 gpus to one \ngpu training leads to the f1 score of 90.71.", "The result is a matrix of shape (number of examples, hidden size). The hidden size is the number of dimensions in the hidden representation. For BERT, the hidden size is 768. The hidden representation is a vector of numbers that represents the text that can be used for many different tasks. In this case, we will use it for classification with [XGBoost](https://github.com/dmlc/xgboost) afterwards.\n\n```python\nimport numpy as np\nimport tqdm\n# Function that transforms a list of texts to their representation\n# learned by the transformer.\ndef text_to_tensor(\n   list_text_X_train: list,\n   transformer_model: AutoModelForSequenceClassification,\n   tokenizer: AutoTokenizer,\n   device: str,\n) -> np.ndarray:\n   # Tokenize each text in the list one by one\n   tokenized_text_X_train_split = []\n   tokenized_text_X_train_split = [\n       tokenizer.encode(text_x_train, return_tensors=\"pt\")\n       for text_x_train in list_text_X_train\n   ]\n\n   # Send the model to the device\n   transformer_model = transformer_model.to(device)\n   output_hidden_states_list = [None] * len(tokenized_text_X_train_split)", "---\n \\\\( {}^1 \\\\) For a simpler explanation, the layer norm layer which is normally applied to \\\\( \\mathbf{\\overline{Z}} \\\\) before being processed by the feed forward layers is omitted for now.\n\n \\\\( {}^2 \\\\) In `bert-base-uncased`, *e.g.* the intermediate dimension \\\\( d_{f} \\\\) is with 3072 four times larger than the output dimension \\\\( d_{h} \\\\).\n\n \\\\( {}^3 \\\\) As a reminder, the output `config.num_attention_heads` is assumed to be 1 for the sake of clarity and illustration in this notebook, so that the output of the self-attention layers can be assumed to be of size `config.hidden_size`.\n\nMore information on chunked linear / feed forward layers can also be found [here](https://huggingface.co/transformers/glossary.html#feed-forward-chunking) on the \ud83e\udd17Transformers docs.\n\n\n### Benchmark\n\nLet's test how much memory can be saved by using chunked feed forward layers.\n\n\n```\n#@title Installs and Imports\n# pip installs\n!pip -qq install git+https://github.com/huggingface/transformers.git\n!pip install -qq py3nvml\n\nfrom transformers import ReformerConfig, PyTorchBenchmark, PyTorchBenchmarkArguments\n```\n\n      Building wheel for transformers (setup.py) ... \u001b[?25l\u001b[?25hdone"], "eval_score_GPT4": "5", "eval_feedback_GPT4": "Feedback: The response correctly identifies the dimension of the feature vector for the base BERT model as 768, which is in line with the reference answer. The additional explanation provided about the contextualized meaning of a word and the self-attention mechanism in the encoder layer of the BERT model, while not required, does not detract from the accuracy of the response. The response is factual and aligns with the reference answer, thus meeting the criteria for the highest score on the rubric."}, {"question": "What special identifier does the WordPiece Model use for continuing subwords?\n", "true_answer": "##", "source_doc": "huggingface/tokenizers/blob/main/docs/source-doc-builder/components.mdx", "generated_answer": "The special identifier that the WordPiece Model uses for continuing subwords is `##`. (Source: 1)", "retrieved_docs": ["| Name | Description |\n| :--- | :--- |\n| ByteLevel | Reverts the ByteLevel PreTokenizer. This PreTokenizer encodes at the byte-level, using a set of visible Unicode characters to represent each byte, so we need a Decoder to revert this process and get something readable again. |\n| Metaspace | Reverts the Metaspace PreTokenizer. This PreTokenizer uses a special identifer `\u2581` to identify whitespaces, and so this Decoder helps with decoding these. |\n| WordPiece | Reverts the WordPiece Model. This model uses a special identifier `##` for continuing subwords, and so this Decoder helps with decoding these. |", "<a id='wordpiece'></a>\n\n### WordPiece\n\nWordPiece is the subword tokenization algorithm used for [BERT](model_doc/bert), [DistilBERT](model_doc/distilbert), and [Electra](model_doc/electra). The algorithm was outlined in [Japanese and Korean\nVoice Search (Schuster et al., 2012)](https://static.googleusercontent.com/media/research.google.com/ja//pubs/archive/37842.pdf) and is very similar to\nBPE. WordPiece first initializes the vocabulary to include every character present in the training data and\nprogressively learns a given number of merge rules. In contrast to BPE, WordPiece does not choose the most frequent\nsymbol pair, but the one that maximizes the likelihood of the training data once added to the vocabulary.\n\nSo what does this mean exactly? Referring to the previous example, maximizing the likelihood of the training data is\nequivalent to finding the symbol pair, whose probability divided by the probabilities of its first symbol followed by\nits second symbol is the greatest among all symbol pairs. *E.g.* `\"u\"`, followed by `\"g\"` would have only been\nmerged if the probability of `\"ug\"` divided by `\"u\"`, `\"g\"` would have been greater than for any other symbol\npair. Intuitively, WordPiece is slightly different to BPE in that it evaluates what it _loses_ by merging two symbols\nto ensure it's _worth it_.\n\n<a id='unigram'></a>\n\n### Unigram", "</Tip>\n\nLike BPE, WordPiece starts from a small vocabulary including the special tokens used by the model and the initial alphabet. Since it identifies subwords by adding a prefix (like `##` for BERT), each word is initially split by adding that prefix to all the characters inside the word. So, for instance, `\"word\"` gets split like this:\n\n```\nw ##o ##r ##d\n```\n\nThus, the initial alphabet contains all the characters present at the beginning of a word and the characters present inside a word preceded by the WordPiece prefix.\n\nThen, again like BPE, WordPiece learns merge rules. The main difference is the way the pair to be merged is selected. Instead of selecting the most frequent pair, WordPiece computes a score for each pair, using the following formula:\n\n$$\\mathrm{score} = (\\mathrm{freq\\_of\\_pair}) / (\\mathrm{freq\\_of\\_first\\_element} \\times \\mathrm{freq\\_of\\_second\\_element})$$", "On this page, we will have a closer look at tokenization.\n\n<Youtube id=\"VFp38yj8h3A\"/>\n\nAs we saw in [the preprocessing tutorial](preprocessing), tokenizing a text is splitting it into words or\nsubwords, which then are converted to ids through a look-up table. Converting words or subwords to ids is\nstraightforward, so in this summary, we will focus on splitting a text into words or subwords (i.e. tokenizing a text).\nMore specifically, we will look at the three main types of tokenizers used in \ud83e\udd17 Transformers: [Byte-Pair Encoding\n(BPE)](#byte-pair-encoding), [WordPiece](#wordpiece), and [SentencePiece](#sentencepiece), and show examples\nof which tokenizer type is used by which model.\n\nNote that on each model page, you can look at the documentation of the associated tokenizer to know which tokenizer\ntype was used by the pretrained model. For instance, if we look at [`BertTokenizer`], we can see\nthat the model uses [WordPiece](#wordpiece).\n\n## Introduction\n\nSplitting a text into smaller chunks is a task that is harder than it looks, and there are multiple ways of doing so.\nFor instance, let's look at the sentence `\"Don't you love \ud83e\udd17 Transformers? We sure do.\"`\n\n<Youtube id=\"nhJxYji1aho\"/>\n\nA simple way of tokenizing this text is to split it by spaces, which would give:", "### 9. Select the sentences that apply to the WordPiece model of tokenization.\n\n<Question\n\tchoices={[\n\t\t{\n\t\t\ttext: \"WordPiece is a subword tokenization algorithm that starts with a small vocabulary and learns merge rules.\",\n\t\t\texplain: \"That's the case indeed!\",\n\t\t\tcorrect: true\n\t\t},\n\t\t{\n\t\t\ttext: \"WordPiece is a subword tokenization algorithm that starts with a big vocabulary and progressively removes tokens from it.\",\n\t\t\texplain: \"No, that's the approach taken by a different tokenization algorithm.\"\n\t\t},\n\t\t{\n\t\t\ttext: \"WordPiece tokenizers learn merge rules by merging the pair of tokens that is the most frequent.\",\n\t\t\texplain: \"No, that's the strategy applied by another tokenization algorithm.\"\n\t\t},\n\t\t{\n\t\t\ttext: \"A WordPiece tokenizer learns a merge rule by merging the pair of tokens that maximizes a score that privileges frequent pairs with less frequent individual parts.\",\n\t\t\texplain: \"That's correct!\",\n\t\t\tcorrect: true\n\t\t},\n\t\t{\n\t\t\ttext: \"WordPiece tokenizes words into subwords by finding the most likely segmentation into tokens, according to the model.\",\n\t\t\texplain: \"No, that's how another tokenization algorithm works.\"\n\t\t},\n\t\t{\n\t\t\ttext: \"WordPiece tokenizes words into subwords by finding the longest subword starting from the beginning that is in the vocabulary, then repeating the process for the rest of the text.\",\n\t\t\texplain: \"Yes, this is how WordPiece proceeds for the encoding.\",\n\t\t\tcorrect: true\n\t\t},\n\t]}\n/>\n\n### 10. Select the sentences that apply to the Unigram model of tokenization.", "**Use case 3: token classification (training), parse_html=False**\n\nFor token classification tasks (such as [SWDE](https://paperswithcode.com/dataset/swde)), one can also provide the\ncorresponding node labels in order to train a model. The processor will then convert these into token-level `labels`.\nBy default, it will only label the first wordpiece of a word, and label the remaining wordpieces with -100, which is the\n`ignore_index` of PyTorch's CrossEntropyLoss. In case you want all wordpieces of a word to be labeled, you can\ninitialize the tokenizer with `only_label_first_subword` set to `False`.\n\n```python\n>>> from transformers import MarkupLMProcessor\n\n>>> processor = MarkupLMProcessor.from_pretrained(\"microsoft/markuplm-base\")\n>>> processor.parse_html = False", "<a id='sentencepiece'></a>\n\n### SentencePiece\n\nAll tokenization algorithms described so far have the same problem: It is assumed that the input text uses spaces to\nseparate words. However, not all languages use spaces to separate words. One possible solution is to use language\nspecific pre-tokenizers, *e.g.* [XLM](model_doc/xlm) uses a specific Chinese, Japanese, and Thai pre-tokenizer).\nTo solve this problem more generally, [SentencePiece: A simple and language independent subword tokenizer and\ndetokenizer for Neural Text Processing (Kudo et al., 2018)](https://arxiv.org/pdf/1808.06226.pdf) treats the input\nas a raw input stream, thus including the space in the set of characters to use. It then uses the BPE or unigram\nalgorithm to construct the appropriate vocabulary.\n\nThe [`XLNetTokenizer`] uses SentencePiece for example, which is also why in the example earlier the\n`\"\u2581\"` character was included in the vocabulary. Decoding with SentencePiece is very easy since all tokens can just be\nconcatenated and `\"\u2581\"` is replaced by a space.\n\nAll transformers models in the library that use SentencePiece use it in combination with unigram. Examples of models\nusing SentencePiece are [ALBERT](model_doc/albert), [XLNet](model_doc/xlnet), [Marian](model_doc/marian), and [T5](model_doc/t5)."], "eval_score_GPT4": "5", "eval_feedback_GPT4": "Feedback: The response provided is completely correct, accurate, and factual. It correctly identifies `##` as the special identifier used by the WordPiece Model for continuing subwords, which matches the reference answer."}, {"question": "What is the purpose of the \ud83e\udde8 Diffusers tutorials?\n", "true_answer": "To provide a gentle introduction to diffusion models and help understand the library fundamentals.", "source_doc": "huggingface/diffusers/blob/main/docs/source/en/tutorials/tutorial_overview.md", "generated_answer": "The purpose of the \ud83e\udde8 Diffusers tutorials, as mentioned in the context, is to allow users to experiment, research, or simply play with text-to-image models using the Diffusers library. These tutorials provide step-by-step guides on how to use the library to generate images from text inputs, as well as how to train or finetune models, access and change schedulers, and optimize inference for different hardware platforms. The tutorials are", "retrieved_docs": ["## \ud83e\udde8 Diffusers\n\n[Diffusers](https://huggingface.co/docs/diffusers) provides pre-trained vision and audio diffusion models, and serves as a modular toolbox for inference and training. With this library, you can generate plausible images from natural language inputs amongst other creative use cases. Here is an example:\n\n```py\nfrom diffusers import DiffusionPipeline\n\ngenerator = DiffusionPipeline.from_pretrained(\"CompVis/stable-diffusion-v1-4\")\ngenerator.to(\u201ccuda\u201d)\n\nimage = generator(\"An image of a squirrel in Picasso style\").images[0]\n```\n\n<div align=\"center\">\n<img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/cv_state/sd_output.png\"/>\n</div>\n\nThis type of technology can empower a new generation of creative applications and also aid artists coming from different backgrounds. To know more about Diffusers and the different use cases, check out the [official documentation](https://huggingface.co/docs/diffusers).\n\nThe literature on Diffusion-based models is developing at a rapid pace which is why we partnered with [Jonathan Whitaker](https://github.com/johnowhitaker) to develop a course on it. The course is free, and you can check it out [here](https://github.com/huggingface/diffusion-models-class).\n\n## Support for third-party libraries", "--\ntitle: \"Happy 1st anniversary \ud83e\udd17 Diffusers!\" \nthumbnail: /blog/assets/diffusers-turns-1/diffusers-turns-1.png\nauthors:\n- user: stevhliu\n- user: sayakpaul\n- user: pcuenq\n---\n\n# Happy 1st anniversary \ud83e\udd17 Diffusers!\n\n\n\ud83e\udd17\u00a0Diffusers is happy to celebrate its first anniversary! It has been an exciting year, and we're proud and grateful for how far we've come thanks to our community and open-source contributors. Last year, text-to-image models like DALL-E 2, Imagen, and Stable Diffusion captured the world's attention with their ability to generate stunningly photorealistic images from text, sparking a massive surge of interest and development in generative AI. But access to these powerful models was limited.\n\nAt Hugging Face, our mission is to democratize good machine learning by collaborating and helping each other build an open and ethical AI future together. Our mission motivated us to create the \ud83e\udd17 Diffusers library so *everyone* can experiment, research, or simply play with text-to-image models. That\u2019s why we designed the library as a modular toolbox, so you can customize a diffusion model\u2019s components or just start using it out-of-the-box.\n\nAs \ud83e\udd17 Diffusers turns 1, here\u2019s an overview of some of the most notable features we\u2019ve added to the library with the help of our community. We are proud and immensely grateful for being part of an engaged community that promotes accessible usage, pushes diffusion models beyond just text-to-image generation, and is an all-around inspiration.\n\n**Table of Contents**", "## Next steps\n\nHopefully, you generated some cool images with \ud83e\udde8 Diffusers in this quicktour! For your next steps, you can:\n\n* Train or finetune a model to generate your own images in the [training](./tutorials/basic_training) tutorial.\n* See example official and community [training or finetuning scripts](https://github.com/huggingface/diffusers/tree/main/examples#-diffusers-examples) for a variety of use cases.\n* Learn more about loading, accessing, changing, and comparing schedulers in the [Using different Schedulers](./using-diffusers/schedulers) guide.\n* Explore prompt engineering, speed and memory optimizations, and tips and tricks for generating higher-quality images with the [Stable Diffusion](./stable_diffusion) guide.\n* Dive deeper into speeding up \ud83e\udde8 Diffusers with guides on [optimized PyTorch on a GPU](./optimization/fp16), and inference guides for running [Stable Diffusion on Apple Silicon (M1/M2)](./optimization/mps) and [ONNX Runtime](./optimization/onnx).", "In addition to visual content (images, videos, 3D assets, etc.), we also added support for audio! Check out [the documentation](https://huggingface.co/docs/diffusers/using-diffusers/audio) to learn more.\n\n## Community highlights\n\nOne of the most gratifying experiences of the past year has been seeing how the community is incorporating \ud83e\udd17 Diffusers into their projects. From adapting Low-rank adaptation (LoRA) for faster training of text-to-image models to building a state-of-the-art inpainting tool, here are a few of our favorite projects:", "--\ntitle: Training Stable Diffusion with Dreambooth using Diffusers\nthumbnail: /blog/assets/sd_dreambooth_training/thumbnail.jpg\nauthors:\n- user: valhalla\n- user: pcuenq\n- user: 9of9\n  guest: true\n---\n\n# Training Stable Diffusion with Dreambooth using \ud83e\udde8 Diffusers\n\n\n[Dreambooth](https://dreambooth.github.io/) is a technique to teach new concepts to [Stable Diffusion](https://huggingface.co/blog/stable_diffusion) using a specialized form of fine-tuning. Some people have been using it with a few of their photos to place themselves in fantastic situations, while others are using it to incorporate new styles. [\ud83e\udde8 Diffusers](https://github.com/huggingface/diffusers) provides a Dreambooth [training script](https://github.com/huggingface/diffusers/tree/main/examples/dreambooth). It doesn't take long to train, but it's hard to select the right set of hyperparameters and it's easy to overfit.\n\nWe conducted a lot of experiments to analyze the effect of different settings in Dreambooth. This post presents our findings and some tips to improve your results when fine-tuning Stable Diffusion with Dreambooth.\n\nBefore we start, please be aware that this method should never be used for malicious purposes, to generate harm in any way, or to impersonate people without their knowledge. Models trained with it are still bound by the [CreativeML Open RAIL-M license](https://huggingface.co/spaces/CompVis/stable-diffusion-license) that governs distribution of Stable Diffusion models.", "If you\u2019re building products on top of \ud83e\udd17\u00a0Diffusers, we\u2019d love to chat to understand how we can make the library better together! Feel free to reach out to patrick@hf.co or sayak@hf.co.\n\n## Looking forward\n\nAs we celebrate our first anniversary, we're grateful to our community and open-source contributors who have helped us come so far in such a short time. We're happy to share that we'll be presenting a \ud83e\udd17\u00a0Diffusers demo at ICCV 2023 this fall \u2013 if you're attending, do come and see us! We'll continue to develop and improve our library, making it easier for everyone to use. We're also excited to see what the community will create next with our tools and resources. Thank you for being a part of our journey so far, and we look forward to continuing to democratize good machine learning together! \ud83e\udd73\n\n\u2764\ufe0f Diffusers team\n\n---\n\n**Acknowledgements**: Thank you to [Omar Sanseviero](https://huggingface.co/osanseviero), [Patrick von Platen](https://huggingface.co/patrickvonplaten), [Giada Pistilli](https://huggingface.co/giadap) for their reviews, and [Chunte Lee](https://huggingface.co/Chunte) for designing the thumbnail.", "--\ntitle: What's new in Diffusers? \ud83c\udfa8\nthumbnail: /blog/assets/102_diffusers_2nd_month/inpainting.png\nauthors:\n- user: osanseviero\n---\n\n# What's new in Diffusers? \ud83c\udfa8\n\n\nA month and a half ago we released `diffusers`, a library that provides a modular toolbox for diffusion models across modalities. A couple of weeks later, we released support for Stable Diffusion, a high quality text-to-image model, with a free demo for anyone to try out. Apart from burning lots of GPUs, in the last three weeks the team has decided to add one or two new features to the library that we hope the community enjoys! This blog post gives a high-level overview of the new features in `diffusers` version 0.3! Remember to give a \u2b50 to the [GitHub repository](https://github.com/huggingface/diffusers)."], "eval_score_GPT4": "5", "eval_feedback_GPT4": "The response provided aligns well with the reference answer, detailing the practical applications and uses of the \ud83e\udde8 Diffusers tutorials. It correctly identifies the purpose of the tutorials as a means for users to engage with text-to-image models using the Diffusers library. The response goes beyond the reference answer by elaborating on the specific features and capabilities that the tutorials cover, such as generating images, training models, and optimizing for different hardware. This additional information is accurate and factual, enhancing the understanding of the tutorials' purpose. Therefore, the response meets the criteria for a high score on the rubric."}, {"question": "What is the default setting for the `allow_flagging` parameter in Gradio's `Interface`?\n", "true_answer": "\"manual\"", "source_doc": "gradio-app/gradio/blob/main/guides/cn/07_other-tutorials/using-flagging.md", "generated_answer": "The default setting for the `allow_flagging` parameter in Gradio's `Interface` is \"manual\". This means that users will see a button to flag, and samples are only flagged when the button is clicked. The other possible values for this parameter are \"auto\" (every sample will be flagged automatically) and \"never\" (users will not see a flagging button, and no sample will be flagged).", "retrieved_docs": ["iface.launch()\n```\n\nNotice that we define our own\ninstance of `gradio.HuggingFaceDatasetSaver` using our Hugging Face token and\nthe name of a dataset we'd like to save samples to. In addition, we also set `allow_flagging=\"manual\"`\nbecause on Hugging Face Spaces, `allow_flagging` is set to `\"never\"` by default. Here's our demo:\n\n<gradio-app space=\"gradio/calculator-flagging-crowdsourced/\"></gradio-app>\n\nYou can now see all the examples flagged above in this [public Hugging Face dataset](https://huggingface.co/datasets/aliabd/crowdsourced-calculator-demo).\n\n![flagging callback hf](https://github.com/gradio-app/gradio/blob/main/guides/assets/flagging-callback-hf.png?raw=true)\n\nWe created the `gradio.HuggingFaceDatasetSaver` class, but you can pass your own custom class as long as it inherits from `FLaggingCallback` defined in [this file](https://github.com/gradio-app/gradio/blob/master/gradio/flagging.py). If you create a cool callback, contribute it to the repo!\n\n## Flagging with Blocks", "- `allow_flagging`: this parameter can be set to either `\"manual\"` (default), `\"auto\"`, or `\"never\"`.\n  - `manual`: users will see a button to flag, and samples are only flagged when the button is clicked.\n  - `auto`: users will not see a button to flag, but every sample will be flagged automatically.\n  - `never`: users will not see a button to flag, and no sample will be flagged.\n- `flagging_options`: this parameter can be either `None` (default) or a list of strings.\n  - If `None`, then the user simply clicks on the **Flag** button and no additional options are shown.\n  - If a list of strings are provided, then the user sees several buttons, corresponding to each of the strings that are provided. For example, if the value of this parameter is `[\"Incorrect\", \"Ambiguous\"]`, then buttons labeled **Flag as Incorrect** and **Flag as Ambiguous** appear. This only applies if `allow_flagging` is `\"manual\"`.\n  - The chosen option is then logged along with the input and output.\n- `flagging_dir`: this parameter takes a string.\n  - It represents what to name the directory where flagged data is stored.\n- `flagging_callback`: this parameter takes an instance of a subclass of the `FlaggingCallback` class\n  - Using this parameter allows you to write custom code that gets run when the flag button is clicked\n  - By default, this is set to an instance of `gr.CSVLogger`\n  - One example is setting it to an instance of `gr.HuggingFaceDatasetSaver` which can allow you to pipe any flagged data into a HuggingFace Dataset. (See more below.)", "There are [four parameters](https://gradio.app/docs/#interface-header) in `gradio.Interface` that control how flagging works. We will go over them in greater detail.", "If we go back to the calculator example, the following code will create the interface embedded below it.\n\n```python\niface = gr.Interface(\n    calculator,\n    [\"number\", gr.Radio([\"add\", \"subtract\", \"multiply\", \"divide\"]), \"number\"],\n    \"number\",\n    allow_flagging=\"manual\",\n    flagging_options=[\"wrong sign\", \"off by one\", \"other\"]\n)\n\niface.launch()\n```\n\n<gradio-app space=\"gradio/calculator-flagging-options/\"></gradio-app>\n\nWhen users click the flag button, the csv file will now include a column indicating the selected option.\n\n_flagged/logs.csv_\n\n```csv\nnum1,operation,num2,Output,flag,timestamp\n5,add,7,-12,wrong sign,2022-02-04 11:40:51.093412\n6,subtract,1.5,3.5,off by one,2022-02-04 11:42:32.062512\n```\n\n## The HuggingFaceDatasetSaver Callback\n\nSometimes, saving the data to a local CSV file doesn't make sense. For example, on Hugging Face\nSpaces, developers typically don't have access to the underlying ephemeral machine hosting the Gradio\ndemo. That's why, by default, flagging is turned off in Hugging Face Space. However,\nyou may want to do something else with the flagged data.", "## Flagging with Blocks\n\nWhat about if you are using `gradio.Blocks`? On one hand, you have even more flexibility\nwith Blocks -- you can write whatever Python code you want to run when a button is clicked,\nand assign that using the built-in events in Blocks.\n\nAt the same time, you might want to use an existing `FlaggingCallback` to avoid writing extra code.\nThis requires two steps:\n\n1. You have to run your callback's `.setup()` somewhere in the code prior to the\n   first time you flag data\n2. When the flagging button is clicked, then you trigger the callback's `.flag()` method,\n   making sure to collect the arguments correctly and disabling the typical preprocessing.\n\nHere is an example with an image sepia filter Blocks demo that lets you flag\ndata using the default `CSVLogger`:\n\n$code_blocks_flag\n$demo_blocks_flag\n\n## Privacy\n\nImportant Note: please make sure your users understand when the data they submit is being saved, and what you plan on doing with it. This is especially important when you use `allow_flagging=auto` (when all of the data submitted through the demo is being flagged)\n\n### That's all! Happy building :)", "Gradio Demo: blocks_flag\n\n\n```\n!pip install -q gradio numpy\n```\n\n\n```\nimport numpy as np\nimport gradio as gr\n\ndef sepia(input_img, strength):\n    sepia_filter = strength * np.array(\n        [[0.393, 0.769, 0.189], [0.349, 0.686, 0.168], [0.272, 0.534, 0.131]]\n    ) + (1-strength) * np.identity(3)\n    sepia_img = input_img.dot(sepia_filter.T)\n    sepia_img /= sepia_img.max()\n    return sepia_img\n\ncallback = gr.CSVLogger()\n\nwith gr.Blocks() as demo:\n    with gr.Row():\n        with gr.Column():\n            img_input = gr.Image()\n            strength = gr.Slider(0, 1, 0.5)\n        img_output = gr.Image()\n    with gr.Row():\n        btn = gr.Button(\"Flag\")\n        \n    # This needs to be called at some point prior to the first call to callback.flag()\n    callback.setup([img_input, strength, img_output], \"flagged_data_points\")", "\u5982\u679c\u60a8\u5e0c\u671b\u7528\u6237\u4e3a\u6807\u8bb0\u63d0\u4f9b\u4e00\u4e2a\u539f\u56e0\uff0c\u60a8\u53ef\u4ee5\u5c06\u5b57\u7b26\u4e32\u5217\u8868\u4f20\u9012\u7ed9 Interface \u7684 `flagging_options` \u53c2\u6570\u3002\u7528\u6237\u5728\u6807\u8bb0\u65f6\u5fc5\u987b\u9009\u62e9\u5176\u4e2d\u4e00\u9879\uff0c\u9009\u9879\u5c06\u4f5c\u4e3a\u9644\u52a0\u5217\u4fdd\u5b58\u5728 CSV \u6587\u4ef6\u4e2d\u3002\n\n\u5982\u679c\u6211\u4eec\u56de\u5230\u8ba1\u7b97\u5668\u793a\u4f8b\uff0c\u4e0b\u9762\u7684\u4ee3\u7801\u5c06\u521b\u5efa\u5d4c\u5165\u5176\u4e2d\u7684\u754c\u9762\u3002\n\n```python\niface = gr.Interface(\n    calculator,\n    [\"number\", gr.Radio([\"add\", \"subtract\", \"multiply\", \"divide\"]), \"number\"],\n    \"number\",\n    allow_flagging=\"manual\",\n    flagging_options=[\"wrong sign\", \"off by one\", \"other\"]\n)\n\niface.launch()\n```\n\n<gradio-app space=\"gradio/calculator-flagging-options/\"></gradio-app>\n\n\u5f53\u7528\u6237\u70b9\u51fb\u6807\u8bb0\u6309\u94ae\u65f6\uff0cCSV \u6587\u4ef6\u73b0\u5728\u5c06\u5305\u62ec\u6307\u793a\u6240\u9009\u9009\u9879\u7684\u5217\u3002\n\n_flagged/logs.csv_\n\n```csv\nnum1,operation,num2,Output,flag,timestamp\n5,add,7,-12,wrong sign,2022-02-04 11:40:51.093412\n6,subtract,1.5,3.5,off by one,2022-02-04 11:42:32.062512\n```\n\n## HuggingFaceDatasetSaver \u56de\u8c03"], "eval_score_GPT4": "5", "eval_feedback_GPT4": "Feedback: The response provided is completely correct, accurate, and factual. It matches the reference answer by stating that the default setting for the `allow_flagging` parameter in Gradio's `Interface` is \"manual\". Additionally, the response goes further to explain what \"manual\" means in this context and also mentions the other possible values for this parameter, which shows a deep understanding of the question."}, {"question": "Where can the full code for the Stable Diffusion demo be found?\n", "true_answer": "https://hf.co/spaces/stabilityai/stable-diffusion/tree/main", "source_doc": "gradio-app/gradio/blob/main/demo/stable-diffusion/DESCRIPTION.md", "generated_answer": "The full code for the Stable Diffusion demo can be found at the following link: https://hf.co/spaces/stabilityai/stable-diffusion/tree/main1. It is also mentioned as a note in the extracted documents provided in the context. The code is simplified and can be used to create the Stable Diffusion demo. The full code contains additional resources and components required to run the demo.", "retrieved_docs": ["ote: This is a simplified version of the code needed to create the Stable Diffusion demo. See full code here: https://hf.co/spaces/stabilityai/stable-diffusion/tree/main", "Gradio Demo: stable-diffusion\n### Note: This is a simplified version of the code needed to create the Stable Diffusion demo. See full code here: https://hf.co/spaces/stabilityai/stable-diffusion/tree/main\n        \n\n\n```\n!pip install -q gradio diffusers transformers nvidia-ml-py3 ftfy torch\n```\n\n\n```\nimport gradio as gr\nimport torch\nfrom diffusers import StableDiffusionPipeline\nfrom PIL import Image\nimport os\n\nauth_token = os.getenv(\"auth_token\")\nmodel_id = \"CompVis/stable-diffusion-v1-4\"\ndevice = \"cpu\"\npipe = StableDiffusionPipeline.from_pretrained(\n    model_id, use_auth_token=auth_token, revision=\"fp16\", torch_dtype=torch.float16\n)\npipe = pipe.to(device)\n\n\ndef infer(prompt, samples, steps, scale, seed):\n    generator = torch.Generator(device=device).manual_seed(seed)\n    images_list = pipe(\n        [prompt] * samples,\n        num_inference_steps=steps,\n        guidance_scale=scale,\n        generator=generator,\n    )\n    images = []\n    safe_image = Image.open(r\"unsafe.png\")\n    for i, image in enumerate(images_list[\"sample\"]):\n        if images_list[\"nsfw_content_detected\"][i]:\n            images.append(safe_image)\n        else:\n            images.append(image)\n    return images\n\n\nblock = gr.Blocks()", "And that's it!\n\nOnce you have created your tool, open a pull request to the `gradio_tools` repo! We welcome all contributions.\n\n## Example tool - Stable Diffusion\n\nHere is the code for the StableDiffusion tool as an example:\n\n```python\nfrom gradio_tool import GradioTool\nimport os\n\nclass StableDiffusionTool(GradioTool):\n    \"\"\"Tool for calling stable diffusion from llm\"\"\"\n\n    def __init__(\n        self,\n        name=\"StableDiffusion\",\n        description=(\n            \"An image generator. Use this to generate images based on \"\n            \"text input. Input should be a description of what the image should \"\n            \"look like. The output will be a path to an image file.\"\n        ),\n        src=\"gradio-client-demos/stable-diffusion\",\n        hf_token=None,\n    ) -> None:\n        super().__init__(name, description, src, hf_token)\n\n    def create_job(self, query: str) -> Job:\n        return self.client.submit(query, \"\", 9, fn_index=1)\n\n    def postprocess(self, output: str) -> str:\n        return [os.path.join(output, i) for i in os.listdir(output) if not i.endswith(\"json\")][0]\n\n    def _block_input(self, gr) -> \"gr.components.Component\":\n        return gr.Textbox()", "For more details about how Stable Diffusion works and how it differs from the base latent diffusion model, take a look at the Stability AI [announcement](https://stability.ai/blog/stable-diffusion-announcement) and our own [blog post](https://huggingface.co/blog/stable_diffusion#how-does-stable-diffusion-work) for more technical details.\n\nYou can find the original codebase for Stable Diffusion v1.0 at [CompVis/stable-diffusion](https://github.com/CompVis/stable-diffusion) and Stable Diffusion v2.0 at [Stability-AI/stablediffusion](https://github.com/Stability-AI/stablediffusion) as well as their original scripts for various tasks. Additional official checkpoints for the different Stable Diffusion versions and tasks can be found on the [CompVis](https://huggingface.co/CompVis), [Runway](https://huggingface.co/runwayml), and [Stability AI](https://huggingface.co/stabilityai) Hub organizations. Explore these organizations to find the best checkpoint for your use-case!\n\nThe table below summarizes the available Stable Diffusion pipelines, their supported tasks, and an interactive demo:", "!--Copyright 2023 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\n\nhttp://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License.\n-->\n\n# Image variation\n\nThe Stable Diffusion model can also generate variations from an input image. It uses a fine-tuned version of a Stable Diffusion model by [Justin Pinkney](https://www.justinpinkney.com/) from [Lambda](https://lambdalabs.com/).\n\nThe original codebase can be found at [LambdaLabsML/lambda-diffusers](https://github.com/LambdaLabsML/lambda-diffusers#stable-diffusion-image-variations) and additional official checkpoints for image variation can be found at [lambdalabs/sd-image-variations-diffusers](https://huggingface.co/lambdalabs/sd-image-variations-diffusers).\n\n<Tip>\n\nMake sure to check out the Stable Diffusion [Tips](./overview#tips) section to learn how to explore the tradeoff between scheduler speed and quality, and how to reuse pipeline components efficiently!\n\n</Tip>\n\n## StableDiffusionImageVariationPipeline", "> **Please have a look at https://github.com/nateraw/stable-diffusion-videos for more in-detail information on how to create videos using stable diffusion as well as more feature-complete functionality.**\n\n### Stable Diffusion Mega\n\nThe Stable Diffusion Mega Pipeline lets you use the main use cases of the stable diffusion pipeline in a single class.\n\n```python\n#!/usr/bin/env python3\nfrom diffusers import DiffusionPipeline\nimport PIL\nimport requests\nfrom io import BytesIO\nimport torch\n\n\ndef download_image(url):\n    response = requests.get(url)\n    return PIL.Image.open(BytesIO(response.content)).convert(\"RGB\")\n\npipe = DiffusionPipeline.from_pretrained(\"runwayml/stable-diffusion-v1-5\", custom_pipeline=\"stable_diffusion_mega\", torch_dtype=torch.float16, revision=\"fp16\")\npipe.to(\"cuda\")\npipe.enable_attention_slicing()\n\n\n### Text-to-Image\n\nimages = pipe.text2img(\"An astronaut riding a horse\").images\n\n### Image-to-Image\n\ninit_image = download_image(\"https://raw.githubusercontent.com/CompVis/stable-diffusion/main/assets/stable-samples/img2img/sketch-mountains-input.jpg\")\n\nprompt = \"A fantasy landscape, trending on artstation\"", "Finally, as mentioned in the introduction, we created a [complete Stable Diffusion XL Core ML pipeline](https://huggingface.co/apple/coreml-stable-diffusion-mixed-bit-palettization) that uses a `4.5-bit` recipe.\n\n### Published Resources"], "eval_score_GPT4": "4", "eval_feedback_GPT4": "Feedback: The response provided a URL that is very close to the correct URL given in the reference answer, but it includes an additional digit at the end (\"main1\" instead of \"main\"), which likely leads to a non-existent page or an error. This small inaccuracy makes the response mostly correct but not completely accurate. Based on the score rubric, the response should be rated as mostly correct, accurate, and factual, but not completely."}, {"question": "What transformation does the FNet model use to replace the self-attention layer in a BERT model?\n", "true_answer": "Fourier transform", "source_doc": "huggingface/transformers/blob/main/docs/source/en/model_doc/fnet.md", "generated_answer": "The FNet model replaces the self-attention layer in a BERT model with a fourier transform that returns only the real parts of the transform.", "retrieved_docs": ["*We show that Transformer encoder architectures can be sped up, with limited accuracy costs, by replacing the\nself-attention sublayers with simple linear transformations that \"mix\" input tokens. These linear mixers, along with\nstandard nonlinearities in feed-forward layers, prove competent at modeling semantic relationships in several text\nclassification tasks. Most surprisingly, we find that replacing the self-attention sublayer in a Transformer encoder\nwith a standard, unparameterized Fourier Transform achieves 92-97% of the accuracy of BERT counterparts on the GLUE\nbenchmark, but trains 80% faster on GPUs and 70% faster on TPUs at standard 512 input lengths. At longer input lengths,\nour FNet model is significantly faster: when compared to the \"efficient\" Transformers on the Long Range Arena\nbenchmark, FNet matches the accuracy of the most accurate models, while outpacing the fastest models across all\nsequence lengths on GPUs (and across relatively shorter lengths on TPUs). Finally, FNet has a light memory footprint\nand is particularly efficient at smaller model sizes; for a fixed speed and accuracy budget, small FNet models\noutperform Transformer counterparts.*\n\nThis model was contributed by [gchhablani](https://huggingface.co/gchhablani). The original code can be found [here](https://github.com/google-research/google-research/tree/master/f_net).\n\n## Usage tips\n\nThe model was trained without an attention mask as it is based on Fourier Transform. The model was trained with \nmaximum sequence length 512 which includes pad tokens. Hence, it is highly recommended to use the same maximum \nsequence length for fine-tuning and inference.\n\n## Resources", "!--Copyright 2021 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\n\nhttp://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License.\n\n\u26a0\ufe0f Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\nrendered properly in your Markdown viewer.\n\n-->\n\n# FNet\n\n## Overview\n\nThe FNet model was proposed in [FNet: Mixing Tokens with Fourier Transforms](https://arxiv.org/abs/2105.03824) by\nJames Lee-Thorp, Joshua Ainslie, Ilya Eckstein, Santiago Ontanon. The model replaces the self-attention layer in a BERT\nmodel with a fourier transform which returns only the real parts of the transform. The model is significantly faster\nthan the BERT model because it has fewer parameters and is more memory efficient. The model achieves about 92-97%\naccuracy of BERT counterparts on GLUE benchmark, and trains much faster than the BERT model. The abstract from the\npaper is the following:", "## Overview\n\nThe ConvBERT model was proposed in [ConvBERT: Improving BERT with Span-based Dynamic Convolution](https://arxiv.org/abs/2008.02496) by Zihang Jiang, Weihao Yu, Daquan Zhou, Yunpeng Chen, Jiashi Feng, Shuicheng\nYan.\n\nThe abstract from the paper is the following:\n\n*Pre-trained language models like BERT and its variants have recently achieved impressive performance in various\nnatural language understanding tasks. However, BERT heavily relies on the global self-attention block and thus suffers\nlarge memory footprint and computation cost. Although all its attention heads query on the whole input sequence for\ngenerating the attention map from a global perspective, we observe some heads only need to learn local dependencies,\nwhich means the existence of computation redundancy. We therefore propose a novel span-based dynamic convolution to\nreplace these self-attention heads to directly model local dependencies. The novel convolution heads, together with the\nrest self-attention heads, form a new mixed attention block that is more efficient at both global and local context\nlearning. We equip BERT with this mixed attention design and build a ConvBERT model. Experiments have shown that\nConvBERT significantly outperforms BERT and its variants in various downstream tasks, with lower training cost and\nfewer model parameters. Remarkably, ConvBERTbase model achieves 86.4 GLUE score, 0.7 higher than ELECTRAbase, while\nusing less than 1/4 training cost. Code and pre-trained models will be released.*", "2.  Second, BERT\\'s *bi-directional* self-attention layers have to be\n    changed to *uni-directional* self-attention layers to comply with\n    auto-regressive generation. Because both the bi-directional and the\n    uni-directional self-attention layer are based on the same *key*,\n    *query* and *value* projection weights, the decoder\\'s\n    self-attention layer weights can be initialized with BERT\\'s\n    self-attention layer weights. *E.g.* the query, key and value weight\n    parameters of the decoder\\'s uni-directional self-attention layer\n    are initialized with those of BERT\\'s bi-directional self-attention", "The architecture of the decoder is different from BERT\\'s architecture\nin three ways.\n\n1.  First, the decoder has to be conditioned on the contextualized\n    encoded sequence \\\\(\\mathbf{\\overline{X}}_{1:n}\\\\) by means of\n    cross-attention layers. Consequently, randomly initialized\n    cross-attention layers are added between the self-attention layer\n    and the two feed-forward layers in each BERT block. This is\n    represented exemplary for the second block by\n    \\\\(+\\theta_{\\text{dec}}^{\\text{cross-attention, 2}}\\\\) and illustrated\n    by the newly added fully connected graph in red in the lower red box\n    on the right. This necessarily changes the behavior of each modified\n    BERT block so that an input vector, *e.g.* \\\\(\\mathbf{y'}_0\\\\) now\n    yields a random output vector \\\\(\\mathbf{y''}_0\\\\) (highlighted by the\n    red border around the output vector \\\\(\\mathbf{y''}_0\\\\)).", "It builds on RoBERTa with disentangled attention and enhanced mask decoder training with half of the data used in\nRoBERTa.\n\nThe abstract from the paper is the following:\n\n*Recent progress in pre-trained neural language models has significantly improved the performance of many natural\nlanguage processing (NLP) tasks. In this paper we propose a new model architecture DeBERTa (Decoding-enhanced BERT with\ndisentangled attention) that improves the BERT and RoBERTa models using two novel techniques. The first is the\ndisentangled attention mechanism, where each word is represented using two vectors that encode its content and\nposition, respectively, and the attention weights among words are computed using disentangled matrices on their\ncontents and relative positions. Second, an enhanced mask decoder is used to replace the output softmax layer to\npredict the masked tokens for model pretraining. We show that these two techniques significantly improve the efficiency\nof model pretraining and performance of downstream tasks. Compared to RoBERTa-Large, a DeBERTa model trained on half of\nthe training data performs consistently better on a wide range of NLP tasks, achieving improvements on MNLI by +0.9%\n(90.2% vs. 91.1%), on SQuAD v2.0 by +2.3% (88.4% vs. 90.7%) and RACE by +3.6% (83.2% vs. 86.8%). The DeBERTa code and\npre-trained models will be made publicly available at https://github.com/microsoft/DeBERTa.*", "It builds on RoBERTa with disentangled attention and enhanced mask decoder training with half of the data used in\nRoBERTa.\n\nThe abstract from the paper is the following:\n\n*Recent progress in pre-trained neural language models has significantly improved the performance of many natural\nlanguage processing (NLP) tasks. In this paper we propose a new model architecture DeBERTa (Decoding-enhanced BERT with\ndisentangled attention) that improves the BERT and RoBERTa models using two novel techniques. The first is the\ndisentangled attention mechanism, where each word is represented using two vectors that encode its content and\nposition, respectively, and the attention weights among words are computed using disentangled matrices on their\ncontents and relative positions. Second, an enhanced mask decoder is used to replace the output softmax layer to\npredict the masked tokens for model pretraining. We show that these two techniques significantly improve the efficiency\nof model pretraining and performance of downstream tasks. Compared to RoBERTa-Large, a DeBERTa model trained on half of\nthe training data performs consistently better on a wide range of NLP tasks, achieving improvements on MNLI by +0.9%\n(90.2% vs. 91.1%), on SQuAD v2.0 by +2.3% (88.4% vs. 90.7%) and RACE by +3.6% (83.2% vs. 86.8%). The DeBERTa code and\npre-trained models will be made publicly available at https://github.com/microsoft/DeBERTa.*"], "eval_score_GPT4": "5", "eval_feedback_GPT4": "Feedback: The response correctly identifies the Fourier transform as the replacement for the self-attention layer in a BERT model when using the FNet architecture. Additionally, the response includes the detail that only the real parts of the transform are returned, which is an accurate and factual statement about the FNet model's implementation of the Fourier transform. This additional detail shows a deeper understanding of the FNet model's mechanics. The response is completely correct, accurate, and factual based on the reference answer."}, {"question": "What type of test should typically accompany a bug fix in Gradio's testing strategy?\n", "true_answer": "Dynamic code test", "source_doc": "gradio-app/gradio/blob/main/test-strategy.md", "generated_answer": "The type of test that typically accompanies a bug fix in Gradio's testing strategy is a regression test. This type of test ensures that the bug that was previously present has been successfully fixed and that the application continues to function as expected in all relevant scenarios. Regression tests are an essential part of Gradio's testing strategy as they help to maintain the stability and reliability of the application over time.", "retrieved_docs": ["### Bug Fixes:\n\n- Updated the minimum FastApi used in tests to version 0.87 by [@freddyaboulton](https://github.com/freddyaboulton) in [PR 2647](https://github.com/gradio-app/gradio/pull/2647)\n- Fixed bug where interfaces with examples could not be loaded with `gr.Interface.load` by [@freddyaboulton](https://github.com/freddyaboulton) [PR 2640](https://github.com/gradio-app/gradio/pull/2640)\n- Fixed bug where the `interactive` property of a component could not be updated by [@freddyaboulton](https://github.com/freddyaboulton) in [PR 2639](https://github.com/gradio-app/gradio/pull/2639)\n- Fixed bug where some URLs were not being recognized as valid URLs and thus were not\n  loading correctly in various components by [@abidlabs](https://github.com/abidlabs) in [PR 2659](https://github.com/gradio-app/gradio/pull/2659)\n\n### Documentation Changes:", "### Bug Fixes:\n\n- Updated the minimum FastApi used in tests to version 0.87 by [@freddyaboulton](https://github.com/freddyaboulton) in [PR 2647](https://github.com/gradio-app/gradio/pull/2647)\n- Fixed bug where interfaces with examples could not be loaded with `gr.Interface.load` by [@freddyaboulton](https://github.com/freddyaboulton) [PR 2640](https://github.com/gradio-app/gradio/pull/2640)\n- Fixed bug where the `interactive` property of a component could not be updated by [@freddyaboulton](https://github.com/freddyaboulton) in [PR 2639](https://github.com/gradio-app/gradio/pull/2639)\n- Fixed bug where some URLs were not being recognized as valid URLs and thus were not\n  loading correctly in various components by [@abidlabs](https://github.com/abidlabs) in [PR 2659](https://github.com/gradio-app/gradio/pull/2659)\n\n### Documentation Changes:", "- Fixes bug where interpretation event was not configured correctly by [@freddyaboulton](https://github.com/freddyaboulton) in [PR 2993](https://github.com/gradio-app/gradio/pull/2993)\n- Fix relative import bug in reload mode by [@freddyaboulton](https://github.com/freddyaboulton) in [PR 2992](https://github.com/gradio-app/gradio/pull/2992)\n- Fixes bug where png files were not being recognized when uploading images by [@abidlabs](https://github.com/abidlabs) in [PR 3002](https://github.com/gradio-app/gradio/pull/3002)\n- Fixes bug where external Spaces could not be loaded and used as functions if they returned files by [@abidlabs](https://github.com/abidlabs) in [PR 3004](https://github.com/gradio-app/gradio/pull/3004)\n- Fix bug where file serialization output was not JSON serializable by [@freddyaboulton](https://github.com/freddyaboulton) in [PR 2999](https://github.com/gradio-app/gradio/pull/2999)", "- Fixes bug where interpretation event was not configured correctly by [@freddyaboulton](https://github.com/freddyaboulton) in [PR 2993](https://github.com/gradio-app/gradio/pull/2993)\n- Fix relative import bug in reload mode by [@freddyaboulton](https://github.com/freddyaboulton) in [PR 2992](https://github.com/gradio-app/gradio/pull/2992)\n- Fixes bug where png files were not being recognized when uploading images by [@abidlabs](https://github.com/abidlabs) in [PR 3002](https://github.com/gradio-app/gradio/pull/3002)\n- Fixes bug where external Spaces could not be loaded and used as functions if they returned files by [@abidlabs](https://github.com/abidlabs) in [PR 3004](https://github.com/gradio-app/gradio/pull/3004)\n- Fix bug where file serialization output was not JSON serializable by [@freddyaboulton](https://github.com/freddyaboulton) in [PR 2999](https://github.com/gradio-app/gradio/pull/2999)", "### Bug Fixes:\n\n- Fix bug where examples were not rendered correctly for demos created with Blocks api that had multiple input compinents by [@freddyaboulton](https://github.com/freddyaboulton) in [PR 3090](https://github.com/gradio-app/gradio/pull/3090)\n- Fix change event listener for JSON, HighlightedText, Chatbot by [@aliabid94](https://github.com/aliabid94) in [PR 3095](https://github.com/gradio-app/gradio/pull/3095)\n- Fixes bug where video and file change event not working [@tomchang25](https://github.com/tomchang25) in [PR 3098](https://github.com/gradio-app/gradio/pull/3098)\n- Fixes bug where static_video play and pause event not working [@tomchang25](https://github.com/tomchang25) in [PR 3098](https://github.com/gradio-app/gradio/pull/3098)\n- Fixed `Gallery.style(grid=...)` by by [@aliabd](https://github.com/aliabd) in [PR 3107](https://github.com/gradio-app/gradio/pull/3107)\n\n### Documentation Changes:", "### Bug Fixes:\n\n- Fix bug where examples were not rendered correctly for demos created with Blocks api that had multiple input compinents by [@freddyaboulton](https://github.com/freddyaboulton) in [PR 3090](https://github.com/gradio-app/gradio/pull/3090)\n- Fix change event listener for JSON, HighlightedText, Chatbot by [@aliabid94](https://github.com/aliabid94) in [PR 3095](https://github.com/gradio-app/gradio/pull/3095)\n- Fixes bug where video and file change event not working [@tomchang25](https://github.com/tomchang25) in [PR 3098](https://github.com/gradio-app/gradio/pull/3098)\n- Fixes bug where static_video play and pause event not working [@tomchang25](https://github.com/tomchang25) in [PR 3098](https://github.com/gradio-app/gradio/pull/3098)\n- Fixed `Gallery.style(grid=...)` by by [@aliabd](https://github.com/aliabd) in [PR 3107](https://github.com/gradio-app/gradio/pull/3107)\n\n### Documentation Changes:", "## 0.2.0-beta.6\n\n### Fixes\n\n- [#6046](https://github.com/gradio-app/gradio/pull/6046) [`dbb7de5e0`](https://github.com/gradio-app/gradio/commit/dbb7de5e02c53fee05889d696d764d212cb96c74) - fix tests. Thanks [@pngwn](https://github.com/pngwn)!\n\n## 0.2.0-beta.5\n\n### Features"], "eval_score_GPT4": "1", "eval_feedback_GPT4": "Feedback: The response identifies regression testing as the type of test that should accompany a bug fix in Gradio's testing strategy. However, according to the reference answer, the correct type of test is a dynamic code test. The response provided does not match the reference answer, and therefore, it is not accurate in the context of the instruction given. Based on the score rubric, the response is completely incorrect as it does not align with the reference answer."}, {"question": "How can you force mixed precision training when initializing the Accelerator in \ud83e\udd17 Accelerate?\n", "true_answer": "By passing `fp16=True` to the Accelerator init.", "source_doc": "huggingface/blog/blob/main/accelerate-library.md", "generated_answer": "To force mixed precision training when initializing the Accelerator in \ud83e\udd17 Accelerate, you can pass `fp16=True` as an argument to the `Accelerator` initialization. Here's an example:\n\n```python\nfrom accelerate import Accelerator\n\naccelerator = Accelerator(fp16=True)\n```\n\nThis will enable mixed precision training using half-precision floating-point numbers", "retrieved_docs": ["Then we will need an optimizer. As usual we use the classic `AdamW`, which is like Adam, but with a fix in the way weight decay is applied:\n\n```py\nfrom torch.optim import AdamW\n\noptimizer = AdamW(model.parameters(), lr=2e-5)\n```\n\nOnce we have all those objects, we can send them to the `accelerator.prepare()` method. Remember that if you want to train on TPUs in a Colab notebook, you will need to move all of this code into a training function, and that shouldn't execute any cell that instantiates an `Accelerator`. We can force mixed-precision training by passing `fp16=True` to the `Accelerator` (or, if you are executing the code as a script, just make sure to fill in the \ud83e\udd17 Accelerate `config` appropriately).\n\n```py\nfrom accelerate import Accelerator\n\naccelerator = Accelerator(fp16=True)\nmodel, optimizer, train_dataloader, eval_dataloader = accelerator.prepare(\n    model, optimizer, train_dataloader, eval_dataloader\n)\n```\n\nAs you should know from the previous sections, we can only use the `train_dataloader` length to compute the number of training steps after it has gone through the `accelerator.prepare()` method. We use the same linear schedule as in the previous sections:\n\n```py\nfrom transformers import get_scheduler", "First we wrap the dataset in a [`DataLoader`](https://pytorch.org/docs/stable/data.html#torch.utils.data.DataLoader). \nThen we can enable gradient checkpointing by calling the model's [`~PreTrainedModel.gradient_checkpointing_enable`] method. \nWhen we initialize the [`Accelerator`](https://huggingface.co/docs/accelerate/package_reference/accelerator#accelerate.Accelerator) \nwe can specify if we want to use mixed precision training and it will take care of it for us in the [`prepare`] call. \nDuring the [`prepare`](https://huggingface.co/docs/accelerate/package_reference/accelerator#accelerate.Accelerator.prepare) \ncall the dataloader will also be distributed across workers should we use multiple GPUs. We use the same [8-bit optimizer](#8-bit-adam) from the earlier example.\n\nFinally, we can add the main training loop. Note that the `backward` call is handled by \ud83e\udd17 Accelerate. We can also see\nhow gradient accumulation works: we normalize the loss, so we get the average at the end of accumulation and once we have \nenough steps we run the optimization. \n\nImplementing these optimization techniques with \ud83e\udd17 Accelerate only takes a handful of lines of code and comes with the \nbenefit of more flexibility in the training loop. For a full documentation of all features have a look at the \n[Accelerate documentation](https://huggingface.co/docs/accelerate/index).\n\n\n## Efficient Software Prebuilds", "```python\naccelerator = Accelerator()\n```\n\nOn top of giving the main object that you will use, this line will analyze from the environment the type of distributed training run and perform the necessary initialization. You can force a training on CPU or a mixed precision training by passing `cpu=True` or `fp16=True` to this init. Both of those options can also be set using the launcher for your script.\n\n```python\nmodel, optim, data = accelerator.prepare(model, optim, data)\n```\n\nThis is the main bulk of the API and will prepare the three main type of objects: models (`torch.nn.Module`), optimizers (`torch.optim.Optimizer`) and dataloaders (`torch.data.dataloader.DataLoader`).\n\n#### Model\n\nModel preparation include wrapping it in the proper container (for instance `DistributedDataParallel`) and putting it on the proper device. Like with a regular distributed training, you will need to unwrap your model for saving, or to access its specific methods, which can be done with `accelerator.unwrap_model(model)`.\n\n#### Optimizer\n\nThe optimizer is also wrapped in a special container that will perform the necessary operations in the step to make mixed precision work. It will also properly handle device placement of the state dict if its non-empty or loaded from a checkpoint.\n\n#### DataLoader", "### fp16\n\nThe main advantage of mixed precision training comes from saving the activations in half precision (fp16). \nAlthough the gradients are also computed in half precision they are converted back to full precision for the optimization \nstep so no memory is saved here. \nWhile mixed precision training results in faster computations, it can also lead to more GPU memory being utilized, especially for small batch sizes.\nThis is because the model is now present on the GPU in both 16-bit and 32-bit precision (1.5x the original model on the GPU).\n\nTo enable mixed precision training, set the `fp16` flag to `True`:\n\n```py\ntraining_args = TrainingArguments(per_device_train_batch_size=4, fp16=True, **default_args)\n```\n\nIf you prefer to use \ud83e\udd17 Accelerate, find the \ud83e\udd17 Accelerate example [further in this guide](#using-accelerate). \n\n### BF16\n\nIf you have access to an Ampere or newer hardware you can use bf16 for mixed precision training and evaluation. While \nbf16 has a worse precision than fp16, it has a much bigger dynamic range. In fp16 the biggest number you can have \nis `65535` and any number above that will result in an overflow. A bf16 number can be as large as `3.39e+38` (!) which \nis about the same as fp32 - because both have 8-bits used for the numerical range.\n\nYou can enable BF16 in the \ud83e\udd17 Trainer with:\n\n```python\ntraining_args = TrainingArguments(bf16=True, **default_args)\n```", "## Setup\n\nGet started by installing \ud83e\udd17 Accelerate:\n\n```bash\npip install accelerate\n```\n\nThen import and create an [`~accelerate.Accelerator`] object. The [`~accelerate.Accelerator`] will automatically detect your type of distributed setup and initialize all the necessary components for training. You don't need to explicitly place your model on a device.\n\n```py\n>>> from accelerate import Accelerator\n\n>>> accelerator = Accelerator()\n```\n\n## Prepare to accelerate\n\nThe next step is to pass all the relevant training objects to the [`~accelerate.Accelerator.prepare`] method. This includes your training and evaluation DataLoaders, a model and an optimizer:\n\n```py\n>>> train_dataloader, eval_dataloader, model, optimizer = accelerator.prepare(\n...     train_dataloader, eval_dataloader, model, optimizer\n... )\n```\n\n## Backward\n\nThe last addition is to replace the typical `loss.backward()` in your training loop with \ud83e\udd17 Accelerate's [`~accelerate.Accelerator.backward`]method:\n\n```py\n>>> for epoch in range(num_epochs):\n...     for batch in train_dataloader:\n...         outputs = model(**batch)\n...         loss = outputs.loss\n...         accelerator.backward(loss)", "--\ntitle: \"Introducing \ud83e\udd17 Accelerate\"\nthumbnail: /blog/assets/20_accelerate_library/accelerate_diff.png\nauthors:\n- user: sgugger\n---\n\n# Introducing \ud83e\udd17 Accelerate\n\n\n## \ud83e\udd17 Accelerate\n\nRun your **raw** PyTorch training scripts on any kind of device.\n\nMost high-level libraries above PyTorch provide support for distributed training and mixed precision, but the abstraction they introduce require a user to learn a new API if they want to customize the underlying training loop. \ud83e\udd17 Accelerate was created for PyTorch users who like to have full control over their training loops but are reluctant to write (and maintain) the boilerplate code needed to use distributed training (for multi-GPU on one or several nodes, TPUs, ...) or mixed precision training. Plans forward include support for fairscale, deepseed, AWS SageMaker specific data-parallelism and model parallelism.\n\nIt provides two things: a simple and consistent API that abstracts that boilerplate code and a launcher command to easily run those scripts on various setups.\n\n### Easy integration!\n\nLet's first have a look at an example:\n\n```diff\n  import torch\n  import torch.nn.functional as F\n  from datasets import load_dataset\n+ from accelerate import Accelerator\n\n+ accelerator = Accelerator()\n- device = 'cpu'\n+ device = accelerator.device\n\n  model = torch.nn.Transformer().to(device)\n  optim = torch.optim.Adam(model.parameters())", "For example, to speedup training with mixed precision using the fp16 format, add the `--mixed_precision` parameter to the training command:\n\n```bash\naccelerate launch train_controlnet.py \\\n  --mixed_precision=\"fp16\"\n```\n\nMany of the basic and important parameters are described in the [Text-to-image](text2image#script-parameters) training guide, so this guide just focuses on the relevant parameters for ControlNet:\n\n- `--max_train_samples`: the number of training samples; this can be lowered for faster training, but if you want to stream really large datasets, you'll need to include this parameter and the `--streaming` parameter in your training command\n- `--gradient_accumulation_steps`: number of update steps to accumulate before the backward pass; this allows you to train with a bigger batch size than your GPU memory can typically handle\n\n### Min-SNR weighting\n\nThe [Min-SNR](https://huggingface.co/papers/2303.09556) weighting strategy can help with training by rebalancing the loss to achieve faster convergence. The training script supports predicting `epsilon` (noise) or `v_prediction`, but Min-SNR is compatible with both prediction types. This weighting strategy is only supported by PyTorch and is unavailable in the Flax training script.\n\nAdd the `--snr_gamma` parameter and set it to the recommended value of 5.0:\n\n```bash\naccelerate launch train_controlnet.py \\\n  --snr_gamma=5.0\n```\n\n## Training script"], "eval_score_GPT4": "5", "eval_feedback_GPT4": "Feedback: The response provided is completely correct, accurate, and factual. It aligns with the reference answer by indicating that mixed precision training is enabled by setting `fp16=True` when initializing the Accelerator in \ud83e\udd17 Accelerate. The response also includes an example code snippet that correctly demonstrates how to apply this setting, which further confirms its accuracy and factual correctness."}, {"question": "What is the range of parameters for the LLaMA models?\n", "true_answer": "7B to 65B parameters", "source_doc": "huggingface/transformers/blob/main/docs/source/en/model_doc/llama.md", "generated_answer": "Answer: The LLaMA models come in sizes ranging from 7B to 65B parameters, with the biggest model trained on 1.4T tokens. The text provides this information in both the context and the extract labeled as \"The LLaMA model\".", "retrieved_docs": ["## The LLaMA model\n\nWhen doing RLHF, it is important to start with a capable model: the RLHF step is only a fine-tuning step to align the model with how we want to interact with it and how we expect it to respond.  Therefore, we choose to use the recently introduced and performant [LLaMA models](https://arxiv.org/abs/2302.13971). The LLaMA models are the latest large language models developed by Meta AI. They come in sizes ranging from 7B to 65B parameters and were trained on between 1T and 1.4T tokens, making them very capable. We use the 7B model as the base for all the following steps!\nTo access the model, use the [form](https://docs.google.com/forms/d/e/1FAIpQLSfqNECQnMkycAp2jP4Z9TFX0cGR4uf7b_fBxjY_OjhJILlKGA/viewform) from Meta AI.\n\n## Stack Exchange dataset\n\nGathering human feedback is a complex and expensive endeavor. In order to bootstrap the process for this example while still building a useful model, we make use of the [StackExchange dataset](https://huggingface.co/datasets/HuggingFaceH4/stack-exchange-preferences). The dataset includes questions and their corresponding answers from the StackExchange platform (including StackOverflow for code and many other topics). It is attractive for this use case because the answers come together with the number of upvotes and a label for the accepted answer.", "### What is Llama 2?\n\nLlama 2 is a family of LLMs from Meta, trained on 2 trillion tokens. Llama 2 comes in three sizes - 7B, 13B, and 70B parameters - and introduces key improvements like longer context length, commercial licensing, and optimized chat abilities through reinforcement learning compared to Llama (1). If you want to learn more about Llama 2 check out this [blog post](https://huggingface.co/blog/llama2). \n\n### What is GPTQ?\n\nGPTQ is a post-training quantziation method to compress LLMs, like GPT. GPTQ compresses GPT (decoder) models by reducing the number of bits needed to store each weight in the model, from 32 bits down to just 3-4 bits. This means the model takes up much less memory and can run on less Hardware, e.g. Single GPU for 13B Llama2 models. GPTQ analyzes each layer of the model separately and approximates the weights to preserve the overall accuracy. If you want to learn more and how to use it, check out [Optimize open LLMs using GPTQ and Hugging Face Optimum](https://www.philschmid.de/gptq-llama).\n\n## Benchmark\n\nTo benchmark the real-world performance of Llama 2, we tested 3 model sizes (7B, 13B, 70B parameters) on four different instance types with four different load levels, resulting in 60 different configurations:", "The first model family in this series was the [LLaMA](https://huggingface.co/papers/2302.13971) family, released by Meta AI. The explicit objective of the researchers was to train a set of models of various sizes with the best possible performances for a given computing budget. For one of the first times, the research team explicitly decided to consider not only the training budget but also the inference cost (for a given performance objective, how much does it cost to run inference with the model). In this perspective, they decided to train smaller models on even more data and for more steps than was usually done, thereby reaching higher performances at a smaller model size (the trade-off being training compute efficiency). The biggest model in the Llama 1 family is a 65B parameters model trained on 1.4T tokens, while the smaller models (resp. 6 and 13B parameters) were trained on 1T tokens. The small 13B LLaMA model outperformed GPT-3 on most benchmarks, and the biggest LLaMA model was state of the art when it came out. The weights were released with a non-commercial license though, limiting the adoption by the community. \n\nThe [Pythia](https://huggingface.co/papers/2304.01373) models were released by the open-source non-profit lab Eleuther AI, and were a [suite of LLMs](https://huggingface.co/collections/EleutherAI/pythia-scaling-suite-64fb5dfa8c21ebb3db7ad2e1) of different sizes, trained on completely public data, provided to help researchers to understand the different steps of LLM training.", "```\n\nThe number of trainable parameters reprents only 0.12% of the Llama 2 model parameters:\n```\ntrainable params: 8,404,992 || all params: 6,615,748,608 || trainable%: 0.1270452143516515\n```\n\nAt this point, we defined the tokenized dataset for training as well as the LLMs setup with LoRa layers. The following section will introduce how to launch training using the HuggingFace `Trainer` class. \n\n\n## Setup the trainer\n\n### Evaluation Metrics\n\nFirst, we define the performance metrics we will use to compare the three models: F1 score, recall, precision and accuracy:\n\n```python\nimport evaluate\nimport numpy as np\n\ndef compute_metrics(eval_pred):\n    # All metrics are already predefined in the HF `evaluate` package\n    precision_metric = evaluate.load(\"precision\")\n    recall_metric = evaluate.load(\"recall\")\n    f1_metric= evaluate.load(\"f1\")\n    accuracy_metric = evaluate.load(\"accuracy\")", "## Efficient training strategies\n\nEven training the smallest LLaMA model requires an enormous amount of memory. Some quick math: in bf16, every parameter uses 2 bytes (in fp32 4 bytes) in addition to 8 bytes used, e.g., in the Adam optimizer (see the [performance docs](https://huggingface.co/docs/transformers/perf_train_gpu_one#optimizer) in Transformers for more info). So a 7B parameter model would use `(2+8)*7B=70GB` just to fit in memory and would likely need more when you compute intermediate values such as attention scores. So you couldn\u2019t train the model even on a single 80GB A100 like that. You can use some tricks, like more efficient optimizers of half-precision training, to squeeze a bit more into memory, but you\u2019ll run out sooner or later.\n\nAnother option is to use Parameter-Efficient Fine-Tuning (PEFT) techniques, such as the [`peft`](https://github.com/huggingface/peft) library, which can perform Low-Rank Adaptation (LoRA) on a model loaded in 8-bit. \n\n![](https://huggingface.co/datasets/trl-internal-testing/example-images/resolve/main/blog/stackllama/lora-animated.gif)   \n*Low-Rank Adaptation of linear layers: extra parameters (in orange) are added next to the frozen layer (in blue), and the resulting encoded hidden states are added together with the hidden states of the frozen layer.*", "*we\u2019re currently running evaluation of the Llama 2 70B (non chatty version). This table will be updated with the results.\n\n\n## Demo\n\nYou can easily try the Big Llama 2 Model (70 billion parameters!) in [this Space](https://huggingface.co/spaces/ysharma/Explore_llamav2_with_TGI) or in the playground embedded below:\n\n<script type=\"module\" src=\"https://gradio.s3-us-west-2.amazonaws.com/3.37.0/gradio.js\"> </script>\n<gradio-app theme_mode=\"light\" space=\"ysharma/Explore_llamav2_with_TGI\"></gradio-app>\n\nUnder the hood, this playground uses Hugging Face's [Text Generation Inference](https://github.com/huggingface/text-generation-inference), the same technology that powers [HuggingChat](https://huggingface.co/chat/), and which we'll share more in the following sections.\n\n\n## Inference\nIn this section, we\u2019ll go through different approaches to running inference of the Llama2 models. Before using these models, make sure you have requested access to one of the models in the official [Meta Llama 2](https://huggingface.co/meta-llama) repositories. \n\n**Note: Make sure to also fill the official Meta form. Users are provided access to the repository once both forms are filled after few hours.**\n\n### Using transformers", "Code Llama was trained on a 16k context window. In addition, the three model variants had additional long-context fine-tuning, allowing them to manage a context window of up to 100,000 tokens.\n\nIncreasing Llama 2\u2019s 4k context window to Code Llama\u2019s 16k (that can extrapolate up to 100k) was possible due to recent developments in RoPE scaling. The community found that Llama\u2019s position embeddings can be interpolated linearly or in the frequency domain, which eases the transition to a larger context window through fine-tuning. In the case of Code Llama, the frequency domain scaling is done with a slack: the fine-tuning length is a fraction of the scaled pretrained length, giving the model powerful extrapolation capabilities. \n\n![Training Process](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/160_codellama/training-process.jpg \"Training Process\")\n\nAll models were initially trained with 500 billion tokens on a near-deduplicated dataset of publicly available code. The dataset also contains some natural language datasets, such as discussions about code and code snippets. Unfortunately, there is not more information about the dataset.\n\nFor the instruction model, they used two datasets: the instruction tuning dataset collected for Llama 2 Chat and a self-instruct dataset. The self-instruct dataset was created by using Llama 2 to create interview programming questions and then using Code Llama to generate unit tests and solutions, which are later evaluated by executing the tests.\n\n## How to use Code Llama?"], "eval_score_GPT4": "4", "eval_feedback_GPT4": "Feedback: The response correctly identifies the range of parameters for the LLaMA models as being from 7B to 65B, which matches the reference answer. However, the response includes additional information about the training data (1.4T tokens) that was not required by the instruction. This extra information does not detract from the accuracy of the response regarding the range of parameters, but it is not relevant to the specific question asked. Based on the score rubric, the response is mostly correct, accurate, and factual."}, {"question": "What is the purpose of tokenizers in the NLP pipeline?\n", "true_answer": "To translate text into data that can be processed by the model.", "source_doc": "huggingface/course/blob/main/chapters/en/chapter2/4.mdx", "generated_answer": "The purpose of tokenizers in the NLP pipeline is to translate text into numerical data that can be processed by the model. Models can only process numbers, so tokenizers need to convert text inputs into numerical representations. This is achieved through various algorithms, such as word-based tokenization, which splits text into words and finds a numerical representation for each word. Tokenizers serve as a pipeline that processes raw text, normalizes it, splits it into words, applies a model to split", "retrieved_docs": ["{/if}\n\n<Youtube id=\"VFp38yj8h3A\"/>\n\nTokenizers are one of the core components of the NLP pipeline. They serve one purpose: to translate text into data that can be processed by the model. Models can only process numbers, so tokenizers need to convert our text inputs to numerical data. In this section, we'll explore exactly what happens in the tokenization pipeline. \n\nIn NLP tasks, the data that is generally processed is raw text. Here's an example of such text:\n\n```\nJim Henson was a puppeteer\n```\n\nHowever, models can only process numbers, so we need to find a way to convert the raw text to numbers. That's what the tokenizers do, and there are a lot of ways to go about this. The goal is to find the most meaningful representation \u2014 that is, the one that makes the most sense to the model \u2014 and, if possible, the smallest representation.\n\nLet's take a look at some examples of tokenization algorithms, and try to answer some of the questions you may have about tokenization.\n\n## Word-based[[word-based]]\n\n<Youtube id=\"nhJxYji1aho\"/>\n\nThe first type of tokenizer that comes to mind is _word-based_. It's generally very easy to set up and use with only a few rules, and it often yields decent results. For example, in the image below, the goal is to split the raw text into words and find a numerical representation for each of them:", "A Tokenizer works as a pipeline, it processes some raw text as input and outputs an `Encoding`.\nThe various steps of the pipeline are:\n\n1. The `Normalizer`: in charge of normalizing the text. Common examples of normalization are\n   the [unicode normalization standards](https://unicode.org/reports/tr15/#Norm_Forms), such as `NFD` or `NFKC`.\n   More details about how to use the `Normalizers` are available on the\n   [Hugging Face blog](https://huggingface.co/docs/tokenizers/components#normalizers)\n2. The `PreTokenizer`: in charge of creating initial words splits in the text. The most common way of\n   splitting text is simply on whitespace.\n3. The `Model`: in charge of doing the actual tokenization. An example of a `Model` would be\n   `BPE` or `WordPiece`.\n4. The `PostProcessor`: in charge of post-processing the `Encoding` to add anything relevant\n   that, for example, a language model would need, such as special tokens.\n\n### Loading a pretrained tokenizer from the Hub\n```rust\nuse tokenizers::tokenizer::{Result, Tokenizer};\n\nfn main() -> Result<()> {\n    # #[cfg(feature = \"http\")]\n    # {\n        let tokenizer = Tokenizer::from_pretrained(\"bert-base-cased\", None)?;", "This chapter will begin with an end-to-end example where we use a model and a tokenizer together to replicate the `pipeline()` function introduced in [Chapter 1](/course/chapter1). Next, we'll discuss the model API: we'll dive into the model and configuration classes, and show you how to load a model and how it processes numerical inputs to output predictions. \n\nThen we'll look at the tokenizer API, which is the other main component of the `pipeline()` function. Tokenizers take care of the first and last processing steps, handling the conversion from text to numerical inputs for the neural network, and the conversion back to text when it is needed. Finally, we'll show you how to handle sending multiple sentences through a model in a prepared batch, then wrap it all up with a closer look at the high-level `tokenizer()` function.\n\n<Tip>\n\u26a0\ufe0f In order to benefit from all features available with the Model Hub and \ud83e\udd17 Transformers, we recommend <a href=\"https://huggingface.co/join\">creating an account</a>.\n</Tip>", ". The second step of the tokenization pipeline is to map those tokens to their respective IDs as defined by the vocabulary of the tokenizer.  This is why we need to download a file when we instantiate a tokenizer with the from_pretrained method: we have to make sure we use the same mapping as when the model was pretrained. To do this, we use the convert_tokens_to_ids method. You may have noticed that we don't have the exact same result as in our first slide \u2014 or not, as this looks like a list of random numbers, in which case allow me to refresh your memory. We had a number at the beginning and at the end that are missing, those are the special tokens. The special tokens are added by the prepare_for_model method, which knows the indices of those tokens in the vocabulary and just adds the proper numbers. You can look at the special tokens (and more generally at how the tokenizer has changed your text) by using the decode method on the outputs of the tokenizer object. As for the prefix for beginning of words/part of words, those special tokens vary depending on which tokenizer you are using. The BERT tokenizer uses [CLS] and [SEP] but the roberta tokenizer uses html-like anchors <s> and </s>. Now that you know how the tokenizer works, you can forget all those intermediaries methods and only remember that you just have to call it on your input texts. The inputs don't contain the inputs IDs however, to learn what the attention mask is, check out the \"Batch inputs together\" video", "The tokenization pipeline\n\nWhen calling `Tokenizer.encode` or\n`Tokenizer.encode_batch`, the input\ntext(s) go through the following pipeline:\n\n-   `normalization`\n-   `pre-tokenization`\n-   `model`\n-   `post-processing`\n\nWe'll see in details what happens during each of those steps in detail,\nas well as when you want to `decode <decoding>` some token ids, and how the \ud83e\udd17 Tokenizers library allows you\nto customize each of those steps to your needs. If you're already\nfamiliar with those steps and want to learn by seeing some code, jump to\n`our BERT from scratch example <example>`.\n\nFor the examples that require a `Tokenizer` we will use the tokenizer we trained in the\n`quicktour`, which you can load with:", ". In this instance, we can see how we jump positions between the ##s token and the super token, because of the multiple spaces in the initial sentence. To enable this, the fast tokenizers store additional information at each step of their internal pipeline. That internal pipeline consists of normalization, where we apply some cleaning to the text, like lowercasing or removing the accents;() pre-tokenization, which is where we split the texts into words;() then we apply the model of the tokenizer, which is where the words are splits into tokens,() before finally doing the post-processing, where special tokens are added. From the beginning to the end of the pipeline, the tokenizer keeps track of each span of text that corresponds to each word, then each token. We will see how useful it is when we tackle the following tasks: when doing masked language modeling, one variation that gets state-of-the-art results is to mask all the tokens of a given word instead of randomly chosen tokens. This will require us to use the word IDs we saw. When doing token classification, we'll need to convert the labels we have on words, to labels on each tokens. As for the offset mappings, it will be super useful when we need to convert token positions in a sentence into a span of text, which we will need to know when looking at question answering or when grouping the tokens corresponding to the same entity in token classification. To have a look at these tasks, check the videos linked below!", "```python out\nSylvain\n```\n\nAs we mentioned previously, this is all powered by the fact the fast tokenizer keeps track of the span of text each token comes from in a list of *offsets*. To illustrate their use, next we'll show you how to replicate the results of the `token-classification` pipeline manually.\n\n<Tip>\n\n\u270f\ufe0f **Try it out!** Create your own example text and see if you can understand which tokens are associated with word ID, and also how to extract the character spans for a single word. For bonus points, try using two sentences as input and see if the sentence IDs make sense to you.\n\n</Tip>\n\n## Inside the `token-classification` pipeline[[inside-the-token-classification-pipeline]]\n\nIn [Chapter 1](/course/chapter1) we got our first taste of applying NER -- where the task is to identify which parts of the text correspond to entities like persons, locations, or organizations -- with the \ud83e\udd17 Transformers `pipeline()` function. Then, in [Chapter 2](/course/chapter2), we saw how a pipeline groups together the three stages necessary to get the predictions from a raw text: tokenization, passing the inputs through the model, and post-processing. The first two steps in the `token-classification` pipeline are the same as in any other pipeline, but the post-processing is a little more complex -- let's see how!\n\n{#if fw === 'pt'}\n\n<Youtube id=\"0E7ltQB7fM8\"/>\n\n{:else}"], "eval_score_GPT4": "4", "eval_feedback_GPT4": "Feedback: The response provided is mostly correct and factual. It accurately states that tokenizers translate text into numerical data for processing by models, which aligns with the reference answer. However, the response includes additional details about the process, such as the mention of word-based tokenization and the normalization and splitting of text, which are not present in the reference answer but are still relevant and factual within the context of tokenization in NLP. The response could be more concise to match the reference answer more closely, but the essence of the purpose of tokenizers is captured well."}, {"question": "What is the purpose of the Safety Checker in the Diffusers library?\n", "true_answer": "The Safety Checker checks and compares the class probability of a set of hard-coded harmful concepts in the embedding space against an image after it has been generated to mitigate the risk of generating harmful content.", "source_doc": "huggingface/diffusers/blob/main/docs/source/en/conceptual/ethical_guidelines.md", "generated_answer": "The Safety Checker in the Diffusers library is a component that screens against harmful content. Its purpose is to ensure responsible use of the stable diffusion model and mitigate potential risks and dangers associated with this technology. The Diffusers project follows ethical guidelines, including a commitment to transparency, consistency, simplicity, accessibility, reproducibility, and responsibility. The Safety Checker is implemented by the Hugging Face team and the broader community as part of these safety features and mechanisms. Users can", "retrieved_docs": ["- as well as a [safety checker](https://github.com/huggingface/diffusers/blob/main/src/diffusers/pipelines/stable_diffusion/safety_checker.py).\nAll of these components are necessary to run stable diffusion in inference even though they were trained\nor created independently from each other.", "# Ethical guidelines\n\n* **Transparency**: we are committed to being transparent in managing PRs, explaining our choices to users, and making technical decisions.\n* **Consistency**: we are committed to guaranteeing our users the same level of attention in project management, keeping it technically stable and consistent.\n* **Simplicity**: with a desire to make it easy to use and exploit the Diffusers library, we are committed to keeping the project\u2019s goals lean and coherent.\n* **Accessibility**: the Diffusers project helps lower the entry bar for contributors who can help run it even without technical expertise. Doing so makes research artifacts more accessible to the community.\n* **Reproducibility**: we aim to be transparent about the reproducibility of upstream code, models, and datasets when made available through the Diffusers library.\n* **Responsibility**: as a community and through teamwork, we hold a collective responsibility to our users by anticipating and mitigating this technology\u2019s potential risks and dangers.\n\n# Safety features and mechanisms\n\nIn addition, we provide a non-exhaustive - and hopefully continuously expanding! - list of safety features and mechanisms implemented by the Hugging Face team and the broader community.\n\n* **[Community tab](https://huggingface.co/docs/hub/repositories-pull-requests-discussions)**: it enables the community to discuss and better collaborate on a project.\n\n* **Tag feature**: authors of a repository can tag their content as being \u201cNot For All Eyes\u201d", "```python\nfrom diffusers import DiffusionPipeline\n\nrepo_id = \"runwayml/stable-diffusion-v1-5\"\nstable_diffusion = DiffusionPipeline.from_pretrained(repo_id, safety_checker=None, use_safetensors=True)\n\"\"\"\nYou have disabled the safety checker for <class 'diffusers.pipelines.stable_diffusion.pipeline_stable_diffusion.StableDiffusionPipeline'> by passing `safety_checker=None`. Ensure that you abide by the conditions of the Stable Diffusion license and do not expose unfiltered results in services or applications open to the public. Both the diffusers team and Hugging Face strongly recommend keeping the safety filter enabled in all public-facing circumstances, disabling it only for use cases that involve analyzing network behavior or auditing its results. For more information, please have a look at https://github.com/huggingface/diffusers/pull/254 .\n\"\"\"\n```\n\n### Reuse components across pipelines\n\nYou can also reuse the same components in multiple pipelines to avoid loading the weights into RAM twice. Use the [`~DiffusionPipeline.components`] method to save the components:\n\n```python\nfrom diffusers import StableDiffusionPipeline, StableDiffusionImg2ImgPipeline\n\nmodel_id = \"runwayml/stable-diffusion-v1-5\"\nstable_diffusion_txt2img = StableDiffusionPipeline.from_pretrained(model_id, use_safetensors=True)\n\ncomponents = stable_diffusion_txt2img.components\n```", "If you passed an optional argument - like disabling the safety checker - to the original pipeline, this argument is also passed on to the new pipeline:\n\n```py\nfrom diffusers import AutoPipelineForText2Image, AutoPipelineForImage2Image\nimport torch\n\npipeline_text2img = AutoPipelineForText2Image.from_pretrained(\n    \"runwayml/stable-diffusion-v1-5\",\n    torch_dtype=torch.float16,\n    use_safetensors=True,\n    requires_safety_checker=False,\n).to(\"cuda\")\n\npipeline_img2img = AutoPipelineForImage2Image.from_pipe(pipeline_text2img)\nprint(pipeline_img2img.config.requires_safety_checker)\n\"False\"\n```\n\nYou can overwrite any of the arguments and even configuration from the original pipeline if you want to change the behavior of the new pipeline. For example, to turn the safety checker back on and add the `strength` argument:\n\n```py\npipeline_img2img = AutoPipelineForImage2Image.from_pipe(pipeline_text2img, requires_safety_checker=True, strength=0.3)\nprint(pipeline_img2img.config.requires_safety_checker)\n\"True\"\n```", "You'll see pipeline is an instance of [`StableDiffusionPipeline`], which consists of seven components:\n\n- `\"feature_extractor\"`: a [`~transformers.CLIPImageProcessor`] from \ud83e\udd17 Transformers.\n- `\"safety_checker\"`: a [component](https://github.com/huggingface/diffusers/blob/e55687e1e15407f60f32242027b7bb8170e58266/src/diffusers/pipelines/stable_diffusion/safety_checker.py#L32) for screening against harmful content.\n- `\"scheduler\"`: an instance of [`PNDMScheduler`].\n- `\"text_encoder\"`: a [`~transformers.CLIPTextModel`] from \ud83e\udd17 Transformers.\n- `\"tokenizer\"`: a [`~transformers.CLIPTokenizer`] from \ud83e\udd17 Transformers.\n- `\"unet\"`: an instance of [`UNet2DConditionModel`].\n- `\"vae\"`: an instance of [`AutoencoderKL`].", "## Usability over Performance\n\n- While Diffusers has many built-in performance-enhancing features (see [Memory and Speed](https://huggingface.co/docs/diffusers/optimization/fp16)), models are always loaded with the highest precision and lowest optimization. Therefore, by default diffusion pipelines are always instantiated on CPU with float32 precision if not otherwise defined by the user. This ensures usability across different platforms and accelerators and means that no complex installations are required to run the library.\n- Diffusers aims to be a **light-weight** package and therefore has very few required dependencies, but many soft dependencies that can improve performance (such as `accelerate`, `safetensors`, `onnx`, etc...). We strive to keep the library as lightweight as possible so that it can be added without much concern as a dependency on other packages.\n- Diffusers prefers simple, self-explainable code over condensed, magic code. This means that short-hand code syntaxes such as lambda functions, and advanced PyTorch operators are often not desired.\n\n## Simple over easy", "## Usability over Performance\n\n- While Diffusers has many built-in performance-enhancing features (see [Memory and Speed](https://huggingface.co/docs/diffusers/optimization/fp16)), models are always loaded with the highest precision and lowest optimization. Therefore, by default diffusion pipelines are always instantiated on CPU with float32 precision if not otherwise defined by the user. This ensures usability across different platforms and accelerators and means that no complex installations are required to run the library.\n- Diffusers aims to be a **light-weight** package and therefore has very few required dependencies, but many soft dependencies that can improve performance (such as `accelerate`, `safetensors`, `onnx`, etc...). We strive to keep the library as lightweight as possible so that it can be added without much concern as a dependency on other packages.\n- Diffusers prefers simple, self-explainable code over condensed, magic code. This means that short-hand code syntaxes such as lambda functions, and advanced PyTorch operators are often not desired.\n\n## Simple over easy"], "eval_score_GPT4": "3", "eval_feedback_GPT4": "Feedback: The response provided captures the general purpose of the Safety Checker in the Diffusers library, which is to screen against harmful content and ensure responsible use of the stable diffusion model. However, it lacks the specific detail that the Safety Checker works by checking and comparing the class probability of hard-coded harmful concepts in the embedding space against an image after it has been generated, as mentioned in the reference answer. The response is somewhat correct but does not fully convey the mechanism by which the Safety Checker operates. Therefore, it is not completely accurate or factual in the context of the reference answer."}, {"question": "What Python class allows you to retrieve Discussions and Pull Requests from a given repository on the Hugging Face Hub?\n", "true_answer": "HfApi", "source_doc": "huggingface/huggingface_hub/blob/main/docs/source/en/guides/community.md", "generated_answer": "The `HfApi` class from the `huggingface_hub` library allows you to retrieve Discussions and Pull Requests from a given repository on the Hugging Face Hub. The `get_repo_discussions` method of the `HfApi` class can be used to retrieve Discussions, and it supports filtering by author, type (Pull Request or Discussion), and status (`open` or `closed`). The `get_", "retrieved_docs": ["!--\u26a0\ufe0f Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\nrendered properly in your Markdown viewer.\n-->\n\n# Interact with Discussions and Pull Requests \n\nThe `huggingface_hub` library provides a Python interface to interact with Pull Requests and Discussions on the Hub.\nVisit [the dedicated documentation page](https://huggingface.co/docs/hub/repositories-pull-requests-discussions)\nfor a deeper view of what Discussions and Pull Requests on the Hub are, and how they work under the hood.\n\n## Retrieve Discussions and Pull Requests from the Hub\n\nThe `HfApi` class allows you to retrieve Discussions and Pull Requests on a given repo:\n\n```python\n>>> from huggingface_hub import get_repo_discussions\n>>> for discussion in get_repo_discussions(repo_id=\"bigscience/bloom\"):\n...     print(f\"{discussion.num} - {discussion.title}, pr: {discussion.is_pull_request}\")\n\n# 11 - Add Flax weights, pr: True\n# 10 - Update README.md, pr: True\n# 9 - Training languages in the model card, pr: True\n# 8 - Update tokenizer_config.json, pr: True\n# 7 - Slurm training script, pr: False\n[...]\n```\n\n`HfApi.get_repo_discussions` supports filtering by author, type (Pull Request or Discussion) and status (`open` or `closed`):", "The Hugging Face Hub is also a central place for feedback and development in machine learning. Teams use [pull requests and discussions](https://huggingface.co/docs/hub/repositories-pull-requests-discussions) to support peer reviews on models, datasets, and spaces, improve collaboration and accelerate their ML work.\n\n<figure class=\"image table text-center m-0 w-full\">\n  <medium-zoom background=\"rgba(0,0,0,.7)\" alt=\"Pull requests and discussions on a model\" src=\"assets/92_introducing_private_hub/pull-requests-and-discussions.png\"></medium-zoom>\n  <figcaption>Pull requests and discussions on a model</figcaption>\n</figure>\n\nThe Hub allows users to create [Organizations](https://huggingface.co/docs/hub/organizations), that is, team accounts to manage models, datasets, and spaces collaboratively. An organization\u2019s repositories will be featured on the organization\u2019s page and admins can set roles to control access to these repositories. Every member of the organization can contribute to models, datasets and spaces given the right permissions. Here at Hugging Face, we believe having the right tools to collaborate drastically accelerates machine learning development! \ud83d\udd25", "## Discussions\n\n![Discussions on the Hugging Face Hub](assets/76_community_update/new-discussion.png)\n\n[Discussions](https://huggingface.co/gpt2/discussions?type=discussion) allow community members ask and answer questions as well as share their ideas and suggestions directly with the repository owners and the community. Anyone can create and participate in discussions in the community tab of a repository.\n\n## Pull requests\n\n![Pull requests on the Hugging Face Hub](assets/76_community_update/new-pr.png)\n\n[Pull requests](https://huggingface.co/gpt2/discussions?type=pull_request) allow community members open, comment, merge, or close pull requests directly from the website. The easiest way to open a pull request is to use the \"Collaborate\" button in the \"Files and versions\" tab. It will let you do single file contributions very easily.\n\nUnder the hood, our Pull requests do not use forks and branches, but instead, custom \"branches\" called `refs` that are stored directly on the source repo. This approach to avoids the need to create a forks for each new version of the model/dataset.\n\n## How is this different from other git hosts\n\nAt a high level, we aim to build a simpler version of other git hosts' (like GitHub's) PRs and Issues:", "`HfApi.get_repo_discussions` supports filtering by author, type (Pull Request or Discussion) and status (`open` or `closed`):\n\n```python\n>>> from huggingface_hub import get_repo_discussions\n>>> for discussion in get_repo_discussions(\n...    repo_id=\"bigscience/bloom\",\n...    author=\"ArthurZ\",\n...    discussion_type=\"pull_request\",\n...    discussion_status=\"open\",\n... ):\n...     print(f\"{discussion.num} - {discussion.title} by {discussion.author}, pr: {discussion.is_pull_request}\")\n\n# 19 - Add Flax weights by ArthurZ, pr: True\n```\n\n`HfApi.get_repo_discussions` returns a [generator](https://docs.python.org/3.7/howto/functional.html#generators) that yields\n[`Discussion`] objects. To get all the Discussions in a single list, run:\n\n```python\n>>> from huggingface_hub import get_repo_discussions\n>>> discussions_list = list(get_repo_discussions(repo_id=\"bert-base-uncased\"))\n```\n\nThe [`Discussion`] object returned by [`HfApi.get_repo_discussions`] contains high-level overview of the\nDiscussion or Pull Request. You can also get more detailed information using [`HfApi.get_discussion_details`]:\n\n```python\n>>> from huggingface_hub import get_discussion_details", "### The client library, `huggingface_hub`\n\nThis repository hosts the `huggingface_hub`, the client library that interfaces any Python script with the Hugging Face Hub.\nIts implementation lives in `src/huggingface_hub` while the tests are located in `tests/`.\n\nThere are many ways you can contribute to this client library:\n* Fixing outstanding issues with the existing code;\n* Contributing to the examples or to the documentation;\n* Submitting issues related to bugs or desired new features.\n\n## Submitting a new issue or feature request\n\nDo your best to follow these guidelines when submitting an issue or a feature\nrequest. It will make it easier for us to come back to you quickly and with good\nfeedback.\n\n### Did you find a bug?\n\nThe `huggingface_hub` library is robust and reliable thanks to the users who notify us of\nthe problems they encounter. So thank you for reporting an issue.\n\nFirst, we would really appreciate it if you could **make sure the bug was not\nalready reported** (use the search bar on Github under Issues).\n\nDid not find it? :( So we can act quickly on it, please follow these steps:\n\n* A short, self-contained, code snippet that allows us to reproduce the bug in less than 30s;\n* Provide the *full* traceback if an exception is raised by copying the text from your terminal in the issue description.\n* Include information about your local setup. You can dump this information by running `huggingface-cli env` in your terminal;\n\n### Do you want a new feature?\n\nA good feature request addresses the following points:", "Pull requests and Discussions\n\nHub Pull requests and Discussions allow users to do community contributions to repositories. Pull requests and discussions work the same for all the repo types.\n\nAt a high level, the aim is to build a simpler version of other git hosts' (like GitHub's) PRs and Issues:\n- no forks are involved: contributors push to a special `ref` branch directly on the source repo.\n- there's no hard distinction between discussions and PRs: they are essentially the same so they are displayed in the same lists.\n- they are streamlined for ML (i.e. models/datasets/spaces repos), not arbitrary repos.\n\n_Note, Pull Requests and discussions can be enabled or disabled from the [repository settings](./repositories-settings#disabling-discussions-pull-requests)_\n\n## List\n\nBy going to the community tab in any repository, you can see all Discussions and Pull requests. You can also filter to only see the ones that are open.\n\n\n<div class=\"flex justify-center\">\n<img class=\"block dark:hidden\" src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/hub/discussions-list.png\"/>\n<img class=\"hidden dark:block\" src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/hub/discussions-list-dark.png\"/>\n</div>\n\n## View", "### Advanced programmatic repository management \n\nThe `Repository` class helps manage both offline Git repositories and Hugging\nFace Hub repositories. Using the `Repository` class requires `git` and `git-lfs`\nto be installed.\n\nInstantiate a `Repository` object by calling it with a path to a local Git\nclone/repository:\n\n```python\n>>> from huggingface_hub import Repository\n>>> repo = Repository(\"<path>/<to>/<folder>\")\n```\n\nThe `Repository` takes a `clone_from` string as parameter. This can stay as\n`None` for offline management, but can also be set to any URL pointing to a Git\nrepo to clone that repository in the specified directory:\n\n```python\n>>> repo = Repository(\"huggingface-hub\", clone_from=\"https://github.com/huggingface/huggingface_hub\")\n```\n\nThe `clone_from` method can also take any Hugging Face model ID as input, and\nwill clone that repository:\n\n```python\n>>> repo = Repository(\"w2v2\", clone_from=\"facebook/wav2vec2-large-960h-lv60\")\n```"], "eval_score_GPT4": "4", "eval_feedback_GPT4": "The response correctly identifies the `HfApi` class from the `huggingface_hub` library as the tool to retrieve Discussions and Pull Requests from a given repository on the Hugging Face Hub. The additional details provided about the `get_repo_discussions` method and its filtering capabilities are accurate and relevant to the instruction, even though the response is cut off and does not finish the explanation about Pull Requests. However, the core requirement of the instruction is met by correctly naming the class."}, {"question": "What is the name of the new library introduced by Hugging Face for hosting scikit-learn models?\n", "true_answer": "Skops", "source_doc": "huggingface/blog/blob/main/skops.md", "generated_answer": "The new library introduced by Hugging Face for hosting scikit-learn models is called Skops. This information can be found in the context provided, specifically in the title and body of the first document (0: --) titled \"Introducing Skops\".", "retrieved_docs": ["--\ntitle: \"Introducing Skops\"\nthumbnail: /blog/assets/94_skops/introducing_skops.png\nauthors:\n- user: merve\n- user: adrin\n- user: BenjaminB\n---\n\n# Introducing Skops\n\n\n##\u00a0Introducing Skops\n\nAt Hugging Face, we are working on tackling various problems in open-source machine learning, including, hosting models securely and openly, enabling reproducibility, explainability and collaboration. We are thrilled to introduce you to our new library: Skops! With Skops, you can host your scikit-learn models on the Hugging Face Hub, create model cards for model documentation and collaborate with others.\n\nLet's go through an end-to-end example: train a model first, and see step-by-step how to leverage Skops for sklearn in production.\n\n```python\n# let's import the libraries first\nimport sklearn\nfrom sklearn.datasets import load_breast_cancer\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.model_selection import train_test_split\n\n# Load the data and split\nX, y = load_breast_cancer(as_frame=True, return_X_y=True)\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.3, random_state=42\n)\n\n# Train the model\nmodel = DecisionTreeClassifier().fit(X_train, y_train)\n```", "## Further References\n\n- This Hugging Face implementation uses [this scikit-learn implementation](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.matthews_corrcoef.html)", "Integrate your library with the Hub\n\nThe Hugging Face Hub aims to facilitate sharing machine learning models, checkpoints, and artifacts. This endeavor includes integrating the Hub into many of the amazing third-party libraries in the community. Some of the ones already integrated include [spaCy](https://spacy.io/usage/projects#huggingface_hub), [AllenNLP](https://allennlp.org/), and [timm](https://rwightman.github.io/pytorch-image-models/), among many others. Integration means users can download and upload files to the Hub directly from your library. We hope you will integrate your library and join us in democratizing artificial intelligence for everyone.\n\nIntegrating the Hub with your library provides many benefits, including:\n\n- Free model hosting for you and your users.\n- Built-in file versioning - even for huge files - made possible by [Git-LFS](https://git-lfs.github.com/).\n- All public models are powered by the [Inference API](https://huggingface.co/docs/api-inference/index).\n- In-browser widgets allow users to interact with your hosted models directly.\n\nThis tutorial will help you integrate the Hub into your library so your users can benefit from all the features offered by the Hub.\n\nBefore you begin, we recommend you create a [Hugging Face account](https://huggingface.co/join) from which you can manage your repositories and files.", "## Using the Hugging Face Client Library\n\nYou can use the [`huggingface_hub`](https://github.com/huggingface/huggingface_hub) library to create, delete, update and retrieve information from repos. You can also download files from repos or integrate them into your library! For example, you can quickly load a Scikit-learn model with a few lines.\n\n```py\nfrom huggingface_hub import hf_hub_download\nimport joblib\n\nREPO_ID = \"YOUR_REPO_ID\"\nFILENAME = \"sklearn_model.joblib\"\n\nmodel = joblib.load(\n    hf_hub_download(repo_id=REPO_ID, filename=FILENAME)\n)\n```\n\n## Using Git\n\nSince all models on the Model Hub are Git repositories, you can clone the models locally by running:\n\n```bash\ngit lfs install\ngit clone git@hf.co:<MODEL ID> # example: git clone git@hf.co:bigscience/bloom\n```\n\nIf you have write-access to the particular model repo, you'll also have the ability to commit and push revisions to the model.\n\nAdd your SSH public key to [your user settings](https://huggingface.co/settings/keys) to push changes and/or access private repos.", "### Visibility for your work \n\nThe Hugging Face Hub has become the central Hub for people to collaborate on open machine learning. Making your datasets available via the Hugging Face Hub ensures it is visible to a wide audience of machine learning researchers. The Hub makes it possible to expose links between datasets, models and demos which makes it easier to see how people are using your datasets for training models and creating demos. \n\n### Tools for exploring and working with datasets\n\nThere are a growing number of tools being created which make it easier to understand datasets hosted on the Hugging Face Hub. \n\n### Tools for loading datasets hosted on the Hugging Face Hub \n\nDatasets shared on the Hugging Face Hub can be loaded via a variety of tools. The [`datasets`](https://huggingface.co/docs/datasets/) library is a Python library which can directly load datasets from the huggingface hub via a `load_dataset` command. The `datasets` library is optimized for working with large datasets (including datasets which won't fit into memory) and supporting machine learning workflows. \n\nAlongside this many of the datasets on the Hub can also be loaded directly into [`Pandas`](https://pandas.pydata.org/), [`Polars`](https://www.pola.rs/), and [`DuckDB`](https://duckdb.org/). This [page](https://huggingface.co/docs/datasets-server/parquet_process) provides a more detailed overview of the different ways you can load datasets from the Hub.", "## Support for third-party libraries\n\nCentral to the Hugging Face ecosystem is the [Hugging Face Hub](https://huggingface.co/docs/hub), which lets people collaborate effectively on Machine Learning. As mentioned earlier, we not only support models from \ud83e\udd17 Transformers on the Hub but also models from other third-party libraries. To this end, we provide [several utilities](https://huggingface.co/docs/hub/models-adding-libraries) so that you can integrate your own library with the Hub. One of the primary advantages of doing this is that it becomes very easy to share artifacts (such as models and datasets) with the community, thereby making it easier for your users to try out your models.\n\nWhen you have your models hosted on the Hub, you can also [add custom inference widgets](https://github.com/huggingface/api-inference-community) for them. Inference widgets allow users to quickly check out the models. This helps with improving user engagement.\n\n<div align=\"center\">\n<img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/cv_state/task_widget_generation.png\"/>\n</div>\n\n## Spaces for computer vision demos", "!--\u26a0\ufe0f Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\nrendered properly in your Markdown viewer.\n-->\n\n# Integrate any ML framework with the Hub\n\nThe Hugging Face Hub makes hosting and sharing models with the community easy. It supports\n[dozens of libraries](https://huggingface.co/docs/hub/models-libraries) in the Open Source ecosystem. We are always\nworking on expanding this support to push collaborative Machine Learning forward. The `huggingface_hub` library plays a\nkey role in this process, allowing any Python script to easily push and load files.\n\nThere are four main ways to integrate a library with the Hub:\n1. **Push to Hub:** implement a method to upload a model to the Hub. This includes the model weights, as well as\n   [the model card](https://huggingface.co/docs/huggingface_hub/how-to-model-cards) and any other relevant information\n   or data necessary to run the model (for example, training logs). This method is often called `push_to_hub()`.\n2. **Download from Hub:** implement a method to load a model from the Hub. The method should download the model\n   configuration/weights and load the model. This method is often called `from_pretrained` or `load_from_hub()`.\n3. **Inference API:** use our servers to run inference on models supported by your library for free.\n4. **Widgets:** display a widget on the landing page of your models on the Hub. It allows users to quickly try a model\n   from the browser."], "eval_score_GPT4": "5", "eval_feedback_GPT4": "The response correctly identifies the name of the new library introduced by Hugging Face for hosting scikit-learn models as \"Skops.\" The information provided aligns with the reference answer, indicating that the response is accurate and factual."}, {"question": "What is the purpose of Textual Inversion?\n", "true_answer": "Textual Inversion is a training method for personalizing models by learning new text embeddings from a few example images.", "source_doc": "huggingface/diffusers/blob/main/docs/source/en/api/loaders/textual_inversion.md", "generated_answer": "Textual Inversion is a technique for learning a specific concept from some images and using it to generate new images conditioned on that concept. Its purpose is to personalize text-to-image models like Stable Diffusion on one's own images by teaching them about new concepts using just 3-5 examples. This allows for the generation of images involving the concept(s) of interest.", "retrieved_docs": ["[Textual inversion](../training/text_inversion) is a technique for learning a specific concept from some images which you can use to generate new images conditioned on that concept.\n\nCreate a pipeline and use the [`~loaders.TextualInversionLoaderMixin.load_textual_inversion`] function to load the textual inversion embeddings (feel free to browse the [Stable Diffusion Conceptualizer](https://huggingface.co/spaces/sd-concepts-library/stable-diffusion-conceptualizer) for 100+ trained concepts):\n\n```py\nimport torch\nfrom diffusers import StableDiffusionPipeline\nfrom compel import Compel, DiffusersTextualInversionManager\n\npipe = StableDiffusionPipeline.from_pretrained(\n  \"runwayml/stable-diffusion-v1-5\", torch_dtype=torch.float16,\n  use_safetensors=True, variant=\"fp16\").to(\"cuda\")\npipe.load_textual_inversion(\"sd-concepts-library/midjourney-style\")\n```\n\nCompel provides a `DiffusersTextualInversionManager` class to simplify prompt weighting with textual inversion. Instantiate `DiffusersTextualInversionManager` and pass it to the `Compel` class:\n\n```py\ntextual_inversion_manager = DiffusersTextualInversionManager(pipe)\ncompel_proc = Compel(\n    tokenizer=pipe.tokenizer,\n    text_encoder=pipe.text_encoder,\n    textual_inversion_manager=textual_inversion_manager)\n```\n\nIncorporate the concept to condition a prompt with using the `<concept>` syntax:", "## Textual Inversion\n\n[Paper](https://arxiv.org/abs/2208.01618)\n\n[Textual Inversion](../training/text_inversion) fine-tunes a model to teach it about a new concept. I.e. a few pictures of a style of artwork can be used to generate images in that style.\n\n## ControlNet\n\n[Paper](https://arxiv.org/abs/2302.05543)\n\n[ControlNet](../api/pipelines/controlnet) is an auxiliary network which adds an extra condition.\nThere are 8 canonical pre-trained ControlNets trained on different conditionings such as edge detection, scribbles,\ndepth maps, and semantic segmentations.\n\n## Prompt Weighting\n\n[Prompt weighting](../using-diffusers/weighted_prompts) is a simple technique that puts more attention weight on certain parts of the text\ninput.\n\n## Custom Diffusion\n\n[Paper](https://arxiv.org/abs/2212.04488)\n\n[Custom Diffusion](../training/custom_diffusion) only fine-tunes the cross-attention maps of a pre-trained\ntext-to-image diffusion model. It also allows for additionally performing Textual Inversion. It supports\nmulti-concept training by design. Like DreamBooth and Textual Inversion, Custom Diffusion is also used to\nteach a pre-trained text-to-image diffusion model about new concepts to generate outputs involving the\nconcept(s) of interest.\n\n## Model Editing", "# Textual Inversion fine-tuning example\n\n[Textual inversion](https://arxiv.org/abs/2208.01618) is a method to personalize text2image models like stable diffusion on your own images using just 3-5 examples.\nThe `textual_inversion.py` script shows how to implement the training procedure and adapt it for stable diffusion.\n\n## Training with Intel Extension for PyTorch\n\nIntel Extension for PyTorch provides the optimizations for faster training and inference on CPUs. You can leverage the training example \"textual_inversion.py\". Follow the [instructions](https://github.com/huggingface/diffusers/tree/main/examples/textual_inversion) to get the model and [dataset](https://huggingface.co/sd-concepts-library/dicoo2) before running the script.\n\nThe example supports both single node and multi-node distributed training:\n\n### Single node training\n\n```bash\nexport MODEL_NAME=\"CompVis/stable-diffusion-v1-4\"\nexport DATA_DIR=\"path-to-dir-containing-dicoo-images\"", "# Textual Inversion fine-tuning example\n\n[Textual inversion](https://arxiv.org/abs/2208.01618) is a method to personalize text2image models like stable diffusion on your own images using just 3-5 examples.\nThe `textual_inversion.py` script shows how to implement the training procedure and adapt it for stable diffusion.\n\n## Running on Colab\n\nColab for training\n[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/huggingface/notebooks/blob/main/diffusers/sd_textual_inversion_training.ipynb)\n\nColab for inference\n[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/huggingface/notebooks/blob/main/diffusers/stable_conceptualizer_inference.ipynb)\n\n## Running locally with PyTorch\n### Installing the dependencies\n\nBefore running the scripts, make sure to install the library's training dependencies:\n\n**Important**", "# Textual Inversion fine-tuning example\n\n[Textual inversion](https://arxiv.org/abs/2208.01618) is a method to personalize text2image models like stable diffusion on your own images using just 3-5 examples.\nThe `textual_inversion.py` script shows how to implement the training procedure and adapt it for stable diffusion.\n\n## Running on Colab\n\nColab for training\n[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/huggingface/notebooks/blob/main/diffusers/sd_textual_inversion_training.ipynb)\n\nColab for inference\n[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/huggingface/notebooks/blob/main/diffusers/stable_conceptualizer_inference.ipynb)\n\n## Running locally with PyTorch\n### Installing the dependencies\n\nBefore running the scripts, make sure to install the library's training dependencies:\n\n**Important**", "Textual inversion can also be trained on undesirable things to create *negative embeddings* to discourage a model from generating images with those undesirable things like blurry images or extra fingers on a hand. This can be an easy way to quickly improve your prompt. You'll also load the embeddings with [`~loaders.TextualInversionLoaderMixin.load_textual_inversion`], but this time, you'll need two more parameters:\n\n- `weight_name`: specifies the weight file to load if the file was saved in the \ud83e\udd17 Diffusers format with a specific name or if the file is stored in the A1111 format\n- `token`: specifies the special word to use in the prompt to trigger the embeddings\n\nLet's load the [sayakpaul/EasyNegative-test](https://huggingface.co/sayakpaul/EasyNegative-test) embeddings:\n\n```py\npipeline.load_textual_inversion(\n    \"sayakpaul/EasyNegative-test\", weight_name=\"EasyNegative.safetensors\", token=\"EasyNegative\"\n)\n```\n\nNow you can use the `token` to generate an image with the negative embeddings:\n\n```py\nprompt = \"A cute brown bear eating a slice of pizza, stunning color scheme, masterpiece, illustration, EasyNegative\"\nnegative_prompt = \"EasyNegative\"\n\nimage = pipeline(prompt, negative_prompt=negative_prompt, num_inference_steps=50).images[0]\nimage\n```", "Distillation for quantization on Textual Inversion models to personalize text2image\n\n[Textual inversion](https://arxiv.org/abs/2208.01618) is a method to personalize text2image models like stable diffusion on your own images._By using just 3-5 images new concepts can be taught to Stable Diffusion and the model personalized on your own images_\nThe `textual_inversion.py` script shows how to implement the training procedure and adapt it for stable diffusion.\nWe have enabled distillation for quantization in `textual_inversion.py` to do quantization aware training as well as distillation on the model generated by Textual Inversion method.\n\n## Installing the dependencies\n\nBefore running the scripts, make sure to install the library's training dependencies:\n\n```bash\npip install -r requirements.txt\n```\n\n## Prepare Datasets\n\nOne picture which is from the huggingface datasets [sd-concepts-library/dicoo2](https://huggingface.co/sd-concepts-library/dicoo2) is needed, and save it to the `./dicoo` directory. The picture is shown below:\n\n<a href=\"https://huggingface.co/sd-concepts-library/dicoo2/blob/main/concept_images/1.jpeg\">\n    <img src=\"https://huggingface.co/sd-concepts-library/dicoo2/resolve/main/concept_images/1.jpeg\" width = \"300\" height=\"300\">\n</a>"], "eval_score_GPT4": "4", "eval_feedback_GPT4": "Feedback: The response provided is mostly correct, accurate, and factual. It correctly identifies Textual Inversion as a technique for learning specific concepts from images and using those concepts to generate new images. It also accurately describes the purpose of personalizing text-to-image models and the process of teaching these models new concepts with a small number of examples. The response aligns well with the reference answer, which states that Textual Inversion is a training method for personalizing models by learning new text embeddings from a few example images. However, the response could be more concise and focused on the aspect of learning new text embeddings, as mentioned in the reference answer."}, {"question": "What is the recommended multiple of batch size for fp16 data type on an A100 GPU?\n", "true_answer": "64", "source_doc": "huggingface/transformers/blob/main/docs/source/en/perf_train_gpu_one.md", "generated_answer": "Based on the context provided, the recommended multiple of batch size for fp16 data type on an A100 GPU is 8 or 16, as mentioned in the extract from the Deepspeed-ZeRO documentation. This allows for further division of throughput when processing multiple generate streams in parallel using Deepspeed-ZeRO. For example, with 8x80 A100 GPUs, a batch size of 64 can be processed with", "retrieved_docs": ["Since Deepspeed-ZeRO can process multiple generate streams in parallel its throughput can be further divided by 8 or 16, depending on whether 8 or 16 GPUs were used during the `generate` call. And, of course, it means that it can process a batch size of 64 in the case of 8x80 A100 (the table above) and thus the throughput is about 4msec - so all 3 solutions are very close to each other.\n\nLet's revisit again how these numbers were calculated. To generate 100 new tokens for a batch size of 128 took 8832 msecs in real time when using Deepspeed-Inference in fp16 mode. So now to calculate the throughput we did: walltime/(batch_size*new_tokens) or `8832/(128*100) = 0.69`.\n\nNow let's look at the power of quantized int8-based models provided by Deepspeed-Inference and BitsAndBytes, as it requires only half the original GPU memory of inference in bfloat16 or float16.\n\nThroughput in msecs 4x80GB A100:\n\n| project       bs  |      1 |     8 |    16 |    32 |   64 | 128  |\n| :---------------- | :----- | :---- | :---- | :---- | :--- | :--- |\n| accelerate   int8 | 284.15 | 40.14 | 21.97 |  oom  |      |      |\n| ds-inference int8 | 156.51 | 20.11 | 10.38 |  5.50 | 2.96 | oom  |", "# using bitsandbytes fp4/fp16 scheme\nCUDA_VISIBLE_DEVICES=0 python benchmark_gptq.py --model meta-llama/Llama-2-13b-chat-hf --sweep --num-batches 10 --task text-generation --prefill --bitsandbytes --generate\n```\n\nThe benchmark below is for a prompt length of 512, measuring only the prefill step on a single NVIDIA A100-SXM4-80GB GPU. The forward is repeated 10 times. This benchmark typically corresponds to the forward during training (to the difference that here `generate` is called, which has some overhead).\n\n### Batch size = 1", "*BS* is the batch size.\nThe Gaudi runs were performed in *bfloat16* precision and the A100 runs in *fp16* precision (more information [here](https://huggingface.co/docs/diffusers/optimization/fp16)). All runs were *single-device* runs.\n\n\n### Fine-tuning T5-3B\n\nWith 96 GB of memory per device, Gaudi2 enables running much bigger models. For instance, we managed to fine-tune T5-3B (containing 3 billion parameters) with gradient checkpointing being the only applied memory optimization. This is not possible on first-gen Gaudi.\n[Here](https://huggingface.co/regisss/t5-3b-summarization-gaudi-2/tensorboard?scroll=1#scalars) are the logs of this run where the model was fine-tuned on the CNN DailyMail dataset for text summarization using [this script](https://github.com/huggingface/optimum-habana/tree/main/examples/summarization).\n\nThe results we achieved are presented in the table below. **Gaudi2 is x2.44 faster than A100 80GB.** We observe that we cannot fit a batch size larger than 1 on Gaudi2 here. This is due to the memory space taken by the graph where operations are accumulated during the first iteration of the run. Habana is working on optimizing the memory footprint in future releases of SynapseAI. We are looking forward to expanding this benchmark using newer versions of Habana's SDK and also using [DeepSpeed](https://www.deepspeed.ai/) to see if the same trend holds.", "</center>\n\n*BS* is the batch size per device. The Gaudi runs were performed in mixed precision (bf16/fp32) and the A100 runs in fp16. All runs were *distributed* runs on *8 devices*.\n\n\n### Generating Images from Text with Stable Diffusion\n\nOne of the main new features of \ud83e\udd17 Optimum Habana release 1.3 is [the support for Stable Diffusion](https://huggingface.co/docs/optimum/habana/usage_guides/stable_diffusion). It is now very easy to generate images from text on Gaudi. Unlike with \ud83e\udd17 Diffusers on GPUs, images are generated by batches. Due to model compilation times, the first two batches will be slower than the following iterations. In this benchmark, these first two iterations were discarded to compute the throughputs for both first-gen Gaudi and Gaudi2.\n\n[This script](https://github.com/huggingface/optimum-habana/tree/main/examples/stable-diffusion) was run for a batch size of 8 samples. It uses the [`Habana/stable-diffusion`](https://huggingface.co/Habana/stable-diffusion) Gaudi configuration.", "# batch size has to be divisible by world_size, but can be bigger than world_size\ntrain_batch_size = 1 * world_size\n\n# ds_config notes\n#\n# - enable bf16 if you use Ampere or higher GPU - this will run in mixed precision and will be\n# faster.\n#\n# - for older GPUs you can enable fp16, but it'll only work for non-bf16 pretrained models - e.g.\n# all official t5 models are bf16-pretrained\n#\n# - set offload_param.device to \"none\" or completely remove the `offload_param` section if you don't\n# - want CPU offload\n#\n# - if using `offload_param` you can manually finetune stage3_param_persistence_threshold to control\n# - which params should remain on gpus - the larger the value the smaller the offload size\n#\n# For indepth info on Deepspeed config see\n# https://huggingface.co/docs/transformers/main/main_classes/deepspeed", "Additional benchmarks could be done in the act-order case.\n\nFrom the bencharmk, it appears that Exllama kernel is the best-in-class for GPTQ, although it is rather slow for larger batch sizes. The memory savings are not exactly of x4 although weights are in int4. This can be explained by the possible static buffers used by the kernels, the CUDA context (taken into account in the measurements), and the KV cache that is still in fp16.\n\nBitsandbytes uses the fp4 scheme, with the compute in fp16.\n\n### Batch size = 1", "Table 2: Benchmarking FSDP on GPT-2 XL (1.5B) model\n\nFrom Table 2, we can observe that DDP (w and w/o fp16) isn\u2019t even able to run with batch size of 1 and results in CUDA OOM error. FSDP with Zero-Stage 3 is able to be run on 2 GPUs with batch size of 5 (effective batch size =10 (5 X 2)). FSDP with CPU offload can further increase the max batch size to 14 per GPU when using 2 GPUs. **FSDP with CPU offload enables training GPT-2 1.5B model on a single GPU with a batch size of 10**. This enables ML practitioners with minimal compute resources to train such large models, thereby democratizing large model training.\n\n## Capabilities and limitations of the FSDP Integration\n\nLet\u2019s dive into the current support that Accelerate provides for FSDP integration and the known limitations.\n\n**Required PyTorch version for FSDP support**: PyTorch Nightly (or 1.12.0 if you read this after it has been released) as the model saving with FSDP activated is only available with recent fixes.\n\n**Configuration through CLI:**\n\n1. **Sharding Strategy**: [1] FULL_SHARD, [2] SHARD_GRAD_OP\n2. **Min Num Params**: FSDP's minimum number of parameters for Default Auto Wrapping.\n3. **Offload Params**: Decides Whether to offload parameters and gradients to CPU."], "eval_score_GPT4": "3", "eval_feedback_GPT4": "The response provided is somewhat correct as it mentions multiples of 8 or 16, which are indeed common multiples used for batch sizes in various scenarios. However, the response does not directly answer the question with a specific recommended multiple, which according to the reference answer is 64. The response also includes additional context about Deepspeed-ZeRO and GPU configurations, which is not directly relevant to the question asked. Based on the score rubric, the response is somewhat correct, accurate, and/or factual, but it lacks the precision and directness of the reference answer."}, {"question": "How do you run a Gradio Blocks app in reload mode using a Python IDE?\n", "true_answer": "Run `gradio run.py` in the terminal.", "source_doc": "gradio-app/gradio/blob/main/guides/09_other-tutorials/developing-faster-with-reload-mode.md", "generated_answer": "To run a Gradio Blocks app in reload mode using a Python IDE, follow these steps:\n\n1. Make sure your app is written in Blocks format using the `gr.Blocks()` function.\n2. Save your app as a Python file, for example `run.py`.\n3. Instead of running `python run.py` to launch your app, run `gradio run.py` in your terminal.\n4. Gradio will now", "retrieved_docs": ["If your demo code is in a script named `app.py`, instead of running `python app.py` you can now run `gradio app.py` and that will launch the demo in reload mode:\n\n```bash\nLaunching in reload mode on: http://127.0.0.1:7860 (Press CTRL+C to quit)\nWatching...\nWARNING: The --reload flag should not be used in production on Windows.\n```\n\nIf you're working from a Jupyter or Colab Notebook, use these magic commands instead: `%load_ext gradio` when you import gradio, and `%%blocks` in the top of the cell with the demo code. Here's an example that shows how much faster the development becomes:\n\n![Blocks](https://user-images.githubusercontent.com/9021060/178986488-ed378cc8-5141-4330-ba41-672b676863d0.gif)\n\n###### 3. Inpainting Support on `gr.Image()` \ud83c\udfa8\n\nWe updated the Image component to add support for inpainting demos. It works by adding `tool=\"sketch\"` as a parameter, that passes both an image and a sketchable mask to your prediction function.\n\nHere's an example from the [LAMA space](https://huggingface.co/spaces/akhaliq/lama):", "If your demo code is in a script named `app.py`, instead of running `python app.py` you can now run `gradio app.py` and that will launch the demo in reload mode:\n\n```bash\nLaunching in reload mode on: http://127.0.0.1:7860 (Press CTRL+C to quit)\nWatching...\nWARNING: The --reload flag should not be used in production on Windows.\n```\n\nIf you're working from a Jupyter or Colab Notebook, use these magic commands instead: `%load_ext gradio` when you import gradio, and `%%blocks` in the top of the cell with the demo code. Here's an example that shows how much faster the development becomes:\n\n![Blocks](https://user-images.githubusercontent.com/9021060/178986488-ed378cc8-5141-4330-ba41-672b676863d0.gif)\n\n###### 3. Inpainting Support on `gr.Image()` \ud83c\udfa8\n\nWe updated the Image component to add support for inpainting demos. It works by adding `tool=\"sketch\"` as a parameter, that passes both an image and a sketchable mask to your prediction function.\n\nHere's an example from the [LAMA space](https://huggingface.co/spaces/akhaliq/lama):", "Developing Faster with Auto-Reloading\n\n**Prerequisite**: This Guide requires you to know about Blocks. Make sure to [read the Guide to Blocks first](https://gradio.app/guides/quickstart/#blocks-more-flexibility-and-control).\n\nThis guide covers auto reloading, reloading in a Python IDE, and using gradio with Jupyter Notebooks.\n\n## Why Auto-Reloading?\n\nWhen you are building a Gradio demo, particularly out of Blocks, you may find it cumbersome to keep re-running your code to test your changes.\n\nTo make it faster and more convenient to write your code, we've made it easier to \"reload\" your Gradio apps instantly when you are developing in a **Python IDE** (like VS Code, Sublime Text, PyCharm, or so on) or generally running your Python code from the terminal. We've also developed an analogous \"magic command\" that allows you to re-run cells faster if you use **Jupyter Notebooks** (or any similar environment like Colab).\n\nThis short Guide will cover both of these methods, so no matter how you write Python, you'll leave knowing how to build Gradio apps faster.\n\n## Python IDE Reload \ud83d\udd25\n\nIf you are building Gradio Blocks using a Python IDE, your file of code (let's name it `run.py`) might look something like this:\n\n```python\nimport gradio as gr", "```python\nimport gradio as gr\n\nwith gr.Blocks() as demo:\n    gr.Markdown(\"# Greetings from Gradio!\")\n    inp = gr.Textbox(placeholder=\"What is your name?\")\n    out = gr.Textbox()\n\n    inp.change(fn=lambda x: f\"Welcome, {x}!\",\n               inputs=inp,\n               outputs=out)\n\nif __name__ == \"__main__\":\n    demo.launch()\n```\n\nThe problem is that anytime that you want to make a change to your layout, events, or components, you have to close and rerun your app by writing `python run.py`.\n\nInstead of doing this, you can run your code in **reload mode** by changing 1 word: `python` to `gradio`:\n\nIn the terminal, run `gradio run.py`. That's it!\n\nNow, you'll see that after you'll see something like this:\n\n```bash\nWatching: '/Users/freddy/sources/gradio/gradio', '/Users/freddy/sources/gradio/demo/'\n\nRunning on local URL:  http://127.0.0.1:7860\n```", "then run:\n\n```\ngradio app.py\n```\n\nThis will start the backend server in reload mode, which will watch for changes in the `gradio` folder and reload the app if changes are made. By default, Gradio will launch on port 7860. You can also just use `python app.py`, but this won't automatically trigger updates. \n\n\nIf you're making frontend changes, start the frontend server:\n\n```\npnpm dev\n```\nThis will open a separate browser tab. By default, Gradio will launch this on port 9876. Any changes to the frontend will also reload automatically in the browser. For more information about developing in the frontend, you can refer to [js/README.md](js/README.md).\n\nWe also have demos of all our components in the `/gradio/demo` directory. To get our simple gradio Chatbot running locally:\n\n```\ngradio demo/chatbot_simple/run.py\n```\n\n\n## \ud83e\uddea Testing\n\nWe use Pytest, Playwright and Vitest to test our code. \n\n- The Python tests are located in `/test`. To run these tests:\n\n```\nbash scripts/run_all_tests.sh\n```\n\n- The frontend unit tests are any defined with the filename `*.test.ts`. To run them:\n\n```\npnpm test\n```", "Type your name in the textbox on the left, drag the slider, and then press the Submit button. You should see a friendly greeting on the right.\n\nTip: When developing locally, you can run your Gradio app in <strong>hot reload mode</strong>, which automatically reloads the Gradio app whenever you make changes to the file. To do this, simply type in <code>gradio</code> before the name of the file instead of <code>python</code>. In the example above, you would type: `gradio app.py` in your terminal. Learn more about hot reloading in the <a href=\"https://www.gradio.app/guides/developing-faster-with-reload-mode\">Hot Reloading Guide</a>.\n\n\n**Understanding the `Interface` Class**\n\nYou'll notice that in order to make your first demo, you created an instance of the `gr.Interface` class. The `Interface` class is designed to create demos for machine learning models which accept one or more inputs, and return one or more outputs. \n\nThe `Interface` class has three core arguments:\n\n- `fn`: the function to wrap a user interface (UI) around\n- `inputs`: the Gradio component(s) to use for the input. The number of components should match the number of arguments in your function.\n- `outputs`: the Gradio component(s) to use for the output. The number of components should match the number of return values from your function.", "if __name__ == \"__main__\":\n    my_demo.launch()\n```\n\nThen you would launch it in reload mode like this: `gradio run.py my_demo`.\n\nBy default, the Gradio use UTF-8 encoding for scripts. **For reload mode**, If you are using encoding formats other than UTF-8 (such as cp1252), make sure you've done like this:\n\n1. Configure encoding declaration of python script, for example: `# -*- coding: cp1252 -*-`\n2. Confirm that your code editor has identified that encoding format. \n3. Run like this: `gradio run.py --encoding cp1252`\n\n\ud83d\udd25 If your application accepts command line arguments, you can pass them in as well. Here's an example:\n\n```python\nimport gradio as gr\nimport argparse\n\nparser = argparse.ArgumentParser()\nparser.add_argument(\"--name\", type=str, default=\"User\")\nargs, unknown = parser.parse_known_args()\n\nwith gr.Blocks() as demo:\n    gr.Markdown(f\"# Greetings {args.name}!\")\n    inp = gr.Textbox()\n    out = gr.Textbox()\n\n    inp.change(fn=lambda x: x, inputs=inp, outputs=out)\n\nif __name__ == \"__main__\":\n    demo.launch()\n```"], "eval_score_GPT4": "3", "eval_feedback_GPT4": "Feedback: The response provided is mostly correct and factual as it correctly identifies the use of `gr.Blocks()` and the need to save the app as a Python file. However, the response inaccurately suggests running `gradio run.py` in the terminal, which is not the correct command for running a Gradio Blocks app in reload mode using a Python IDE. The correct command should be `python run.py` with the appropriate flags for reload mode if necessary. The response also ends abruptly without completing the fourth step, which leaves the instructions incomplete. Based on the score rubric, the response is mostly correct but has a factual inaccuracy regarding the command to use and is incomplete."}, {"question": "What command is used to install the development version of the \ud83e\udd17 Transformers library in a Python virtual environment?\n", "true_answer": "pip install \"transformers[sentencepiece]\"", "source_doc": "huggingface/course/blob/main/chapters/en/chapter0/1.mdx", "generated_answer": "To install the development version of the \ud83e\udd17 Transformers library in a Python virtual environment, you can use the following command:\n\n```bash\npip install transformers[sentencepiece]\n```\n\nThis will install a very light version of \ud83e\udd17 Transformers that comes with all the required dependencies for most use cases. If you prefer to use a Python virtual environment, you should create one using the `python -m v", "retrieved_docs": ["You can make sure the package was correctly installed by importing it within your Python runtime:\n\n```\nimport transformers\n```\n\n<div class=\"flex justify-center\">\n<img src=\"https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter0/install.gif\" alt=\"A gif showing the result of the two commands above: installation and import\" width=\"80%\"/>\n</div>\n\nThis installs a very light version of \ud83e\udd17 Transformers. In particular, no specific machine learning frameworks (like PyTorch or TensorFlow) are installed. Since we'll be using a lot of different features of the library, we recommend installing the development version, which comes with all the required dependencies for pretty much any imaginable use case:\n\n```\n!pip install transformers[sentencepiece]\n```\n\nThis will take a bit of time, but then you'll be ready to go for the rest of the course!\n\n## Using a Python virtual environment[[using-a-python-virtual-environment]]\n\nIf you prefer to use a Python virtual environment, the first step is to install Python on your system. We recommend following [this guide](https://realpython.com/installing-python/) to get started.\n\nOnce you have Python installed, you should be able to run Python commands in your terminal. You can start by running the following command to ensure that it is correctly installed before proceeding to the next steps: `python --version`. This should print out the Python version now available on your system.", "```\nls -a\n```\n\n```out\n.      ..    .env\n```\n\nYou can jump in and out of your virtual environment with the `activate` and `deactivate` scripts:\n\n```\n# Activate the virtual environment\nsource .env/bin/activate\n\n# Deactivate the virtual environment\nsource .env/bin/deactivate\n```\n\nYou can make sure that the environment is activated by running the `which python` command: if it points to the virtual environment, then you have successfully activated it!\n\n```\nwhich python\n```\n\n```out\n/home/<user>/transformers-course/.env/bin/python\n```\n\n### Installing dependencies[[installing-dependencies]]\n\nAs in the previous section on using Google Colab instances, you'll now need to install the packages required to continue. Again, you can install the development version of \ud83e\udd17 Transformers using the `pip` package manager:\n\n```\npip install \"transformers[sentencepiece]\"\n```\n\nYou're now all set up and ready to go!", "## Install with pip\n\nYou should install \ud83e\udd17 Transformers in a [virtual environment](https://docs.python.org/3/library/venv.html). If you're unfamiliar with Python virtual environments, take a look at this [guide](https://packaging.python.org/guides/installing-using-pip-and-virtual-environments/). A virtual environment makes it easier to manage different projects, and avoid compatibility issues between dependencies.\n\nStart by creating a virtual environment in your project directory:\n\n```bash\npython -m venv .env\n```\n\nActivate the virtual environment. On Linux and MacOs:\n\n```bash\nsource .env/bin/activate\n```\nActivate Virtual environment on Windows\n\n```bash\n.env/Scripts/activate\n```\n\nNow you're ready to install \ud83e\udd17 Transformers with the following command:\n\n```bash\npip install transformers\n```\n\nFor CPU-support only, you can conveniently install \ud83e\udd17 Transformers and a deep learning library in one line. For example, install \ud83e\udd17 Transformers and PyTorch with:\n\n```bash\npip install 'transformers[torch]'\n```\n\n\ud83e\udd17 Transformers and TensorFlow 2.0:\n\n```bash\npip install 'transformers[tf-cpu]'\n```\n\n<Tip warning={true}>\n\nM1 / ARM Users\n    \nYou will need to install the following before installing TensorFLow 2.0\n```\nbrew install cmake\nbrew install pkg-config\n```\n\n</Tip>\n\n\ud83e\udd17 Transformers and Flax:", "First, create a virtual environment with the version of Python you're going to use and activate it.\n\nThen, you will need to install at least one of Flax, PyTorch, or TensorFlow.\nPlease refer to [TensorFlow installation page](https://www.tensorflow.org/install/), [PyTorch installation page](https://pytorch.org/get-started/locally/#start-locally) and/or [Flax](https://github.com/google/flax#quick-install) and [Jax](https://github.com/google/jax#installation) installation pages regarding the specific installation command for your platform.\n\nWhen one of those backends has been installed, \ud83e\udd17 Transformers can be installed using pip as follows:\n\n```bash\npip install transformers\n```\n\nIf you'd like to play with the examples or need the bleeding edge of the code and can't wait for a new release, you must [install the library from source](https://huggingface.co/docs/transformers/installation#installing-from-source).\n\n### With conda\n\nSince Transformers version v4.0.0, we now have a conda channel: `huggingface`.\n\n\ud83e\udd17 Transformers can be installed using conda as follows:\n\n```shell script\nconda install -c huggingface transformers\n```\n\nFollow the installation pages of Flax, PyTorch or TensorFlow to see how to install them with conda.", "The following libraries are required to train a JAX/Flax model with \ud83e\udd17 Transformers and \ud83e\udd17 Datasets on TPU VM:\n\n- [JAX](https://github.com/google/jax/)\n- [Flax](https://github.com/google/flax)\n- [Optax](https://github.com/deepmind/optax)\n- [Transformers](https://github.com/huggingface/transformers)\n- [Datasets](https://github.com/huggingface/datasets)\n\nYou should install the above libraries in a [virtual environment](https://docs.python.org/3/library/venv.html). \nIf you're unfamiliar with Python virtual environments, check out the [user guide](https://packaging.python.org/guides/installing-using-pip-and-virtual-environments/). Create a virtual environment with the version of Python you're going\nto use and activate it.\n\nYou should be able to run the command:\n\n```bash\npython3 -m venv <your-venv-name>\n```\n\nIf this doesn't work, you first might to have install `python3-venv`. You can do this as follows:\n\n```bash\nsudo apt-get install python3-venv\n```\n\nYou can activate your venv by running\n\n```bash\nsource ~/<your-venv-name>/bin/activate\n```", "### Local computer\n\nThe following libraries are required to train a JAX/Flax model with \ud83e\udd17 Transformers and \ud83e\udd17 Datasets:\n\n- [JAX](https://github.com/google/jax/)\n- [Flax](https://github.com/google/flax)\n- [Optax](https://github.com/deepmind/optax)\n- [Transformers](https://github.com/huggingface/transformers)\n- [Datasets](https://github.com/huggingface/datasets)\n\nYou should install the above libraries in a [virtual environment](https://docs.python.org/3/library/venv.html). \nIf you're unfamiliar with Python virtual environments, check out the [user guide](https://packaging.python.org/guides/installing-using-pip-and-virtual-environments/). Create a virtual environment with the version of Python you're going\nto use and activate it.\n\nYou should be able to run the command:\n\n```bash\npython3 -m venv <your-venv-name>\n```\n\nYou can activate your venv by running\n\n```bash\nsource ~/<your-venv-name>/bin/activate\n```", "Check if \ud83e\udd17 Transformers has been properly installed by running the following command:\n\n```bash\npython -c \"from transformers import pipeline; print(pipeline('sentiment-analysis')('I love you'))\"\n```\n\n## Editable install\n\nYou will need an editable install if you'd like to:\n\n* Use the `main` version of the source code.\n* Contribute to \ud83e\udd17 Transformers and need to test changes in the code.\n\nClone the repository and install \ud83e\udd17 Transformers with the following commands:\n\n```bash\ngit clone https://github.com/huggingface/transformers.git\ncd transformers\npip install -e .\n```\n\nThese commands will link the folder you cloned the repository to and your Python library paths. Python will now look inside the folder you cloned to in addition to the normal library paths. For example, if your Python packages are typically installed in `~/anaconda3/envs/main/lib/python3.7/site-packages/`, Python will also search the folder you cloned to: `~/transformers/`.\n\n<Tip warning={true}>\n\nYou must keep the `transformers` folder if you want to keep using the library.\n\n</Tip>\n\nNow you can easily update your clone to the latest version of \ud83e\udd17 Transformers with the following command:\n\n```bash\ncd ~/transformers/\ngit pull\n```\n\nYour Python environment will find the `main` version of \ud83e\udd17 Transformers on the next run.\n\n## Install with conda\n\nInstall from the conda channel `huggingface`:"], "eval_score_GPT4": "3", "eval_feedback_GPT4": "The response provided is partially correct. The command `pip install transformers[sentencepiece]` does indeed install the Transformers library along with the sentencepiece dependency. However, the response fails to specify that this command installs the development version of the library, which is a critical part of the instruction. The response also incorrectly suggests that this command installs a \"very light version\" of the library, which is not accurate in the context of installing the development version. Additionally, the response is cut off and does not finish the explanation about using a Python virtual environment. Based on the score rubric, the response is somewhat correct but lacks the specificity and accuracy required for a full score."}, {"question": "How can you install the Hugging Face Unity API in your Unity project?\n", "true_answer": "To install the Hugging Face Unity API in your Unity project, go to `Window` -> `Package Manager`, click `+` and select `Add Package from git URL`, then enter `https://github.com/huggingface/unity-api.git`.", "source_doc": "huggingface/blog/blob/main/unity-api.md", "generated_answer": "To install the Hugging Face Unity API in your Unity project, follow these steps:\n\n1. Open your Unity project.\n2. Go to \"Window\" > \"Package Manager\".\n3. Click the \"+\" button and select \"Add Package from git URL\".\n4. Enter \"https://github.com/huggingface/unity-api.git\" and click \"Add\".\n5. Once the package is installed, the Hugging Face API", "retrieved_docs": ["--\ntitle: \"How to Install and Use the Hugging Face Unity API\"\nthumbnail: /blog/assets/124_ml-for-games/unity-api-thumbnail.png\nauthors:\n- user: dylanebert\n---\n\n# How to Install and Use the Hugging Face Unity API\n\n<!-- {authors} --> \n\nThe [Hugging Face Unity API](https://github.com/huggingface/unity-api) is an easy-to-use integration of the [Hugging Face Inference API](https://huggingface.co/inference-api), allowing developers to access and use Hugging Face AI models in their Unity projects. In this blog post, we'll walk through the steps to install and use the Hugging Face Unity API.\n\n## Installation\n\n1. Open your Unity project\n2. Go to `Window` -> `Package Manager`\n3. Click `+` and select `Add Package from git URL`\n4. Enter `https://github.com/huggingface/unity-api.git`\n5. Once installed, the Unity API wizard should pop up. If not, go to `Window` -> `Hugging Face API Wizard`\n\n<figure class=\"image text-center\">\n  <img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/124_ml-for-games/packagemanager.gif\">\n</figure>", "6. Enter your API key. Your API key can be created in your [Hugging Face account settings](https://huggingface.co/settings/tokens).\n7. Test the API key by clicking `Test API key` in the API Wizard.\n8. Optionally, change the model endpoints to change which model to use. The model endpoint for any model that supports the inference API can be found by going to the model on the Hugging Face website, clicking `Deploy` -> `Inference API`, and copying the url from the `API_URL` field.\n9. Configure advanced settings if desired. For up-to-date information, visit the project repository at `https://github.com/huggingface/unity-api`\n10. To see examples of how to use the API, click `Install Examples`. You can now close the API Wizard.\n\n<figure class=\"image text-center\">\n  <img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/124_ml-for-games/apiwizard.png\">\n</figure> \n\nNow that the API is set up, you can make calls from your scripts to the API. Let's look at an example of performing a Sentence Similarity task:\n\n```\nusing HuggingFace.API;\n\n/* other code */", "/* other code */\n```\n\n## Supported Tasks and Custom Models\n\nThe Hugging Face Unity API also currently supports the following tasks:\n\n- [Conversation](https://huggingface.co/tasks/conversational)\n- [Text Generation](https://huggingface.co/tasks/text-generation)\n- [Text to Image](https://huggingface.co/tasks/text-to-image)\n- [Text Classification](https://huggingface.co/tasks/text-classification)\n- [Question Answering](https://huggingface.co/tasks/question-answering)\n- [Translation](https://huggingface.co/tasks/translation)\n- [Summarization](https://huggingface.co/tasks/summarization)\n- [Speech Recognition](https://huggingface.co/tasks/automatic-speech-recognition)\n\nUse the corresponding methods provided by the `HuggingFaceAPI` class to perform these tasks.\n\nTo use your own custom model hosted on Hugging Face, change the model endpoint in the API Wizard.\n\n## Usage Tips\n\n1. Keep in mind that the API makes calls asynchronously, and returns a response or error via callbacks.\n2. Address slow response times or performance issues by changing model endpoints to lower resource models.\n\n## Conclusion\n\nThe Hugging Face Unity API offers a simple way to integrate AI models into your Unity projects. We hope you found this tutorial helpful. If you have any questions or would like to get more involved in using Hugging Face for Games, join the [Hugging Face Discord](https://hf.co/join/discord)!", "### 4. Speech Recognition\n\nNext, we'll want to use the Hugging Face Unity API to run speech recognition on our encoded audio. To do so, we'll create a `SendRecording()` method:\n```\nusing HuggingFace.API;\n\nprivate void SendRecording() {\n    HuggingFaceAPI.AutomaticSpeechRecognition(bytes, response => {\n        text.color = Color.white;\n        text.text = response;\n    }, error => {\n        text.color = Color.red;\n        text.text = error;\n    });\n}\n```\nThis will send the encoded audio to the API, displaying the response in white if successful, otherwise the error message in red.\n\nDon't forget to call `SendRecording()` at the end of the `StopRecording()` method:\n```\nprivate void StopRecording() {\n    /* other code */\n    SendRecording();\n}\n```\n\n### 5. Final Touches\n\nFinally, let's improve the UX of this demo a bit using button interactability and status messages.\n\nThe Start and Stop buttons should only be interactable when appropriate, i.e. when a recording is ready to be started/stopped.\n\nThen, set the response text to a simple status message while recording or waiting for the API.\n\nThe finished script should look something like this:\n```\nusing System.IO;\nusing HuggingFace.API;\nusing TMPro;\nusing UnityEngine;\nusing UnityEngine.UI;", "```\nusing HuggingFace.API;\n\n/* other code */\n\n// Make a call to the API\nvoid Query() {\n    string inputText = \"I'm on my way to the forest.\";\n    string[] candidates = {\n        \"The player is going to the city\",\n        \"The player is going to the wilderness\",\n        \"The player is wandering aimlessly\"\n    };\n    HuggingFaceAPI.SentenceSimilarity(inputText, OnSuccess, OnError, candidates);\n}\n\n// If successful, handle the result\nvoid OnSuccess(float[] result) {\n    foreach(float value in result) {\n        Debug.Log(value);\n    }\n}\n\n// Otherwise, handle the error\nvoid OnError(string error) {\n    Debug.LogError(error);\n}\n\n/* other code */\n```\n\n## Supported Tasks and Custom Models\n\nThe Hugging Face Unity API also currently supports the following tasks:", "## Step 5: Open Player Settings\n\nIn the Build Settings window, click the \"Player Settings\" button to open the Player Settings panel.\n\n<figure class=\"image text-center\">\n  <img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/124_ml-for-games/games-in-spaces/5.png\">\n</figure> \n\n## Step 6: Optionally, Download the Hugging Face Unity WebGL Template\n\nYou can enhance your game's appearance in a Space by downloading the Hugging Face Unity WebGL template, available [here](https://github.com/huggingface/Unity-WebGL-template-for-Hugging-Face-Spaces). Just download the repository and drop it in your project files.\n\nThen, in the Player Settings panel, switch the WebGL template to Hugging Face. To do so, in Player Settings, click \"Resolution and Presentation\", then select the Hugging Face WebGL template.\n\n<figure class=\"image text-center\">\n  <img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/124_ml-for-games/games-in-spaces/6.png\">\n</figure> \n\n## Step 7: Change the Compression Format to Disabled\n\nIn the Player Settings panel, navigate to the \"Publishing Settings\" section and change the Compression Format to \"Disabled\".", "## Step 10: Enable Git-LFS for Large File Storage\n\nNavigate to your repository. Use the following commands to track large build files.\n\n```\ngit lfs install\ngit lfs track Build/* \n```\n\n## Step 11: Push your Changes\n\nFinally, use the following Git commands to push your changes:\n\n```\ngit add .\ngit commit -m \"Add Unity WebGL build files\"\ngit push\n```\n\n## Done!\n\nCongratulations! Refresh your Space. You should now be able to play your game in a Hugging Face Space.\n\nWe hope you found this tutorial helpful. If you have any questions or would like to get more involved in using Hugging Face for Games, join the [Hugging Face Discord](https://hf.co/join/discord)!"], "eval_score_GPT4": "4", "eval_feedback_GPT4": "The response provided is mostly correct and factual. It follows the same steps as the reference answer, including opening the Unity project, navigating to the \"Package Manager\" through the \"Window\" menu, and adding a package from a git URL. The URL provided is accurate and would lead to the installation of the Hugging Face Unity API. However, the response is incomplete as it ends abruptly without finishing the last sentence. This suggests that there might have been additional steps or information that the respondent intended to include but did not. Therefore, the response cannot be scored as completely correct because it lacks a complete conclusion."}, {"question": "What is the pretraining objective of the Wav2Vec2 context network?\n", "true_answer": "The pretraining objective of the Wav2Vec2 context network is a contrastive task where the model has to predict the true quantized speech representation of the masked prediction from a set of false ones.", "source_doc": "huggingface/transformers/blob/main/docs/source/en/tasks_explained.md", "generated_answer": "The pretraining objective of the Wav2Vec2 context network is a contrastive task. The model has to predict the true quantized speech representation of the masked prediction from a set of false ones, encouraging the model to find the most similar context vector and quantized speech unit (the target label). (Extracted from source document: 0)", "retrieved_docs": ["4. The pretraining objective of the context network is a *contrastive task*. The model has to predict the true quantized speech representation of the masked prediction from a set of false ones, encouraging the model to find the most similar context vector and quantized speech unit (the target label).\n\nNow that wav2vec2 is pretrained, you can finetune it on your data for audio classification or automatic speech recognition!\n\n### Audio classification\n\nTo use the pretrained model for audio classification, add a sequence classification head on top of the base Wav2Vec2 model. The classification head is a linear layer that accepts the encoder's hidden states. The hidden states represent the learned features from each audio frame which can have varying lengths. To create one vector of fixed-length, the hidden states are pooled first and then transformed into logits over the class labels. The cross-entropy loss is calculated between the logits and target to find the most likely class.\n\nReady to try your hand at audio classification? Check out our complete [audio classification guide](tasks/audio_classification) to learn how to finetune Wav2Vec2 and use it for inference!\n\n### Automatic speech recognition", "### Create Wav2Vec2CTCTokenizer\n\nThe [pretrained Wav2Vec2 checkpoint](https://huggingface.co/facebook/wav2vec2-base) maps the speech signal to a\nsequence of context representations as illustrated in the figure above.\nA fine-tuned Wav2Vec2 checkpoint needs to map this sequence of context\nrepresentations to its corresponding transcription so that a linear\nlayer has to be added on top of the transformer block (shown in yellow).\nThis linear layer is used to classifies each context representation to a\ntoken class analogous how, *e.g.*, after pretraining a linear layer is\nadded on top of BERT\\'s embeddings for further classification - *cf.*\nwith *\\\"BERT\\\"* section of this [blog post](https://huggingface.co/blog/warm-starting-encoder-decoder).\n\nThe output size of this layer corresponds to the number of tokens in the\nvocabulary, which does **not** depend on Wav2Vec2\\'s pretraining task,\nbut only on the labeled dataset used for fine-tuning. So in the first\nstep, we will take a look at Timit and define a vocabulary based on the\ndataset\\'s transcriptions.\n\nLet\\'s start by loading the dataset and taking a look at its structure.\n\n```python\nfrom datasets import load_dataset, load_metric\n\ntimit = load_dataset(\"timit_asr\")\n\nprint(timit)\n```", "### Create `Wav2Vec2CTCTokenizer`\n\nA pre-trained XLS-R model maps the speech signal to a sequence of\ncontext representations as illustrated in the figure above. However, for\nspeech recognition the model has to to map this sequence of context\nrepresentations to its corresponding transcription which means that a\nlinear layer has to be added on top of the transformer block (shown in\nyellow in the diagram above). This linear layer is used to classify\neach context representation to a token class analogous to how\na linear layer is added on top of BERT\\'s embeddings\nfor further classification after pre-training (*cf.* with *\\'BERT\\'* section of the following [blog\npost](https://huggingface.co/blog/warm-starting-encoder-decoder)).\nafter pretraining a linear layer is added on top of BERT\\'s embeddings\nfor further classification - *cf.* with *\\'BERT\\'* section of this [blog\npost](https://huggingface.co/blog/warm-starting-encoder-decoder).\n\nThe output size of this layer corresponds to the number of tokens in the\nvocabulary, which does **not** depend on XLS-R\\'s pretraining task, but\nonly on the labeled dataset used for fine-tuning. So in the first step,\nwe will take a look at the chosen dataset of Common Voice and define a\nvocabulary based on the transcriptions.\n\nFirst, let\\'s go to Common Voice [official\nwebsite](https://commonvoice.mozilla.org/en/datasets) and pick a\nlanguage to fine-tune XLS-R on. For this notebook, we will use Turkish.", "### Pretraining Wav2Vec2\n\nThe `run_pretrain.py` script allows one to pretrain a Wav2Vec2 model from scratch using Wav2Vec2's contrastive loss objective (see official [paper](https://arxiv.org/abs/2006.11477) for more information). \nIt is recommended to pre-train Wav2Vec2 with Trainer + Deepspeed (please refer to [this guide](https://huggingface.co/transformers/main/main_classes/deepspeed.html#deepspeed-trainer-integration) for more information).\n\nHere is an example of how you can use DeepSpeed ZeRO-2 to pretrain a small Wav2Vec2 model:", "Similar to [BERT\\'s masked language modeling\nobjective](http://jalammar.github.io/illustrated-bert/), XLS-R learns\ncontextualized speech representations by randomly masking feature\nvectors before passing them to a transformer network during\nself-supervised pre-training (*i.e.* diagram on the left below).\n\nFor fine-tuning, a single linear layer is added on top of the\npre-trained network to train the model on labeled data of audio\ndownstream tasks such as speech recognition, speech translation and\naudio classification (*i.e.* diagram on the right below).\n\n![wav2vec2\\_structure](https://raw.githubusercontent.com/patrickvonplaten/scientific_images/master/xls_r.png)\n\nXLS-R shows impressive improvements over previous state-of-the-art\nresults on both speech recognition, speech translation and\nspeaker/language identification, *cf.* with Table 3-6, Table 7-10, and\nTable 11-12 respectively of the official [paper](https://ai.facebook.com/blog/xls-r-self-supervised-speech-processing-for-128-languages).\n\nSetup\n--------------\n\nIn this blog, we will give an in-detail explanation of how XLS-R -\nmore specifically the pre-trained checkpoint\n[**Wav2Vec2-XLS-R-300M**](https://huggingface.co/facebook/wav2vec2-xls-r-300m) - can be fine-tuned for ASR.", "The first component of Wav2Vec2 consists of a stack of CNN layers that\nare used to extract acoustically meaningful - but contextually\nindependent - features from the raw speech signal. This part of the\nmodel has already been sufficiently trained during pretrainind and as\nstated in the [paper](https://arxiv.org/abs/2006.11477) does not need to\nbe fine-tuned anymore. Thus, we can set the `requires_grad` to `False`\nfor all parameters of the *feature extraction* part.\n\n```python\nmodel.freeze_feature_extractor()\n```\n\nIn a final step, we define all parameters related to training. To give\nmore explanation on some of the parameters:\n\n-   `group_by_length` makes training more efficient by grouping training\n    samples of similar input length into one batch. This can\n    significantly speed up training time by heavily reducing the overall\n    number of useless padding tokens that are passed through the model\n-   `learning_rate` and `weight_decay` were heuristically tuned until\n    fine-tuning has become stable. Note that those parameters strongly\n    depend on the Timit dataset and might be suboptimal for other speech\n    datasets.\n\nFor more explanations on other parameters, one can take a look at the\n[docs](https://huggingface.co/transformers/master/main_classes/trainer.html?highlight=trainer#trainingarguments).\n\nDuring training, a checkpoint will be uploaded asynchronously to the hub every 400 training steps. It allows you to also play around with the demo widget even while your model is still training.", "**Wav2Vec2** is a popular pre-trained model for speech recognition.\nReleased in [September 2020](https://ai.facebook.com/blog/wav2vec-20-learning-the-structure-of-speech-from-raw-audio/)\nby Meta AI Research, the novel architecture catalyzed progress in\nself-supervised pretraining for speech recognition, *e.g.* [*G. Ng et\nal.*, 2021](https://arxiv.org/pdf/2104.03416.pdf), [*Chen et al*,\n2021](https://arxiv.org/abs/2110.13900), [*Hsu et al.*,\n2021](https://arxiv.org/abs/2106.07447) and [*Babu et al.*,\n2021](https://arxiv.org/abs/2111.09296). On the Hugging Face Hub,\nWav2Vec2's most popular pre-trained checkpoint currently amounts to\nover [**250,000** monthly\ndownloads](https://huggingface.co/facebook/wav2vec2-base-960h).\n\nUsing Connectionist Temporal Classification (CTC), pre-trained\nWav2Vec2-like checkpoints are extremely easy to fine-tune on downstream\nspeech recognition tasks. In a nutshell, fine-tuning pre-trained\nWav2Vec2 checkpoints works as follows:\n\nA single randomly initialized linear layer is stacked on top of the\npre-trained checkpoint and trained to classify raw audio input to a\nsequence of letters. It does so by:"], "eval_score_GPT4": "5", "eval_feedback_GPT4": "Feedback: The response provided is completely correct, accurate, and factual. It aligns with the reference answer by stating that the pretraining objective of the Wav2Vec2 context network is a contrastive task and further explains that the model must distinguish the true quantized speech representation from false ones. The additional detail about encouraging the model to find the most similar context vector and quantized speech unit adds depth to the explanation without deviating from the factual accuracy."}, {"question": "What is the default checkpoint used by the sentiment analysis pipeline in the Transformers library?\n", "true_answer": "distilbert base uncased finetuned sst2 english", "source_doc": "huggingface/course/blob/main/subtitles/en/raw/chapter2/02_inside-pipeline-pt.md", "generated_answer": "The default checkpoint used by the sentiment analysis pipeline in the Transformers library is \"distilbert-base-uncased-finetuned-sst-2-english\". This information can be found in the context provided, specifically in the third and fourth code snippets where the AutoTokenizer is loaded from the pretrained model's checkpoint. The checkpoint is used to download and cache the configuration and vocabulary associated with the model's tokenizer,", "retrieved_docs": [". Here, the checkpoint used by default for the sentiment analysis pipeline is distilbert base uncased finetuned sst2 english. We instantiate a tokenizer associated with that checkpoint, then feed it the two sentences. Since those two sentences are not of the same size, we will need to pad the shortest one to be able to build an array. This is done by the tokenizer with the option padding=True. With truncation=True, we ensure that any sentence longer than the maximum the model can handle is truncated. Lastly, the return_tensors option tells the tokenizer to return a PyTorch tensor. Looking at the result, we see we have a dictionary with two keys. Input IDs contains the IDs of both sentences, with 0s where the padding is applied. The second key, attention mask, indicates where padding has been applied, so the model does not pay attention to it. This is all what is inside the tokenization step. Now let's have a look at the second step, the model. As for the tokenizer, there is an AutoModel API, with a from_pretrained method. It will download and cache the configuration of the model as well as the pretrained weights. However, the AutoModel API will only instantiate the body of the model, that is, the part of the model that is left once the pretraining head is removed. It will output a high-dimensional tensor that is a representation of the sentences passed, but which is not directly useful for our classification problem", ". Here, the checkpoint used by default for the sentiment analysis pipeline is distilbert base uncased finetuned sst2 english. We instantiate a tokenizer associated with that checkpoint, then feed it the two sentences. Since those two sentences are not of the same size, we will need to pad the shortest one to be able to build an array. This is done by the tokenizer with the option padding=True. With truncation=True, we ensure that any sentence longer than the maximum the model can handle is truncated. Lastly, the return_tensors option tells the tokenizer to return a TensorFlow tensor. Looking at the result, we see we have a dictionary with two keys. Input IDs contains the IDs of both sentences, with 0s where the padding is applied. The second key, attention mask, indicates where padding has been applied, so the model does not pay attention to it. This is all what is inside the tokenization step. Now let's have a look at the second step, the model. As for the tokenizer, there is an TFAutoModel API, with a from_pretrained method. It will download and cache the configuration of the model as well as the pretrained weights. However, the TFAutoModel API will only instantiate the body of the model, that is, the part of the model that is left once the pretraining head is removed. It will output a high-dimensional tensor that is a representation of the sentences passed, but which is not directly useful for our classification problem", "hat happens inside the pipeline function? In this video, we will look at what actually happens when we use the pipeline function of the Transformers library. More specifically, we will look at the sentiment analysis pipeline, and how it went from the two following sentences to the positive labels with their respective scores. As we have seen in the pipeline presentation, there are three stages in the pipeline. First, we convert the raw texts to numbers the model can make sense of, using a tokenizer. Then, those numbers go through the model, which outputs logits. Finally, the post-processing steps transforms those logits into labels and scores. Let's look in detail at those three steps, and how to replicate them using the Transformers library, beginning with the first stage, tokenization. The tokenization process has several steps. First, the text is split into small chunks called tokens. They can be words, parts of words or punctuation symbols. Then the tokenizer will had some special tokens (if the model expect them). Here the model uses expects a CLS token at the beginning and a SEP token at the end of the sentence to classify. Lastly, the tokenizer matches each token to its unique ID in the vocabulary of the pretrained model. To load such a tokenizer, the Transformers library provides the AutoTokenizer API. The most important method of this class is from_pretrained, which will download and cache the configuration and the vocabulary associated to a given checkpoint. Here, the checkpoint used by default for the sentiment analysis pipeline is distilbert base uncased finetuned sst2 english", "hat happens inside the pipeline function? In this video, we will look at what actually happens when we use the pipeline function of the Transformers library. More specifically, we will look at the sentiment analysis pipeline, and how it went from the two following sentences to the positive labels with their respective scores. As we have seen in the pipeline presentation, there are three stages in the pipeline. First, we convert the raw texts to numbers the model can make sense of, using a tokenizer. Then, those numbers go through the model, which outputs logits. Finally, the post-processing steps transforms those logits into labels and scores. Let's look in detail at those three steps, and how to replicate them using the Transformers library, beginning with the first stage, tokenization. The tokenization process has several steps. First, the text is split into small chunks called tokens. They can be words, parts of words or punctuation symbols. Then the tokenizer will had some special tokens (if the model expect them). Here the model uses expects a CLS token at the beginning and a SEP token at the end of the sentence to classify. Lastly, the tokenizer matches each token to its unique ID in the vocabulary of the pretrained model. To load such a tokenizer, the Transformers library provides the AutoTokenizer API. The most important method of this class is from_pretrained, which will download and cache the configuration and the vocabulary associated to a given checkpoint. Here, the checkpoint used by default for the sentiment analysis pipeline is distilbert base uncased finetuned sst2 english", "All this preprocessing needs to be done in exactly the same way as when the model was pretrained, so we first need to download that information from the [Model Hub](https://huggingface.co/models). To do this, we use the `AutoTokenizer` class and its `from_pretrained()` method. Using the checkpoint name of our model, it will automatically fetch the data associated with the model's tokenizer and cache it (so it's only downloaded the first time you run the code below).\n\nSince the default checkpoint of the `sentiment-analysis` pipeline is `distilbert-base-uncased-finetuned-sst-2-english` (you can see its model card [here](https://huggingface.co/distilbert-base-uncased-finetuned-sst-2-english)), we run the following:\n\n```python\nfrom transformers import AutoTokenizer\n\ncheckpoint = \"distilbert-base-uncased-finetuned-sst-2-english\"\ntokenizer = AutoTokenizer.from_pretrained(checkpoint)\n```\n\nOnce we have the tokenizer, we can directly pass our sentences to it and we'll get back a dictionary that's ready to feed to our model! The only thing left to do is to convert the list of input IDs to tensors.", "## The Transformers library\nI was a bit confused at the beginning when I downloaded the .h5 file. I thought it would be compatible with `tensorflow.keras.models.load_model`, but this wasn't the case. After a few minutes of research I was able to figure out that the file was a weights checkpoint rather than a Keras model.\nAfter that, I tried out the API that Hugging Face offers and read a bit more about the pipeline feature they offer. Since the results of the API & the pipeline were great, I decided that I could serve the model through the pipeline on my own server.\n\nBelow is the [official example](https://github.com/huggingface/transformers#quick-tour) from the Transformers GitHub page.\n\n```python\nfrom transformers import pipeline\n\n# Allocate a pipeline for sentiment-analysis\nclassifier = pipeline('sentiment-analysis')\nclassifier('We are very happy to include pipeline into the transformers repository.')\n[{'label': 'POSITIVE', 'score': 0.9978193640708923}]\n```\n\n\n## Deploy transformers to Google Cloud\n> GCP is chosen as it is the cloud environment I am using in my personal organization.", "Start by creating an instance of [`pipeline`] and specifying a task you want to use it for. In this guide, you'll use the [`pipeline`] for sentiment analysis as an example:\n\n```py\n>>> from transformers import pipeline\n\n>>> classifier = pipeline(\"sentiment-analysis\")\n```\n\nThe [`pipeline`] downloads and caches a default [pretrained model](https://huggingface.co/distilbert-base-uncased-finetuned-sst-2-english) and tokenizer for sentiment analysis. Now you can use the `classifier` on your target text:\n\n```py\n>>> classifier(\"We are very happy to show you the \ud83e\udd17 Transformers library.\")\n[{'label': 'POSITIVE', 'score': 0.9998}]\n```\n\nIf you have more than one input, pass your inputs as a list to the [`pipeline`] to return a list of dictionaries:\n\n```py\n>>> results = classifier([\"We are very happy to show you the \ud83e\udd17 Transformers library.\", \"We hope you don't hate it.\"])\n>>> for result in results:\n...     print(f\"label: {result['label']}, with score: {round(result['score'], 4)}\")\nlabel: POSITIVE, with score: 0.9998\nlabel: NEGATIVE, with score: 0.5309\n```\n\nThe [`pipeline`] can also iterate over an entire dataset for any task you like. For this example, let's choose automatic speech recognition as our task:"], "eval_score_GPT4": "5", "eval_feedback_GPT4": "Feedback: The response provided is completely correct, accurate, and factual. It matches the reference answer by stating the correct default checkpoint \"distilbert-base-uncased-finetuned-sst-2-english\" used by the sentiment analysis pipeline in the Transformers library. The additional information about the AutoTokenizer and the context of the code snippets is not required for the correctness but shows a deeper understanding of how the checkpoint is used, which does not detract from the accuracy of the response."}, {"question": "What is the purpose of the notebook \"How to use DeepSpeed to train models with billions of parameters on Habana Gaudi\"?\n", "true_answer": "To show how to use DeepSpeed to pre-train/fine-tune the 1.6B-parameter GPT2-XL for causal language modeling on Habana Gaudi.", "source_doc": "huggingface/optimum/blob/main/docs/source/notebooks.md", "generated_answer": "The purpose of the notebook \"How to use DeepSpeed to train models with billions of parameters on Habana Gaudi\" is to demonstrate how to use DeepSpeed, a library designed for speed and scale for distributed training of large models with billions of parameters, to train models on Habana Gaudi, a second-generation AI hardware accelerator designed by Habana Labs. The notebook specifically shows how to use DeepSpeed to pre-train/fine-tune the", "retrieved_docs": ["| [How to use DeepSpeed to train models with billions of parameters on Habana Gaudi](https://github.com/huggingface/optimum-habana/blob/main/notebooks/AI_HW_Summit_2022.ipynb) | Show how to use DeepSpeed to pre-train/fine-tune the 1.6B-parameter GPT2-XL for causal language modeling on Habana Gaudi. |  [![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/huggingface/optimum-habana/blob/main/notebooks/AI_HW_Summit_2022.ipynb) | [![Open in AWS Studio](https://studiolab.sagemaker.aws/studiolab.svg)](https://studiolab.sagemaker.aws/import/github/huggingface/optimum-habana/blob/main/notebooks/AI_HW_Summit_2022.ipynb) |", "| [How to use DeepSpeed to train models with billions of parameters on Habana Gaudi](https://github.com/huggingface/optimum-habana/blob/main/notebooks/AI_HW_Summit_2022.ipynb) | Show how to use DeepSpeed to pre-train/fine-tune the 1.6B-parameter GPT2-XL for causal language modeling on Habana Gaudi. |  [![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/huggingface/optimum-habana/blob/main/notebooks/AI_HW_Summit_2022.ipynb) | [![Open in AWS Studio](https://studiolab.sagemaker.aws/studiolab.svg)](https://studiolab.sagemaker.aws/import/github/huggingface/optimum-habana/blob/main/notebooks/AI_HW_Summit_2022.ipynb) |", "## Conclusion\n\nWhen dealing with images, we presented two solutions to speed up your training workflows: allocating more resources to the dataloader, and decoding and augmenting images directly on accelerator devices rather than on CPU.\nWe showed that it leads to dramatic speedups when training a SOTA vision-language model like BridgeTower: **Habana Gaudi2 with Optimum Habana is about x1.4 faster than Nvidia H100 and x2.5 faster than Nvidia A100 80GB with Transformers!**\nAnd this is super easy to use as you just need to provide a few additional training arguments.\n\nTo go further, we are looking forward to using HPU graphs for training models even faster and to presenting how to use DeepSpeed ZeRO-3 on Gaudi2 to accelerate the training of your LLMs. Stay tuned!\n\nIf you are interested in accelerating your Machine Learning training and inference workflows using the latest AI hardware accelerators and software libraries, check out our [Expert Acceleration Program](https://huggingface.co/support). To learn more about Habana solutions, [read about our partnership and contact them here](https://huggingface.co/hardware/habana). To learn more about Hugging Face efforts to make AI hardware accelerators easy to use, check out our [Hardware Partner Program](https://huggingface.co/hardware).\n\n\n### Related Topics", "## Habana Gaudi2\n\n[Gaudi2](https://habana.ai/training/gaudi2/) is the second-generation AI hardware accelerator designed by Habana Labs. A single server contains 8 accelerator devices (called Habana Processing Units, or HPUs) with 96GB of memory each, which provides room to make very large models fit in. However, hosting the model is not very interesting if the computation is slow. Fortunately, Gaudi2 shines on that aspect: it differs from GPUs in that its architecture enables the accelerator to perform General Matrix Multiplication (GeMM) and other operations in parallel, which speeds up deep learning workflows. These features make Gaudi2 a great candidate for LLM training and inference.\n\nHabana's SDK, SynapseAI\u2122, supports PyTorch and DeepSpeed for accelerating LLM training and inference. The [SynapseAI graph compiler](https://docs.habana.ai/en/latest/Gaudi_Overview/SynapseAI_Software_Suite.html#graph-compiler-and-runtime) will optimize the execution of the operations accumulated in the graph (e.g. operator fusion, data layout management, parallelization, pipelining and memory management, and graph-level optimizations).", "!--\u26a0\ufe0f Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\nrendered properly in your Markdown viewer.\n-->\n\n# DeepSpeed\n\n[DeepSpeed](https://www.deepspeed.ai/) is a library designed for speed and scale for distributed training of large models with billions of parameters. At its core is the Zero Redundancy Optimizer (ZeRO) that shards optimizer states (ZeRO-1), gradients (ZeRO-2), and parameters (ZeRO-3) across data parallel processes. This drastically reduces memory usage, allowing you to scale your training to billion parameter models. To unlock even more memory efficiency, ZeRO-Offload reduces GPU compute and memory by leveraging CPU resources during optimization.\n\nBoth of these features are supported in \ud83e\udd17 Accelerate, and you can use them with \ud83e\udd17 PEFT. This guide will help you learn how to use our DeepSpeed [training script](https://github.com/huggingface/peft/blob/main/examples/conditional_generation/peft_lora_seq2seq_accelerate_ds_zero3_offload.py). You'll configure the script to train a large model for conditional generation with ZeRO-3 and ZeRO-Offload.\n\n<Tip>", "### Related Topics\n\n- [Faster Training and Inference: Habana Gaudi-2 vs Nvidia A100 80GB](https://huggingface.co/blog/habana-gaudi-2-benchmark)\n- [Leverage DeepSpeed to Train Faster and Cheaper Large Scale Transformer Models with Hugging Face and Habana Labs Gaudi](https://developer.habana.ai/events/leverage-deepspeed-to-train-faster-and-cheaper-large-scale-transformer-models-with-hugging-face-and-habana-labs-gaudi/)\n\n---\n\nThanks for reading! If you have any questions, feel free to contact me, either through [Github](https://github.com/huggingface/optimum-habana) or on the [forum](https://discuss.huggingface.co/c/optimum/59). You can also connect with me on [LinkedIn](https://www.linkedin.com/in/regispierrard/).\n\n[^1]: \u201cZero-shot\u201d refers to the ability of a model to complete a task on new or unseen input data, i.e. without having been provided any training examples of this kind of data. We provide the model with a prompt and a sequence of text that describes what we want our model to do, in natural language. Zero-shot classification excludes any examples of the desired task being completed. This differs from single or few-shot classification, as these tasks include a single or a few examples of the selected task.", "The Habana team recently introduced support for DeepSpeed-inference in SynapseAI 1.8, and thereby quickly enabled inference for 100+ billion parameter models. **For the 176-billion-parameter checkpoint, Gaudi2 is 1.42x faster than A100 80GB**. Smaller checkpoints present interesting results too. **Gaudi2 is 2.89x faster than A100 for BLOOMZ-7B!** It is also interesting to note that it manages to benefit from model parallelism whereas A100 is faster on a single device.\n\nWe also ran these models on first-gen Gaudi. While it is slower than Gaudi2, it is interesting from a price perspective as a DL1 instance on AWS costs approximately 13\\$ per hour. Latency for BLOOMZ-7B on first-gen Gaudi is 2.387 seconds. Thus, **first-gen Gaudi offers for the 7-billion checkpoint a better price-performance ratio than A100** which costs more than 30\\$ per hour!\n\nWe expect the Habana team will optimize the performance of these models in the upcoming SynapseAI releases. For example, in our last benchmark, we saw that [Gaudi2 performs Stable Diffusion inference 2.2x faster than A100](https://huggingface.co/blog/habana-gaudi-2-benchmark#generating-images-from-text-with-stable-diffusion) and this has since been improved further to 2.37x with the latest optimizations provided by Habana. We will update these numbers as new versions of SynapseAI are released and integrated within Optimum Habana.\n\n\n### Running inference on a complete dataset"], "eval_score_GPT4": "4", "eval_feedback_GPT4": "The response provided is mostly correct and factual. It accurately identifies the purpose of the notebook as demonstrating the use of DeepSpeed for distributed training of large models on Habana Gaudi hardware. However, the response is incomplete as it does not mention the specific model (1.6B-parameter GPT2-XL) or the type of modeling (causal language modeling) that the notebook focuses on. This information is crucial to fully satisfy the purpose of the notebook as described in the reference answer. Therefore, the response does not fully meet the highest standard set by the score rubric for complete correctness and factual accuracy."}, {"question": "What command line module does PyTorch provide to run a script on multiple GPUs?\n", "true_answer": "torchrun", "source_doc": "huggingface/blog/blob/main/pytorch-ddp-accelerate-transformers.md", "generated_answer": "PyTorch provides the `torch.distributed` module to run a script on multiple GPUs. This module is meant to work on distributed setups, including multi-node and multi-GPU configurations. To convert a PyTorch script to work with distributed setups, a few setup configurations must be defined, as detailed in the PyTorch Distributed Data Parallelism section provided in the context. The `torchrun` command is used to launch the distributed script, and the `", "retrieved_docs": ["```bash\ntorchrun --nproc_per_node=2 --nnodes=1 example_script.py\n```\n\nThe above will run the training script on two GPUs that live on a single machine and this is the barebones for performing only distributed training with PyTorch.\n\nNow let's talk about Accelerate, a library aimed to make this process more seameless and also help with a few best practices\n\n## \ud83e\udd17 Accelerate\n\n[Accelerate](https://huggingface.co/docs/accelerate) is a library designed to allow you to perform what we just did above, without needing to modify your code greatly. On top of this, the data pipeline innate to Accelerate can also improve performance to your code as well.\n\nFirst, let's wrap all of the above code we just performed into a single function, to help us visualize the difference:\n\n```python\ndef train_ddp(rank, world_size):\n    setup(rank, world_size)\n    # Build DataLoaders\n    transform = transforms.Compose([\n        transforms.ToTensor(),\n        transforms.Normalize((0.1307), (0.3081))\n    ])\n\n    train_dset = datasets.MNIST('data', train=True, download=True, transform=transform)\n    test_dset = datasets.MNIST('data', train=False, transform=transform)", "model.eval()\ncorrect = 0\nwith torch.no_grad():\n    for data, target in test_loader:\n        output = model(data)\n        pred = output.argmax(dim=1, keepdim=True)\n        correct += pred.eq(target.view_as(pred)).sum().item()\nprint(f'Accuracy: {100. * correct / len(test_loader.dataset)}')\n```\n\nTypically from here, one could either throw all of this into a python script or run it on a Jupyter Notebook.\n\nHowever, how would you then get this script to run on say two GPUs or on multiple machines if these resources are available, which could improve training speed through *distributed* training? Just doing `python myscript.py` will only ever run the script using a single GPU. This is where `torch.distributed` comes into play\n\n## PyTorch Distributed Data Parallelism\n\nAs the name implies, `torch.distributed` is meant to work on *distributed* setups. This can include multi-node, where you have a number of machines each with a single GPU, or multi-gpu where a single system has multiple GPUs, or some combination of both.\n\nTo convert our above code to work within a distributed setup, a few setup configurations must first be defined, detailed in the [Getting Started with DDP Tutorial](https://pytorch.org/tutorials/intermediate/ddp_tutorial.html)\n\nFirst a `setup` and a `cleanup` function must be declared. This will open up a processing group that all of the compute processes can communicate through", "```bash\nCUDA_VISIBLE_DEVICES=0,2 torchrun trainer-program.py ...\n```\n\nOnly the 2 physical GPUs (0 and 2) are \"visible\" to PyTorch and these are mapped to `cuda:0` and `cuda:1` respectively. You can also reverse the order of the GPUs to use 2 first. Now, the mapping is `cuda:1` for GPU 0 and `cuda:0` for GPU 2.\n\n```bash\nCUDA_VISIBLE_DEVICES=2,0 torchrun trainer-program.py ...\n```\n\nYou can also set the `CUDA_VISIBLE_DEVICES` environment variable to an empty value to create an environment without GPUs.\n\n```bash\nCUDA_VISIBLE_DEVICES= python trainer-program.py ...\n```\n\n<Tip warning={true}>\n\nAs with any environment variable, they can be exported instead of being added to the command line. However, this is not recommended because it can be confusing if you forget how the environment variable was setup and you end up using the wrong GPUs. Instead, it is common practice to set the environment variable for a specific training run on the same command line.\n\n</Tip>\n\n`CUDA_DEVICE_ORDER` is an alternative environment variable you can use to control how the GPUs are ordered. You can either order them by:", "### Script to compare pre-training with PyTorch on 8 GPU V100's\n\nFor comparison you can run the same pre-training with PyTorch on GPU. Note that we have to make use of `gradient_accumulation` \nbecause the maximum batch size that fits on a single V100 GPU is 32 instead of 128.\nHaving created the tokenzier and configuration in `norwegian-roberta-base`, we create the following symbolic links:\n\n```bash\nln -s ~/transformers/examples/pytorch/language-modeling/run_mlm.py ./\n```\n\n, set some environment variables:\n\n```bash\nexport NUM_GPUS=8\nexport TOKENIZERS_PARALLELISM=0\nexport MODEL_DIR=\"./norwegian-roberta-base\"\nmkdir -p ${MODEL_DIR}\n```\n\n, and can start training as follows:", "Scripts\n\nA train, validation, inference, and checkpoint cleaning script included in the github root folder. Scripts are not currently packaged in the pip release.\n\nThe training and validation scripts evolved from early versions of the [PyTorch Imagenet Examples](https://github.com/pytorch/examples). I have added significant functionality over time, including CUDA specific performance enhancements based on\n[NVIDIA's APEX Examples](https://github.com/NVIDIA/apex/tree/master/examples).\n\n## Training Script\n\nThe variety of training args is large and not all combinations of options (or even options) have been fully tested. For the training dataset folder, specify the folder to the base that contains a `train` and `validation` folder.\n\nTo train an SE-ResNet34 on ImageNet, locally distributed, 4 GPUs, one process per GPU w/ cosine schedule, random-erasing prob of 50% and per-pixel random value:\n\n```bash\n./distributed_train.sh 4 /data/imagenet --model seresnet34 --sched cosine --epochs 150 --warmup-epochs 5 --lr 0.4 --reprob 0.5 --remode pixel --batch-size 256 --amp -j 4\n```", "For some practical usage examples, please, see this [post](https://github.com/huggingface/transformers/issues/8771#issuecomment-759248400).\n\n\n\n<a id='deepspeed-one-gpu'></a>\n\n### Deployment with one GPU\n\nTo deploy DeepSpeed with one GPU adjust the [`Trainer`] command line arguments as follows:\n\n```bash\ndeepspeed --num_gpus=1 examples/pytorch/translation/run_translation.py \\\n--deepspeed tests/deepspeed/ds_config_zero2.json \\\n--model_name_or_path t5-small --per_device_train_batch_size 1 \\\n--output_dir output_dir --overwrite_output_dir --fp16 \\\n--do_train --max_train_samples 500 --num_train_epochs 1 \\\n--dataset_name wmt16 --dataset_config \"ro-en\" \\\n--source_lang en --target_lang ro\n```\n\nThis is almost the same as with multiple-GPUs, but here we tell DeepSpeed explicitly to use just one GPU via\n`--num_gpus=1`. By default, DeepSpeed deploys all GPUs it can see on the given node. If you have only 1 GPU to start\nwith, then you don't need this argument. The following [documentation](https://www.deepspeed.ai/getting-started/#resource-configuration-multi-node) discusses the launcher options.", "- Add the `fp16` argument to enable mixed precision.\n- Set the number of GPUs to use with the `nproc_per_node` argument.\n\n```bash\ntorchrun \\\n    --nproc_per_node 8 pytorch/summarization/run_summarization.py \\\n    --fp16 \\\n    --model_name_or_path t5-small \\\n    --do_train \\\n    --do_eval \\\n    --dataset_name cnn_dailymail \\\n    --dataset_config \"3.0.0\" \\\n    --source_prefix \"summarize: \" \\\n    --output_dir /tmp/tst-summarization \\\n    --per_device_train_batch_size=4 \\\n    --per_device_eval_batch_size=4 \\\n    --overwrite_output_dir \\\n    --predict_with_generate\n```\n\nTensorFlow scripts utilize a [`MirroredStrategy`](https://www.tensorflow.org/guide/distributed_training#mirroredstrategy) for distributed training, and you don't need to add any additional arguments to the training script. The TensorFlow script will use multiple GPUs by default if they are available.\n\n## Run a script on a TPU"], "eval_score_GPT4": "3", "eval_feedback_GPT4": "Feedback: The response correctly identifies `torch.distributed` as the module provided by PyTorch for running scripts on multiple GPUs, which is factually accurate. However, the response fails to mention `torchrun` as the specific command line tool, which is the direct answer to the question. The response provides additional context about the module's purpose and usage, which is accurate but not necessary for the direct question asked. Based on the rubric, the response is somewhat correct and factual but not completely, as it does not specify `torchrun` directly."}, {"question": "What is the most popular vision transformer model on the Hugging Face Model Hub for image classification?\n", "true_answer": "google/vit-base-patch16-224", "source_doc": "gradio-app/gradio/blob/main/guides/06_integrating-other-frameworks/image-classification-with-vision-transformers.md", "generated_answer": "The most popular vision transformer model for image classification on the Hugging Face Model Hub, as mentioned in the context, is the one contributed by nielsr with the paper \"An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale\" by Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner", "retrieved_docs": ["Image Classification with Vision Transformers\n\nRelated spaces: https://huggingface.co/spaces/abidlabs/vision-transformer\nTags: VISION, TRANSFORMERS, HUB\n\n## Introduction\n\nImage classification is a central task in computer vision. Building better classifiers to classify what object is present in a picture is an active area of research, as it has applications stretching from facial recognition to manufacturing quality control.\n\nState-of-the-art image classifiers are based on the _transformers_ architectures, originally popularized for NLP tasks. Such architectures are typically called vision transformers (ViT). Such models are perfect to use with Gradio's _image_ input component, so in this tutorial we will build a web demo to classify images using Gradio. We will be able to build the whole web application in a **single line of Python**, and it will look like the demo on the bottom of the page.\n\nLet's get started!\n\n### Prerequisites\n\nMake sure you have the `gradio` Python package already [installed](/getting_started).\n\n## Step 1 \u2014 Choosing a Vision Image Classification Model\n\nFirst, we will need an image classification model. For this tutorial, we will use a model from the [Hugging Face Model Hub](https://huggingface.co/models?pipeline_tag=image-classification). The Hub contains thousands of models covering dozens of different machine learning tasks.\n\nExpand the Tasks category on the left sidebar and select \"Image Classification\" as our task of interest. You will then see all of the models on the Hub that are designed to classify images.", "The abstract from the paper is the following:\n\n*While the Transformer architecture has become the de-facto standard for natural language processing tasks, its\napplications to computer vision remain limited. In vision, attention is either applied in conjunction with\nconvolutional networks, or used to replace certain components of convolutional networks while keeping their overall\nstructure in place. We show that this reliance on CNNs is not necessary and a pure transformer applied directly to\nsequences of image patches can perform very well on image classification tasks. When pre-trained on large amounts of\ndata and transferred to multiple mid-sized or small image recognition benchmarks (ImageNet, CIFAR-100, VTAB, etc.),\nVision Transformer (ViT) attains excellent results compared to state-of-the-art convolutional networks while requiring\nsubstantially fewer computational resources to train.*\n\nThis model was contributed by [nielsr](https://huggingface.co/nielsr). The original code (written in JAX) can be\nfound [here](https://github.com/google-research/vision_transformer).\n\n## Resources\n\nA list of official Hugging Face and community (indicated by \ud83c\udf0e) resources to help you get started with ViT Hybrid.\n\n<PipelineTag pipeline=\"image-classification\"/>", "![LayoutLM object detection](assets/27_summer_at_huggingface/layout.png)\n\n- [BEiT](https://huggingface.co/transformers/model_doc/beit.html) by Microsoft Research makes self-supervised Vision Transformers outperform supervised ones, using a clever pre-training objective inspired by BERT.\n- [RemBERT](https://huggingface.co/transformers/model_doc/rembert.html?), a large multilingual Transformer that outperforms XLM-R (and mT5 with a similar number of parameters) in zero-shot transfer.\n- [Splinter](https://huggingface.co/transformers/model_doc/splinter.html) which can be used for few-shot question answering. Given only 128 examples, Splinter is able to reach ~73% F1 on SQuAD, outperforming MLM-based models by 24 points!\n\nThe Hub is now integrated into `transformers`, with the ability to push to the Hub configuration, model, and tokenizer files without leaving the Python runtime! The `Trainer` can now push directly to the Hub every time a checkpoint is saved: \n\n![Saving a checkpoint](assets/27_summer_at_huggingface/save_checkpoint.png)\n\n### New in Datasets\n\nYou can find 1400 public datasets in [https://huggingface.co/datasets](https://huggingface.co/datasets) thanks to the awesome contributions from all our community. \ud83d\udcaf", "The abstract from the paper is the following:\n\n*While the Transformer architecture has become the de-facto standard for natural language processing tasks, its\napplications to computer vision remain limited. In vision, attention is either applied in conjunction with\nconvolutional networks, or used to replace certain components of convolutional networks while keeping their overall\nstructure in place. We show that this reliance on CNNs is not necessary and a pure transformer applied directly to\nsequences of image patches can perform very well on image classification tasks. When pre-trained on large amounts of\ndata and transferred to multiple mid-sized or small image recognition benchmarks (ImageNet, CIFAR-100, VTAB, etc.),\nVision Transformer (ViT) attains excellent results compared to state-of-the-art convolutional networks while requiring\nsubstantially fewer computational resources to train.*\n\n<img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/model_doc/vit_architecture.jpg\"\nalt=\"drawing\" width=\"600\"/>\n\n<small> ViT architecture. Taken from the <a href=\"https://arxiv.org/abs/2010.11929\">original paper.</a> </small>\n\nFollowing the original Vision Transformer, some follow-up works have been made:", "1. **[Vision Transformer (ViT)](https://huggingface.co/docs/transformers/model_doc/vit)** (\u6765\u81ea Google AI) \u4f34\u968f\u8bba\u6587 [An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale](https://arxiv.org/abs/2010.11929) \u7531 Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, Neil Houlsby \u53d1\u5e03\u3002\n1. **[VisualBERT](https://huggingface.co/docs/transformers/model_doc/visual_bert)** (\u6765\u81ea UCLA NLP) \u4f34\u968f\u8bba\u6587 [VisualBERT: A Simple and Performant Baseline for Vision and Language](https://arxiv.org/pdf/1908.03557) \u7531 Liunian Harold Li, Mark Yatskar, Da Yin, Cho-Jui Hsieh, Kai-Wei Chang \u53d1\u5e03\u3002", "1. **[VipLlava](https://huggingface.co/docs/transformers/model_doc/vipllava)** (from University of Wisconsin\u2013Madison) released with the paper [Making Large Multimodal Models Understand Arbitrary Visual Prompts](https://arxiv.org/abs/2312.00784) by Mu Cai, Haotian Liu, Siva Karthik Mustikovela, Gregory P. Meyer, Yuning Chai, Dennis Park, Yong Jae Lee.\n1. **[Vision Transformer (ViT)](https://huggingface.co/docs/transformers/model_doc/vit)** (from Google AI) released with the paper [An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale](https://arxiv.org/abs/2010.11929) by Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, Neil Houlsby.\n1. **[VisualBERT](https://huggingface.co/docs/transformers/model_doc/visual_bert)** (from UCLA NLP) released with the paper [VisualBERT: A Simple and Performant Baseline for Vision and Language](https://arxiv.org/pdf/1908.03557) by Liunian Harold Li, Mark Yatskar, Da Yin, Cho-Jui Hsieh, Kai-Wei Chang.", "1. **[VipLlava](https://huggingface.co/docs/transformers/model_doc/vipllava)** (from University of Wisconsin\u2013Madison) released with the paper [Making Large Multimodal Models Understand Arbitrary Visual Prompts](https://arxiv.org/abs/2312.00784) by Mu Cai, Haotian Liu, Siva Karthik Mustikovela, Gregory P. Meyer, Yuning Chai, Dennis Park, Yong Jae Lee.\n1. **[Vision Transformer (ViT)](https://huggingface.co/docs/transformers/model_doc/vit)** (from Google AI) released with the paper [An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale](https://arxiv.org/abs/2010.11929) by Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, Neil Houlsby.\n1. **[VisualBERT](https://huggingface.co/docs/transformers/model_doc/visual_bert)** (from UCLA NLP) released with the paper [VisualBERT: A Simple and Performant Baseline for Vision and Language](https://arxiv.org/pdf/1908.03557) by Liunian Harold Li, Mark Yatskar, Da Yin, Cho-Jui Hsieh, Kai-Wei Chang."], "eval_score_GPT4": "2", "eval_feedback_GPT4": "The response provided does not directly answer the question with a specific model name as the reference answer does. Instead, it gives a general description of a vision transformer model and references the paper associated with the development of vision transformers. The response fails to mention the exact model \"google/vit-base-patch16-224\" which is the most popular vision transformer model on the Hugging Face Model Hub for image classification. Therefore, the response is not completely correct, accurate, or factual in the context of the specific question asked."}, {"question": "What is the command to upload an ESPnet model to a Hugging Face repository?\n", "true_answer": "./run.sh --stage 15 --skip_upload_hf false --hf_repo username/model_repo", "source_doc": "huggingface/hub-docs/blob/main/docs/hub/espnet.md", "generated_answer": "To upload an ESPnet model to a Hugging Face repository, you can use the `run.sh` script provided by ESPnet or the `push_to_hub` argument in the `espnet` scripts. Here's an example command:\n\n```bash\npython examples/pytorch/summarization/run_summarization.py\n    --model_name_or_path t5-small \\\n    --do_train \\\n   ", "retrieved_docs": ["## Sharing your models\n\n`ESPnet` outputs a `zip` file that can be uploaded to Hugging Face easily. For a full guide on sharing  models, we recommend checking out the [official guide](https://github.com/espnet/espnet_model_zoo#register-your-model)).\n\nThe `run.sh` script allows to upload a given model to a Hugging Face repository.\n\n```bash\n./run.sh --stage 15 --skip_upload_hf false --hf_repo username/model_repo\n```\n\n## Additional resources\n\n* ESPnet [docs](https://espnet.github.io/espnet/index.html).\n* ESPnet model zoo [repository](https://github.com/espnet/espnet_model_zoo).\n* Integration [docs](https://github.com/asteroid-team/asteroid/blob/master/docs/source/readmes/pretrained_models.md).", "Using ESPnet at Hugging Face\n\n`espnet` is an end-to-end toolkit for speech processing, including automatic speech recognition, text to speech, speech enhancement, dirarization and other tasks.\n\n## Exploring ESPnet in the Hub\n\nYou can find hundreds of `espnet` models by filtering at the left of the [models page](https://huggingface.co/models?library=espnet&sort=downloads). \n\nAll models on the Hub come up with useful features:\n1. An automatically generated model card with a description, a training configuration, licenses and more.\n2. Metadata tags that help for discoverability and contain information such as license, language and datasets.\n3. An interactive widget you can use to play out with the model directly in the browser.\n4. An Inference API that allows to make inference requests.\n\n<div class=\"flex justify-center\">\n<img class=\"block dark:hidden\" src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/hub/libraries-espnet_widget.png\"/>\n<img class=\"hidden dark:block\" src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/hub/libraries-espnet_widget-dark.png\"/>\n</div>\n\n## Using existing models\n\nFor a full guide on loading pre-trained models, we recommend checking out the [official guide](https://github.com/espnet/espnet_model_zoo)).", "![](https://i.imgur.com/E9MiMi9.png)\n\n\n![](image/media/image4.png)\n\nIf you prefer to use a model via an API, clicking the\n`deploy` button in a model repository gives you various options for hosting the model behind an API. This can be particularly useful if you want to try out a model on a larger amount of data but need the infrastructure to run models locally.\n\nA similar approach can also be used to find relevant models and datasets\non the Hugging Face Hub.\n\n## Walkthrough: how can you add a GLAM dataset to the Hub?\n\nWe can make datasets available via the Hugging Face hub in various ways. I'll walk through an example of adding a CSV dataset to the Hugging Face hub. \n\n<figure class=\"image table text-center m-0 w-full\">\n    <video \n        alt=\"Uploading a file to the Hugging Face Hub\"\n        style=\"max-width: 70%; margin: auto;\"\n        autoplay loop autobuffer muted playsinline\n    >\n      <source src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/hf-hub-glam-guide/Upload%20dataset%20to%20hub.mp4\" type=\"video/mp4\">\n  </video>\n</figure>\n\n*Overview of the process of uploading a dataset to the Hub via the browser interface*", "## Share your model\n\nAll scripts can upload your final model to the [Model Hub](https://huggingface.co/models). Make sure you are logged into Hugging Face before you begin:\n\n```bash\nhuggingface-cli login\n```\n\nThen add the `push_to_hub` argument to the script. This argument will create a repository with your Hugging Face username and the folder name specified in `output_dir`.\n\nTo give your repository a specific name, use the `push_to_hub_model_id` argument to add it. The repository will be automatically listed under your namespace.\n\nThe following example shows how to upload a model with a specific repository name:\n\n```bash\npython examples/pytorch/summarization/run_summarization.py\n    --model_name_or_path t5-small \\\n    --do_train \\\n    --do_eval \\\n    --dataset_name cnn_dailymail \\\n    --dataset_config \"3.0.0\" \\\n    --source_prefix \"summarize: \" \\\n    --push_to_hub \\\n    --push_to_hub_model_id finetuned-t5-cnn_dailymail \\\n    --output_dir /tmp/tst-summarization \\\n    --per_device_train_batch_size=4 \\\n    --per_device_eval_batch_size=4 \\\n    --overwrite_output_dir \\\n    --predict_with_generate\n```", "If you don't want to use a Google Colab or a Jupyter Notebook, you need to use this command instead: `huggingface-cli login`\n\n3\ufe0f\u20e3 We're now ready to push our trained agent to the \ud83e\udd17 Hub \ud83d\udd25\n\nLet's run push_to_hub.py file to upload our trained agent to the Hub.\n\n`--repo-name `: The name of the repo\n\n`-orga`: Your Hugging Face username\n\n`-f`: Where the trained model folder is (in our case `logs`)\n\n<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/notebooks/unit3/select-id.png\" alt=\"Select Id\">\n\n```bash\npython -m rl_zoo3.push_to_hub  --algo dqn  --env SpaceInvadersNoFrameskip-v4  --repo-name _____________________ -orga _____________________ -f logs/\n```\n\n#### Solution\n\n```bash\npython -m rl_zoo3.push_to_hub  --algo dqn  --env SpaceInvadersNoFrameskip-v4  --repo-name dqn-SpaceInvadersNoFrameskip-v4  -orga ThomasSimonini  -f logs/\n```", "```py\n>>> pt_model.push_to_hub(\"my-awesome-org/my-awesome-model\")\n```\n\nThe `push_to_hub` function can also be used to add other files to a model repository. For example, add a tokenizer to a model repository:\n\n```py\n>>> tokenizer.push_to_hub(\"my-awesome-model\")\n```\n\nOr perhaps you'd like to add the TensorFlow version of your fine-tuned PyTorch model:\n\n```py\n>>> tf_model.push_to_hub(\"my-awesome-model\")\n```\n\nNow when you navigate to your Hugging Face profile, you should see your newly created model repository. Clicking on the **Files** tab will display all the files you've uploaded to the repository.\n\nFor more details on how to create and upload files to a repository, refer to the Hub documentation [here](https://huggingface.co/docs/hub/how-to-upstream).\n\n## Upload with the web interface\n\nUsers who prefer a no-code approach are able to upload a model through the Hub's web interface. Visit [huggingface.co/new](https://huggingface.co/new) to create a new repository:\n\n![new_model_repo](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/new_model_repo.png)\n\nFrom here, add some information about your model:", "def package_to_hub(\n    repo_id,\n    model,\n    hyperparameters,\n    eval_env,\n    video_fps=30,\n    commit_message=\"Push agent to the Hub\",\n    token=None,\n    logs=None,\n):\n    \"\"\"\n    Evaluate, Generate a video and Upload a model to Hugging Face Hub.\n    This method does the complete pipeline:\n    - It evaluates the model\n    - It generates the model card\n    - It generates a replay video of the agent\n    - It pushes everything to the hub\n    :param repo_id: id of the model repository from the Hugging Face Hub\n    :param model: trained model\n    :param eval_env: environment used to evaluate the agent\n    :param fps: number of fps for rendering the video\n    :param commit_message: commit message\n    :param logs: directory on local machine of tensorboard logs you'd like to upload\n    \"\"\"\n    msg.info(\n        \"This function will save, evaluate, generate a video of your agent, \"\n        \"create a model card and push everything to the hub. \"\n        \"It might take up to 1min. \\n \"\n        \"This is a work in progress: if you encounter a bug, please open an issue.\"\n    )\n    # Step 1: Clone or create the repo\n    repo_url = HfApi().create_repo(\n        repo_id=repo_id,\n        token=token,\n        private=False,\n        exist_ok=True,\n    )\n\n    with tempfile.TemporaryDirectory() as tmpdirname:\n        tmpdirname = Path(tmpdirname)\n\n        # Step 2: Save the model\n        torch.save(model.state_dict(), tmpdirname / \"model.pt\")"], "eval_score_GPT4": "2", "eval_feedback_GPT4": "The response provided does not accurately reflect the correct command to upload an ESPnet model to a Hugging Face repository. The reference answer gives a specific command using `./run.sh` with appropriate flags, while the response suggests using a `run_summarization.py` script with incomplete arguments and is not specific to ESPnet. The response also incorrectly refers to a summarization example which is not relevant to the task of uploading an ESPnet model. Therefore, the response is mostly incorrect and not factual in the context of the given instruction."}, {"question": "What file should be added to a model repository to install custom Python dependencies for Inference Endpoints?\n", "true_answer": "requirements.txt", "source_doc": "huggingface/hf-endpoints-documentation/blob/main/docs/source/guides/custom_dependencies.mdx", "generated_answer": "The file that should be added to a model repository to install custom Python dependencies for Inference Endpoints is a `requirements.txt` file, as mentioned in the context provided. This file should be placed in the model repository on the Hugging Face Hub, and Inference Endpoints will install the dependencies listed within when the Endpoint and Image artifacts are created. Examples of `requirements.txt` files can be found in the model repositories listed in the context.", "retrieved_docs": ["Add custom Dependencies\n\nInference Endpoints\u2019 base image includes all required libraries to run inference on \ud83e\udd17 Transformers models, but it also supports custom dependencies. This is useful if you want to:\n\n* [customize your inference pipeline](/docs/inference-endpoints/guides/custom_handler) and need additional Python dependencies\n* run a model which requires special dependencies like the newest or a fixed version of a library (for example, `tapas` (`torch-scatter`)).\n\nTo add custom dependencies, add a `requirements.txt` [file](https://huggingface.co/philschmid/distilbert-onnx-banking77/blob/main/requirements.txt) with the Python dependencies you want to install in your model repository on the Hugging Face Hub. When your Endpoint and Image artifacts are created, Inference Endpoints checks if the model repository contains a `requirements.txt ` file and installs the dependencies listed within.\n\n```bash\noptimum[onnxruntime]==1.2.3\nmkl-include\nmkl\n```\n\nCheck out the `requirements.txt` files in the following model repositories for examples:\n\n* [Optimum and onnxruntime](https://huggingface.co/philschmid/distilbert-onnx-banking77/blob/main/requirements.txt)\n* [diffusers](https://huggingface.co/philschmid/stable-diffusion-v1-4-endpoints/blob/main/requirements.txt)", "Inference Endpoints Version\n\nHugging Face Inference Endpoints comes with a default serving container which is used for all [supported Transformers and Sentence-Transformers tasks](/docs/inference-endpoints/supported_tasks) and for [custom inference handler](/docs/inference-endpoints/guides/custom_handler) and implement batching.\nBelow you will find information about the installed packages and versions used.\n\nYou can always upgrade installed packages and a custom packages by adding a `requirements.txt` file to your model repository. Read more in [Add custom Dependencies](/docs/inference-endpoints/guides/custom_dependencies).\n\n## Installed packages & version\n\nThe installed packages are split into `general`, `CPU` & `GPU` packages. The `general` packages are installed in all containers, the `CPU` and `GPU` packages are only installed in the corresponding containers.\nThe Hugging Face Inference Runtime has separate versions for `PyTorch` and `TensorFlow` for `CPU` and `GPU`, which are used based on the selected `framework` when creating an Inference Endpoint. The `TensorFlow` and `PyTorch` flavors are grouped together in the list below.\n\n### General\n\n- `Python`: `3.9.13`\n\n### CPU", "## Set up the Inference API\n\nOur Inference API powers models uploaded to the Hub through your library.\n\n### Create an Inference API Docker image \n\nAll third-party libraries are Dockerized, so you can install the dependencies you'll need for your library to work correctly. Add your library to the existing Docker images by navigating to the [Docker images folder](https://github.com/huggingface/api-inference-community/tree/main/docker_images).\n\n1. Copy the `common` folder and rename it with the name of your library (e.g. `docker/common` to `docker/your-awesome-library`).\n2. There are four files you need to edit:\n    * List the packages required for your library to work in `requirements.txt`.\n    * Update `app/main.py` with the tasks supported by your model (see [here](https://github.com/huggingface/api-inference-community) for a complete list of available tasks). Look out for the `IMPLEMENT_THIS` flag to add your supported task.\n\n       ```python\n       ALLOWED_TASKS: Dict[str, Type[Pipeline]] = {\n           \"token-classification\": TokenClassificationPipeline\n       }\n       ```", "The `inference.py` file contains your custom inference module, and the `requirements.txt` file contains additional dependencies that should be added. The custom module can override the following methods:", "_Default: min 1; max 2_\n\n**Task** \n\nSelect a [supported Machine Learning Task](/docs/inference-endpoints/supported_tasks), or set to [Custom](/docs/inference-endpoints/guides/custom_handler). [Custom](/docs/inference-endpoints/guides/custom_handler) can/should be used when you are not using a Transformers-based model or when you want to customize the inference pipeline, see [Create your own Inference handler](/docs/inference-endpoints/guides/custom_handler).\n\n_Default: derived from the model repository._\n\n**Framework**\n\nFor Transformers models, if both PyTorch and TensorFlow weights are both available, you can select which model weights to use. This will help reduce the image artifact size and accelerate startups/scaling of your endpoints.\n\n_Default: PyTorch if available._\n\n**Revision**\n\nCreate your Endpoint targeting a specific revision commit for its source Hugging Face Model Repository. This allows you to version your endpoint and make sure you are always using the same weights even if you are updating the Model Repository. \n\n_Default: The most recent commit._\n\n**Image**", "</Tip>\n\n### Using a specific URL\n\nThe examples we saw above use the free-hosted Inference API. This proves to be very useful for prototyping\nand testing things quickly. Once you're ready to deploy your model to production, you'll need to use a dedicated infrastructure.\nThat's where [Inference Endpoints](https://huggingface.co/docs/inference-endpoints/index) comes into play. It allows you to deploy\nany model and expose it as a private API. Once deployed, you'll get a URL that you can connect to using exactly the same\ncode as before, changing only the `model` parameter:\n\n```python\n>>> from huggingface_hub import InferenceClient\n>>> client = InferenceClient(model=\"https://uu149rez6gw9ehej.eu-west-1.aws.endpoints.huggingface.cloud/deepfloyd-if\")\n# or\n>>> client = InferenceClient()\n>>> client.text_to_image(..., model=\"https://uu149rez6gw9ehej.eu-west-1.aws.endpoints.huggingface.cloud/deepfloyd-if\")\n```\n\n### Authentication", "For more information, take a look at how you can create and install dependencies when you [use your own custom container](/docs/inference-endpoints/guides/custom_container) for inference."], "eval_score_GPT4": "5", "eval_feedback_GPT4": "The response correctly identifies the `requirements.txt` file as the one needed to install custom Python dependencies for Inference Endpoints, which aligns with the reference answer. The additional details provided about the placement of the file and the process of dependency installation during the creation of Endpoint and Image artifacts are accurate and factual, enhancing the quality of the response. There is no indication of incorrect or inaccurate information presented in the response."}, {"question": "How many images are needed to teach new concepts to Stable Diffusion using Textual Inversion?\n", "true_answer": "3-5 images", "source_doc": "huggingface/diffusers/blob/main/examples/research_projects/intel_opts/textual_inversion_dfq/README.md", "generated_answer": "According to the context provided, just 3-5 images are needed to teach new concepts to Stable Diffusion using Textual Inversion. (Source: Extract 5)", "retrieved_docs": ["Distillation for quantization on Textual Inversion models to personalize text2image\n\n[Textual inversion](https://arxiv.org/abs/2208.01618) is a method to personalize text2image models like stable diffusion on your own images._By using just 3-5 images new concepts can be taught to Stable Diffusion and the model personalized on your own images_\nThe `textual_inversion.py` script shows how to implement the training procedure and adapt it for stable diffusion.\nWe have enabled distillation for quantization in `textual_inversion.py` to do quantization aware training as well as distillation on the model generated by Textual Inversion method.\n\n## Installing the dependencies\n\nBefore running the scripts, make sure to install the library's training dependencies:\n\n```bash\npip install -r requirements.txt\n```\n\n## Prepare Datasets\n\nOne picture which is from the huggingface datasets [sd-concepts-library/dicoo2](https://huggingface.co/sd-concepts-library/dicoo2) is needed, and save it to the `./dicoo` directory. The picture is shown below:\n\n<a href=\"https://huggingface.co/sd-concepts-library/dicoo2/blob/main/concept_images/1.jpeg\">\n    <img src=\"https://huggingface.co/sd-concepts-library/dicoo2/resolve/main/concept_images/1.jpeg\" width = \"300\" height=\"300\">\n</a>", "# Textual Inversion fine-tuning example\n\n[Textual inversion](https://arxiv.org/abs/2208.01618) is a method to personalize text2image models like stable diffusion on your own images using just 3-5 examples.\nThe `textual_inversion.py` script shows how to implement the training procedure and adapt it for stable diffusion.\n\n## Training with Intel Extension for PyTorch\n\nIntel Extension for PyTorch provides the optimizations for faster training and inference on CPUs. You can leverage the training example \"textual_inversion.py\". Follow the [instructions](https://github.com/huggingface/diffusers/tree/main/examples/textual_inversion) to get the model and [dataset](https://huggingface.co/sd-concepts-library/dicoo2) before running the script.\n\nThe example supports both single node and multi-node distributed training:\n\n### Single node training\n\n```bash\nexport MODEL_NAME=\"CompVis/stable-diffusion-v1-4\"\nexport DATA_DIR=\"path-to-dir-containing-dicoo-images\"", "In just a couple of days, the community shared over 200 concepts! Check them out!\n\n* [Organization](https://huggingface.co/sd-concepts-library) with the concepts. \n* [Navigator Colab](https://colab.research.google.com/github/huggingface/notebooks/blob/main/diffusers/stable_diffusion_textual_inversion_library_navigator.ipynb): Browse visually and use over 150 concepts created by the community.\n* [Training Colab](https://colab.research.google.com/github/huggingface/notebooks/blob/main/diffusers/sd_textual_inversion_training.ipynb): Teach Stable Diffusion a new concept and share it with the rest of the community.\n* [Inference Colab](https://colab.research.google.com/github/huggingface/notebooks/blob/main/diffusers/stable_conceptualizer_inference.ipynb): Run Stable Diffusion with the learned concepts.\n\n\n## Experimental inpainting pipeline\n\nInpainting allows to provide an image, then select an area in the image (or provide a mask), and use Stable Diffusion to replace the mask. Here is an example:\n\n<figure class=\"image table text-center m-0 w-full\">\n  <img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/diffusers-2nd-month/inpainting.png\" alt=\"Example inpaint of owl being generated from an initial image and a prompt\"/>\n</figure>", "[Textual inversion](../training/text_inversion) is a technique for learning a specific concept from some images which you can use to generate new images conditioned on that concept.\n\nCreate a pipeline and use the [`~loaders.TextualInversionLoaderMixin.load_textual_inversion`] function to load the textual inversion embeddings (feel free to browse the [Stable Diffusion Conceptualizer](https://huggingface.co/spaces/sd-concepts-library/stable-diffusion-conceptualizer) for 100+ trained concepts):\n\n```py\nimport torch\nfrom diffusers import StableDiffusionPipeline\nfrom compel import Compel, DiffusersTextualInversionManager\n\npipe = StableDiffusionPipeline.from_pretrained(\n  \"runwayml/stable-diffusion-v1-5\", torch_dtype=torch.float16,\n  use_safetensors=True, variant=\"fp16\").to(\"cuda\")\npipe.load_textual_inversion(\"sd-concepts-library/midjourney-style\")\n```\n\nCompel provides a `DiffusersTextualInversionManager` class to simplify prompt weighting with textual inversion. Instantiate `DiffusersTextualInversionManager` and pass it to the `Compel` class:\n\n```py\ntextual_inversion_manager = DiffusersTextualInversionManager(pipe)\ncompel_proc = Compel(\n    tokenizer=pipe.tokenizer,\n    text_encoder=pipe.text_encoder,\n    textual_inversion_manager=textual_inversion_manager)\n```\n\nIncorporate the concept to condition a prompt with using the `<concept>` syntax:", "```py\nall_images = []\nfor _ in range(num_rows):\n    images = pipeline(prompt, num_images_per_prompt=num_samples_per_row, num_inference_steps=50, guidance_scale=7.5).images\n    all_images.extend(images)\n\ngrid = make_image_grid(all_images, num_rows, num_samples_per_row)\ngrid\n```\n\n<div class=\"flex justify-center\">\n    <img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/diffusers/textual_inversion_inference.png\">\n</div>\n\n## Stable Diffusion XL\n\nStable Diffusion XL (SDXL) can also use textual inversion vectors for inference. In contrast to Stable Diffusion 1 and 2, SDXL has two text encoders so you'll need two textual inversion embeddings - one for each text encoder model.\n\nLet's download the SDXL textual inversion embeddings and have a closer look at it's structure:\n\n```py\nfrom huggingface_hub import hf_hub_download\nfrom safetensors.torch import load_file\n\nfile = hf_hub_download(\"dn118/unaestheticXL\", filename=\"unaestheticXLv31.safetensors\")\nstate_dict = load_file(file)\nstate_dict\n```", "# Textual Inversion fine-tuning example\n\n[Textual inversion](https://arxiv.org/abs/2208.01618) is a method to personalize text2image models like stable diffusion on your own images using just 3-5 examples.\nThe `textual_inversion.py` script shows how to implement the training procedure and adapt it for stable diffusion.\n\n## Running on Colab\n\nColab for training\n[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/huggingface/notebooks/blob/main/diffusers/sd_textual_inversion_training.ipynb)\n\nColab for inference\n[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/huggingface/notebooks/blob/main/diffusers/stable_conceptualizer_inference.ipynb)\n\n## Running locally with PyTorch\n### Installing the dependencies\n\nBefore running the scripts, make sure to install the library's training dependencies:\n\n**Important**", "# Textual Inversion fine-tuning example\n\n[Textual inversion](https://arxiv.org/abs/2208.01618) is a method to personalize text2image models like stable diffusion on your own images using just 3-5 examples.\nThe `textual_inversion.py` script shows how to implement the training procedure and adapt it for stable diffusion.\n\n## Running on Colab\n\nColab for training\n[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/huggingface/notebooks/blob/main/diffusers/sd_textual_inversion_training.ipynb)\n\nColab for inference\n[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/huggingface/notebooks/blob/main/diffusers/stable_conceptualizer_inference.ipynb)\n\n## Running locally with PyTorch\n### Installing the dependencies\n\nBefore running the scripts, make sure to install the library's training dependencies:\n\n**Important**"], "eval_score_GPT4": "5", "eval_feedback_GPT4": "The response provided is completely correct, accurate, and factual. It matches the reference answer by stating that 3-5 images are needed to teach new concepts to Stable Diffusion using Textual Inversion. The inclusion of a source reference does not detract from the accuracy of the answer."}, {"question": "What is the maximum size of a model checkpoint before it is automatically sharded in Transformers version 4.18.0?\n", "true_answer": "10GB", "source_doc": "huggingface/transformers/blob/main/docs/source/en/big_models.md", "generated_answer": "The maximum size of a model checkpoint before it is automatically sharded in Transformers version 4.18.0 is 10GB. This feature was introduced in version 4.18.0 to handle the increasing size of transformer models. If a model checkpoint exceeds 10GB, it will be automatically sharded into smaller pieces with a maximum size specified by the user with the `max_shard_size` parameter. This allows for more", "retrieved_docs": ["<Tip>\n\nNote that the randomly created model is initialized with \"empty\" tensors, which take the space in memory without filling it (thus the random values are whatever was in this chunk of memory at a given time). The random initialization following the appropriate distribution for the kind of model/parameters instantiated (like a normal distribution for instance) is only performed after step 3 on the non-initialized weights, to be as fast as possible! \n\n</Tip>\n\nIn this guide, we explore the solutions Transformers offer to deal with this issue. Note that this is an area of active development, so the APIs explained here may change slightly in the future.\n\n## Sharded checkpoints\n\nSince version 4.18.0, model checkpoints that end up taking more than 10GB of space are automatically sharded in smaller pieces. In terms of having one single checkpoint when you do `model.save_pretrained(save_dir)`, you will end up with several partial checkpoints (each of which being of size < 10GB) and an index that maps parameter names to the files they are stored in.\n\nYou can control the maximum size before sharding with the `max_shard_size` parameter, so for the sake of an example, we'll use a normal-size models with a small shard size: let's take a traditional BERT model.\n\n```py\nfrom transformers import AutoModel\n\nmodel = AutoModel.from_pretrained(\"bert-base-cased\")\n```", "```py\n>>> from transformers.modeling_utils import load_sharded_checkpoint\n\n>>> with tempfile.TemporaryDirectory() as tmp_dir:\n...     model.save_pretrained(tmp_dir, max_shard_size=\"200MB\")\n...     load_sharded_checkpoint(model, tmp_dir)\n```\n\n## Low memory loading\n\nSharded checkpoints reduce the memory usage during step 2 of the workflow mentioned above, but in order to use that model in a low memory setting, we recommend leveraging our tools based on the Accelerate library.\n\nPlease read the following guide for more information: [Large model loading using Accelerate](./main_classes/model#large-model-loading)", "This option can be activated with `low_cpu_mem_usage=True`. The model is first created on the Meta device (with empty weights) and the state dict is then loaded inside it (shard by shard in the case of a sharded checkpoint). This way the maximum RAM used is the full size of the model only.\n\n```py\nfrom transformers import AutoModelForSeq2SeqLM\n\nt0pp = AutoModelForSeq2SeqLM.from_pretrained(\"bigscience/T0pp\", low_cpu_mem_usage=True)\n```\n\nMoreover, you can directly place the model on different devices if it doesn't fully fit in RAM (only works for inference for now). With `device_map=\"auto\"`, Accelerate will determine where to put each layer to maximize the use of your fastest devices (GPUs) and offload the rest on the CPU, or even the hard drive if you don't have enough GPU RAM (or CPU RAM). Even if the model is split across several devices, it will run as you would normally expect.\n\nWhen passing a `device_map`, `low_cpu_mem_usage` is automatically set to `True`, so you don't need to specify it:\n\n```py\nfrom transformers import AutoModelForSeq2SeqLM\n\nt0pp = AutoModelForSeq2SeqLM.from_pretrained(\"bigscience/T0pp\", device_map=\"auto\")\n```\n\nYou can inspect how the model was split across devices by looking at its `hf_device_map` attribute:", "TODO: Add more explantion.\n\nBefore training, let's prepare our model first. To be able to shard the model, the sharded dimention needs to be a multiple of devices it'll be sharded on. But GPTNeo's vocab size is 50257, so we need to resize the embeddings accordingly. \n\n```python\nfrom transformers import FlaxGPTNeoForCausalLM, GPTNeoConfig \nmodel = FlaxGPTNeoForCausalLM.from_pretrained(\"EleutherAI/gpt-neo-1.3B\")\n\nemb = jnp.zeros((50264, model.config.hidden_size))\n# update the first 50257 weights using pre-trained weights\nemb = emb.at[:50257, :].set(model.params[\"transformer\"][\"wte\"][\"embedding\"])\nparams = model.params\nparams[\"transformer\"][\"wte\"][\"embedding\"] = emb\n\n# initialize a random model with the right vocab_size\nconfig = GPTNeoConfig.from_pretrained(\"EleutherAI/gpt-neo-1.3B\", vocab_size=50264)\nmodel = FlaxGPTNeoForCausalLM(config)\n\n# assign the pre-trained weights and save the model.\nmodel.params = params\nmodel.save_pretrained(\"gpt-neo-1.3B\")\n```\n\n\n### Train Model", "On top of the configuration of the model, we see three different weights files, and an `index.json` file which is our index. A checkpoint like this can be fully reloaded using the [`~PreTrainedModel.from_pretrained`] method:\n\n```py\n>>> with tempfile.TemporaryDirectory() as tmp_dir:\n...     model.save_pretrained(tmp_dir, max_shard_size=\"200MB\")\n...     new_model = AutoModel.from_pretrained(tmp_dir)\n```\n\nThe main advantage of doing this for big models is that during step 2 of the workflow shown above, each shard of the checkpoint is loaded after the previous one, capping the memory usage in RAM to the model size plus the size of the biggest shard.\n\nBehind the scenes, the index file is used to determine which keys are in the checkpoint, and where the corresponding weights are stored. We can load that index like any json and get a dictionary:\n\n```py\n>>> import json\n\n>>> with tempfile.TemporaryDirectory() as tmp_dir:\n...     model.save_pretrained(tmp_dir, max_shard_size=\"200MB\")\n...     with open(os.path.join(tmp_dir, \"pytorch_model.bin.index.json\"), \"r\") as f:\n...         index = json.load(f)", ">>> model = AutoModelForDocumentQuestionAnswering.from_pretrained(model_checkpoint)\n```\n\nIn the [`TrainingArguments`] use `output_dir` to specify where to save your model, and configure hyperparameters as you see fit.\nIf you wish to share your model with the community, set `push_to_hub` to `True` (you must be signed in to Hugging Face to upload your model).\nIn this case the `output_dir` will also be the name of the repo where your model checkpoint will be pushed.\n\n```py\n>>> from transformers import TrainingArguments\n\n>>> # REPLACE THIS WITH YOUR REPO ID\n>>> repo_id = \"MariaK/layoutlmv2-base-uncased_finetuned_docvqa\"\n\n>>> training_args = TrainingArguments(\n...     output_dir=repo_id,\n...     per_device_train_batch_size=4,\n...     num_train_epochs=20,\n...     save_steps=200,\n...     logging_steps=50,\n...     evaluation_strategy=\"steps\",\n...     learning_rate=5e-5,\n...     save_total_limit=2,\n...     remove_unused_columns=False,\n...     push_to_hub=True,\n... )\n```\n\nDefine a simple data collator to batch examples together.\n\n```py\n>>> from transformers import DefaultDataCollator", "This model was contributed by [Stella Biderman](https://huggingface.co/stellaathena).\n\n## Usage tips\n\n- To load [GPT-J](https://huggingface.co/EleutherAI/gpt-j-6B) in float32 one would need at least 2x model size\n  RAM: 1x for initial weights and another 1x to load the checkpoint. So for GPT-J it would take at least 48GB\n  RAM to just load the model. To reduce the RAM usage there are a few options. The `torch_dtype` argument can be\n  used to initialize the model in half-precision on a CUDA device only. There is also a fp16 branch which stores the fp16 weights,\n  which could be used to further minimize the RAM usage:\n\n```python\n>>> from transformers import GPTJForCausalLM\n>>> import torch\n\n>>> device = \"cuda\"\n>>> model = GPTJForCausalLM.from_pretrained(\n...     \"EleutherAI/gpt-j-6B\",\n...     revision=\"float16\",\n...     torch_dtype=torch.float16,\n... ).to(device)\n```"], "eval_score_GPT4": "5", "eval_feedback_GPT4": "The response correctly identifies the maximum size of a model checkpoint before it is automatically sharded as 10GB, which aligns with the reference answer. The additional information provided about the feature's introduction and the `max_shard_size` parameter is not required to answer the question but is accurate and relevant to the topic."}, {"question": "What is the purpose of Weights and Biases (W&B) for data scientists and machine learning scientists?\n", "true_answer": "To track their machine learning experiments at every stage, from training to production.", "source_doc": "gradio-app/gradio/blob/main/guides/06_integrating-other-frameworks/Gradio-and-Wandb-Integration.md", "generated_answer": "The purpose of Weights and Biases (W&B) for data scientists and machine learning scientists is to allow them to track their machine learning experiments from training to production at every stage. W&B provides a way to aggregate any metric and display it in a customizable and searchable dashboard, as shown in the provided screenshot. This tool helps data scientists and machine learning scientists to monitor and analyze the performance of their models throughout the development process, making it easier to identify and address issues", "retrieved_docs": ["--\ntitle: \"Let's talk about biases in machine learning! Ethics and Society Newsletter #2\" \nthumbnail: /blog/assets/122_ethics_soc_2/thumbnail-solstice.png\nauthors:\n- user: yjernite\n---\n\n# Machine Learning in development: Let's talk about bias!\n\n\n_Bias in ML is ubiquitous, and Bias in ML is complex; so complex in fact that no single technical intervention is likely to meaningfully address the problems it engenders. ML models, as sociotechnical systems, amplify social trends that may exacerbate inequities and harmful biases in ways that depend on their deployment context and are constantly evolving._\n\n_This means that developing ML systems with care requires vigilance and responding to feedback from those deployment contexts, which in turn we can facilitate by sharing lessons across contexts and developing tools to analyze signs of bias at every level of ML development._\n\n_This blog post from the [Ethics and Society regulars @\ud83e\udd17](https://huggingface.co/blog/ethics-soc-1) shares some of the lessons we have learned along with tools we have developed to support ourselves and others in our community\u2019s efforts to better address bias in Machine Learning. The first part is a broader reflection on bias and its context. If you\u2019ve already read it and are coming back specifically for the tools, feel free to jump to the [datasets](#i-am-curatingpicking-a-dataset-for-my-ml-system-how-can-i-address-bias) or [models](#i-am-trainingselecting-a-model-for-my-ml-system-how-can-i-address-bias)\nsection!_", "Hopefully, we can focus on the things that are most beneficial and continue heading in that direction.\n\n### Fav ML papers?\n\n**Meg:** Most recently I\u2019ve really loved what [Abeba Birhane](https://abebabirhane.github.io) has been doing on [values that are encoded in machine learning](https://arxiv.org/abs/2106.15590). My own team at Google had been working on [data genealogies](https://journals.sagepub.com/doi/full/10.1177/20539517211035955), bringing critical analysis on how ML data is handled which they have a few papers on - for example, [Data and its (dis)contents: A survey of dataset development and use in machine learning research](https://arxiv.org/abs/2012.05345). Really love that work and might be biased because it included my team and direct reports, I\u2019m very proud of them but it really is fundamentally good work.\n\nEarlier papers that I\u2019m interested in are more reflective of what I was doing at that time. Really love the work of [Herbert Clark](https://neurotree.org/beta/publications.php?pid=4636) who was a psycholinguistics/communications person and he did a lot of work that is easily ported to computational models about how humans communicate. Really love his work and cite him a lot throughout my thesis.\n\n### Anything else you would like to mention?", "**Sasha:** Yeah, something I'm working on outside of Big Science is on evaluation and how we evaluate models. Well kind of to what Ababa talks about in her paper, but even from just a pure machine learning perspective, what are the different ways that we can evaluate models and compare them on different aspects, I guess. Not only accuracy but efficiency and carbon emissions and things like that. So there's a project that started a month or ago on how to evaluate in a way that's not only performance-driven, but takes into account different aspects essentially. And I think that this has been a really overlooked aspect of machine learning, like people typically just once again and just check off like oh, you have to evaluate this and that and that, and then submit the paper. There are also these interesting trade-offs that we could be doing and things that we could be measuring that we're not.\n \nFor example, if you have a data set and you have an average accuracy, is the accuracy the same again in different subsets of the data set, like are there for example, patterns that you can pick up on that will help you improve your model, but also make it fairer? I guess the typical example is like image recognition, does it do the same in different\u2026 Well the famous [Gender Shades](http://gendershades.org/) paper about the algorithm did better on white men than African American women, but you could do that about anything. Not only gender and race, but you could do that for images, color or types of objects or angles. Like is it good for images from above or images from street level. There are all these different ways of analyzing accuracy or performance that we haven't really looked at because it's typically more time-consuming. And so we want to make tools to help people delve deeper into the results and understand their models better.", "So, who\u2019s on the hook for machine biases in ML? These three cases illustrate one of the reasons why discussions about the responsibility of ML developers in addressing bias can get so complicated: depending on decisions made at other points in the ML system development process by other people, the biases in an ML dataset or model may land anywhere between being irrelevant to the application settings and directly leading to grievous harm. However, in all of these cases, **stronger biases in the model/dataset increase the risk of negative outcomes**. The European Union has started to develop frameworks that address this phenomenon in [recent regulatory efforts](https://ec.europa.eu/info/business-economy-euro/doing-business-eu/contract-rules/digital-contracts/liability-rules-artificial-intelligence_en): in short, a company that deploys an AI system based on a measurably biased model is liable for harm caused by the system.\n\nConceptualizing bias as a risk factor then allows us to better understand the **shared responsibility** for machine biases between developers at all stages. Bias can never be fully removed, not least because the definitions of social biases and the power dynamics that tie them to discrimination vary vastly across social contexts. However:", "## \u4ec0\u4e48\u662f Wandb\uff1f\n\nWeights and Biases (W&B) \u5141\u8bb8\u6570\u636e\u79d1\u5b66\u5bb6\u548c\u673a\u5668\u5b66\u4e60\u79d1\u5b66\u5bb6\u5728\u4ece\u8bad\u7ec3\u5230\u751f\u4ea7\u7684\u6bcf\u4e2a\u9636\u6bb5\u8ddf\u8e2a\u4ed6\u4eec\u7684\u673a\u5668\u5b66\u4e60\u5b9e\u9a8c\u3002\u4efb\u4f55\u6307\u6807\u90fd\u53ef\u4ee5\u5bf9\u6837\u672c\u8fdb\u884c\u805a\u5408\uff0c\u5e76\u5728\u53ef\u81ea\u5b9a\u4e49\u548c\u53ef\u641c\u7d22\u7684\u4eea\u8868\u677f\u4e2d\u663e\u793a\uff0c\u5982\u4e0b\u6240\u793a\uff1a\n\n<img alt=\"Screen Shot 2022-08-01 at 5 54 59 PM\" src=\"https://user-images.githubusercontent.com/81195143/182252755-4a0e1ca8-fd25-40ff-8c91-c9da38aaa9ec.png\">\n\n## \u4ec0\u4e48\u662f Hugging Face Spaces \u548c Gradio\uff1f\n\n### Gradio\n\nGradio \u8ba9\u7528\u6237\u53ef\u4ee5\u4f7f\u7528\u51e0\u884c Python \u4ee3\u7801\u5c06\u5176\u673a\u5668\u5b66\u4e60\u6a21\u578b\u6f14\u793a\u4e3a Web \u5e94\u7528\u7a0b\u5e8f\u3002Gradio \u5c06\u4efb\u4f55 Python \u51fd\u6570\uff08\u4f8b\u5982\u673a\u5668\u5b66\u4e60\u6a21\u578b\u7684\u63a8\u65ad\u51fd\u6570\uff09\u5305\u88c5\u6210\u4e00\u4e2a\u7528\u6237\u754c\u9762\uff0c\u8fd9\u4e9b\u6f14\u793a\u53ef\u4ee5\u5728 jupyter \u7b14\u8bb0\u672c\u3001colab \u7b14\u8bb0\u672c\u4e2d\u542f\u52a8\uff0c\u4e5f\u53ef\u4ee5\u5d4c\u5165\u5230\u60a8\u81ea\u5df1\u7684\u7f51\u7ad9\u4e2d\uff0c\u514d\u8d39\u6258\u7ba1\u5728 Hugging Face Spaces \u4e0a\u3002\n\n\u5728\u8fd9\u91cc\u5f00\u59cb [here](https://gradio.app/getting_started)\n\n### Hugging Face Spaces", "## Citation(s)\n```bibtex\n@article{scikit-learn,\n    title={Scikit-learn: Machine Learning in {P}ython},\n    author={Pedregosa, F. and Varoquaux, G. and Gramfort, A. and Michel, V.\n    and Thirion, B. and Grisel, O. and Blondel, M. and Prettenhofer, P.\n    and Weiss, R. and Dubourg, V. and Vanderplas, J. and Passos, A. and\n    Cournapeau, D. and Brucher, M. and Perrot, M. and Duchesnay, E.},\n    journal={Journal of Machine Learning Research},\n    volume={12},\n    pages={2825--2830},\n    year={2011}\n}\n```\n\n\n## Further References\n- [Wikipedia -- Precision and recall](https://en.wikipedia.org/wiki/Precision_and_recall)", "## Citation(s)\n```bibtex\n@article{scikit-learn,\n    title={Scikit-learn: Machine Learning in {P}ython},\n    author={Pedregosa, F. and Varoquaux, G. and Gramfort, A. and Michel, V.\n    and Thirion, B. and Grisel, O. and Blondel, M. and Prettenhofer, P.\n    and Weiss, R. and Dubourg, V. and Vanderplas, J. and Passos, A. and\n    Cournapeau, D. and Brucher, M. and Perrot, M. and Duchesnay, E.},\n    journal={Journal of Machine Learning Research},\n    volume={12},\n    pages={2825--2830},\n    year={2011}\n}\n```\n\n\n## Further References\n- [Wikipedia -- Precision and recall](https://en.wikipedia.org/wiki/Precision_and_recall)"], "eval_score_GPT4": "5", "eval_feedback_GPT4": "The response provided aligns well with the reference answer, stating that the purpose of Weights and Biases (W&B) is to track machine learning experiments at every stage, which includes training to production. The response expands on the reference answer by explaining that W&B allows for the aggregation of metrics and displays them in a customizable dashboard, which is a factual statement about the capabilities of W&B. It also correctly identifies the benefit of using W&B for monitoring and analyzing model performance, which is a key aspect of the tool for data scientists and machine learning scientists. The response is therefore correct, accurate, and factual."}, {"question": "What is the name of the open-source library created by Hugging Face to simplify Transformer acceleration?\n", "true_answer": "Optimum", "source_doc": "huggingface/blog/blob/main/intel.md", "generated_answer": "The name of the open-source library created by Hugging Face to simplify Transformer acceleration is \ud83e\udd17 Optimum. (Source: Extract 1)", "retrieved_docs": ["## Optimum Library\n\nHugging Face is a fast-growing open community and platform aiming to democratize good machine learning. We extended modalities from NLP to audio and vision, and now covers use cases across Machine Learning to meet our community's needs following the success of the [Transformers library](https://huggingface.co/docs/transformers/index). Now on [Hugging Face Hub](https://huggingface.co/models), there are more than 120K free and accessible model checkpoints for various machine learning tasks, 18K datasets, and 20K ML demo apps. However, scaling transformer models into production is still a challenge for the industry. Despite high accuracy, training and inference of transformer-based models can be time-consuming and expensive.\n\nTo target these needs, Hugging Face built two open-sourced libraries: __Accelerate__ and __Optimum__. While [\ud83e\udd17 Accelerate](https://huggingface.co/docs/accelerate/index) focuses on out-of-the-box distributed training, [\ud83e\udd17 Optimum](https://huggingface.co/docs/optimum/index), as an extension of transformers, accelerates model training and inference by leveraging the maximum efficiency of users\u2019 targeted hardware. Optimum integrated machine learning accelerators like ONNX Runtime and specialized hardware like [Intel's Habana Gaudi](https://huggingface.co/blog/habana-gaudi-2-benchmark), so users can benefit from considerable speedup in both training and inference. Besides, Optimum seamlessly integrates other Hugging Face\u2019s tools while inheriting the same ease of use as Transformers. Developers can easily adapt their work to achieve lower latency with less computing power.", "At Hugging Face, we have been working a lot to facilitate the optimization of models, especially with our open-source [Optimum library](https://huggingface.co/hardware). Optimum makes it extremely simple to optimize most \ud83e\udd17 Transformers models.\n\nIf you're looking for **highly optimized** solutions which don't require any technical knowledge, you might be interested in the [Inference API](https://huggingface.co/inference-api), a plug & play solution to serve in production a wide variety of machine learning tasks, including sentiment analysis.\n\nMoreover, if you are searching for **support for your custom use cases**, Hugging Face's team of experts can help accelerate your ML projects! Our team answer questions and find solutions as needed in your machine learning journey from research to production. Visit [hf.co/support](https://huggingface.co/support) to learn more and request a quote.", "!--Copyright 2022 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\n\nhttp://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License.\n\n\u26a0\ufe0f Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\nrendered properly in your Markdown viewer.\n\n-->\n\n# Distributed training with \ud83e\udd17 Accelerate\n\nAs models get bigger, parallelism has emerged as a strategy for training larger models on limited hardware and accelerating training speed by several orders of magnitude. At Hugging Face, we created the [\ud83e\udd17 Accelerate](https://huggingface.co/docs/accelerate) library to help users easily train a \ud83e\udd17 Transformers model on any type of distributed setup, whether it is multiple GPU's on one machine or multiple GPU's across several machines. In this tutorial, learn how to customize your native PyTorch training loop to enable training in a distributed environment.\n\n## Setup\n\nGet started by installing \ud83e\udd17 Accelerate:\n\n```bash\npip install accelerate\n```", "![Filter of all libraries](assets/27_summer_at_huggingface/filters.png)\n\n\n## Solutions\n\n### **Coming soon: Infinity**\n\nTransformers latency down to 1ms? \ud83e\udd2f\ud83e\udd2f\ud83e\udd2f\n\nWe have been working on a really sleek solution to achieve unmatched efficiency for state-of-the-art Transformer models, for companies to deploy in their own infrastructure.\n\n- Infinity comes as a single-container and can be deployed in any production environment.\n- It can achieve 1ms latency for BERT-like models on GPU and 4-10ms on CPU \ud83e\udd2f\ud83e\udd2f\ud83e\udd2f\n- Infinity meets the highest security requirements and can be integrated into your system without the need for internet access. You have control over all incoming and outgoing traffic.\n\n\u26a0\ufe0f  Join us for a [live announcement and demo on Sep 28](https://app.livestorm.co/hugging-face/hugging-face-infinity-launch?type=detailed), where we will be showcasing Infinity for the first time in public!\n\n### **NEW: Hardware Acceleration**\n\nHugging Face is [partnering with leading AI hardware accelerators](http://hf.co/hardware) such as Intel, Qualcomm and GraphCore to make state-of-the-art production performance accessible and extend training capabilities on SOTA hardware. As the first step in this journey, we [introduced a new Open Source library](https://huggingface.co/blog/hardware-partners-program): \ud83e\udd17 Optimum - the ML optimization toolkit for production performance \ud83c\udfce. Learn more in this [blog post](https://huggingface.co/blog/graphcore).", "Transformers have proven to be extremely efficient for a wide range of functions, including feature extraction, text generation, sentiment analysis, translation and many more. Models like BERT are widely used by Graphcore customers in a huge array of applications including cybersecurity, voice call automation, drug discovery, and translation.\n\nOptimizing their performance in the real world requires considerable time, effort and skills that are beyond the reach of many companies and organizations. In providing an open-source library of transformer models, Hugging Face has directly addressed these issues. Integrating IPUs with HuggingFace also allows developers to leverage not just the models, but also datasets available in the HuggingFace Hub.\n\nDevelopers can now use Graphcore systems to train 10 different types of state-of-the-art transformer models and access thousands of datasets with minimal coding complexity. With this partnership, we are providing users with the tools and ecosystem to easily download and fine-tune state-of-the-art pretrained models to various domains and downstream tasks.\n\n## Bringing Graphcore\u2019s latest hardware and software to the table\n\nWhile members of Hugging Face\u2019s ever-expanding user base have already been able to benefit from the speed, performance, and power- and cost-efficiency of IPU technology, a combination of recent hardware and software releases from Graphcore will unlock even more potential.", "## @Hugging Face \ud83e\udd17: Long-range modeling\n\nThe Longformer implementation and the associated open-source checkpoints are available through the Transformers library and the [model hub](https://huggingface.co/models?search=longformer). Performer and Big Bird, which is a long-range model based on sparse attention, are currently in the works as part of our [call for models](https://twitter.com/huggingface/status/1359903233976762368), an effort involving the community in order to promote open-source contributions. We would be pumped to hear from you if you\u2019ve wondered how to contribute to `transformers` but did not know where to start!\n\nFor further reading, we recommend checking Patrick Platen\u2019s blog on [Reformer](https://arxiv.org/abs/2001.04451), Teven Le Scao\u2019s post on [Johnson-Lindenstrauss approximation](https://tevenlescao.github.io/blog/fastpages/jupyter/2020/06/18/JL-Lemma-+-Linformer.html), [Efficient Transfomers: A Survey](https://arxiv.org/abs/2009.06732), and [Long Range Arena: A Benchmark for Efficient Transformers](https://arxiv.org/abs/2011.04006).\n\nNext month, we'll cover self-training methods and applications. See you in March!", "## If you are looking for custom support from the Hugging Face team\n\n<a target=\"_blank\" href=\"https://huggingface.co/support\">\n    <img alt=\"HuggingFace Expert Acceleration Program\" src=\"https://cdn-media.huggingface.co/marketing/transformers/new-support-improved.png\" style=\"width: 100%; max-width: 600px; border: 1px solid #eee; border-radius: 4px; box-shadow: 0 1px 2px 0 rgba(0, 0, 0, 0.05);\">\n</a>\n\n## Contents\n\nThe documentation is organized into five sections:\n\n- **GET STARTED** provides a quick tour of the library and installation instructions to get up and running.\n- **TUTORIALS** are a great place to start if you're a beginner. This section will help you gain the basic skills you need to start using the library.\n- **HOW-TO GUIDES** show you how to achieve a specific goal, like finetuning a pretrained model for language modeling or how to write and share a custom model.\n- **CONCEPTUAL GUIDES** offers more discussion and explanation of the underlying concepts and ideas behind models, tasks, and the design philosophy of \ud83e\udd17 Transformers.\n- **API** describes all classes and functions:\n\n  - **MAIN CLASSES** details the most important classes like configuration, model, tokenizer, and pipeline.\n  - **MODELS** details the classes and functions related to each model implemented in the library.\n  - **INTERNAL HELPERS** details utility classes and functions used internally.\n\n\n## Supported models and frameworks"], "eval_score_GPT4": "5", "eval_feedback_GPT4": "The response correctly identifies the name of the open-source library as \"Optimum,\" which matches the reference answer. The inclusion of the Hugging Face emoji \ud83e\udd17 is not necessary for the correctness of the answer but does not detract from the factual accuracy of the response. Therefore, the response is completely correct, accurate, and factual."}, {"question": "What parameter is used to ensure that elements in a row have the same height in Gradio?\n", "true_answer": "equal_height", "source_doc": "gradio-app/gradio/blob/main/guides/cn/03_building-with-blocks/02_controlling-layout.md", "generated_answer": "The parameter used to ensure that elements in a row have the same height in Gradio is `equal_height`, which can be passed as an argument to the `style` method of the `Row` component. This ensures that all elements in the row have the same height, even if they have different content.", "retrieved_docs": ["$code_rows_and_columns\n$demo_rows_and_columns\n\nSee how the first column has two Textboxes arranged vertically. The second column has an Image and Button arranged vertically. Notice how the relative widths of the two columns is set by the `scale` parameter. The column with twice the `scale` value takes up twice the width.\n\nLearn more about Columns in the [docs](https://gradio.app/docs/#column).\n\n# Dimensions\n\nYou can control the height and width of various components, where the parameters are available. These parameters accept either a number (interpreted as pixels) or a string. Using a string allows the direct application of any CSS unit to the encapsulating Block element, catering to more specifc design requirements. When omitted, Gradio uses default dimensions suited for most use cases.\n\nBelow is an example illustrating the use of viewport width (vw):\n\n```python\nimport gradio as gr\n\nwith gr.Blocks() as demo:\n    im = gr.ImageEditor(\n        width=\"50vw\",\n    )\n\ndemo.launch()\n```\n\nWhen using percentage values for dimensions, you may want to define a parent component with an absolute unit (e.g. `px` or `vw`). This approach ensures that child components with relative dimensions are sized appropriately:\n\n\n```python\nimport gradio as gr\n\ncss = \"\"\"\n.container {\n    height: 100vh;\n}\n\"\"\"", "```python\nimport gradio as gr\n\ncss = \"\"\"\n.container {\n    height: 100vh;\n}\n\"\"\"\n\nwith gr.Blocks(css=css) as demo:\n    with gr.Column(elem_classes=[\"container\"]):\n        name = gr.Chatbot(value=[[\"1\", \"2\"]], height=\"70%\")\n\ndemo.launch()\n```\n\nIn this example, the Column layout component is given a height of 100% of the viewport height (100vh), and the Chatbot component inside it takes up 70% of the Column's height.\n\nYou can apply any valid CSS unit for these parameters. For a comprehensive list of CSS units, refer to [this guide](https://www.w3schools.com/cssref/css_units.php). We recommend you always consider responsiveness and test your interfaces on various screen sizes to ensure a consistent user experience.\n\n\n\n## Tabs and Accordions\n\nYou can also create Tabs using the `with gr.Tab('tab_name'):` clause. Any component created inside of a `with gr.Tab('tab_name'):` context appears in that tab. Consecutive Tab clauses are grouped together so that a single tab can be selected at one time, and only the components within that Tab's context are shown.\n\nFor example:\n\n$code_blocks_flipper\n$demo_blocks_flipper", "The widths of elements in a Row can be controlled via a combination of `scale` and `min_width` arguments that are present in every Component.\n\n- `scale` is an integer that defines how an element will take up space in a Row. If scale is set to `0`, and element will not expand to take up space. If scale is set to `1` or greater, the element well expand. Multiple elements in a row will expand proportional to their scale. Below, `btn1` will expand twice as much as `btn2`, while `btn0` will not expand at all:\n\n```python\nwith gr.Blocks() as demo:\n    with gr.Row():\n        btn0 = gr.Button(\"Button 0\", scale=0)\n        btn1 = gr.Button(\"Button 1\", scale=1)\n        btn2 = gr.Button(\"Button 2\", scale=2)\n```\n\n- `min_width` will set the minimum width the element will take. The Row will wrap if there isn't sufficient space to satisfy all `min_width` values.\n\nLearn more about Rows in the [docs](https://gradio.app/docs/#row).\n\n## Columns and Nesting\n\nComponents within a Column will be placed vertically atop each other. Since the vertical layout is the default layout for Blocks apps anyway, to be useful, Columns are usually nested within Rows. For example:\n\n$code_rows_and_columns\n$demo_rows_and_columns", "Controlling Layout\n\nBy default, Components in Blocks are arranged vertically. Let's take a look at how we can rearrange Components. Under the hood, this layout structure uses the [flexbox model of web development](https://developer.mozilla.org/en-US/docs/Web/CSS/CSS_Flexible_Box_Layout/Basic_Concepts_of_Flexbox).\n\n## Rows\n\nElements within a `with gr.Row` clause will all be displayed horizontally. For example, to display two Buttons side by side:\n\n```python\nwith gr.Blocks() as demo:\n    with gr.Row():\n        btn1 = gr.Button(\"Button 1\")\n        btn2 = gr.Button(\"Button 2\")\n```\n\nTo make every element in a Row have the same height, use the `equal_height` argument of the `style` method.\n\n```python\nwith gr.Blocks() as demo:\n    with gr.Row(equal_height=True):\n        textbox = gr.Textbox()\n        btn2 = gr.Button(\"Button 2\")\n```\n\nThe widths of elements in a Row can be controlled via a combination of `scale` and `min_width` arguments that are present in every Component.", "* Add missing `display: flex` property to `Row` so that flex styling is applied to children by [@hannahblair] in [PR 4896](https://github.com/gradio-app/gradio/pull/4896)\n* Fixed bug where `gr.Video` could not preprocess urls by [@freddyaboulton](https://github.com/freddyaboulton) in [PR 4904](https://github.com/gradio-app/gradio/pull/4904)\n* Fixed copy button rendering in API page on Safari by [@aliabid94](https://github.com/aliabid94) in [PR 4924](https://github.com/gradio-app/gradio/pull/4924)\n* Fixed `gr.Group` and `container=False`. `container` parameter only available for `Textbox`, `Number`, and `Dropdown`, the only elements where it makes sense. By [@aliabid94](https://github.com/aliabid94) in [PR 4916](https://github.com/gradio-app/gradio/pull/4916)\n* Fixed broken image link in auto-generated `app.py` from `ThemeClass.push_to_hub` by [@deepkyu](https://github.com/deepkyu) in [PR 4944](https://github.com/gradio-app/gradio/pull/4944)\n\n### Other Changes:", "* Add missing `display: flex` property to `Row` so that flex styling is applied to children by [@hannahblair] in [PR 4896](https://github.com/gradio-app/gradio/pull/4896)\n* Fixed bug where `gr.Video` could not preprocess urls by [@freddyaboulton](https://github.com/freddyaboulton) in [PR 4904](https://github.com/gradio-app/gradio/pull/4904)\n* Fixed copy button rendering in API page on Safari by [@aliabid94](https://github.com/aliabid94) in [PR 4924](https://github.com/gradio-app/gradio/pull/4924)\n* Fixed `gr.Group` and `container=False`. `container` parameter only available for `Textbox`, `Number`, and `Dropdown`, the only elements where it makes sense. By [@aliabid94](https://github.com/aliabid94) in [PR 4916](https://github.com/gradio-app/gradio/pull/4916)\n* Fixed broken image link in auto-generated `app.py` from `ThemeClass.push_to_hub` by [@deepkyu](https://github.com/deepkyu) in [PR 4944](https://github.com/gradio-app/gradio/pull/4944)\n\n### Other Changes:", "- white space for class concatenation by [@radames](https://github.com/radames) in [PR 1228](https://github.com/gradio-app/gradio/pull/1228)\n- Tweaks by [@pngwn](https://github.com/pngwn) in [PR 1230](https://github.com/gradio-app/gradio/pull/1230)\n- css tweaks by [@pngwn](https://github.com/pngwn) in [PR 1235](https://github.com/gradio-app/gradio/pull/1235)\n- ensure defaults height match for media inputs by [@pngwn](https://github.com/pngwn) in [PR 1236](https://github.com/gradio-app/gradio/pull/1236)\n- Default Label label value by [@radames](https://github.com/radames) in [PR 1239](https://github.com/gradio-app/gradio/pull/1239)\n- update-shortcut-syntax by [@FarukOzderim](https://github.com/FarukOzderim) in [PR 1234](https://github.com/gradio-app/gradio/pull/1234)"], "eval_score_GPT4": "5", "eval_feedback_GPT4": "The response correctly identifies `equal_height` as the parameter used in Gradio to ensure that elements in a row have the same height. The explanation provided is accurate and aligns with the reference answer. The response is factual, providing additional context on how the parameter is used within the `style` method of the `Row` component, which demonstrates a clear understanding of the functionality."}, {"question": "What is the command to install the latest version of Optimum with OpenVINO support?\n", "true_answer": "pip install --upgrade-strategy eager optimum[\"openvino\"]", "source_doc": "huggingface/diffusers/blob/main/docs/source/en/optimization/open_vino.md", "generated_answer": "The command to install the latest version of Optimum with OpenVINO support is:\n\n```bash\npip install --upgrade-strategy eager optimum[openvino]\n```\n\nThis command will install the current main dev version of Optimum, which could include the latest developments (new features, bug fixes), but it might not be very stable. If you encounter any issues, please open an issue on the Optimum GitHub page.", "retrieved_docs": ["The `--upgrade-strategy eager` option is needed to ensure the different packages are upgraded to the latest possible version.\n\nTo install from source:\n\n```bash\npython -m pip install git+https://github.com/huggingface/optimum.git\n```\n\nFor the accelerator-specific features, append `optimum[accelerator_type]` to the above command:\n\n```bash\npython -m pip install optimum[onnxruntime]@git+https://github.com/huggingface/optimum.git\n```\n\n## Accelerated Inference\n\n\ud83e\udd17 Optimum provides multiple tools to export and run optimized models on various ecosystems: \n\n- [ONNX](https://huggingface.co/docs/optimum/exporters/onnx/usage_guides/export_a_model) / [ONNX Runtime](https://huggingface.co/docs/optimum/onnxruntime/usage_guides/models)\n- TensorFlow Lite\n- [OpenVINO](https://huggingface.co/docs/optimum/intel/inference)\n- Habana first-gen Gaudi / Gaudi2, more details [here](https://huggingface.co/docs/optimum/main/en/habana/usage_guides/accelerate_inference)\n\nThe [export](https://huggingface.co/docs/optimum/exporters/overview) and optimizations can be done both programmatically and with a command line.\n\n### Features summary", "!--Copyright 2023 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\n\nhttp://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License.\n-->\n\n# OpenVINO\n\n\ud83e\udd17 [Optimum](https://github.com/huggingface/optimum-intel) provides Stable Diffusion pipelines compatible with OpenVINO to perform inference on a variety of Intel processors (see the [full list](https://docs.openvino.ai/latest/openvino_docs_OV_UG_supported_plugins_Supported_Devices.html) of supported devices).\n\nYou'll need to install \ud83e\udd17 Optimum Intel with the `--upgrade-strategy eager` option to ensure [`optimum-intel`](https://github.com/huggingface/optimum-intel) is using the latest version:\n\n```bash\npip install --upgrade-strategy eager optimum[\"openvino\"]\n```\n\nThis guide will show you how to use the Stable Diffusion and Stable Diffusion XL (SDXL) pipelines with OpenVINO.\n\n## Stable Diffusion", "Out of the box, we can see that Sapphire Rapids CPUs are quite faster without any code change!\n\nNow, let's accelerate!\n\n## Optimum Intel and OpenVINO\n\n[Optimum Intel](https://huggingface.co/docs/optimum/intel/index) accelerates end-to-end pipelines on Intel architectures. Its API is extremely similar to the vanilla [Diffusers](https://huggingface.co/docs/diffusers/index) API, making it trivial to adapt existing code.\n\nOptimum Intel supports [OpenVINO](https://docs.openvino.ai/latest/index.html), an Intel open-source toolkit for high-performance inference. \n\nOptimum Intel and OpenVINO can be installed as follows:\n\n```\npip install optimum[openvino]\n```\n\nStarting from the code above, we only need to replace `StableDiffusionPipeline` with `OVStableDiffusionPipeline`. To load a PyTorch model and convert it to the OpenVINO format on-the-fly, you can set `export=True` when loading your model.\n\n```python\nfrom optimum.intel.openvino import OVStableDiffusionPipeline\n...\nov_pipe = OVStableDiffusionPipeline.from_pretrained(model_id, export=True)\nlatency = elapsed_time(ov_pipe, prompt)\nprint(latency)\n\n# Don't forget to save the exported model\nov_pipe.save_pretrained(\"./openvino\")\n```", "```bash\npip install git+https://github.com/huggingface/optimum.git\n```\n\nThis command installs the current main dev version of Optimum, which could include latest developments(new features, bug fixes). However, the\nmain version might not be very stable. If you run into any problem, please open an [issue](https://github.com/huggingface/optimum/issues) so\nthat we can fix it as soon as possible.\n\n## ORTTrainer\n\nThe [`ORTTrainer`] class inherits the [`Trainer`](https://huggingface.co/docs/transformers/main/en/main_classes/trainer#trainer)\nof Transformers. You can easily adapt the codes by replacing `Trainer` of transformers with `ORTTrainer` to take advantage of the acceleration\nempowered by ONNX Runtime. Here is an example of how to use `ORTTrainer` compared with `Trainer`:\n\n```diff\n-from transformers import Trainer, TrainingArguments\n+from optimum.onnxruntime import ORTTrainer, ORTTrainingArguments\n\n# Step 1: Define training arguments\n-training_args = TrainingArguments(\n+training_args = ORTTrainingArguments(\n    output_dir=\"path/to/save/folder/\",\n-   optim = \"adamw_hf\",\n+   optim=\"adamw_ort_fused\",\n    ...\n)", "### OpenVINO\n\nBefore you begin, make sure you have all the necessary libraries installed :\n\n```bash\npip install --upgrade-strategy eager optimum[openvino,nncf]\n```\n\nIt is possible to export \ud83e\udd17 Transformers and Diffusers models to the OpenVINO format easily:\n\n```bash\noptimum-cli export openvino --model distilbert-base-uncased-finetuned-sst-2-english distilbert_sst2_ov\n```\n\nIf you add `--int8`, the weights will be quantized to INT8. Static quantization can also be applied on the activations using [NNCF](https://github.com/openvinotoolkit/nncf), more information can be found in the [documentation](https://huggingface.co/docs/optimum/main/en/intel/optimization_ov).\n\nTo load a model and run inference with OpenVINO Runtime, you can just replace your `AutoModelForXxx` class with the corresponding `OVModelForXxx` class. To load a PyTorch checkpoint and convert it to the OpenVINO format on-the-fly, you can set `export=True` when loading your model.\n\n```diff\n- from transformers import AutoModelForSequenceClassification\n+ from optimum.intel import OVModelForSequenceClassification\n  from transformers import AutoTokenizer, pipeline", "## Optimum Intel\n\n### OpenVINO", "## Optimum Intel\n\n### OpenVINO"], "eval_score_GPT4": "4", "eval_feedback_GPT4": "The response provides the correct command to install the latest version of Optimum with OpenVINO support, including the correct use of the `pip install` command and the `--upgrade-strategy eager` option. However, there is a minor discrepancy in the use of quotation marks around \"openvino\" compared to the reference answer. The reference answer uses double quotes around \"openvino\", while the response does not use quotes. This difference is not likely to affect the functionality of the command, as pip typically handles both cases similarly. Therefore, the response is mostly correct, accurate, and factual."}]