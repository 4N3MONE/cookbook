[{"question": "What architecture is the `tokenizers-linux-x64-musl` binary designed for?\n", "true_answer": "x86_64-unknown-linux-musl", "source_doc": "huggingface/tokenizers/blob/main/bindings/node/npm/linux-x64-musl/README.md", "generated_answer": "The `tokenizers-linux-x64-musl` binary is designed for the **x86_64-unknown-linux-musl** architecture. (Answer according to context provided.)", "retrieved_docs": ["`tokenizers-linux-x64-musl`\n\nThis is the **x86_64-unknown-linux-musl** binary for `tokenizers`", "`tokenizers-linux-arm64-musl`\n\nThis is the **aarch64-unknown-linux-musl** binary for `tokenizers`", "`tokenizers-linux-x64-gnu`\n\nThis is the **x86_64-unknown-linux-gnu** binary for `tokenizers`", "`tokenizers-linux-arm64-gnu`\n\nThis is the **aarch64-unknown-linux-gnu** binary for `tokenizers`", "`tokenizers-freebsd-x64`\n\nThis is the **x86_64-unknown-freebsd** binary for `tokenizers`", "`tokenizers-win32-x64-msvc`\n\nThis is the **x86_64-pc-windows-msvc** binary for `tokenizers`", "`tokenizers-win32-arm64-msvc`\n\nThis is the **aarch64-pc-windows-msvc** binary for `tokenizers`"], "eval_score_GPT4": "5", "eval_feedback_GPT4": "Feedback: The response provided is completely correct, accurate, and factual. It matches the reference answer exactly, indicating that the `tokenizers-linux-x64-musl` binary is designed for the x86_64-unknown-linux-musl architecture."}, {"question": "What is the purpose of the BLIP-Diffusion model?\n", "true_answer": "The BLIP-Diffusion model is designed for controllable text-to-image generation and editing.", "source_doc": "huggingface/diffusers/blob/main/docs/source/en/api/pipelines/blip_diffusion.md", "generated_answer": "The provided context does not include information about the BLIP-Diffusion model. Therefore, it is not possible to answer the question. Please provide more context or clarify the question to enable a comprehensive answer.", "retrieved_docs": ["We also want to thank @heejkoo for the very helpful overview of papers, code and resources on diffusion models, available [here](https://github.com/heejkoo/Awesome-Diffusion-Models) as well as @crowsonkb and @rromb for useful discussions and insights.\n\n## Citation\n\n```bibtex\n@misc{von-platen-etal-2022-diffusers,\n  author = {Patrick von Platen and Suraj Patil and Anton Lozhkov and Pedro Cuenca and Nathan Lambert and Kashif Rasul and Mishig Davaadorj and Thomas Wolf},\n  title = {Diffusers: State-of-the-art diffusion models},\n  year = {2022},\n  publisher = {GitHub},\n  journal = {GitHub repository},\n  howpublished = {\\url{https://github.com/huggingface/diffusers}}\n}\n```", "**Note**: It is highly recommended to have a basic understanding of how diffusion models work. If diffusion \nmodels are completely new to you, we recommend reading one of the following blog posts:\n- [The Annotated Diffusion Model](https://huggingface.co/blog/annotated-diffusion)\n- [Getting started with \ud83e\udde8 Diffusers](https://colab.research.google.com/github/huggingface/notebooks/blob/main/diffusers/diffusers_intro.ipynb)\n\nNow, let's get started by generating some images \ud83c\udfa8.\n\n## Running Stable Diffusion\n\n### License\n\nBefore using the model, you need to accept the model [license](https://huggingface.co/spaces/CompVis/stable-diffusion-license) in order to download and use the weights. **Note: the license does not need to be explicitly accepted through the UI anymore**.\n\nThe license is designed to mitigate the potential harmful effects of such a powerful machine learning system. \nWe request users to **read the license entirely and carefully**. Here we offer a summary:\n1. You can't use the model to deliberately produce nor share illegal or harmful outputs or content,\n2. We claim no rights on the outputs you generate, you are free to use them and are accountable for their use which should not go against the provisions set in the license, and\n3. You may re-distribute the weights and use the model commercially and/or as a service. If you do, please be aware you have to include the same use restrictions as the ones in the license and share a copy of the CreativeML OpenRAIL-M to all your users.\n\n### Usage\n\nFirst, you should install `diffusers==0.10.2` to run the following code snippets:\n\n```bash\npip install diffusers==0.10.2 transformers scipy ftfy accelerate\n```", "!--Copyright 2023 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\n\nhttp://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License.\n-->\n\n# Overview\n\nWelcome to \ud83e\udde8 Diffusers! If you're new to diffusion models and generative AI, and want to learn more, then you've come to the right place. These beginner-friendly tutorials are designed to provide a gentle introduction to diffusion models and help you understand the library fundamentals - the core components and how \ud83e\udde8 Diffusers is meant to be used.\n\nYou'll learn how to use a pipeline for inference to rapidly generate things, and then deconstruct that pipeline to really understand how to use the library as a modular toolbox for building your own diffusion systems. In the next lesson, you'll learn how to train your own diffusion model to generate what you want.\n\nAfter completing the tutorials, you'll have gained the necessary skills to start exploring the library on your own and see how to use it for your own projects and applications.\n\nFeel free to join our community on [Discord](https://discord.com/invite/JfAtkvEtRb) or the [forums](https://discuss.huggingface.co/c/discussion-related-to-httpsgithubcomhuggingfacediffusers/63) to connect and collaborate with other users and developers!\n\nLet's start diffusing! \ud83e\udde8", "--\ntitle: The Annotated Diffusion Model\nthumbnail: /blog/assets/78_annotated-diffusion/thumbnail.png\nauthors:\n- user: nielsr\n- user: kashif\n---\n\n# The Annotated Diffusion Model\n\n\n<script async defer src=\"https://unpkg.com/medium-zoom-element@0/dist/medium-zoom-element.min.js\"></script>\n\n<a target=\"_blank\" href=\"https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/annotated_diffusion.ipynb\">\n    <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n</a>\n\nIn this blog post, we'll take a deeper look into **Denoising Diffusion Probabilistic Models** (also known as DDPMs, diffusion models, score-based generative models or simply [autoencoders](https://benanne.github.io/2022/01/31/diffusion.html)) as researchers have been able to achieve remarkable results with them for (un)conditional image/audio/video generation. Popular examples (at the time of writing) include [GLIDE](https://arxiv.org/abs/2112.10741) and [DALL-E 2](https://openai.com/dall-e-2/) by OpenAI, [Latent Diffusion](https://github.com/CompVis/latent-diffusion) by the University of Heidelberg and [ImageGen](https://imagen.research.google/) by Google Brain.", "<div class=\"mt-10\">\n  <div class=\"w-full flex flex-col space-y-4 md:space-y-0 md:grid md:grid-cols-2 md:gap-y-4 md:gap-x-5\">\n    <a class=\"!no-underline border dark:border-gray-700 p-5 rounded-lg shadow hover:shadow-lg\" href=\"./tutorials/tutorial_overview\"\n      ><div class=\"w-full text-center bg-gradient-to-br from-blue-400 to-blue-500 rounded-lg py-1.5 font-semibold mb-5 text-white text-lg leading-relaxed\">Tutorials</div>\n      <p class=\"text-gray-700\">Learn the fundamental skills you need to start generating outputs, build your own diffusion system, and train a diffusion model. We recommend starting here if you're using \ud83e\udd17 Diffusers for the first time!</p>\n    </a>\n    <a class=\"!no-underline border dark:border-gray-700 p-5 rounded-lg shadow hover:shadow-lg\" href=\"./using-diffusers/loading_overview\"\n      ><div class=\"w-full text-center bg-gradient-to-br from-indigo-400 to-indigo-500 rounded-lg py-1.5 font-semibold mb-5 text-white text-lg leading-relaxed\">How-to guides</div>\n      <p class=\"text-gray-700\">Practical guides for helping you load pipelines, models, and schedulers. You'll also learn how to use pipelines for specific tasks, control how outputs are generated, optimize for inference speed, and different training techniques.</p>\n    </a>", "## Scenarios\n\nWe cover Diffusion models with the following pipelines:\n\n- Text-guided image generation (such as the [`StableDiffusionPipeline`](https://huggingface.co/docs/diffusers/main/en/api/pipelines/stable_diffusion/text2img)).\n- Text-guided image generation, additionally conditioned on an input image (such as the [`StableDiffusionImg2ImgPipeline`](https://huggingface.co/docs/diffusers/main/en/api/pipelines/stable_diffusion/img2img) and [`StableDiffusionInstructPix2PixPipeline`](https://huggingface.co/docs/diffusers/main/en/api/pipelines/pix2pix)).\n- Class-conditioned image generation models (such as the [`DiTPipeline`](https://huggingface.co/docs/diffusers/main/en/api/pipelines/dit)).\n\n## Qualitative Evaluation\n\nQualitative evaluation typically involves human assessment of generated images. Quality is measured across aspects such as compositionality, image-text alignment, and spatial relations. Common prompts provide a degree of uniformity for subjective metrics.\nDrawBench and PartiPrompts are prompt datasets used for qualitative benchmarking. DrawBench and PartiPrompts were introduced by [Imagen](https://imagen.research.google/) and [Parti](https://parti.research.google/) respectively.\n\nFrom the [official Parti website](https://parti.research.google/):\n\n> PartiPrompts (P2) is a rich set of over 1600 prompts in English that we release as part of this work. P2 can be used to measure model capabilities across various categories and challenge aspects.\n\n![parti-prompts](https://huggingface.co/datasets/diffusers/docs-images/resolve/main/evaluation_diffusion_models/parti-prompts.png)", "!--Copyright 2023 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\n\nhttp://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License.\n-->\n\n<p align=\"center\">\n    <br>\n    <img src=\"https://raw.githubusercontent.com/huggingface/diffusers/77aadfee6a891ab9fcfb780f87c693f7a5beeb8e/docs/source/imgs/diffusers_library.jpg\" width=\"400\"/>\n    <br>\n</p>\n\n# Diffusers\n\n\ud83e\udd17 Diffusers is the go-to library for state-of-the-art pretrained diffusion models for generating images, audio, and even 3D structures of molecules. Whether you're looking for a simple inference solution or want to train your own diffusion model, \ud83e\udd17 Diffusers is a modular toolbox that supports both. Our library is designed with a focus on [usability over performance](conceptual/philosophy#usability-over-performance), [simple over easy](conceptual/philosophy#simple-over-easy), and [customizability over abstractions](conceptual/philosophy#tweakable-contributorfriendly-over-abstraction).\n\nThe library has three main components:"], "eval_score_GPT4": "1", "eval_feedback_GPT4": "Feedback: The response does not provide any information about the purpose of the BLIP-Diffusion model. Instead, it deflects the question by stating that there is not enough context to provide an answer. This does not align with the reference answer, which clearly states the purpose of the BLIP-Diffusion model. According to the score rubric, the response should be rated as completely incorrect, inaccurate, and/or not factual because it fails to mention the model's purpose for controllable text-to-image generation and editing."}, {"question": "How can a user claim authorship of a paper on the Hugging Face Hub?\n", "true_answer": "By clicking their name on the corresponding Paper page and clicking \"claim authorship\", then confirming the request in paper settings for admin team validation.", "source_doc": "huggingface/hub-docs/blob/main/docs/hub/paper-pages.md", "generated_answer": "To claim authorship of a paper on the Hugging Face Hub, a user can follow these steps:\n\n1. Ensure that the paper is linked to their account by checking if their name appears in the corresponding Paper page. If it does not, proceed to step 2.\n\n2. Click on the user's name in the Paper page and select \"claim authorship\". This will redirect the user to their Paper settings page.\n\n3. Confirm the request by clicking", "retrieved_docs": ["## Claiming authorship to a Paper\n\nThe Hub will attempt to automatically match paper to users based on their email. \n\n<div class=\"flex justify-center\">\n<img class=\"block dark:hidden\" width=\"300\" src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/hub/papers-authors.png\"/>\n<img class=\"hidden dark:block\" width=\"300\" src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/hub/papers-authors-dark.png\"/>\n</div>\n\nIf your paper is not linked to your account, you can click in your name in the corresponding Paper page and click \"claim authorship\". This will automatically re-direct to your paper settings where you can confirm the request. The admin team will validate your request soon. Once confirmed, the Paper page will show as verified.\n\n<div class=\"flex justify-center\">\n<img class=\"block dark:hidden\" width=\"300\" src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/hub/papers-settings.png\"/>\n<img class=\"hidden dark:block\" width=\"300\" src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/hub/papers-settings-dark.png\"/>\n</div>\n\n\n## Frequently Asked Questions\n\n### Can I control which Paper pages show in my profile?\n\nYes! You can visit your Papers in [settings](https://huggingface.co/settings/papers), where you will see a list of verified papers. There, you can click the \"Show on profile\" checkbox to hide/show it in your profile. \n\n### Do you support ACL anthology?", "Paper Pages\n\nPaper pages allow people to find artifacts related to a paper such as models, datasets and apps/demos (Spaces). Paper pages also enable the community to discuss about the paper.\n\n<div class=\"flex justify-center\">\n<img class=\"block dark:hidden\" width=\"300\" src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/hub/papers-discussions.png\"/>\n<img class=\"hidden dark:block\" width=\"300\" src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/hub/papers-discussions-dark.png\"/>\n</div>\n\n## Linking a Paper to a model, dataset or Space\n\nIf the repository card (`README.md`) includes a link to a paper on arXiv, the Hugging Face Hub will extract the arXiv ID and include it in the repository's tags. Clicking on the arxiv tag will let you:\n\n* Visit the Paper page.\n* Filter for other models or datasets on the Hub that cite the same paper.\n\n<div class=\"flex justify-center\">\n<img class=\"block dark:hidden\" width=\"300\" src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/hub/datasets-arxiv.png\"/>\n<img class=\"hidden dark:block\" width=\"300\" src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/hub/datasets-arxiv-dark.png\"/>\n</div>\n\n## Claiming authorship to a Paper\n\nThe Hub will attempt to automatically match paper to users based on their email.", "!---\nCopyright 2023 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\");\nyou may not use this file except in compliance with the License.\nYou may obtain a copy of the License at\n\n    http://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software\ndistributed under the License is distributed on an \"AS IS\" BASIS,\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\nSee the License for the specific language governing permissions and\nlimitations under the License.\n-->\n\n# \ud83e\udd17 Transformers Notebooks\n\nYou can find here a list of the official notebooks provided by Hugging Face.\n\nAlso, we would like to list here interesting content created by the community.\nIf you wrote some notebook(s) leveraging \ud83e\udd17 Transformers and would like to be listed here, please open a\nPull Request so it can be included under the Community notebooks.\n\n\n## Hugging Face's notebooks \ud83e\udd17\n\n### Documentation notebooks\n\nYou can open any page of the documentation as a notebook in Colab (there is a button directly on said pages) but they are also listed here if you need them:", "!---\nCopyright 2023 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\");\nyou may not use this file except in compliance with the License.\nYou may obtain a copy of the License at\n\n    http://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software\ndistributed under the License is distributed on an \"AS IS\" BASIS,\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\nSee the License for the specific language governing permissions and\nlimitations under the License.\n-->\n\n# \ud83e\udd17 Transformers Notebooks\n\nYou can find here a list of the official notebooks provided by Hugging Face.\n\nAlso, we would like to list here interesting content created by the community.\nIf you wrote some notebook(s) leveraging \ud83e\udd17 Transformers and would like to be listed here, please open a\nPull Request so it can be included under the Community notebooks.\n\n\n## Hugging Face's notebooks \ud83e\udd17\n\n### Documentation notebooks\n\nYou can open any page of the documentation as a notebook in Colab (there is a button directly on said pages) but they are also listed here if you need them:", "!---\nCopyright 2023 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\");\nyou may not use this file except in compliance with the License.\nYou may obtain a copy of the License at\n\n    http://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software\ndistributed under the License is distributed on an \"AS IS\" BASIS,\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\nSee the License for the specific language governing permissions and\nlimitations under the License.\n-->\n\n# \ud83e\udd17 Datasets Notebooks\n\nYou can find here a list of the official notebooks provided by Hugging Face.\n\nAlso, we would like to list here interesting content created by the community.\nIf you wrote some notebook(s) leveraging \ud83e\udd17 Datasets and would like it to be listed here, please open a\nPull Request so it can be included under the Community notebooks.\n\n## Hugging Face's notebooks \ud83e\udd17\n\n### Documentation notebooks\n\nYou can open any page of the documentation as a notebook in Colab (there is a button directly on said pages) but they are also listed here if you need them:", "By clicking on **Agree**, you agree to share your username and email address with the model authors. In some cases, additional fields might be requested. To help the model authors decide whether to grant you access, try to fill out the form as completely as possible.\n\nOnce the access request is sent, there are two possibilities. If the approval mechanism is automatic, you immediately get access to the model files. Otherwise, the requests have to be approved manually by the authors, which can take more time. \n\n<Tip warning={true}>\n\nThe model authors have complete control over model access. In particular, they can decide at any time to block your access to the model without prior notice, regardless of approval mechanism or if your request has already been approved.\n\n</Tip>\n\n### Download files\n\nTo download files from a gated model you'll need to be authenticated. In the browser, this is automatic as long as you are logged in with your account. If you are using a script, you will need to provide a [user token](./security-tokens). In the Hugging Face Python ecosystem (`transformers`, `diffusers`, `datasets`, etc.), you can login your machine using the [`huggingface_hub`](https://huggingface.co/docs/huggingface_hub/index) library and running in your terminal:\n\n```bash\nhuggingface-cli login\n```\n\nAlternatively, you can programmatically login using `login()` in a notebook or a script:\n\n```python\n>>> from huggingface_hub import login\n>>> login()\n```\n\nYou can also provide the `token` parameter to most loading methods in the libraries (`from_pretrained`, `hf_hub_download`, `load_dataset`, etc.), directly from your scripts.\n\nFor more details about how to login, check out the [login guide](https://huggingface.co/docs/huggingface_hub/quick-start#login).", "!---\nCopyright 2020 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\");\nyou may not use this file except in compliance with the License.\nYou may obtain a copy of the License at\n\n    http://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software\ndistributed under the License is distributed on an \"AS IS\" BASIS,\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\nSee the License for the specific language governing permissions and\nlimitations under the License.\n-->\n\n# How to contribute to huggingface_hub, the GitHub repository?\n\nEveryone is welcome to contribute, and we value everybody's contribution. Code is not the only way to help the community.\nAnswering questions, helping others, reaching out and improving the documentations are immensely valuable to the community.\n\nIt also helps us if you spread the word: reference the library from blog posts\non the awesome projects it made possible, shout out on Twitter every time it has\nhelped you, or simply star the repo to say \"thank you\".\n\nWhichever way you choose to contribute, please be mindful to respect our\n[code of conduct](https://github.com/huggingface/huggingface_hub/blob/main/CODE_OF_CONDUCT.md).\n\n> Looking for a good first issue to work on?\n> Please check out our contributing guide below and then select an issue from our [curated list](https://github.com/huggingface/huggingface_hub/contribute).\n> Pick one and get started with it!\n\n### The client library, `huggingface_hub`\n\nThis repository hosts the `huggingface_hub`, the client library that interfaces any Python script with the Hugging Face Hub.\nIts implementation lives in `src/huggingface_hub` while the tests are located in `tests/`.\n\nThere are many ways you can contribute to this client library:\n* Fixing outstanding issues with the existing code;\n* Contributing to the examples or to the documentation;\n* Submitting issues related to bugs or desired new features."], "eval_score_GPT4": "4", "eval_feedback_GPT4": "Feedback: The response provided is mostly correct and factual. It aligns with the reference answer in terms of the steps required to claim authorship on the Hugging Face Hub. However, the response is incomplete as it ends abruptly at step 3 without explaining what needs to be confirmed or the final steps of the process. The reference answer mentions confirming the request in paper settings for admin team validation, which is not fully covered in the evaluated response. Therefore, the response does not fully meet the criteria for a score of 5."}, {"question": "What is the purpose of the /healthcheck endpoint in the Datasets server API?\n", "true_answer": "Ensure the app is running", "source_doc": "huggingface/datasets-server/blob/main/services/api/README.md", "generated_answer": "The purpose of the `/healthcheck` endpoint in the Datasets server API is to ensure that the app is running properly. When this endpoint is accessed, the server returns a response indicating whether it is healthy or not. This endpoint is commonly used to check the availability and responsiveness of the server, as well as to monitor its overall health and performance. In the context provided, the `/healthcheck` endpoint is listed as one of the endpoints in the configuration and API service sections for", "retrieved_docs": ["Datasets server SSE API\n\n> Server-sent events API for the Datasets server. It's used to update the Hub's backend cache.\n\n## Configuration\n\nThe service can be configured using environment variables. They are grouped by scope.\n\n### Common\n\nSee [../../libs/libcommon/README.md](../../libs/libcommon/README.md) for more information about the common configuration.\n\n## Endpoints\n\nSee https://huggingface.co/docs/datasets-server\n\n- /healthcheck: Ensure the app is running\n- /metrics: Return a list of metrics in the Prometheus format\n- /hub-cache: Return a dataset information as a Server-Sent Event (SSE) when a dataset is updated. If `?all=true` is passed in the parameters, and if the cache already has some entries, one SSE per cached dataset is sent to the client. Then, a SSE is sent when a dataset is inserted, modified or deleted. The event data is a JSON with the following structure. The `hub_cache` field is null for deleted entries, or when the response is an error. The `num_rows` value is `0` if it could not be determined.\n\n```typescript\n{\n    dataset: string;\n    hub_cache: null | {\n        preview: boolean;\n        viewer: boolean;\n        partial: boolean;\n        num_rows: int;\n    };\n}\n```", "Datasets server API - rows endpoint\n\n> /rows endpoint\n\n## Configuration\n\nThe service can be configured using environment variables. They are grouped by scope.\n\n### API service\n\nSee [../../libs/libapi/README.md](../../libs/libapi/README.md) for more information about the API configuration.\n\n### Common\n\nSee [../../libs/libcommon/README.md](../../libs/libcommon/README.md) for more information about the common configuration.\n\n## Endpoints\n\nSee https://huggingface.co/docs/datasets-server\n\n- /healthcheck: ensure the app is running\n- /metrics: return a list of metrics in the Prometheus format\n- /rows: get a slice of rows of a dataset split", "Datasets server API\n\n> API on \ud83e\udd17 datasets\n\n## Configuration\n\nThe service can be configured using environment variables. They are grouped by scope.\n\n### API service\n\nSee [../../libs/libapi/README.md](../../libs/libapi/README.md) for more information about the API configuration.\n\n### Common\n\nSee [../../libs/libcommon/README.md](../../libs/libcommon/README.md) for more information about the common configuration.\n\n## Endpoints\n\nSee https://huggingface.co/docs/datasets-server\n\n- /healthcheck: Ensure the app is running\n- /metrics: Return a list of metrics in the Prometheus format\n- /webhook: Add, update or remove a dataset\n- /is-valid: Tell if a dataset is [valid](https://huggingface.co/docs/datasets-server/valid)\n- /splits: List the [splits](https://huggingface.co/docs/datasets-server/splits) names for a dataset\n- /first-rows: Extract the [first rows](https://huggingface.co/docs/datasets-server/first_rows) for a dataset split\n- /parquet: List the [parquet files](https://huggingface.co/docs/datasets-server/parquet) auto-converted for a dataset", "## Check if a dataset is valid\n\n`/is-valid` checks whether a specific dataset loads without any error. This endpoint's query parameter requires you to specify the name of the dataset:\n\n<inferencesnippet>\n<python>\n```python\nimport requests\nheaders = {\"Authorization\": f\"Bearer {API_TOKEN}\"}\nAPI_URL = \"https://datasets-server.huggingface.co/is-valid?dataset=rotten_tomatoes\"\ndef query():\n    response = requests.get(API_URL, headers=headers)\n    return response.json()\ndata = query()\n```\n</python>\n<js>\n```js\nimport fetch from \"node-fetch\";\nasync function query(data) {\n    const response = await fetch(\n        \"https://datasets-server.huggingface.co/is-valid?dataset=rotten_tomatoes\",\n        {\n            headers: { Authorization: `Bearer ${API_TOKEN}` },\n            method: \"GET\"\n        }\n    );\n    const result = await response.json();\n    return result;\n}\nquery().then((response) => {\n    console.log(JSON.stringify(response));\n});\n```\n</js>\n<curl>\n```curl\ncurl https://datasets-server.huggingface.co/is-valid?dataset=rotten_tomatoes \\\n        -X GET \\\n        -H \"Authorization: Bearer ${API_TOKEN}\"\n```\n</curl>\n</inferencesnippet>\n\nThe response looks like this if a dataset is valid:\n\n```json\n{\n  \"viewer\": true,\n  \"preview\": true\n}\n```\n\nIf only the first rows of a dataset are available, then the response looks like:\n\n```json\n{\n  \"viewer\": false,\n  \"preview\": true\n}\n```\n\nFinally, if the dataset is not valid at all, then the response is:\n\n```json\n{\n  \"viewer\": false,\n  \"preview\": false\n}\n```", "Quickstart\n\n[[open-in-colab]]\n\nIn this quickstart, you'll learn how to use the Datasets Server's REST API to:\n\n- Check whether a dataset on the Hub is functional.\n- Return the configuration and splits of a dataset.\n- Preview the first 100 rows of a dataset.\n- Download slices of rows of a dataset.\n- Search a word in a dataset.\n- Access the dataset as parquet files.\n\n## API endpoints\n\nEach feature is served through an endpoint summarized in the table below:", "Datasets server API - search service\n\n> /search endpoint\n> /filter endpoint\n\n## Configuration\n\nThe service can be configured using environment variables. They are grouped by scope.\n\n### Duckdb index full text search\n- `DUCKDB_INDEX_CACHE_DIRECTORY`: directory where the temporal duckdb index files are downloaded. Defaults to empty.\n- `DUCKDB_INDEX_TARGET_REVISION`: the git revision of the dataset where the index file is stored in the dataset repository.\n\n### API service\n\nSee [../../libs/libapi/README.md](../../libs/libapi/README.md) for more information about the API configuration.\n\n### Common\n\nSee [../../libs/libcommon/README.md](../../libs/libcommon/README.md) for more information about the common configuration.\n\n## Endpoints\n\nSee https://huggingface.co/docs/datasets-server\n\n- /healthcheck: ensure the app is running\n- /metrics: return a list of metrics in the Prometheus format\n- /search: get a slice of a search result over a dataset split\n- /filter: filter rows of a dataset split", "Server infrastructure\n\nThe [Datasets Server](https://github.com/huggingface/datasets-server) has two main components that work together to return queries about a dataset instantly:\n\n- a user-facing web API for exploring and returning information about a dataset\n- a server runs the queries ahead of time and caches them in a database\n\nWhile most of the documentation is focused on the web API, the server is crucial because it performs all the time-consuming preprocessing and stores the results so the web API can retrieve and serve them to the user. This saves a user time because instead of generating the response every time it gets requested, Datasets Server can return the preprocessed results instantly from the cache.\n\nThere are three elements that keep the server running: the job queue, workers, and the cache.\n\n## Job queue\n\nThe job queue is a list of jobs stored in a Mongo database that should be completed by the workers. The jobs are practically identical to the endpoints the user uses; only the server runs the jobs ahead of time, and the user gets the results when they use the endpoint.\n\nThere are three jobs:\n\n- `/splits` corresponds to the `/splits` endpoint. It refreshes a dataset and then returns that dataset's splits and configurations. For every split in the dataset, it'll create a new job.\n- `/first-rows` corresponds to the `/first-rows` endpoint. It gets the first 100 rows and columns of a dataset split.\n- `/parquet` corresponds to the `/parquet` endpoint. It downloads the whole dataset, converts it to [parquet](https://parquet.apache.org/) and publishes the parquet files to the Hub.\n\nYou might've noticed the `/rows` and `/search` endpoints don't have a job in the queue. The responses from these endpoints are generated on demand.\n\n## Workers"], "eval_score_GPT4": "5", "eval_feedback_GPT4": "The response provided aligns well with the reference answer, stating that the purpose of the `/healthcheck` endpoint is to ensure that the app is running properly. It expands on the reference answer by explaining what happens when the endpoint is accessed and the types of checks it performs, such as availability, responsiveness, and overall health and performance of the server. The response is correct, accurate, and factual, providing a more detailed explanation without deviating from the core purpose mentioned in the reference answer."}, {"question": "What is the default context window size for Local Attention in the LongT5 model?\n", "true_answer": "127 tokens", "source_doc": "huggingface/transformers/blob/main/docs/source/en/model_doc/longt5.md", "generated_answer": "The default context window size for Local Attention in the LongT5 model is 127 tokens (`r=127`). This is mentioned in the context provided, specifically in the section titled \"Longformer Self Attention\". The text states, \"The sparse sliding-window local attention operation allows a given token to attend only `r` tokens to the left and right of it (with `r=127` by default).\"", "retrieved_docs": ["## Local attention\n\n[Longformer](#longformer) uses local attention: often, the local context (e.g., what are the two tokens to the\nleft and right?) is enough to take action for a given token. Also, by stacking attention layers that have a small\nwindow, the last layer will have a receptive field of more than just the tokens in the window, allowing them to build a\nrepresentation of the whole sentence.\n\nSome preselected input tokens are also given global attention: for those few tokens, the attention matrix can access\nall tokens and this process is symmetric: all other tokens have access to those specific tokens (on top of the ones in\ntheir local window). This is shown in Figure 2d of the paper, see below for a sample attention mask:\n\n<div class=\"flex justify-center\">\n    <img scale=\"50 %\" align=\"center\" src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/local_attention_mask.png\"/>\n</div>\n\nUsing those attention matrices with less parameters then allows the model to have inputs having a bigger sequence\nlength.\n\n## Other tricks\n\n### Axial positional encodings", "## Longformer Self Attention\n\nLongformer self attention employs self attention on both a \"local\" context and a \"global\" context. Most tokens only\nattend \"locally\" to each other meaning that each token attends to its \\\\(\\frac{1}{2} w\\\\) previous tokens and\n\\\\(\\frac{1}{2} w\\\\) succeeding tokens with \\\\(w\\\\) being the window length as defined in\n`config.attention_window`. Note that `config.attention_window` can be of type `List` to define a\ndifferent \\\\(w\\\\) for each layer. A selected few tokens attend \"globally\" to all other tokens, as it is\nconventionally done for all tokens in `BertSelfAttention`.\n\nNote that \"locally\" and \"globally\" attending tokens are projected by different query, key and value matrices. Also note\nthat every \"locally\" attending token not only attends to tokens within its window \\\\(w\\\\), but also to all \"globally\"\nattending tokens so that global attention is *symmetric*.\n\nThe user can define which tokens attend \"locally\" and which tokens attend \"globally\" by setting the tensor\n`global_attention_mask` at run-time appropriately. All Longformer models employ the following logic for\n`global_attention_mask`:\n\n- 0: the token attends \"locally\",\n- 1: the token attends \"globally\".\n\nFor more information please also refer to [`~LongformerModel.forward`] method.\n\nUsing Longformer self attention, the memory and time complexity of the query-key matmul operation, which usually\nrepresents the memory and time bottleneck, can be reduced from \\\\(\\mathcal{O}(n_s \\times n_s)\\\\) to\n\\\\(\\mathcal{O}(n_s \\times w)\\\\), with \\\\(n_s\\\\) being the sequence length and \\\\(w\\\\) being the average window\nsize. It is assumed that the number of \"globally\" attending tokens is insignificant as compared to the number of\n\"locally\" attending tokens.", "## Usage tips\n\n- [`LongT5ForConditionalGeneration`] is an extension of [`T5ForConditionalGeneration`] exchanging the traditional\nencoder *self-attention* layer with efficient either *local* attention or *transient-global* (*tglobal*) attention.\n- Unlike the T5 model, LongT5 does not use a task prefix. Furthermore, it uses a different pre-training objective\ninspired by the pre-training of [`PegasusForConditionalGeneration`].\n- LongT5 model is designed to work efficiently and very well on long-range *sequence-to-sequence* tasks where the\ninput sequence exceeds commonly used 512 tokens. It is capable of handling input sequences of a length up to 16,384 tokens.\n- For *Local Attention*, the sparse sliding-window local attention operation allows a given token to attend only `r`\ntokens to the left and right of it (with `r=127` by default). *Local Attention* does not introduce any new parameters\nto the model. The complexity of the mechanism is linear in input sequence length `l`: `O(l*r)`.\n- *Transient Global Attention* is an extension of the *Local Attention*. It, furthermore, allows each input token to\ninteract with all other tokens in the layer. This is achieved via splitting an input sequence into blocks of a fixed\nlength `k` (with a default `k=16`). Then, a global token for such a block is obtained via summing and normalizing the embeddings of every token\nin the block. Thanks to this, the attention allows each token to attend to both nearby tokens like in Local attention, and\nalso every global token like in the case of standard global attention (*transient* represents the fact the global tokens\nare constructed dynamically within each attention operation).  As a consequence, *TGlobal* attention introduces\na few new parameters -- global relative position biases and a layer normalization for global token's embedding.\nThe complexity of this mechanism is `O(l(r + l/k))`.\n- An example showing how to evaluate a fine-tuned LongT5 model on the [pubmed dataset](https://huggingface.co/datasets/scientific_papers) is below.", "Recent Changes\n\n### Aug 29, 2022\n* MaxVit window size scales with img_size by default. Add new RelPosMlp MaxViT weight that leverages this:\n  * `maxvit_rmlp_nano_rw_256` - 83.0 @ 256, 83.6 @ 320  (T)", "Iz Beltagy, Matthew E. Peters, Arman Cohan\n\nLongformer addresses the memory bottleneck of transformers by replacing conventional self-attention with a combination of windowed/local/sparse (cf. [Sparse Transformers (2019)](https://arxiv.org/abs/1904.10509)) attention and global attention that scales linearly with the sequence length. As opposed to previous long-range transformer models (e.g. [Transformer-XL (2019)](https://arxiv.org/abs/1901.02860), [Reformer (2020)](https://arxiv.org/abs/2001.04451), [Adaptive Attention Span (2019)](https://arxiv.org/abs/1905.07799)), Longformer\u2019s self-attention layer is designed as a drop-in replacement for the standard self-attention, thus making it possible to leverage pre-trained checkpoints for further pre-training and/or fine-tuning on long sequence tasks.\n\nThe standard self-attention matrix (Figure a) scales quadratically with the input length:\n\n<figure>\n  <img src=\"/blog/assets/14_long_range_transformers/Longformer.png\" alt=\"Longformer attention\"/>\n  <figcaption>Figure taken from Longformer</figcaption>\n</figure>", "RNNs natively support very long context lengths - only limited by the context length seen in training, but this can be extended to millions of tokens with careful coding. Currently, there are RWKV models trained on a context length of 8192 (`ctx8192`) and they are as fast as `ctx1024` models and require the same amount of RAM.\n\nThe major drawbacks of traditional RNN models and how RWKV is different:\n\n1. Traditional RNN models are unable to utilize very long contexts (LSTM can only manage ~100 tokens when used as a LM). However, RWKV can utilize thousands of tokens and beyond, as shown below: \n\n| ![rwkv_loss](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/142_rwkv/RWKV-loss.png) |\n|:--:|\n| <b>LM loss with respect to different context lengths and model sizes. Source: <a href=\"https://raw.githubusercontent.com/BlinkDL/RWKV-LM/main/RWKV-ctxlen.png\" rel=\"noopener\" target=\"_blank\" >RWKV original repository</a>  </b>|\n\n2. Traditional RNN models cannot be parallelized when training. RWKV is similar to a \u201clinearized GPT\u201d and it trains faster than GPT.\n\nBy combining both advantages into a single architecture, the hope is that RWKV can grow to become more than the sum of its parts.\n\n### RWKV attention formulation\n\nThe model architecture is very similar to classic transformer-based models (i.e. an embedding layer, multiple identical layers, layer normalization, and a Causal Language Modeling head to predict the next token). The only difference is on the attention layer, which is completely different from the traditional transformer-based models.", "<img\nsrc=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/dilated-neighborhood-attention-pattern.jpg\"\nalt=\"drawing\" width=\"600\"/>\n\n<small> Neighborhood Attention with different dilation values.\nTaken from the <a href=\"https://arxiv.org/abs/2209.15001\">original paper</a>.</small>\n\nThis model was contributed by [Ali Hassani](https://huggingface.co/alihassanijr).\nThe original code can be found [here](https://github.com/SHI-Labs/Neighborhood-Attention-Transformer).\n\n## Usage tips\n\nDiNAT can be used as a *backbone*. When `output_hidden_states = True`,\nit will output both `hidden_states` and `reshaped_hidden_states`. The `reshaped_hidden_states` have a shape of `(batch, num_channels, height, width)` rather than `(batch_size, height, width, num_channels)`.\n\nNotes:\n- DiNAT depends on [NATTEN](https://github.com/SHI-Labs/NATTEN/)'s implementation of Neighborhood Attention and Dilated Neighborhood Attention.\nYou can install it with pre-built wheels for Linux by referring to [shi-labs.com/natten](https://shi-labs.com/natten), or build on your system by running `pip install natten`.\nNote that the latter will likely take time to compile. NATTEN does not support Windows devices yet.\n- Patch size of 4 is only supported at the moment.\n\n## Resources\n\nA list of official Hugging Face and community (indicated by \ud83c\udf0e) resources to help you get started with DiNAT.\n\n<PipelineTag pipeline=\"image-classification\"/>"], "eval_score_GPT4": "5", "eval_feedback_GPT4": "Feedback: The response provided is completely correct, accurate, and factual. It matches the reference answer by stating that the default context window size for Local Attention in the LongT5 model is 127 tokens. Additionally, the response gives a bit more detail by referencing the specific section where this information can be found, which supports the accuracy of the answer."}, {"question": "What method is used to load a checkpoint for a task using `AutoPipeline`?\n", "true_answer": "from_pretrained()", "source_doc": "huggingface/diffusers/blob/main/docs/source/en/api/pipelines/auto_pipeline.md", "generated_answer": "The `AutoPipeline` class in the `diffusers` library is used to load a checkpoint for a task without knowing the specific pipeline class to use. This is achieved by calling the `from_pretrained()` method of the `AutoPipeline` class with the name or path to the pretrained weights. The method automatically detects the relevant pipeline given the task and returns a pipeline instance ready for inference. This method supports text-to-image, image-to-image,", "retrieved_docs": ["If you try to load an unsupported checkpoint, it'll throw an error:\n\n```py\nfrom diffusers import AutoPipelineForImage2Image\nimport torch\n\npipeline = AutoPipelineForImage2Image.from_pretrained(\n    \"openai/shap-e-img2img\", torch_dtype=torch.float16, use_safetensors=True\n)\n\"ValueError: AutoPipeline can't find a pipeline linked to ShapEImg2ImgPipeline for None\"\n```\n\n## Use multiple pipelines\n\nFor some workflows or if you're loading many pipelines, it is more memory-efficient to reuse the same components from a checkpoint instead of reloading them which would unnecessarily consume additional memory. For example, if you're using a checkpoint for text-to-image and you want to use it again for image-to-image, use the [`~AutoPipelineForImage2Image.from_pipe`] method. This method creates a new pipeline from the components of a previously loaded pipeline at no additional memory cost.\n\nThe [`~AutoPipelineForImage2Image.from_pipe`] method detects the original pipeline class and maps it to the new pipeline class corresponding to the task you want to do. For example, if you load a `\"stable-diffusion\"` class pipeline for text-to-image:\n\n```py\nfrom diffusers import AutoPipelineForText2Image, AutoPipelineForImage2Image\nimport torch\n\npipeline_text2img = AutoPipelineForText2Image.from_pretrained(\n    \"runwayml/stable-diffusion-v1-5\", torch_dtype=torch.float16, use_safetensors=True\n)\nprint(type(pipeline_text2img))\n\"<class 'diffusers.pipelines.stable_diffusion.pipeline_stable_diffusion.StableDiffusionPipeline'>\"\n```", "</Tip>\n\nThis tutorial shows you how to use an `AutoPipeline` to automatically infer the pipeline class to load for a specific task, given the pretrained weights.\n\n## Choose an AutoPipeline for your task\n\nStart by picking a checkpoint. For example, if you're interested in text-to-image with the [runwayml/stable-diffusion-v1-5](https://huggingface.co/runwayml/stable-diffusion-v1-5) checkpoint, use [`AutoPipelineForText2Image`]:\n\n```py\nfrom diffusers import AutoPipelineForText2Image\nimport torch\n\npipeline = AutoPipelineForText2Image.from_pretrained(\n    \"runwayml/stable-diffusion-v1-5\", torch_dtype=torch.float16, use_safetensors=True\n).to(\"cuda\")\nprompt = \"peasant and dragon combat, wood cutting style, viking era, bevel with rune\"\n\nimage = pipeline(prompt, num_inference_steps=25).images[0]\nimage\n```\n\n<div class=\"flex justify-center\">\n    <img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/diffusers/autopipeline-text2img.png\" alt=\"generated image of peasant fighting dragon in wood cutting style\"/>\n</div>\n\nUnder the hood, [`AutoPipelineForText2Image`]:\n\n1. automatically detects a `\"stable-diffusion\"` class from the [`model_index.json`](https://huggingface.co/runwayml/stable-diffusion-v1-5/blob/main/model_index.json) file\n2. loads the corresponding text-to-image [`StableDiffusionPipeline`] based on the `\"stable-diffusion\"` class name", "!--Copyright 2023 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\n\nhttp://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License.\n-->\n\n# AutoPipeline\n\n`AutoPipeline` is designed to:\n\n1. make it easy for you to load a checkpoint for a task without knowing the specific pipeline class to use\n2. use multiple pipelines in your workflow\n\nBased on the task, the `AutoPipeline` class automatically retrieves the relevant pipeline given the name or path to the pretrained weights with the `from_pretrained()` method.\n\nTo seamlessly switch between tasks with the same checkpoint without reallocating additional memory, use the `from_pipe()` method to transfer the components from the original pipeline to the new one.\n\n```py\nfrom diffusers import AutoPipelineForText2Image\nimport torch\n\npipeline = AutoPipelineForText2Image.from_pretrained(\n    \"runwayml/stable-diffusion-v1-5\", torch_dtype=torch.float16, use_safetensors=True\n).to(\"cuda\")\nprompt = \"Astronaut in a jungle, cold color palette, muted colors, detailed, 8k\"\n\nimage = pipeline(prompt, num_inference_steps=25).images[0]\n```\n\n<Tip>\n\nCheck out the [AutoPipeline](../../tutorials/autopipeline) tutorial to learn how to use this API!\n\n</Tip>\n\n`AutoPipeline` supports text-to-image, image-to-image, and inpainting for the following diffusion models:", "\ud83d\udca1 Skip to the [DiffusionPipeline explained](#diffusionpipeline-explained) section if you are interested in learning in more detail about how the [`DiffusionPipeline`] class works.\n\n</Tip>\n\nThe [`DiffusionPipeline`] class is the simplest and most generic way to load the latest trending diffusion model from the [Hub](https://huggingface.co/models?library=diffusers&sort=trending). The [`DiffusionPipeline.from_pretrained`] method automatically detects the correct pipeline class from the checkpoint, downloads, and caches all the required configuration and weight files, and returns a pipeline instance ready for inference.\n\n```python\nfrom diffusers import DiffusionPipeline\n\nrepo_id = \"runwayml/stable-diffusion-v1-5\"\npipe = DiffusionPipeline.from_pretrained(repo_id, use_safetensors=True)\n```\n\nYou can also load a checkpoint with its specific pipeline class. The example above loaded a Stable Diffusion model; to get the same result, use the [`StableDiffusionPipeline`] class:\n\n```python\nfrom diffusers import StableDiffusionPipeline\n\nrepo_id = \"runwayml/stable-diffusion-v1-5\"\npipe = StableDiffusionPipeline.from_pretrained(repo_id, use_safetensors=True)\n```\n\nA checkpoint (such as [`CompVis/stable-diffusion-v1-4`](https://huggingface.co/CompVis/stable-diffusion-v1-4) or [`runwayml/stable-diffusion-v1-5`](https://huggingface.co/runwayml/stable-diffusion-v1-5)) may also be used for more than one task, like text-to-image or image-to-image. To differentiate what task you want to use the checkpoint for, you have to load it directly with its corresponding task-specific pipeline class:\n\n```python\nfrom diffusers import StableDiffusionImg2ImgPipeline", "You can also instantiate the checkpoint using the model architecture directly:\n\n{#if fw === 'pt'}\n```py\nfrom transformers import CamembertTokenizer, CamembertForMaskedLM\n\ntokenizer = CamembertTokenizer.from_pretrained(\"camembert-base\")\nmodel = CamembertForMaskedLM.from_pretrained(\"camembert-base\")\n```\n\nHowever, we recommend using the [`Auto*` classes](https://huggingface.co/transformers/model_doc/auto.html?highlight=auto#auto-classes) instead, as these are by design architecture-agnostic. While the previous code sample limits users to checkpoints loadable in the CamemBERT architecture, using the `Auto*` classes makes switching checkpoints simple:\n\n```py\nfrom transformers import AutoTokenizer, AutoModelForMaskedLM\n\ntokenizer = AutoTokenizer.from_pretrained(\"camembert-base\")\nmodel = AutoModelForMaskedLM.from_pretrained(\"camembert-base\")\n```\n{:else}\n```py\nfrom transformers import CamembertTokenizer, TFCamembertForMaskedLM\n\ntokenizer = CamembertTokenizer.from_pretrained(\"camembert-base\")\nmodel = TFCamembertForMaskedLM.from_pretrained(\"camembert-base\")\n```\n\nHowever, we recommend using the [`TFAuto*` classes](https://huggingface.co/transformers/model_doc/auto.html?highlight=auto#auto-classes) instead, as these are by design architecture-agnostic. While the previous code sample limits users to checkpoints loadable in the CamemBERT architecture, using the `TFAuto*` classes makes switching checkpoints simple:\n\n```py\nfrom transformers import AutoTokenizer, TFAutoModelForMaskedLM", "And if you want to do inpainting, then [`AutoPipelineForInpainting`] loads the underlying [`StableDiffusionInpaintPipeline`] class in the same way:\n\n```py\nfrom diffusers import AutoPipelineForInpainting\nfrom diffusers.utils import load_image\nimport torch\n\npipeline = AutoPipelineForInpainting.from_pretrained(\n    \"stabilityai/stable-diffusion-xl-base-1.0\", torch_dtype=torch.float16, use_safetensors=True\n).to(\"cuda\")\n\nimg_url = \"https://raw.githubusercontent.com/CompVis/latent-diffusion/main/data/inpainting_examples/overture-creations-5sI6fQgYIuo.png\"\nmask_url = \"https://raw.githubusercontent.com/CompVis/latent-diffusion/main/data/inpainting_examples/overture-creations-5sI6fQgYIuo_mask.png\"\n\ninit_image = load_image(img_url).convert(\"RGB\")\nmask_image = load_image(mask_url).convert(\"RGB\")\n\nprompt = \"A majestic tiger sitting on a bench\"\nimage = pipeline(prompt, image=init_image, mask_image=mask_image, num_inference_steps=50, strength=0.80).images[0]\nimage\n```\n\n<div class=\"flex justify-center\">\n    <img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/diffusers/autopipeline-inpaint.png\" alt=\"generated image of a tiger sitting on a bench\"/>\n</div>\n\nIf you try to load an unsupported checkpoint, it'll throw an error:\n\n```py\nfrom diffusers import AutoPipelineForImage2Image\nimport torch", "These ready-to-use checkpoints can be downloaded and used via the HuggingFace Hub:\n\n```python\n>>> from transformers import AutoModelForCausalLM, AutoTokenizer\n>>> device = \"cuda\" # the device to load the model onto\n\n>>> model = AutoModelForCausalLM.from_pretrained(\"mistralai/Mixtral-8x7B-v0.1\")\n>>> tokenizer = AutoTokenizer.from_pretrained(\"mistralai/Mistral-8x7B\")\n\n>>> prompt = \"My favourite condiment is\"\n\n>>> model_inputs = tokenizer([prompt], return_tensors=\"pt\").to(device)\n>>> model.to(device)\n\n>>> generated_ids = model.generate(**model_inputs, max_new_tokens=100, do_sample=True)\n>>> tokenizer.batch_decode(generated_ids)[0]\n\"The expected output\"\n```\n\nTo use the raw checkpoints with HuggingFace you can use the `convert_mixtral_weights_to_hf.py` script to convert them to the HuggingFace format:\n\n```bash\npython src/transformers/models/mixtral/convert_mixtral_weights_to_hf.py \\\n    --input_dir /path/to/downloaded/mistral/weights --output_dir /output/path\n```\n\nYou can then load the converted model from the `output/path`:\n\n```python\nfrom transformers import MixtralForCausalLM, LlamaTokenizer\n\ntokenizer = LlamaTokenizer.from_pretrained(\"/output/path\")\nmodel = MixtralForCausalLM.from_pretrained(\"/output/path\")\n```\n\n## Combining Mixtral and Flash Attention 2\n\nFirst, make sure to install the latest version of Flash Attention 2 to include the sliding window attention feature."], "eval_score_GPT4": "5", "eval_feedback_GPT4": "Feedback: The response correctly identifies the `from_pretrained()` method as the way to load a checkpoint using `AutoPipeline`. It provides additional accurate information about the `AutoPipeline` class and its functionality, which aligns with the reference answer. The response is factual and directly addresses the instruction given."}, {"question": "What is the purpose of Diffusers library?\n", "true_answer": "To serve as a modular toolbox for both inference and training of state-of-the-art pretrained diffusion models across multiple modalities.", "source_doc": "huggingface/diffusers/blob/main/docs/source/en/conceptual/philosophy.md", "generated_answer": "The purpose of the Diffusers library, as stated in the context, is to provide a modular toolbox for state-of-the-art pretrained diffusion models for generating images, audio, and even 3D structures of molecules. It offers three core components: diffusion pipelines, noise schedulers, and pretrained models, which can be used for both simple inference solutions and training custom diffusion systems. The library is designed with a focus on usability over performance, simple over", "retrieved_docs": ["\ud83e\udd17 Diffusers is the go-to library for state-of-the-art pretrained diffusion models for generating images, audio, and even 3D structures of molecules. Whether you're looking for a simple inference solution or training your own diffusion models, \ud83e\udd17 Diffusers is a modular toolbox that supports both. Our library is designed with a focus on [usability over performance](https://huggingface.co/docs/diffusers/conceptual/philosophy#usability-over-performance), [simple over easy](https://huggingface.co/docs/diffusers/conceptual/philosophy#simple-over-easy), and [customizability over abstractions](https://huggingface.co/docs/diffusers/conceptual/philosophy#tweakable-contributorfriendly-over-abstraction).\n\n\ud83e\udd17 Diffusers offers three core components:\n\n- State-of-the-art [diffusion pipelines](https://huggingface.co/docs/diffusers/api/pipelines/overview) that can be run in inference with just a few lines of code.\n- Interchangeable noise [schedulers](https://huggingface.co/docs/diffusers/api/schedulers/overview) for different diffusion speeds and output quality.\n- Pretrained [models](https://huggingface.co/docs/diffusers/api/models/overview) that can be used as building blocks, and combined with schedulers, for creating your own end-to-end diffusion systems.\n\n## Installation\n\nWe recommend installing \ud83e\udd17 Diffusers in a virtual environment from PyPI or Conda. For more details about installing [PyTorch](https://pytorch.org/get-started/locally/) and [Flax](https://flax.readthedocs.io/en/latest/#installation), please refer to their official documentation.\n\n### PyTorch\n\nWith `pip` (official package):\n\n```bash\npip install --upgrade diffusers[torch]\n```\n\nWith `conda` (maintained by the community):", "Using \ud83e\udde8 `diffusers` at Hugging Face\n\nDiffusers is the go-to library for state-of-the-art pretrained diffusion models for generating images, audio, and even 3D structures of molecules. Whether you\u2019re looking for a simple inference solution or want to train your own diffusion model, Diffusers is a modular toolbox that supports both. The library is designed with a focus on usability over performance, simple over easy, and customizability over abstractions.\n\n## Exploring Diffusers in the Hub\n\nThere are over 10,000 `diffusers` compatible pipelines on the Hub which you can find by filtering at the left of [the models page](https://huggingface.co/models?library=diffusers&sort=downloads). Diffusion systems are typically composed of multiple components such as text encoder, UNet, VAE, and scheduler. Even though they are not standalone models, the pipeline abstraction makes it easy to use them for inference or training.\n\nYou can find diffusion pipelines for many different tasks:\n\n* Generating images from natural language text prompts ([text-to-image](https://huggingface.co/models?library=diffusers&pipeline_tag=text-to-image&sort=downloads)).\n* Transforming images using natural language text prompts ([image-to-image](https://huggingface.co/models?library=diffusers&pipeline_tag=image-to-image&sort=downloads)).\n* Generating videos from natural language descriptions ([text-to-video](https://huggingface.co/models?library=diffusers&pipeline_tag=text-to-video&sort=downloads)).\n\n\nYou can try out the models directly in the browser if you want to test them out without downloading them, thanks to the in-browser widgets! \n\n<div class=\"flex justify-center\">\n<img class=\"hidden dark:block\" src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/refs%2Fpr%2F35/hub/libraries-diffusers_widget.png\"/>\n</div>", "## Usability over Performance\n\n- While Diffusers has many built-in performance-enhancing features (see [Memory and Speed](https://huggingface.co/docs/diffusers/optimization/fp16)), models are always loaded with the highest precision and lowest optimization. Therefore, by default diffusion pipelines are always instantiated on CPU with float32 precision if not otherwise defined by the user. This ensures usability across different platforms and accelerators and means that no complex installations are required to run the library.\n- Diffusers aims to be a **light-weight** package and therefore has very few required dependencies, but many soft dependencies that can improve performance (such as `accelerate`, `safetensors`, `onnx`, etc...). We strive to keep the library as lightweight as possible so that it can be added without much concern as a dependency on other packages.\n- Diffusers prefers simple, self-explainable code over condensed, magic code. This means that short-hand code syntaxes such as lambda functions, and advanced PyTorch operators are often not desired.\n\n## Simple over easy", "## Usability over Performance\n\n- While Diffusers has many built-in performance-enhancing features (see [Memory and Speed](https://huggingface.co/docs/diffusers/optimization/fp16)), models are always loaded with the highest precision and lowest optimization. Therefore, by default diffusion pipelines are always instantiated on CPU with float32 precision if not otherwise defined by the user. This ensures usability across different platforms and accelerators and means that no complex installations are required to run the library.\n- Diffusers aims to be a **light-weight** package and therefore has very few required dependencies, but many soft dependencies that can improve performance (such as `accelerate`, `safetensors`, `onnx`, etc...). We strive to keep the library as lightweight as possible so that it can be added without much concern as a dependency on other packages.\n- Diffusers prefers simple, self-explainable code over condensed, magic code. This means that short-hand code syntaxes such as lambda functions, and advanced PyTorch operators are often not desired.\n\n## Simple over easy", "!---\nCopyright 2023 The HuggingFace Team. All rights reserved.\nLicensed under the Apache License, Version 2.0 (the \"License\");\nyou may not use this file except in compliance with the License.\nYou may obtain a copy of the License at\n\n    http://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software\ndistributed under the License is distributed on an \"AS IS\" BASIS,\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\nSee the License for the specific language governing permissions and\nlimitations under the License.\n-->\n\n# \ud83e\udde8 Diffusers Examples\n\nDiffusers examples are a collection of scripts to demonstrate how to effectively use the `diffusers` library\nfor a variety of use cases involving training or fine-tuning.\n\n**Note**: If you are looking for **official** examples on how to use `diffusers` for inference, please have a look at [src/diffusers/pipelines](https://github.com/huggingface/diffusers/tree/main/src/diffusers/pipelines).\n\nOur examples aspire to be **self-contained**, **easy-to-tweak**, **beginner-friendly** and for **one-purpose-only**.\nMore specifically, this means:", "\ud83e\udde8 Diffusers Experimental\n\nWe are adding experimental code to support novel applications and usages of the Diffusers library.\nCurrently, the following experiments are supported:\n* Reinforcement learning via an implementation of the [Diffuser](https://arxiv.org/abs/2205.09991) model.", "</a>\n    <a class=\"!no-underline border dark:border-gray-700 p-5 rounded-lg shadow hover:shadow-lg\" href=\"./conceptual/philosophy\"\n      ><div class=\"w-full text-center bg-gradient-to-br from-pink-400 to-pink-500 rounded-lg py-1.5 font-semibold mb-5 text-white text-lg leading-relaxed\">Conceptual guides</div>\n      <p class=\"text-gray-700\">Understand why the library was designed the way it was, and learn more about the ethical guidelines and safety implementations for using the library.</p>\n   </a>\n    <a class=\"!no-underline border dark:border-gray-700 p-5 rounded-lg shadow hover:shadow-lg\" href=\"./api/models/overview\"\n      ><div class=\"w-full text-center bg-gradient-to-br from-purple-400 to-purple-500 rounded-lg py-1.5 font-semibold mb-5 text-white text-lg leading-relaxed\">Reference</div>\n      <p class=\"text-gray-700\">Technical descriptions of how \ud83e\udd17 Diffusers classes and methods work.</p>\n    </a>\n  </div>\n</div>"], "eval_score_GPT4": "4", "eval_feedback_GPT4": "The response accurately identifies the Diffusers library as a modular toolbox for pretrained diffusion models and mentions its application in generating images, audio, and 3D structures of molecules. It correctly outlines the core components of the library, such as diffusion pipelines, noise schedulers, and pretrained models, and notes that it can be used for inference and training custom diffusion systems. The response also touches on the library's design focus, which aligns with the reference answer's emphasis on modularity for inference and training. However, the response does not explicitly mention \"state-of-the-art\" as the reference answer does, but this is a minor omission and does not significantly detract from the overall accuracy and factuality of the response."}, {"question": "What method does the EulerAncestralDiscreteScheduler use for sampling?\n", "true_answer": "Ancestral sampling with Euler method steps.", "source_doc": "huggingface/diffusers/blob/main/docs/source/en/api/schedulers/euler_ancestral.md", "generated_answer": "The EulerAncestralDiscreteScheduler uses ancestral sampling with Euler method steps for sampling. This is based on the original implementation of k-diffusion by Katherine Crowson. It is a fast scheduler that can often generate good outputs in 20-30 steps.", "retrieved_docs": ["!--Copyright 2023 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\n\nhttp://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License.\n-->\n\n# EulerAncestralDiscreteScheduler\n\nA scheduler that uses ancestral sampling with Euler method steps. This is a fast scheduler which can often generate good outputs in 20-30 steps. The scheduler is based on the original [k-diffusion](https://github.com/crowsonkb/k-diffusion/blob/481677d114f6ea445aa009cf5bd7a9cdee909e47/k_diffusion/sampling.py#L72) implementation by [Katherine Crowson](https://github.com/crowsonkb/).\n\n## EulerAncestralDiscreteScheduler\n[[autodoc]] EulerAncestralDiscreteScheduler\n\n## EulerAncestralDiscreteSchedulerOutput\n[[autodoc]] schedulers.scheduling_euler_ancestral_discrete.EulerAncestralDiscreteSchedulerOutput", "!--Copyright 2023 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\n\nhttp://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License.\n-->\n\n# EulerDiscreteScheduler\n\nThe Euler scheduler (Algorithm 2) is from the [Elucidating the Design Space of Diffusion-Based Generative Models](https://huggingface.co/papers/2206.00364) paper by Karras et al. This is a fast scheduler which can often generate good outputs in 20-30 steps. The scheduler is based on the original [k-diffusion](https://github.com/crowsonkb/k-diffusion/blob/481677d114f6ea445aa009cf5bd7a9cdee909e47/k_diffusion/sampling.py#L51) implementation by [Katherine Crowson](https://github.com/crowsonkb/).\n\n\n## EulerDiscreteScheduler\n[[autodoc]] EulerDiscreteScheduler\n\n## EulerDiscreteSchedulerOutput\n[[autodoc]] schedulers.scheduling_euler_discrete.EulerDiscreteSchedulerOutput", "generator = torch.Generator(device=\"cuda\").manual_seed(8)\nimage = pipeline(prompt, generator=generator, num_inference_steps=30).images[0]\nimage\n```\n\n<p align=\"center\">\n    <br>\n    <img src=\"https://huggingface.co/datasets/patrickvonplaten/images/resolve/main/diffusers_docs/astronaut_euler_discrete.png\" width=\"400\"/>\n    <br>\n</p>\n\n\nand:\n\n```python\nfrom diffusers import EulerAncestralDiscreteScheduler\n\npipeline.scheduler = EulerAncestralDiscreteScheduler.from_config(pipeline.scheduler.config)\n\ngenerator = torch.Generator(device=\"cuda\").manual_seed(8)\nimage = pipeline(prompt, generator=generator, num_inference_steps=30).images[0]\nimage\n```\n\n<p align=\"center\">\n    <br>\n    <img src=\"https://huggingface.co/datasets/patrickvonplaten/images/resolve/main/diffusers_docs/astronaut_euler_ancestral.png\" width=\"400\"/>\n    <br>\n</p>\n\n\n[`DPMSolverMultistepScheduler`] gives a reasonable speed/quality trade-off and can be run with as little as 20 steps.\n\n```python\nfrom diffusers import DPMSolverMultistepScheduler\n\npipeline.scheduler = DPMSolverMultistepScheduler.from_config(pipeline.scheduler.config)\n\ngenerator = torch.Generator(device=\"cuda\").manual_seed(8)\nimage = pipeline(prompt, generator=generator, num_inference_steps=20).images[0]\nimage\n```", "### Stable Diffusion Reference\n\nThis pipeline uses the Reference Control. Refer to the [sd-webui-controlnet discussion: Reference-only Control](https://github.com/Mikubill/sd-webui-controlnet/discussions/1236)[sd-webui-controlnet discussion: Reference-adain Control](https://github.com/Mikubill/sd-webui-controlnet/discussions/1280).\n\nBased on [this issue](https://github.com/huggingface/diffusers/issues/3566),\n- `EulerAncestralDiscreteScheduler` got poor results.\n\n```py\nimport torch\nfrom diffusers import UniPCMultistepScheduler\nfrom diffusers.utils import load_image\n\ninput_image = load_image(\"https://hf.co/datasets/huggingface/documentation-images/resolve/main/diffusers/input_image_vermeer.png\")\n\npipe = StableDiffusionReferencePipeline.from_pretrained(\n       \"runwayml/stable-diffusion-v1-5\",\n       safety_checker=None,\n       torch_dtype=torch.float16\n       ).to('cuda:0')\n\npipe.scheduler = UniPCMultistepScheduler.from_config(pipe.scheduler.config)\n\nresult_img = pipe(ref_image=input_image,\n      prompt=\"1girl\",\n      num_inference_steps=20,\n      reference_attn=True,\n      reference_adain=True).images[0]\n```\n\nReference Image\n\n![reference_image](https://hf.co/datasets/huggingface/documentation-images/resolve/main/diffusers/input_image_vermeer.png)\n\nOutput Image of `reference_attn=True` and `reference_adain=False`", "sample_size = model.config.sample_size\nnoise = torch.randn((1, 3, sample_size, sample_size), device=\"cuda\")\ninput = noise\n\nfor t in scheduler.timesteps:\n    with torch.no_grad():\n        noisy_residual = model(input, t).sample\n        prev_noisy_sample = scheduler.step(noisy_residual, t, input).prev_sample\n        input = prev_noisy_sample\n\nimage = (input / 2 + 0.5).clamp(0, 1)\nimage = image.cpu().permute(0, 2, 3, 1).numpy()[0]\nimage = Image.fromarray((image * 255).round().astype(\"uint8\"))\nimage\n```\n\nCheck out the [Quickstart](https://huggingface.co/docs/diffusers/quicktour) to launch your diffusion journey today!\n\n## How to navigate the documentation", "|   accuracy |   total_time_in_seconds |   samples_per_second |   latency_in_seconds | task_name   |\n|------------:|---------------------:|--------------------------:|:----------------|:-----------|\n|        0.3 |                4.62804  |              2.16074 |            0.462804  | imdb        |\n|        0   |                0.686388 |             14.569   |            0.0686388 | sst2        |", "image.save(\"./astronaut_heun_k_diffusion.png\")\n```\n\nTo make sure that K Diffusion and `diffusers` yield the same results:\n\n**Diffusers**:\n```python\nfrom diffusers import DiffusionPipeline, EulerDiscreteScheduler\n\nseed = 33\n\npipe = DiffusionPipeline.from_pretrained(\"CompVis/stable-diffusion-v1-4\")\npipe.scheduler = EulerDiscreteScheduler.from_config(pipe.scheduler.config)\npipe = pipe.to(\"cuda\")\n\ngenerator = torch.Generator(device=\"cuda\").manual_seed(seed)\nimage = pipe(prompt, generator=generator, num_inference_steps=50).images[0]\n```\n\n![diffusers_euler](https://huggingface.co/datasets/patrickvonplaten/images/resolve/main/k_diffusion/astronaut_euler.png)\n\n**K Diffusion**:\n```python\nfrom diffusers import DiffusionPipeline, EulerDiscreteScheduler\n\nseed = 33\n\npipe = DiffusionPipeline.from_pretrained(\"CompVis/stable-diffusion-v1-4\", custom_pipeline=\"sd_text2img_k_diffusion\")\npipe.scheduler = EulerDiscreteScheduler.from_config(pipe.scheduler.config)\npipe = pipe.to(\"cuda\")\n\npipe.set_scheduler(\"sample_euler\")\ngenerator = torch.Generator(device=\"cuda\").manual_seed(seed)\nimage = pipe(prompt, generator=generator, num_inference_steps=50).images[0]\n```\n\n![diffusers_euler](https://huggingface.co/datasets/patrickvonplaten/images/resolve/main/k_diffusion/astronaut_euler_k_diffusion.png)"], "eval_score_GPT4": "5", "eval_feedback_GPT4": "The response provided is completely correct, accurate, and factual. It correctly identifies that the EulerAncestralDiscreteScheduler uses ancestral sampling with Euler method steps for sampling, which aligns with the reference answer. Additionally, the response provides extra context about the original implementation and the efficiency of the scheduler, which, while not necessary for the correctness, does not detract from the factual accuracy of the answer."}, {"question": "What is the name of the large multimodal model that can solve image-text tasks and is based on Flamingo?\n", "true_answer": "IDEFICS", "source_doc": "huggingface/transformers/blob/main/docs/source/en/tasks/idefics.md", "generated_answer": "The name of the large multimodal model that can solve image-text tasks and is based on Flamingo is IDEFICS. It was introduced by Hugging Face and is an open-access reproduction of Flamingo, comparable in performance with the original closed-source model across various image-text understanding benchmarks. IDEFICS comes in two variants, an 80 billion parameters model and a 9 billion parameters model, and both are available for use on the", "retrieved_docs": ["The development of state-of-the-art AI models should be more transparent. Our goal with IDEFICS is to reproduce and provide the AI community with systems that match the capabilities of large proprietary models like Flamingo. As such, we took important steps contributing to bringing transparency to these AI systems: we used only publicly available data, we provided tooling to explore training datasets, we shared [technical lessons and mistakes](https://github.com/huggingface/m4-logs/blob/master/memos/README.md) of building such artifacts and assessed the model\u2019s harmfulness by adversarially prompting it before releasing it. We are hopeful that IDEFICS will serve as a solid foundation for more open research in multimodal AI systems, alongside models like [OpenFlamingo](https://huggingface.co/openflamingo)-another open reproduction of Flamingo at the 9 billion parameter scale.\n\nTry out the [demo](https://huggingface.co/spaces/HuggingFaceM4/idefics_playground) and the [models](https://huggingface.co/HuggingFaceM4/idefics-80b-instruct) on the Hub!\n\n<p align=\"center\">\n    <img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/idefics/woodstock_ai.png\" width=\"600\" alt=\"Screenshot of IDEFICS generation for HF Woodstock of AI\"/>\n</p>\n\n## What is IDEFICS?\n\nIDEFICS is an 80 billion parameters multimodal model that accepts sequences of images and texts as input and generates coherent text as output. It can answer questions about images, describe visual content, create stories grounded in multiple images, etc.\n\nIDEFICS is an open-access reproduction of Flamingo and is comparable in performance with the original closed-source model across various image-text understanding benchmarks. It comes in two variants - 80 billion parameters and 9 billion parameters.", "IDEFICS is an open-access reproduction of Flamingo and is comparable in performance with the original closed-source model across various image-text understanding benchmarks. It comes in two variants - 80 billion parameters and 9 billion parameters.\n\n<p align=\"center\">\n    <img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/idefics/Figure_Evals_IDEFICS.png\" width=\"600\" alt=\"Plot comparing the performance of Flamingo, OpenFlamingo and IDEFICS\"/>\n</p>\n\nWe also provide fine-tuned versions [idefics-80B-instruct](https://huggingface.co/HuggingFaceM4/idefics-80b-instruct) and [idefics-9B-instruct](https://huggingface.co/HuggingFaceM4/idefics-9b-instruct) adapted for conversational use cases.\n\n## Training Data\n\nIDEFICS was trained on a mixture of openly available datasets: Wikipedia, Public Multimodal Dataset, and LAION, as well as a new 115B token dataset called [OBELICS](https://huggingface.co/datasets/HuggingFaceM4/OBELICS) that we created. OBELICS consists of 141 million interleaved image-text documents scraped from the web and contains 353 million images.\n\nWe provide an [interactive visualization](https://atlas.nomic.ai/map/f2fba2aa-3647-4f49-a0f3-9347daeee499/ee4a84bd-f125-4bcc-a683-1b4e231cb10f) of OBELICS that allows exploring the content of the dataset with [Nomic AI](https://home.nomic.ai/).", "--\ntitle: \"New ViT and ALIGN Models From Kakao Brain\" \nthumbnail: /blog//assets/132_vit_align/thumbnail.png\nauthors:\n- user: adirik\n- user: Unso\n- user: dylan-m\n- user: jun-untitled\n---\n\n\n# Kakao Brain\u2019s Open Source ViT, ALIGN, and the New COYO Text-Image Dataset\n\n\nKakao Brain and Hugging Face are excited to release a new open-source image-text dataset [COYO](https://github.com/kakaobrain/coyo-dataset) of 700 million pairs and two new visual language models trained on it, [ViT](https://github.com/kakaobrain/coyo-vit) and [ALIGN](https://github.com/kakaobrain/coyo-align). This is the first time ever the ALIGN model is made public for free and open-source use and the first release of ViT and ALIGN models that come with the train dataset.  \n\nKakao Brain\u2019s ViT and ALIGN models follow the same architecture and hyperparameters as provided in the original respective Google models but are trained on the open source [COYO](https://github.com/kakaobrain/coyo-dataset) dataset. Google\u2019s [ViT](https://ai.googleblog.com/2020/12/transformers-for-image-recognition-at.html) and [ALIGN](https://ai.googleblog.com/2021/05/align-scaling-up-visual-and-vision.html) models, while trained on huge datasets (ViT trained on 300 million images and ALIGN trained on 1.8 billion image-text pairs respectively), cannot be replicated because the datasets are not public. This contribution is particularly valuable to researchers who want to reproduce visual language modeling with access to the data as well. More detailed information on the Kakao ViT and ALIGN models can be found [here](https://huggingface.co/kakaobrain).", "The abstract from the paper is the following:\n\n*Large multimodal models trained on natural documents, which interleave images and text, outperform models trained on image-text pairs on various multimodal benchmarks that require reasoning over one or multiple images to generate a text. However, the datasets used to train these models have not been released, and the collection process has not been fully specified. We introduce the OBELICS dataset, an open web-scale filtered dataset of interleaved image-text documents comprising 141 million web pages extracted from Common Crawl, 353 million associated images, and 115 billion text tokens. We describe the dataset creation process, present comprehensive filtering rules, and provide an analysis of the dataset's content. To show the viability of OBELISC, we train an 80 billion parameters vision and language model on the dataset and obtain competitive performance on various multimodal benchmarks. We release the code to reproduce the dataset along with the dataset itself.*\n\nThis model was contributed by [HuggingFaceM4](https://huggingface.co/HuggingFaceM4). The original code can be found [here](<INSERT LINK TO GITHUB REPO HERE>). (TODO: don't have a public link yet).\n\n\n<Tip warning={true}>\n\nIDEFICS modeling code in Transformers is for finetuning and inferencing the pre-trained IDEFICS models.\n\nTo train a new IDEFICS model from scratch use the m4 codebase (a link will be provided once it's made public)\n\n</Tip>\n\n\n## IdeficsConfig\n\n[[autodoc]] IdeficsConfig\n\n## IdeficsModel\n\n[[autodoc]] IdeficsModel\n    - forward\n\n## IdeficsForVisionText2Text\n\n[[autodoc]] IdeficsForVisionText2Text\n    - forward\n\n## IdeficsImageProcessor\n\n[[autodoc]] IdeficsImageProcessor\n    - preprocess\n\n## IdeficsProcessor", "</blockquote>\n    <div class=\"flex items-center gap-4\">\n      <img src=\"https://avatars.githubusercontent.com/u/25863658?s=48&v=4\" class=\"h-12 w-12 rounded-full object-cover\" />\n      <div class=\"text-sm\">\n        <p class=\"font-medium\">kiui</p>\n      </div>\n    </div>\n  </div>\n  <div class=\"mb-8 sm:break-inside-avoid\">\n    <blockquote class=\"rounded-xl !mb-0 bg-gray-50 p-6 shadow dark:bg-gray-800\">\n      <p class=\"leading-relaxed text-gray-700\">MMagic (Multimodal Advanced, Generative, and Intelligent Creation) is an advanced and comprehensive Generative AI toolbox that provides state-of-the-art AI models (e.g., diffusion models powered by \ud83e\udd17 Diffusers and GAN) to synthesize, edit and enhance images and videos. In MMagic, users can use rich components to customize their own models like playing with Legos and manage the training loop easily.</p>\n    </blockquote>\n    <div class=\"flex items-center gap-4\">\n      <img src=\"https://avatars.githubusercontent.com/u/10245193?s=48&v=4\" class=\"h-12 w-12 rounded-full object-cover\" />\n      <div class=\"text-sm\">\n        <p class=\"font-medium\">mmagic</p>\n      </div>\n    </div>\n  </div>\n  <div class=\"mb-8 sm:break-inside-avoid\">", "MagicMix                                                                                                                              | Diffusion Pipeline for semantic mixing of an image and a text prompt                                                                                                                                                                                                                                                                                                                                                                                                                                     | [MagicMix](#magic-mix)                                                                    | - |                    [Partho Das](https://github.com/daspartho) |\n| Stable UnCLIP                                                                                                                         | Diffusion Pipeline for combining prior model (generate clip image embedding from text, UnCLIPPipeline `\"kakaobrain/karlo-v1-alpha\"`) and decoder pipeline (decode clip image embedding to image, StableDiffusionImageVariationPipeline `\"lambdalabs/sd-image-variations-diffusers\"` ).                                                                                                                                                                                                                   | [Stable UnCLIP](#stable-unclip)                                                           | -  |                                [Ray Wang](https://wrong.wang) |\n| UnCLIP Text Interpolation Pipeline                                                                                                    | Diffusion Pipeline that allows passing two prompts and produces images while interpolating between the text-embeddings of the two prompts                                                                                                                                                                                                                                                                                                                                                                | [UnCLIP Text Interpolation Pipeline](#unclip-text-interpolation-pipeline)                 | -                                                                                                                                                                                                                  | [Naga Sai Abhinay Devarinti](https://github.com/Abhinay1997/) |\n| UnCLIP Image Interpolation Pipeline                                                                                                   | Diffusion Pipeline that allows passing two images/image_embeddings and produces images while interpolating between their image-embeddings                                                                                                                                                                                                                                                                                                                                                                | [UnCLIP Image Interpolation Pipeline](#unclip-image-interpolation-pipeline)               | -                                                                                                                                                                                                                  | [Naga Sai Abhinay Devarinti](https://github.com/Abhinay1997/) |\n| DDIM Noise Comparative Analysis Pipeline                                                                                              | Investigating how the diffusion models learn visual concepts from each noise level (which is a contribution of [P2 weighting (CVPR 2022)](https://arxiv.org/abs/2204.00227))                                                                                                                                                                                                                                                                                                                             | [DDIM Noise Comparative Analysis Pipeline](#ddim-noise-comparative-analysis-pipeline)     | - |              [Aengus (Duc-Anh)](https://github.com/aengusng8) |", "!--Copyright 2023 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\n\nhttp://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License.\n\n\u26a0\ufe0f Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\nrendered properly in your Markdown viewer.\n\n-->\n\n# Image tasks with IDEFICS\n\n[[open-in-colab]]\n\nWhile individual tasks can be tackled by fine-tuning specialized models, an alternative approach \nthat has recently emerged and gained popularity is to use large models for a diverse set of tasks without fine-tuning. \nFor instance, large language models can handle such NLP tasks as summarization, translation, classification, and more. \nThis approach is no longer limited to a single modality, such as text, and in this guide, we will illustrate how you can \nsolve image-text tasks with a large multimodal model called IDEFICS."], "eval_score_GPT4": "1", "eval_feedback_GPT4": "The response provided is incorrect. The name IDEFICS is not associated with the large multimodal model based on Flamingo that can solve image-text tasks. The correct name of the model was not mentioned, and the additional information provided about IDEFICS being introduced by Hugging Face and its variants is not relevant to the question asked. The response does not match the reference answer, which is simply \"IDEFICS.\" Therefore, based on the score rubric, the response does not meet the criteria for a correct, accurate, and factual answer."}, {"question": "What is the purpose of the `gradio.Blocks` API?\n", "true_answer": "The `gradio.Blocks` API allows you to have full control over the data flows and layout of your application, enabling the building of complex, multi-step applications.", "source_doc": "huggingface/course/blob/main/chapters/en/chapter9/7.mdx", "generated_answer": "The purpose of the `gradio.Blocks` API is to provide a low-level, flexible API for building complex, multi-step machine learning applications with full control over data flows and layout. It allows users to have more customization options than the high-level `Interface` API, such as grouping related demos as multiple tabs, changing the layout of demos, having multi-step interfaces, and changing component properties or visibility based on user input. The `Blocks`", "retrieved_docs": ["### Contributors Shoutout:\n\n- [@nhankiet](https://github.com/nhankiet) made their first contribution in [PR 1819](https://github.com/gradio-app/gradio/pull/1819)\n\n## 3.0\n\n###### \ud83d\udd25 Gradio 3.0 is the biggest update to the library, ever.\n\n### New Features:\n\n###### 1. Blocks \ud83e\uddf1\n\nBlocks is a new, low-level API that allows you to have full control over the data flows and layout of your application. It allows you to build very complex, multi-step applications. For example, you might want to:\n\n- Group together related demos as multiple tabs in one web app\n- Change the layout of your demo instead of just having all of the inputs on the left and outputs on the right\n- Have multi-step interfaces, in which the output of one model becomes the input to the next model, or have more flexible data flows in general\n- Change a component's properties (for example, the choices in a Dropdown) or its visibility based on user input\n\nHere's a simple example that creates the demo below it:\n\n```python\nimport gradio as gr\n\ndef update(name):\n    return f\"Welcome to Gradio, {name}!\"\n\ndemo = gr.Blocks()\n\nwith demo:\n    gr.Markdown(\n    \"\"\"\n    # Hello World!\n    Start typing below to see the output.\n    \"\"\")\n    inp = gr.Textbox(placeholder=\"What is your name?\")\n    out = gr.Textbox()\n\n    inp.change(fn=update,\n               inputs=inp,\n               outputs=out)\n\ndemo.launch()\n```\n\n![hello-blocks](https://user-images.githubusercontent.com/9021060/168684108-78cbd24b-e6bd-4a04-a8d9-20d535203434.gif)", "### Contributors Shoutout:\n\n- [@nhankiet](https://github.com/nhankiet) made their first contribution in [PR 1819](https://github.com/gradio-app/gradio/pull/1819)\n\n## 3.0\n\n###### \ud83d\udd25 Gradio 3.0 is the biggest update to the library, ever.\n\n### New Features:\n\n###### 1. Blocks \ud83e\uddf1\n\nBlocks is a new, low-level API that allows you to have full control over the data flows and layout of your application. It allows you to build very complex, multi-step applications. For example, you might want to:\n\n- Group together related demos as multiple tabs in one web app\n- Change the layout of your demo instead of just having all of the inputs on the left and outputs on the right\n- Have multi-step interfaces, in which the output of one model becomes the input to the next model, or have more flexible data flows in general\n- Change a component's properties (for example, the choices in a Dropdown) or its visibility based on user input\n\nHere's a simple example that creates the demo below it:\n\n```python\nimport gradio as gr\n\ndef update(name):\n    return f\"Welcome to Gradio, {name}!\"\n\ndemo = gr.Blocks()\n\nwith demo:\n    gr.Markdown(\n    \"\"\"\n    # Hello World!\n    Start typing below to see the output.\n    \"\"\")\n    inp = gr.Textbox(placeholder=\"What is your name?\")\n    out = gr.Textbox()\n\n    inp.change(fn=update,\n               inputs=inp,\n               outputs=out)\n\ndemo.launch()\n```\n\n![hello-blocks](https://user-images.githubusercontent.com/9021060/168684108-78cbd24b-e6bd-4a04-a8d9-20d535203434.gif)", "Introduction to Gradio Blocks[[introduction-to-gradio-blocks]]\n\n<CourseFloatingBanner chapter={9}\n  classNames=\"absolute z-10 right-0 top-0\"\n  notebooks={[\n    {label: \"Google Colab\", value: \"https://colab.research.google.com/github/huggingface/notebooks/blob/master/course/en/chapter9/section7.ipynb\"},\n    {label: \"Aws Studio\", value: \"https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/master/course/en/chapter9/section7.ipynb\"},\n]} />\n\nIn the previous sections we have explored and created demos using the `Interface` class. In this section we will introduce our **newly developed** low-level API called `gradio.Blocks`.\n\nNow, what's the difference between `Interface` and `Blocks`?\n\n- \u26a1 `Interface`: a high-level API that allows you to create a full machine learning demo simply by providing a list of inputs and outputs.\n\n- \ud83e\uddf1 `Blocks`: a low-level API that allows you to have full control over the data flows and layout of your application. You can build very complex, multi-step applications using `Blocks` (as in \"building blocks\").\n\n\n### Why Blocks \ud83e\uddf1?[[why-blocks-]]\n\nAs we saw in the previous sections, the `Interface` class allows you to easily create full-fledged machine learning demos with just a few lines of code. The `Interface` API is extremely easy to use but lacks the flexibility that the `Blocks` API provides. For example, you might want to:", "Gradio Demo: blocks_js_methods\n\n\n```\n!pip install -q gradio \n```\n\n\n```\nimport gradio as gr\n\nblocks = gr.Blocks()\n\nwith blocks as demo:\n    subject = gr.Textbox(placeholder=\"subject\")\n    verb = gr.Radio([\"ate\", \"loved\", \"hated\"])\n    object = gr.Textbox(placeholder=\"object\")\n\n    with gr.Row():\n        btn = gr.Button(\"Create sentence.\")\n        reverse_btn = gr.Button(\"Reverse sentence.\")\n        foo_bar_btn = gr.Button(\"Append foo\")\n        reverse_then_to_the_server_btn = gr.Button(\n            \"Reverse sentence and send to server.\"\n        )\n\n    def sentence_maker(w1, w2, w3):\n        return f\"{w1} {w2} {w3}\"\n\n    output1 = gr.Textbox(label=\"output 1\")\n    output2 = gr.Textbox(label=\"verb\")\n    output3 = gr.Textbox(label=\"verb reversed\")\n    output4 = gr.Textbox(label=\"front end process and then send to backend\")\n\n    btn.click(sentence_maker, [subject, verb, object], output1)\n    reverse_btn.click(\n        None, [subject, verb, object], output2, js=\"(s, v, o) => o + ' ' + v + ' ' + s\"\n    )\n    verb.change(lambda x: x, verb, output3, js=\"(x) => [...x].reverse().join('')\")\n    foo_bar_btn.click(None, [], subject, js=\"(x) => x + ' foo'\")", "You can build very custom and complex applications using `gr.Blocks()`. For example, the popular image generation [Automatic1111 Web UI](https://github.com/AUTOMATIC1111/stable-diffusion-webui) is built using Gradio Blocks. We dive deeper into the `gr.Blocks` on our series on [building with Blocks](https://www.gradio.app/guides/blocks-and-event-listeners).\n\n\n#### The Gradio Python & JavaScript Ecosystem\n\nThat's the gist of the core `gradio` Python library, but Gradio is actually so much more! Its an entire ecosystem of Python and JavaScript libraries that let you build machine learning applications, or query them programmatically, in Python or JavaScript. Here are other related parts of the Gradio ecosystem:\n\n* [Gradio Python Client](https://www.gradio.app/guides/getting-started-with-the-python-client) (`gradio_client`): query any Gradio app programmatically in Python.\n* [Gradio JavaScript Client](https://www.gradio.app/guides/getting-started-with-the-js-client) (`@gradio/client`): query any Gradio app programmatically in JavaScript.\n* [Gradio-Lite](https://www.gradio.app/guides/gradio-lite) (`@gradio/lite`): write Gradio apps in Python that run entirely in the browser (no server needed!), thanks to Pyodide. \n* [Hugging Face Spaces](https://huggingface.co/spaces): the most popular place to host Gradio applications \u2014 for free!\n\n### What's Next?\n\nKeep learning about Gradio sequentially using the Gradio Guides, which include explanations as well as example code and embedded interactive demos. Next up: [key features about Gradio demos](https://www.gradio.app/guides/key-features).", "This simple example above introduces 4 concepts that underlie Blocks:\n\n1. Blocks allow you to build web applications that combine markdown, HTML, buttons, and interactive components simply by instantiating objects in Python inside of a `with gradio.Blocks` context.\n<Tip>\n\ud83d\ude4bIf you're not familiar with the `with` statement in Python, we recommend checking out the excellent [tutorial](https://realpython.com/python-with-statement/) from Real Python. Come back here after reading that \ud83e\udd17\n</Tip>\nThe order in which you instantiate components matters as each element gets rendered into the web app in the order it was created. (More complex layouts are discussed below)\n\n2. You can define regular Python functions anywhere in your code and run them with user input using `Blocks`. In our example, we have a simple function that \"flips\" the input text, but you can write any Python function, from a simple calculation to processing the predictions from a machine learning model.\n\n3. You can assign events to any `Blocks` component. This will run your function when the component is clicked, changed, etc. When you assign an event, you pass in three parameters: `fn`: the function that should be called, `inputs`: the (list) of input component(s), and `outputs`: the (list) of output components that should be called.\n\n   In the example above, we run the `flip_text()` function when the value in the `Textbox` named input `input` changes. The event reads the value in `input`, passes it as the name parameter to `flip_text()`, which then returns a value that gets assigned to our second `Textbox` named `output`.\n\n   To see a list of events that each component supports, see the Gradio [documentation](https://www.gradio.app/docs/).", "You can build very custom and complex applications using `gr.Blocks()`. For example, the popular image generation [Automatic1111 Web UI](https://github.com/AUTOMATIC1111/stable-diffusion-webui) is built using Gradio Blocks. We dive deeper into the `gr.Blocks` on our series on [building with Blocks](https://www.gradio.app/guides/blocks-and-event-listeners).\n\n\n### The Gradio Python & JavaScript Ecosystem\n\nThat's the gist of the core `gradio` Python library, but Gradio is actually so much more! Its an entire ecosystem of Python and JavaScript libraries that let you build machine learning applications, or query them programmatically, in Python or JavaScript. Here are other related parts of the Gradio ecosystem:\n\n* [Gradio Python Client](https://www.gradio.app/guides/getting-started-with-the-python-client) (`gradio_client`): query any Gradio app programmatically in Python.\n* [Gradio JavaScript Client](https://www.gradio.app/guides/getting-started-with-the-js-client) (`@gradio/client`): query any Gradio app programmatically in JavaScript.\n* [Gradio-Lite](https://www.gradio.app/guides/gradio-lite) (`@gradio/lite`): write Gradio apps in Python that run entirely in the browser (no server needed!), thanks to Pyodide. \n* [Hugging Face Spaces](https://huggingface.co/spaces): the most popular place to host Gradio applications \u2014 for free!\n\n## What's Next?\n\nKeep learning about Gradio sequentially using the Gradio Guides, which include explanations as well as example code and embedded interactive demos. Next up: [key features about Gradio demos](https://www.gradio.app/guides/key-features)."], "eval_score_GPT4": "4", "eval_feedback_GPT4": "The response provided is mostly correct and aligns well with the reference answer. It accurately describes the purpose of the `gradio.Blocks` API, mentioning the control it offers over data flows and layout, which is a key point in the reference answer. Additionally, the response expands on the reference answer by providing examples of the customization options available through the `Blocks` API, such as grouping demos into tabs, changing layouts, and having multi-step interfaces. This additional information is accurate and factual, enhancing the response's completeness. However, the response seems to be cut off at the end, as the last sentence is incomplete. Despite this, the information given is still mostly correct and factual."}, {"question": "What is the purpose of the two-stage model proposed in the paper \"Hierarchical Text-Conditional Image Generation with CLIP Latents\"?\n", "true_answer": "The purpose of the two-stage model is to generate a CLIP image embedding given a text caption and then generate an image conditioned on the image embedding.", "source_doc": "huggingface/diffusers/blob/main/docs/source/en/api/pipelines/unclip.md", "generated_answer": "The purpose of the two-stage model proposed in the paper \"Hierarchical Text-Conditional Image Generation with CLIP Latents\" is to leverage the robust representations of images learned by contrastive models like CLIP for image generation. The first stage, called the prior, generates a CLIP image embedding from a text caption using a denoising diffusion process. The second stage, called the decoder, generates an image conditioned on the image embedding. This two-stage approach", "retrieved_docs": ["!--Copyright 2023 The HuggingFace Team. All rights reserved.\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\nhttp://www.apache.org/licenses/LICENSE-2.0\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License.\n-->\n\n# unCLIP\n\n[Hierarchical Text-Conditional Image Generation with CLIP Latents](https://huggingface.co/papers/2204.06125) is by Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu, Mark Chen. The unCLIP model in \ud83e\udd17 Diffusers comes from kakaobrain's [karlo](https://github.com/kakaobrain/karlo).\n\nThe abstract from the paper is following:\n\n*Contrastive models like CLIP have been shown to learn robust representations of images that capture both semantics and style. To leverage these representations for image generation, we propose a two-stage model: a prior that generates a CLIP image embedding given a text caption, and a decoder that generates an image conditioned on the image embedding. We show that explicitly generating image representations improves image diversity with minimal loss in photorealism and caption similarity. Our decoders conditioned on image representations can also produce variations of an image that preserve both its semantics and style, while varying the non-essential details absent from the image representation. Moreover, the joint embedding space of CLIP enables language-guided image manipulations in a zero-shot fashion. We use diffusion models for the decoder and experiment with both autoregressive and diffusion models for the prior, finding that the latter are computationally more efficient and produce higher-quality samples.*\n\nYou can find lucidrains' DALL-E 2 recreation at [lucidrains/DALLE2-pytorch](https://github.com/lucidrains/DALLE2-pytorch).", "!--Copyright 2023 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\n\nhttp://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License.\n-->\n\n# Prior Transformer\n\nThe Prior Transformer was originally introduced in [Hierarchical Text-Conditional Image Generation with CLIP Latents](https://huggingface.co/papers/2204.06125) by Ramesh et al. It is used to predict CLIP image embeddings from CLIP text embeddings; image embeddings are predicted through a denoising diffusion process.\n\nThe abstract from the paper is:\n\n*Contrastive models like CLIP have been shown to learn robust representations of images that capture both semantics and style. To leverage these representations for image generation, we propose a two-stage model: a prior that generates a CLIP image embedding given a text caption, and a decoder that generates an image conditioned on the image embedding. We show that explicitly generating image representations improves image diversity with minimal loss in photorealism and caption similarity. Our decoders conditioned on image representations can also produce variations of an image that preserve both its semantics and style, while varying the non-essential details absent from the image representation. Moreover, the joint embedding space of CLIP enables language-guided image manipulations in a zero-shot fashion. We use diffusion models for the decoder and experiment with both autoregressive and diffusion models for the prior, finding that the latter are computationally more efficient and produce higher-quality samples.*\n\n## PriorTransformer\n\n[[autodoc]] PriorTransformer\n\n## PriorTransformerOutput\n\n[[autodoc]] models.prior_transformer.PriorTransformerOutput", "!--Copyright 2023 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\n\nhttp://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License.\n-->\n\n# Kandinsky\n\n[[open-in-colab]]\n\nThe Kandinsky models are a series of multilingual text-to-image generation models. The Kandinsky 2.0 model uses two multilingual text encoders and concatenates those results for the UNet.\n\n[Kandinsky 2.1](../api/pipelines/kandinsky) changes the architecture to include an image prior model ([`CLIP`](https://huggingface.co/docs/transformers/model_doc/clip)) to generate a mapping between text and image embeddings. The mapping provides better text-image alignment and it is used with the text embeddings during training, leading to higher quality results. Finally, Kandinsky 2.1 uses a [Modulating Quantized Vectors (MoVQ)](https://huggingface.co/papers/2209.09002) decoder - which adds a spatial conditional normalization layer to increase photorealism - to decode the latents into images.\n\n[Kandinsky 2.2](../api/pipelines/kandinsky_v22) improves on the previous model by replacing the image encoder of the image prior model with a larger CLIP-ViT-G model to improve quality. The image prior model was also retrained on images with different resolutions and aspect ratios to generate higher-resolution images and different image sizes.", "## Usage tips\n\n- BLIP-2 can be used for conditional text generation given an image and an optional text prompt. At inference time, it's recommended to use the [`generate`] method.\n- One can use [`Blip2Processor`] to prepare images for the model, and decode the predicted tokens ID's back to text.\n\n## Resources\n\nA list of official Hugging Face and community (indicated by \ud83c\udf0e) resources to help you get started with BLIP-2.\n\n- Demo notebooks for BLIP-2 for image captioning, visual question answering (VQA) and chat-like conversations can be found [here](https://github.com/NielsRogge/Transformers-Tutorials/tree/master/BLIP-2).\n\nIf you're interested in submitting a resource to be included here, please feel free to open a Pull Request and we'll review it! The resource should ideally demonstrate something new instead of duplicating an existing resource.\n\n## Blip2Config\n\n[[autodoc]] Blip2Config\n    - from_vision_qformer_text_configs\n\n## Blip2VisionConfig\n\n[[autodoc]] Blip2VisionConfig\n\n## Blip2QFormerConfig\n\n[[autodoc]] Blip2QFormerConfig\n\n## Blip2Processor\n\n[[autodoc]] Blip2Processor\n\n## Blip2VisionModel\n\n[[autodoc]] Blip2VisionModel\n    - forward\n\n## Blip2QFormerModel\n\n[[autodoc]] Blip2QFormerModel\n    - forward\n\n## Blip2Model\n\n[[autodoc]] Blip2Model\n    - forward\n    - get_text_features\n    - get_image_features\n    - get_qformer_features\n\n## Blip2ForConditionalGeneration", "## CLIPSegTextModel\n\n[[autodoc]] CLIPSegTextModel\n    - forward\n\n## CLIPSegVisionModel\n\n[[autodoc]] CLIPSegVisionModel\n    - forward\n\n## CLIPSegForImageSegmentation\n\n[[autodoc]] CLIPSegForImageSegmentation\n    - forward", "## Introduction\n\nIF is a pixel-based text-to-image generation model and was [released in\nlate April 2023 by DeepFloyd](https://github.com/deep-floyd/IF). The\nmodel architecture is strongly inspired by [Google's closed-sourced\nImagen](https://imagen.research.google/).\n\nIF has two distinct advantages compared to existing text-to-image models\nlike Stable Diffusion:\n\n- The model operates directly in \"pixel space\" (*i.e.,* on\nuncompressed images) instead of running the denoising process in the\nlatent space such as [Stable Diffusion](http://hf.co/blog/stable_diffusion).\n- The model is trained on outputs of\n[T5-XXL](https://huggingface.co/google/t5-v1_1-xxl), a more powerful\ntext encoder than [CLIP](https://openai.com/research/clip), used by\nStable Diffusion as the text encoder.\n\nAs a result, IF is better at generating images with high-frequency\ndetails (*e.g.,* human faces and hands) and is the first open-source\nimage generation model that can reliably generate images with text.\n\nThe downside of operating in pixel space and using a more powerful text\nencoder is that IF has a significantly higher amount of parameters. T5,\nIF\\'s text-to-image UNet, and IF\\'s upscaler UNet have 4.5B, 4.3B, and\n1.2B parameters respectively. Compared to [Stable Diffusion\n2.1](https://huggingface.co/stabilityai/stable-diffusion-2-1)\\'s text\nencoder and UNet having just 400M and 900M parameters, respectively.\n\nNevertheless, it is possible to run IF on consumer hardware if one\noptimizes the model for low-memory usage. We will show you can do this\nwith \ud83e\udde8 diffusers in this blog post.", "Therefore, in the 2nd stage, we train to output more Japanese-style images.\n\n#### 2nd stage: Fine-tune the text encoder and the latent diffusion model jointly\nIn the 2nd stage, we will train both the text encoder and the latent diffusion model to generate Japanese-style images. This stage is essential to make the model become a more language-specific model. After this, the model can finally generate a businessman with a Japanese face, as shown in the image below.\n\n<p align=\"center\">\n<img src=\"assets/106_japanese_stable_diffusion/jsd-stage2.jpeg\" alt=\"salary man of japanese stable diffusion\" title=\"salary man of japanese stable diffusion\">\n</p>\n\n*\"\u30b5\u30e9\u30ea\u30fc\u30de\u30f3 \u6cb9\u7d75\", which means exactly \"salary man, oil painting\", from the 2nd-stage Japanese Stable Diffusion*\n\n## rinna\u2019s Open Strategy\nNumerous research institutes are releasing their research results based on the idea of democratization of AI, aiming for a world where anyone can easily use AI. In particular, recently, pre-trained models with a large number of parameters based on large-scale training data have become the mainstream, and there are concerns about a monopoly of high-performance AI by research institutes with computational resources. Still, fortunately, many pre-trained models have been released and are contributing to the development of AI technology. However, pre-trained models on text often target English, the world's most popular language. For a world in which anyone can easily use AI, we believe that it is desirable to be able to use state-of-the-art AI in languages other than English."], "eval_score_GPT4": "4", "eval_feedback_GPT4": "Feedback: The response provided is mostly correct, accurate, and factual. It correctly identifies the two-stage model's purpose as leveraging CLIP's robust representations for image generation and accurately describes the functions of both the prior and decoder stages. However, the response is incomplete as it ends abruptly with \"This two-stage approach\" without finishing the sentence or providing additional information that might have been intended. Despite this, the information given aligns well with the reference answer, which states the model's purpose is to generate a CLIP image embedding from a text caption and then generate an image conditioned on that embedding."}, {"question": "What command is used to install the requirements for a research project using \ud83e\udd17 Transformers?\n", "true_answer": "pip install -r requirements.txt", "source_doc": "huggingface/transformers/blob/main/examples/research_projects/README.md", "generated_answer": "The command to install the requirements for a research project using \ud83e\udd17 Transformers is:\n\n```\npip install -r requirements.txt\n```\n\nThis command should be run inside the folder of the chosen research project. It will install the necessary dependencies listed in the `requirements.txt` file. This is required before running any code or experiments using the project.", "retrieved_docs": ["!---\nCopyright 2020 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\");\nyou may not use this file except in compliance with the License.\nYou may obtain a copy of the License at\n\n    http://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software\ndistributed under the License is distributed on an \"AS IS\" BASIS,\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\nSee the License for the specific language governing permissions and\nlimitations under the License.\n-->\n\n# Research projects\n\nThis folder contains various research projects using \ud83e\udd17 Transformers. They are not maintained and require a specific\nversion of \ud83e\udd17 Transformers that is indicated in the requirements file of each folder. Updating them to the most recent version of the library will require some work.\n\nTo use any of them, just run the command\n```\npip install -r requirements.txt\n```\ninside the folder of your choice.\n\nIf you need help with any of those, contact the author(s), indicated at the top of the `README` of each folder.", "Research projects\n\nThis folder contains various research projects using \ud83e\udde8 Diffusers.\nThey are not really maintained by the core maintainers of this library and often require a specific version of Diffusers that is indicated in the requirements file of each folder.\nUpdating them to the most recent version of the library will require some work.\n\nTo use any of them, just run the command\n\n```\npip install -r requirements.txt\n```\ninside the folder of your choice.\n\nIf you need help with any of those, please open an issue where you directly ping the author(s), as indicated at the top of the README of each folder.", "## Install from source\n\nInstall \ud83e\udd17 Transformers from source with the following command:\n\n```bash\npip install git+https://github.com/huggingface/transformers\n```\n\nThis command installs the bleeding edge `main` version rather than the latest `stable` version. The `main` version is useful for staying up-to-date with the latest developments. For instance, if a bug has been fixed since the last official release but a new release hasn't been rolled out yet. However, this means the `main` version may not always be stable. We strive to keep the `main` version operational, and most issues are usually resolved within a few hours or a day. If you run into a problem, please open an [Issue](https://github.com/huggingface/transformers/issues) so we can fix it even sooner!\n\nCheck if \ud83e\udd17 Transformers has been properly installed by running the following command:\n\n```bash\npython -c \"from transformers import pipeline; print(pipeline('sentiment-analysis')('I love you'))\"\n```\n\n## Editable install\n\nYou will need an editable install if you'd like to:\n\n* Use the `main` version of the source code.\n* Contribute to \ud83e\udd17 Transformers and need to test changes in the code.\n\nClone the repository and install \ud83e\udd17 Transformers with the following commands:\n\n```bash\ngit clone https://github.com/huggingface/transformers.git\ncd transformers\npip install -e .\n```\n\nThese commands will link the folder you cloned the repository to and your Python library paths. Python will now look inside the folder you cloned to in addition to the normal library paths. For example, if your Python packages are typically installed in `~/anaconda3/envs/main/lib/python3.7/site-packages/`, Python will also search the folder you cloned to: `~/transformers/`.\n\n<Tip warning={true}>\n\nYou must keep the `transformers` folder if you want to keep using the library.\n\n</Tip>", "# install transformers\ngit clone https://github.com/huggingface/transformers/\npip install -e .[dev]\n\n```\n\n## Files\n\nAs a quick overview, the following files needed to be created and written:", "2.  Clone your `transformers` fork to your local disk, and add the base\n    repository as a remote:\n\n    ```bash\n    git clone https://github.com/[your Github handle]/transformers.git\n    cd transformers\n    git remote add upstream https://github.com/huggingface/transformers.git\n    ```\n\n3.  Set up a development environment, for instance by running the\n    following command:\n\n    ```bash\n    python -m venv .env\n    source .env/bin/activate\n    pip install -e \".[dev]\"\n    ```\n\nand return to the parent directory\n\n```bash\ncd ..\n```\n\n4.  We recommend adding the PyTorch version of *BigBird* to\n    Transformers. To install PyTorch, please follow the instructions [here](https://pytorch.org/get-started/locally/).\n\n**Note:** You don't need to have CUDA installed. Making the new model\nwork on CPU is sufficient.\n\n5.  To port *BigBird*, you will also need access to its\n    original repository:\n\n```bash\ngit clone https://github.com/google-research/bigbird.git \ncd big_bird\npip install -e .\n```\n\nNow you have set up a development environment to port *BigBird*\nto \ud83e\udd17 Transformers.\n\n### Run a pretrained checkpoint using the original repository\n\n**3. Set up debugging environment**", "## Translating the Transformers documentation into your language\n\nAs part of our mission to democratize machine learning, we'd love to make the Transformers library available in many more languages! Follow the steps below if you want to help translate the documentation into your language \ud83d\ude4f.\n\n**\ud83d\uddde\ufe0f Open an issue**\n\nTo get started, navigate to the [Issues](https://github.com/huggingface/transformers/issues) page of this repo and check if anyone else has opened an issue for your language. If not, open a new issue by selecting the \"Translation template\" from the \"New issue\" button.\n\nOnce an issue exists, post a comment to indicate which chapters you'd like to work on, and we'll add your name to the list.\n\n\n**\ud83c\udf74 Fork the repository**\n\nFirst, you'll need to [fork the Transformers repo](https://docs.github.com/en/get-started/quickstart/fork-a-repo). You can do this by clicking on the **Fork** button on the top-right corner of this repo's page.\n\nOnce you've forked the repo, you'll want to get the files on your local machine for editing. You can do that by cloning the fork with Git as follows:\n\n```bash\ngit clone https://github.com/YOUR-USERNAME/transformers.git\n```\n\n**\ud83d\udccb Copy-paste the English version with a new language code**\n\nThe documentation files are in one leading directory:\n\n- [`docs/source`](https://github.com/huggingface/transformers/tree/main/docs/source): All the documentation materials are organized here by language.\n\nYou'll only need to copy the files in the [`docs/source/en`](https://github.com/huggingface/transformers/tree/main/docs/source/en) directory, so first navigate to your fork of the repo and run the following:", "7.  [ ] Successfully ran forward pass in Transformers that gives\n    identical output to original checkpoint\n\n8.  [ ] Finished model tests in Transformers\n\n9.  [ ] Successfully added Tokenizer in Transformers\n\n10. [ ] Run end-to-end integration tests\n\n11. [ ] Finished docs\n\n12. [ ] Uploaded model weights to the hub\n\n13. [ ] Submitted the pull request for review\n\n14. [ ] (Optional) Added a demo notebook\n\nTo begin with, we usually recommend to start by getting a good\ntheoretical understanding of `[camelcase name of model]`. However, if you prefer to\nunderstand the theoretical aspects of the model *on-the-job*, then it is\ntotally fine to directly dive into the `[camelcase name of model]`'s code-base. This\noption might suit you better, if your engineering skills are better than\nyour theoretical skill, if you have trouble understanding\n`[camelcase name of model]`'s paper, or if you just enjoy programming much more than\nreading scientific papers.\n\n### 1. (Optional) Theoretical aspects of [camelcase name of model]\n\nYou should take some time to read *[camelcase name of model]'s* paper, if such\ndescriptive work exists. There might be large sections of the paper that\nare difficult to understand. If this is the case, this is fine - don't\nworry! The goal is not to get a deep theoretical understanding of the\npaper, but to extract the necessary information required to effectively\nre-implement the model in \ud83e\udd17 Transformers. That being said, you don't\nhave to spend too much time on the theoretical aspects, but rather focus\non the practical ones, namely:"], "eval_score_GPT4": "5", "eval_feedback_GPT4": "The response provided is completely correct, accurate, and factual. It matches the reference answer and includes additional helpful context about where and how to run the command, which aligns with the correct usage of the command in question."}, {"question": "What task does the `roberta-large-mnli` checkpoint perform?\n", "true_answer": "Text classification", "source_doc": "huggingface/course/blob/main/chapters/en/chapter1/10.mdx", "generated_answer": "The `roberta-large-mnli` checkpoint performs the task of natural language inference, specifically the classification task of determining if two sentences are logically linked across three labels (contradiction, neutral, entailment). This is also known as the Multiple Natural Language Inference (MNLI) task. The checkpoint is available on the Hugging Face Hub and is part of the RoBERTa model family.", "retrieved_docs": ["!-- DISABLE-FRONTMATTER-SECTIONS -->\n\n# End-of-chapter quiz[[end-of-chapter-quiz]]\n\n<CourseFloatingBanner\n    chapter={1}\n    classNames=\"absolute z-10 right-0 top-0\"\n/>\n\nThis chapter covered a lot of ground! Don't worry if you didn't grasp all the details; the next chapters will help you understand how things work under the hood.\n\nFirst, though, let's test what you learned in this chapter!\n\n\n### 1. Explore the Hub and look for the `roberta-large-mnli` checkpoint. What task does it perform?\n\n\n<Question\n\tchoices={[\n\t\t{\n\t\t\ttext: \"Summarization\",\n\t\t\texplain: \"Look again on the <a href=\\\"https://huggingface.co/roberta-large-mnli\\\">roberta-large-mnli page</a>.\"\n\t\t},\n\t\t{\n\t\t\ttext: \"Text classification\",\n\t\t\texplain: \"More precisely, it classifies if two sentences are logically linked across three labels (contradiction, neutral, entailment) \u2014 a task also called <em>natural language inference</em>.\",\n\t\t\tcorrect: true\n\t\t},\n\t\t{\n\t\t\ttext: \"Text generation\",\n\t\t\texplain: \"Look again on the <a href=\\\"https://huggingface.co/roberta-large-mnli\\\">roberta-large-mnli page</a>.\"\n\t\t}\n\t]}\n/>\n\n### 2. What will the following code return?\n\n```py\nfrom transformers import pipeline\n\nner = pipeline(\"ner\", grouped_entities=True)\nner(\"My name is Sylvain and I work at Hugging Face in Brooklyn.\")\n```", "The results of this run can be seen [here](https://wandb.ai/patrickvonplaten/pretraining-wav2vec2/reports/Wav2Vec2-Large--VmlldzoxMTAwODM4?accessToken=wm3qzcnldrwsa31tkvf2pdmilw3f63d4twtffs86ou016xjbyilh55uoi3mo1qzc) and the checkpoint pretrained for 120,000 steps can be accessed [here](https://huggingface.co/patrickvonplaten/wav2vec2-large-repro-960h-libri-120k-steps)", "# Data collator for padding a batch of examples to the maximum length seen in the batch\nllama_data_collator = DataCollatorWithPadding(tokenizer=llama_tokenizer)\n```\n\nNow that we have prepared the tokenized datasets, the next section will showcase how to load the pre-trained LLMs checkpoints and how to set the LoRa weights. \n\n## Models\n\n### RoBERTa\n\n#### Load RoBERTa Checkpoints for the Classification Task\n\nWe load the pre-trained RoBERTa model with a sequence classification head using the Hugging Face `AutoModelForSequenceClassification` class:\n\n```python\nfrom transformers import AutoModelForSequenceClassification \nroberta_model = AutoModelForSequenceClassification.from_pretrained(roberta_checkpoint, num_labels=2)\n```\n\n\n####  LoRA setup for RoBERTa classifier\n\nWe import LoRa configuration and set some parameters for RoBERTa classifier:\n- TaskType: Sequence classification\n- r(rank): Rank for our decomposition matrices\n- lora_alpha: Alpha parameter to scale the learned weights. LoRA paper advises fixing alpha at 16\n- lora_dropout: Dropout probability of the LoRA layers\n- bias: Whether to add bias term to LoRa layers\n\nThe code below uses the values recommended by the [Lora paper](https://arxiv.org/abs/2106.09685). [Later in this post](#hyperparameter-tuning) we will perform hyperparameter tuning of these parameters using `wandb`.\n\n```python\nfrom peft import get_peft_model, LoraConfig, TaskType\nroberta_peft_config = LoraConfig(\n    task_type=TaskType.SEQ_CLS, r=2, lora_alpha=16, lora_dropout=0.1, bias=\"none\",\n)\nroberta_model = get_peft_model(roberta_model, roberta_peft_config)\nroberta_model.print_trainable_parameters()\n```", "When work with a larger dataset, you may need to run training process for a long time and it\u2019s useful to save regular checkpoints during the process. You can use the following argument to enable intermediate checkpointing:\n\n```bash\n --checkpointing_steps=500\n```\nThis will save the trained model in subfolders of your output_dir. Subfolder names is the number of steps performed so far; for example: a checkpoint saved after 500 training steps would be saved in a subfolder named 500 \n\nYou can then start your training from this saved checkpoint with \n\n```bash\n --controlnet_model_name_or_path=\"./control_out/500\" \n```\n\nWe support training with the Min-SNR weighting strategy proposed in [Efficient Diffusion Training via Min-SNR Weighting Strategy](https://arxiv.org/abs/2303.09556) which helps to achieve faster convergence by rebalancing the loss. To use it, one needs to set the `--snr_gamma` argument. The recommended value when using it is `5.0`.\n\nWe also support gradient accumulation - it is a technique that lets you use a bigger batch size than your machine would normally be able to fit into memory. You can use `gradient_accumulation_steps` argument to set gradient accumulation steps. The ControlNet author recommends using gradient accumulation to achieve better convergence. Read more [here](https://github.com/lllyasviel/ControlNet/blob/main/docs/train.md#more-consideration-sudden-converge-phenomenon-and-gradient-accumulation).\n\nYou can **profile your code** with:\n\n```bash\n --profile_steps==5\n```\n\nRefer to the [JAX documentation on profiling](https://jax.readthedocs.io/en/latest/profiling.html). To inspect the profile trace, you'll have to install and start Tensorboard with the profile plugin:", "The Whisper checkpoints come in five configurations of varying model sizes.\nThe smallest four are trained on either English-only or multilingual data.\nThe largest checkpoint is multilingual only. All nine of the pre-trained checkpoints \nare available on the [Hugging Face Hub](https://huggingface.co/models?search=openai/whisper). The \ncheckpoints are summarised in the following table with links to the models on the Hub:", "The Whisper checkpoints come in five configurations of varying model sizes.\nThe smallest four are trained on either English-only or multilingual data.\nThe largest checkpoint is multilingual only. All nine of the pre-trained checkpoints \nare available on the [Hugging Face Hub](https://huggingface.co/models?search=openai/whisper). The \ncheckpoints are summarised in the following table with links to the models on the Hub:", "To gain a more comprehensive understanding of the attention layer, we recommend to delve into the detailed explanation provided in [a blog post by Johan Sokrates Wind](https://johanwind.github.io/2023/03/23/rwkv_details.html).\n\n### Existing checkpoints\n\n#### Pure language models: RWKV-4 models\n\nMost adopted RWKV models range from ~170M parameters to 14B parameters. According to the RWKV overview [blog post](https://johanwind.github.io/2023/03/23/rwkv_overview.html), these models have been trained on the Pile dataset and evaluated against other SoTA models on different benchmarks, and they seem to perform quite well, with very comparable results against them.\n\n| ![rwkv_loss](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/142_rwkv/RWKV-eval.png) |\n|:--:|\n| <b>RWKV-4 compared to other common architectures. Source: <a href=\"https://johanwind.github.io/2023/03/23/rwkv_overview.html\" rel=\"noopener\" target=\"_blank\" >Johan Wind's blogpost</a>  </b>|\n\n\n#### Instruction Fine-tuned/Chat Version: RWKV-4 Raven\n\nBo has also trained a \u201cchat\u201d version of the RWKV architecture, the RWKV-4 Raven model. It is a RWKV-4 pile (RWKV model pretrained on The Pile dataset) model fine-tuned on ALPACA, CodeAlpaca, Guanaco, GPT4All, ShareGPT and more. The model is available in multiple versions, with models trained on different languages (English only, English + Chinese + Japanese, English + Japanese, etc.) and different sizes (1.5B parameters, 7B parameters, 14B parameters)."], "eval_score_GPT4": "5", "eval_feedback_GPT4": "The response provides a detailed explanation of the task performed by the `roberta-large-mnli` checkpoint, correctly identifying it as a model for natural language inference and explaining the classification into three labels. This aligns well with the reference answer, which succinctly categorizes the task as text classification. The response goes further to specify the nature of the text classification, which is the MNLI task. Therefore, the response is completely correct, accurate, and factual."}, {"question": "What service is replacing the Paid tier of the Inference API at Hugging Face?\n", "true_answer": "Inference Endpoints", "source_doc": "huggingface/blog/blob/main/pricing-update.md", "generated_answer": "The service replacing the Paid tier of the Inference API at Hugging Face is Inference Endpoints. This new solution offers dedicated support, 24/7 SLAs, and uptime guarantees for enterprise users, and pricing is based on hourly compute usage and billed monthly. Inference Endpoints can be accessed through the Inference Endpoints web application or integrated with cloud providers like AWS and Azure. For more information, refer to the provided context and resources.", "retrieved_docs": ["Access \ud83e\udd17 Inference Endpoints\n\nTo access the [Inference Endpoints web application](https://ui.endpoints.huggingface.co/), you or your organization need to [add a valid payment method](https://huggingface.co/settings/billing) to your Hugging Face account.\n\n<Tip>\n  \nYou can check your [billing](https://huggingface.co/settings/billing) if you're unsure whether you have an active payment method.\n  \n</Tip>\n\nThere are two pricing plans:\n\n- Inference Endpoints pricing is based on your hourly compute, and billed monthly. This can be as low as $0.06 per CPU core/hr and $0.6 per GPU/hr depending on your needs. \n- There is also an [Enterprise](https://huggingface.co/support) plan for Inference Endpoints which offers dedicated support, 24/7 SLAs, and uptime guarantees. Pricing for Enterprise is custom and based on volume commit and annual contracts; [contact us](https://huggingface.co/inference-endpoints/enterprise) for a quote if you're interested!\n\nAfter you've added a valid payment method to your account, access the [Inference Endpoints web application](https://ui.endpoints.huggingface.co/) and start deploying! \ud83e\udd73", "--\ntitle: Introducing our new pricing\nthumbnail: /blog/assets/114_pricing-update/thumbnail.png\nauthors:\n- user: sbrandeis\n- user: pierric\n---\n\n# Introducing our new pricing\n\n\nAs you might have noticed, our [pricing page](https://huggingface.co/pricing) has changed a lot recently.\n\nFirst of all, we are sunsetting the Paid tier of the Inference API service. The Inference API will still be available for everyone to use for free. But if you're looking for a fast, enterprise-grade inference as a service, we recommend checking out our brand new solution for this: [Inference Endpoints](https://huggingface.co/inference-endpoints).\n\nAlong with Inference Endpoints, we've recently introduced hardware upgrades for [Spaces](https://huggingface.co/spaces/launch), which allows running ML demos with the hardware of your choice. No subscription is required to use these services; you only need to add a credit card to your account from your [billing settings](https://huggingface.co/settings/billing). You can also attach a payment method to any of [your organizations](https://huggingface.co/settings/organizations).\n\nYour billing settings centralize everything about our paid services. From there, you can manage your personal PRO subscription, update your payment method, and visualize your usage for the past three months. Usage for all our paid services and subscriptions will be charged at the start of each month, and a consolidated invoice will be available for your records.\n\n**TL;DR**: **At HF we monetize by providing simple access to compute for AI**, with services like AutoTrain, Spaces and Inference Endpoints, directly accessible from the Hub. [Read more](https://huggingface.co/docs/hub/billing) about our pricing and billing system.\n\nIf you have any questions, feel free to reach out. We welcome your feedback \ud83d\udd25", "--\ntitle: \"Hugging Face Platform on the AWS Marketplace: Pay with your AWS Account\"\nthumbnail: /blog/assets/158_aws_marketplace/thumbnail.jpg\nauthors:\n- user: philschmid\n- user: sbrandeis\n- user: jeffboudier\n---\n\n# Hugging Face Platform on the AWS Marketplace: Pay with your AWS Account\n\n\nThe [Hugging Face Platform](https://aws.amazon.com/marketplace/pp/prodview-n6vsyhdjkfng2) has landed on the AWS Marketplace. Starting today, you can subscribe to the Hugging Face Platform through AWS Marketplace to pay for your Hugging Face usage directly with your AWS account. This new integrated billing method makes it easy to manage payment for usage of all our managed services by all members of your organization, including Inference Endpoints, Spaces Hardware Upgrades, and AutoTrain to easily train, test and deploy the most popular machine learning models like Llama 2, StarCoder, or BERT.\n\nBy making [Hugging Face available on AWS Marketplace](https://aws.amazon.com/marketplace/pp/prodview-n6vsyhdjkfng2), we are removing barriers to adopting AI and making it easier for companies to leverage large language models. Now with just a few clicks, AWS customers can subscribe and connect their Hugging Face Account with their AWS account. \n\nBy subscribing through AWS Marketplace, Hugging Face organization usage charges for services like Inference Endpoints will automatically appear on your AWS bill, instead of being charged by Hugging Face to the credit card on file for your organization.\n\nWe are excited about this launch as it will bring our technology to more developers who rely on AWS, and make it easier for businesses to consume Hugging Face services.\n\n## Getting Started\n\nBefore you can connect your AWS Account with your Hugging Face account, you need to fulfill the following prerequisites:", "One of the main problems developers and organizations face is how difficult it is to deploy and scale production-grade inference APIs. Of course, an easy option is to rely on cloud-based AI services. Although they\u2019re extremely simple to use, these services are usually powered by a limited set of models that may not support the [task type](https://huggingface.co/tasks) you need, and that cannot be deeply customized, if at all. Alternatively, cloud-based ML services or in-house platforms give you full control, but at the expense of more time, complexity and cost. In addition, many companies have strict security, compliance, and privacy requirements mandating that they only deploy models on infrastructure over which they have administrative control.\n\n_\u201cWith the new Hugging Face Hub model catalog, natively integrated within Azure Machine Learning, we are opening a new page in our partnership with Microsoft, offering a super easy way for enterprise customers to deploy Hugging Face models for real-time inference, all within their secure Azure environment.\u201d_ said Julien Simon, Chief Evangelist at Hugging Face. \n\n_\"The integration of Hugging Face's open-source models into Azure Machine Learning represents our commitment to empowering developers with industry-leading AI tools,\"_ said John Montgomery, Corporate Vice President, Azure AI Platform at Microsoft. _\"This collaboration not only simplifies the deployment process of large language models but also provides a secure and scalable environment for real-time inferencing. It's an exciting milestone in our mission to accelerate AI initiatives and bring innovative solutions to the market swiftly and securely, backed by the power of Azure infrastructure.\"_\n\nDeploying Hugging Face models on Azure Machine Learning has never been easier:\n* Open the Hugging Face registry in Azure Machine Learning Studio. \n* Click on the Hugging Face Model Catalog. \n* Filter by task or license and search the models. \n* Click the model tile to open the model page and choose the real-time deployment option to deploy the model. \n* Select an Azure instance type and click deploy.\n\n\n![Creating a Hugging Face Endpoint on Azure](assets/75_hugging_face_endpoints_on_azure/03.jpg \"Creating a Hugging Face Endpoint on Azure\")\n\nWithin minutes, you can test your endpoint and add its inference API to your application. It\u2019s never been easier!", "Billing\n\nAt Hugging Face, we build a collaboration platform for the ML community (i.e., the Hub), and we **monetize by providing simple access to compute for AI**, with services like AutoTrain, Spaces and Inference Endpoints, directly accessible from the Hub, and billed by Hugging Face to the credit card on file.\n\nWe also partner with cloud providers, like [AWS](https://huggingface.co/blog/aws-partnership) and [Azure](https://huggingface.co/blog/hugging-face-endpoints-on-azure), to make it easy for customers to use Hugging Face directly in their cloud of choice. These solutions and usage are billed directly by the cloud provider. Ultimately we want people to be able to have great options to use Hugging Face wherever they build Machine Learning.\n\nAll user and organization accounts have a billing system. You can submit your payment info to access \"pay-as-you-go\" services.\n\nFrom the [Settings > Billing](https://huggingface.co/settings/billing) page, you can see a real time view of your paid usage across all HF services, for instance:\n\n- [Spaces](./spaces),\n- [AutoTrain](https://huggingface.co/docs/autotrain/index),\n- [Inference Endpoints](https://huggingface.co/docs/inference-endpoints/index)\n- [PRO subscription](https://huggingface.co/pricing) (for users)\n- [Enterprise Hub subscription](https://huggingface.co/enterprise) (for organizations)\n\n<div class=\"flex justify-center\">\n<img class=\"block dark:hidden\" src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/hub/billing.png\"/>\n<img class=\"hidden dark:block\" src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/hub/billing-dark.png\"/>\n</div>", "Within minutes, you can test your endpoint and add its inference API to your application. It\u2019s never been easier! \n\n![Predicting with a Hugging Face Endpoint on Azure](assets/75_hugging_face_endpoints_on_azure/04.jpg \"Predicting with a Hugging Face Endpoint on Azure\")\n\nIf you'd like to see the service in action, you can click on the image below to launch a video walkthrough.\n\n[![Video walkthrough of Hugging Face Endpoints](assets/75_hugging_face_endpoints_on_azure/05.jpg)](https://youtu.be/cjXYjN2mNVM \"Video walkthrough of Hugging Face Endpoints\")\n\nHugging Face Model Catalog on Azure Machine Learning is available today in public preview in all Azure Regions where Azure Machine Learning is available. Give the service a try and [let us know your feedback and questions in the forum](https://discuss.huggingface.co/c/azureml/68)!", "```python\n>>> from huggingface_hub import InferenceClient\n>>> client = InferenceClient(model=\"https://uu149rez6gw9ehej.eu-west-1.aws.endpoints.huggingface.cloud/deepfloyd-if\")\n# or\n>>> client = InferenceClient()\n>>> client.text_to_image(..., model=\"https://uu149rez6gw9ehej.eu-west-1.aws.endpoints.huggingface.cloud/deepfloyd-if\")\n```\n\n### Authentication\n\nCalls made with the [`InferenceClient`] can be authenticated using a [User Access Token](https://huggingface.co/docs/hub/security-tokens).\nBy default, it will use the token saved on your machine if you are logged in (check out\n[how to authenticate](https://huggingface.co/docs/huggingface_hub/quick-start#authentication)). If you are not logged in, you can pass\nyour token as an instance parameter:\n\n```python\n>>> from huggingface_hub import InferenceClient\n>>> client = InferenceClient(token=\"hf_***\")\n```\n\n<Tip>\n\nAuthentication is NOT mandatory when using the Inference API. However, authenticated users get a higher free-tier to\nplay with the service. Token is also mandatory if you want to run inference on your private models or on private\nendpoints.\n\n</Tip>\n\n## Supported tasks\n\n[`InferenceClient`]'s goal is to provide the easiest interface to run inference on Hugging Face models. It\nhas a simple API that supports the most common tasks. Here is a list of the currently supported tasks:"], "eval_score_GPT4": "5", "eval_feedback_GPT4": "Feedback: The response provided is completely correct, accurate, and factual. It not only names the service, \"Inference Endpoints,\" which matches the reference answer but also gives additional relevant details about the service, such as dedicated support, SLAs, uptime guarantees, pricing model, and integration options with cloud providers. These details are consistent with what one would expect from a service replacing a paid tier, suggesting a high level of accuracy and factuality."}, {"question": "What architectural feature does SqueezeBERT use instead of fully-connected layers for the Q, K, V, and FFN layers?\n", "true_answer": "Grouped convolutions", "source_doc": "huggingface/transformers/blob/main/docs/source/en/model_doc/squeezebert.md", "generated_answer": "SqueezeBERT uses grouped convolutions instead of fully-connected layers for the Q, K, V, and FFN layers, as described in the paper \"SqueezeBERT: What can computer vision teach NLP about efficient neural networks?\" by Forrest N. Iandola, Albert E. Shaw, Ravi Krishna, and Kurt W. Keutzer. This architectural feature is also used in the Legacy SE ResNeXt models and the", "retrieved_docs": ["!--Copyright 2020 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\n\nhttp://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License.\n\n\u26a0\ufe0f Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\nrendered properly in your Markdown viewer.\n\n-->\n\n# SqueezeBERT\n\n## Overview\n\nThe SqueezeBERT model was proposed in [SqueezeBERT: What can computer vision teach NLP about efficient neural networks?](https://arxiv.org/abs/2006.11316) by Forrest N. Iandola, Albert E. Shaw, Ravi Krishna, Kurt W. Keutzer. It's a\nbidirectional transformer similar to the BERT model. The key difference between the BERT architecture and the\nSqueezeBERT architecture is that SqueezeBERT uses [grouped convolutions](https://blog.yani.io/filter-group-tutorial)\ninstead of fully-connected layers for the Q, K, V and FFN layers.\n\nThe abstract from the paper is the following:", "(0): TransformerEncoderLayer(\n        (self_attn): MultiheadAttention(\n          (dropout_module): FairseqDropout()\n          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n        )\n      [...]\n# the full output is in the notebook\n```\nwhich looked very similar to BART's architecture, with some slight differences in a few layers - some were added, others removed. So this was great news as I didn't have to re-invent the wheel, but to only tweak a well-working design.", "<!--\nType: model-index\nCollections:\n- Name: Legacy SE ResNeXt\n  Paper:\n    Title: Squeeze-and-Excitation Networks\n    URL: https://paperswithcode.com/paper/squeeze-and-excitation-networks\nModels:\n- Name: legacy_seresnext101_32x4d\n  In Collection: Legacy SE ResNeXt\n  Metadata:\n    FLOPs: 10287698672\n    Parameters: 48960000\n    File Size: 196466866\n    Architecture:\n    - 1x1 Convolution\n    - Batch Normalization\n    - Convolution\n    - Global Average Pooling\n    - Grouped Convolution\n    - Max Pooling\n    - ReLU\n    - ResNeXt Block\n    - Residual Connection\n    - Softmax\n    - Squeeze-and-Excitation Block\n    Tasks:\n    - Image Classification\n    Training Techniques:\n    - Label Smoothing\n    - SGD with Momentum\n    - Weight Decay\n    Training Data:\n    - ImageNet\n    Training Resources: 8x NVIDIA Titan X GPUs\n    ID: legacy_seresnext101_32x4d\n    LR: 0.6\n    Epochs: 100\n    Layers: 101\n    Dropout: 0.2\n    Crop Pct: '0.875'\n    Momentum: 0.9\n    Batch Size: 1024\n    Image Size: '224'\n    Interpolation: bilinear", "<!--\nType: model-index\nCollections:\n- Name: Legacy SE ResNeXt\n  Paper:\n    Title: Squeeze-and-Excitation Networks\n    URL: https://paperswithcode.com/paper/squeeze-and-excitation-networks\nModels:\n- Name: legacy_seresnext101_32x4d\n  In Collection: Legacy SE ResNeXt\n  Metadata:\n    FLOPs: 10287698672\n    Parameters: 48960000\n    File Size: 196466866\n    Architecture:\n    - 1x1 Convolution\n    - Batch Normalization\n    - Convolution\n    - Global Average Pooling\n    - Grouped Convolution\n    - Max Pooling\n    - ReLU\n    - ResNeXt Block\n    - Residual Connection\n    - Softmax\n    - Squeeze-and-Excitation Block\n    Tasks:\n    - Image Classification\n    Training Techniques:\n    - Label Smoothing\n    - SGD with Momentum\n    - Weight Decay\n    Training Data:\n    - ImageNet\n    Training Resources: 8x NVIDIA Titan X GPUs\n    ID: legacy_seresnext101_32x4d\n    LR: 0.6\n    Epochs: 100\n    Layers: 101\n    Dropout: 0.2\n    Crop Pct: '0.875'\n    Momentum: 0.9\n    Batch Size: 1024\n    Image Size: '224'\n    Interpolation: bilinear", "* Squeeze-and-Excitation Networks\n  * Paper: `Squeeze-and-Excitation Networks` - https://arxiv.org/abs/1709.01507\n  * Code: Added to ResNet base, this is current version going forward, old `senet.py` is being deprecated\n* ECAResNet (ECA-Net)\n  * Paper: `ECA-Net: Efficient Channel Attention for Deep CNN` - https://arxiv.org/abs/1910.03151v4\n  * Code: Added to ResNet base, ECA module contributed by @VRandme, reference https://github.com/BangguWu/ECANet", "* Squeeze-and-Excitation Networks\n  * Paper: `Squeeze-and-Excitation Networks` - https://arxiv.org/abs/1709.01507\n  * Code: Added to ResNet base, this is current version going forward, old `senet.py` is being deprecated\n* ECAResNet (ECA-Net)\n  * Paper: `ECA-Net: Efficient Channel Attention for Deep CNN` - https://arxiv.org/abs/1910.03151v4\n  * Code: Added to ResNet base, ECA module contributed by @VRandme, reference https://github.com/BangguWu/ECANet", "<!--\nType: model-index\nCollections:\n- Name: SEResNeXt\n  Paper:\n    Title: Squeeze-and-Excitation Networks\n    URL: https://paperswithcode.com/paper/squeeze-and-excitation-networks\nModels:\n- Name: seresnext26d_32x4d\n  In Collection: SEResNeXt\n  Metadata:\n    FLOPs: 3507053024\n    Parameters: 16810000\n    File Size: 67425193\n    Architecture:\n    - 1x1 Convolution\n    - Batch Normalization\n    - Convolution\n    - Global Average Pooling\n    - Grouped Convolution\n    - Max Pooling\n    - ReLU\n    - ResNeXt Block\n    - Residual Connection\n    - Softmax\n    - Squeeze-and-Excitation Block\n    Tasks:\n    - Image Classification\n    Training Techniques:\n    - Label Smoothing\n    - SGD with Momentum\n    - Weight Decay\n    Training Data:\n    - ImageNet\n    Training Resources: 8x NVIDIA Titan X GPUs\n    ID: seresnext26d_32x4d\n    LR: 0.6\n    Epochs: 100\n    Layers: 26\n    Dropout: 0.2\n    Crop Pct: '0.875'\n    Momentum: 0.9\n    Batch Size: 1024\n    Image Size: '224'\n    Interpolation: bicubic"], "eval_score_GPT4": "4", "eval_feedback_GPT4": "The response correctly identifies that SqueezeBERT uses grouped convolutions instead of fully-connected layers for the Q, K, V, and FFN layers. The information provided is accurate and factual, aligning with the reference answer. However, the response includes additional information about the use of this architectural feature in Legacy SE ResNeXt models, which was not required by the instruction and is not relevant to the question about SqueezeBERT. Despite this, the core of the response directly addresses the question posed in the instruction. Based on the score rubric, the response is mostly correct, accurate, and factual, but not completely so due to the unnecessary additional information."}, {"question": "What type of license is the HuggingFace Team's software distributed under?\n", "true_answer": "Apache License, Version 2.0", "source_doc": "huggingface/simulate/blob/main/docs/source/tutorials/interaction.mdx", "generated_answer": "The HuggingFace Team's software is distributed under the Apache License, Version 2.0, as stated in the copyright notices provided in the context. The license allows for use of the software only in compliance with its terms, and it is distributed \"AS IS\" without any warranties or conditions, either express or implied. The specific language governing permissions and limitations under the license can be found on the Apache website.", "retrieved_docs": ["!---\nCopyright 2022 - The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\");\nyou may not use this file except in compliance with the License.\nYou may obtain a copy of the License at\n\n    http://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software\ndistributed under the License is distributed on an \"AS IS\" BASIS,\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\nSee the License for the specific language governing permissions and\nlimitations under the License.\n-->", "!--Copyright 2022 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\n\nhttp://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License.\n-->\n\n# Under construction", "!--Copyright 2022 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\n\nhttp://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License.\n-->\n\n# Under construction", "!---\nCopyright 2020 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\");\nyou may not use this file except in compliance with the License.\nYou may obtain a copy of the License at\n\n    http://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software\ndistributed under the License is distributed on an \"AS IS\" BASIS,\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\nSee the License for the specific language governing permissions and\nlimitations under the License.\n-->", "!---\nCopyright 2020 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\");\nyou may not use this file except in compliance with the License.\nYou may obtain a copy of the License at\n\n    http://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software\ndistributed under the License is distributed on an \"AS IS\" BASIS,\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\nSee the License for the specific language governing permissions and\nlimitations under the License.\n-->", "!--Copyright 2022 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\n\nhttp://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License.\n-->\n\n# Scenes\n\nUnder construction \ud83d\udea7.", "!--Copyright 2022 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\n\nhttp://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License.\n-->\n\n# Lights\n\n[[autodoc]] Light\n\n\nUnder construction \ud83d\udea7."], "eval_score_GPT4": "5", "eval_feedback_GPT4": "The response accurately identifies the license as the Apache License, Version 2.0, which is in agreement with the reference answer. The additional details provided about the license terms and where to find the language governing permissions and limitations are correct and factual, although they go beyond the reference answer's simplicity. The response is correct and provides a complete understanding of the license under which the HuggingFace Team's software is distributed."}, {"question": "What are the two parameter-reduction techniques proposed in the ALBERT model to lower memory consumption and increase training speed?\n", "true_answer": "Splitting the embedding matrix into two smaller matrices and using repeating layers split among groups.", "source_doc": "huggingface/transformers/blob/main/docs/source/en/model_doc/albert.md", "generated_answer": "The two parameter-reduction techniques proposed in the ALBERT model to lower memory consumption and increase training speed are:\n\n1. Splitting the embedding matrix into two smaller matrices.\n2. Using repeating layers split among groups.", "retrieved_docs": ["* Paper: [ZeRO: Memory Optimizations Toward Training Trillion Parameter Models](https://arxiv.org/abs/1910.02054). The paper is very interesting, but it's very terse.\n* Here is a good [video discussion](https://www.youtube.com/watch?v=tC01FRB0M7w) of the paper with visuals\n* Paper: [ZeRO-Offload: Democratizing Billion-Scale Model Training](https://arxiv.org/abs/2101.06840). Just published - this one goes into the details of ZeRO Offload feature.\n* DeepSpeed [configuration and tutorials](https://www.deepspeed.ai/getting-started/)\n* In addition to the paper, I highly recommend to read the following detailed blog posts with diagrams:\n  - [DeepSpeed: Extreme-scale model training for everyone]( https://www.microsoft.com/en-us/research/blog/deepspeed-extreme-scale-model-training-for-everyone/)\n  - [ZeRO & DeepSpeed: New system optimizations enable training models with over 100 billion parameters](https://www.microsoft.com/en-us/research/blog/zero-deepspeed-new-system-optimizations-enable-training-models-with-over-100-billion-parameters/)\n  - [Turing-NLG: A 17-billion-parameter language model by Microsoft](https://www.microsoft.com/en-us/research/blog/turing-nlg-a-17-billion-parameter-language-model-by-microsoft/)\n* DeepSpeed [examples on GitHub](https://github.com/microsoft/DeepSpeedExamples)\n\n# Gratitude\n\nWe were quite astonished at the amazing level of support we received from the FairScale and DeepSpeed developer teams while working on integrating those projects into `transformers`.\n\nIn particular I'd like to thank:", "!--Copyright 2022 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\n\nhttp://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\n\n\u26a0\ufe0f Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\nrendered properly in your Markdown viewer.\n\n-->\n\n# Methods and tools for efficient training on a single GPU\n\nThis guide demonstrates practical techniques that you can use to increase the efficiency of your model's training by \noptimizing memory utilization, speeding up the training, or both. If you'd like to understand how GPU is utilized during \ntraining, please refer to the [Model training anatomy](model_memory_anatomy) conceptual guide first. This guide \nfocuses on practical techniques.  \n\n<Tip>\n\nIf you have access to a machine with multiple GPUs, these approaches are still valid, plus you can leverage additional methods outlined in the [multi-GPU section](perf_train_gpu_many).\n\n</Tip>\n\nWhen training large models, there are two aspects that should be considered at the same time: \n\n* Data throughput/training time\n* Model performance\n\nMaximizing the throughput (samples/second) leads to lower training cost. This is generally achieved by utilizing the GPU \nas much as possible and thus filling GPU memory to its limit. If the desired batch size exceeds the limits of the GPU memory, \nthe memory optimization techniques, such as gradient accumulation, can help.", "If you plan to run multiple experiments, in order to properly clear the memory between experiments, restart the Python \n kernel between experiments.\n\n</Tip>\n\n## Memory utilization at vanilla training\n\nLet's use the [`Trainer`] and train the model without using any GPU performance optimization techniques and a batch size of 4:\n\n```py\n>>> from transformers import TrainingArguments, Trainer, logging\n\n>>> logging.set_verbosity_error()\n\n\n>>> training_args = TrainingArguments(per_device_train_batch_size=4, **default_args)\n>>> trainer = Trainer(model=model, args=training_args, train_dataset=ds)\n>>> result = trainer.train()\n>>> print_summary(result)\n```\n\n```\nTime: 57.82\nSamples/second: 8.86\nGPU memory occupied: 14949 MB.\n```\n\nWe see that already a relatively small batch size almost fills up our GPU's entire memory. However, a larger batch size \ncan often result in faster model convergence or better end performance. So ideally we want to tune the batch size to our\nmodel's needs and not to the GPU limitations. What's interesting is that we use much more memory than the size of the model. \nTo understand a bit better why this is the case let's have a look at a model's operations and memory needs.\n\n## Anatomy of Model's Operations\n\nTransformers architecture includes 3 main groups of operations grouped below by compute-intensity.\n\n1. **Tensor Contractions**\n\n    Linear layers and components of Multi-Head Attention all do batched **matrix-matrix multiplications**. These operations are the most compute-intensive part of training a transformer.\n\n2. **Statistical Normalizations**\n\n    Softmax and layer normalization are less compute-intensive than tensor contractions, and involve one or more **reduction operations**, the result of which is then applied via a map.\n\n3. **Element-wise Operators**", "!--Copyright 2020 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\n\nhttp://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License.\n\n\u26a0\ufe0f Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\nrendered properly in your Markdown viewer.\n\n-->\n\n# ALBERT\n\n<div class=\"flex flex-wrap space-x-1\">\n<a href=\"https://huggingface.co/models?filter=albert\">\n<img alt=\"Models\" src=\"https://img.shields.io/badge/All_model_pages-albert-blueviolet\">\n</a>\n<a href=\"https://huggingface.co/spaces/docs-demos/albert-base-v2\">\n<img alt=\"Spaces\" src=\"https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-Spaces-blue\">\n</a>\n</div>\n\n## Overview\n\nThe ALBERT model was proposed in [ALBERT: A Lite BERT for Self-supervised Learning of Language Representations](https://arxiv.org/abs/1909.11942) by Zhenzhong Lan, Mingda Chen, Sebastian Goodman, Kevin Gimpel, Piyush Sharma,\nRadu Soricut. It presents two parameter-reduction techniques to lower memory consumption and increase the training\nspeed of BERT:\n\n- Splitting the embedding matrix into two smaller matrices.\n- Using repeating layers split among groups.\n\nThe abstract from the paper is the following:", "The main drawback of this approach is that it requires staggering amounts of GPU memory - almost an order of magnitude \nlarger than its dense equivalent. Various distillation and approaches are proposed to how to overcome the much higher memory requirements.\n\nThere is direct trade-off though, you can use just a few experts with a 2-3x smaller base model instead of dozens or \nhundreds experts leading to a 5x smaller model and thus increase the training speed moderately while increasing the \nmemory requirements moderately as well.\n\nMost related papers and implementations are built around Tensorflow/TPUs:\n\n- [GShard: Scaling Giant Models with Conditional Computation and Automatic Sharding](https://arxiv.org/abs/2006.16668)\n- [Switch Transformers: Scaling to Trillion Parameter Models with Simple and Efficient Sparsity](https://arxiv.org/abs/2101.03961)\n- [GLaM: Generalist Language Model (GLaM)](https://ai.googleblog.com/2021/12/more-efficient-in-context-learning-with.html)", "> trainable params: 110,428,160 || all params: 15,627,884,544 || trainable%: 0.7066097761926236\n\n1. Base model Weight: 0.5 bytes * 15.51B frozen params  = 7.755 GB\n2. Adapter weight: 2 bytes * 0.11B trainable params        = 0.22GB\n3. Weight gradient: 2 bytes * 0.11B trainable params       = 0.12GB\n4. Optimizer state when using Adam:  4 bytes * 0.11B trainable params * 3 = 1.32GB\n5. **Adding all of the above -> 9.51 GB ~10GB -> 1 A100 40GB GPU required** \ud83e\udd2f. The reason for A100 40GB GPU is that the intermediate activations for long sequence lengths of 2048 and batch size of 4 for training lead to higher memory requirements. As we will see below, GPU memory required is 26GB which can be accommodated on A100 40GB GPU. Also, A100 GPUs have better compatibilty with Flash Attention 2.\n\nIn the above calculations, we didn't consider memory required for intermediate activation checkpointing which is considerably huge. We leverage Flash Attention V2 and Gradient Checkpointing to overcome this issue. \n\n1. For QLoRA along with flash attention V2 and gradient checkpointing, the total memory occupied by the model on a single A100 40GB GPU is **26 GB** with a **batch size of 4**.\n2. For full fine-tuning using FSDP along with Flash Attention V2 and Gradient Checkpointing, the memory occupied per GPU ranges between **70 GB to 77.6 GB** with a **per_gpu_batch_size of 1**.\n\nPlease refer to the [model-memory-usage](https://huggingface.co/spaces/hf-accelerate/model-memory-usage) to easily calculate how much vRAM is needed to train and perform big model inference on a model hosted on the \ud83e\udd17 Hugging Face Hub.\n\n## Full Finetuning", "## Efficient training strategies\n\nEven training the smallest LLaMA model requires an enormous amount of memory. Some quick math: in bf16, every parameter uses 2 bytes (in fp32 4 bytes) in addition to 8 bytes used, e.g., in the Adam optimizer (see the [performance docs](https://huggingface.co/docs/transformers/perf_train_gpu_one#optimizer) in Transformers for more info). So a 7B parameter model would use `(2+8)*7B=70GB` just to fit in memory and would likely need more when you compute intermediate values such as attention scores. So you couldn\u2019t train the model even on a single 80GB A100 like that. You can use some tricks, like more efficient optimizers of half-precision training, to squeeze a bit more into memory, but you\u2019ll run out sooner or later.\n\nAnother option is to use Parameter-Efficient Fine-Tuning (PEFT) techniques, such as the [`peft`](https://github.com/huggingface/peft) library, which can perform Low-Rank Adaptation (LoRA) on a model loaded in 8-bit. \n\n![](https://huggingface.co/datasets/trl-internal-testing/example-images/resolve/main/blog/stackllama/lora-animated.gif)   \n*Low-Rank Adaptation of linear layers: extra parameters (in orange) are added next to the frozen layer (in blue), and the resulting encoded hidden states are added together with the hidden states of the frozen layer.*\n\nLoading the model in 8bit reduces the memory footprint drastically since you only need one byte per parameter for the weights (e.g. 7B LlaMa is 7GB in memory). Instead of training the original weights directly, LoRA adds small adapter layers on top of some specific layers (usually the attention layers); thus, the number of trainable parameters is drastically reduced."], "eval_score_GPT4": "4", "eval_feedback_GPT4": "Feedback: The response provided is mostly correct and factual. It correctly identifies the two parameter-reduction techniques used in the ALBERT model as splitting the embedding matrix and using repeating layers. However, the response could be more accurate by specifying that the repeating layers are not just split among groups but are shared across the model, which is a key aspect of the ALBERT architecture. This slight inaccuracy in the explanation of the second technique results in a small deduction from the highest score."}, {"question": "What are the three main steps for fine-tuning a model with the \ud83e\udd17 Datasets library?\n", "true_answer": "1. Load a dataset from the Hugging Face Hub. 2. Preprocess the data with `Dataset.map()`. 3. Load and compute metrics.", "source_doc": "huggingface/course/blob/main/chapters/en/chapter5/1.mdx", "generated_answer": "The three main steps for fine-tuning a model with the \ud83e\udd17 Datasets library are:\n1. Load a dataset from the Hugging Face Hub.\n2. Preprocess the data with `Dataset.map()`.\n3. Load and compute metrics.\n\nThese steps are mentioned in the context provided, specifically in the introduction of the tutorial. The tutorial then goes on to explain how to fine-tune a pretrained model using different frameworks, but", "retrieved_docs": ["Introduction[[introduction]]\n\n<CourseFloatingBanner\n    chapter={5}\n    classNames=\"absolute z-10 right-0 top-0\"\n/>\n\nIn [Chapter 3](/course/chapter3) you got your first taste of the \ud83e\udd17 Datasets library and saw that there were three main steps when it came to fine-tuning a model:\n\n1. Load a dataset from the Hugging Face Hub.\n2. Preprocess the data with `Dataset.map()`.\n3. Load and compute metrics.\n\nBut this is just scratching the surface of what \ud83e\udd17 Datasets can do! In this chapter, we will take a deep dive into the library. Along the way, we'll find answers to the following questions:\n\n* What do you do when your dataset is not on the Hub?\n* How can you slice and dice a dataset? (And what if you _really_ need to use Pandas?)\n* What do you do when your dataset is huge and will melt your laptop's RAM?\n* What the heck are \"memory mapping\" and Apache Arrow?\n* How can you create your own dataset and push it to the Hub?\n\nThe techniques you learn here will prepare you for the advanced tokenization and fine-tuning tasks in [Chapter 6](/course/chapter6) and [Chapter 7](/course/chapter7) -- so grab a coffee and let's get started!", "This concludes the introduction to fine-tuning using the Keras API. An example of doing this for most common NLP tasks will be given in [Chapter 7](/course/chapter7). If you would like to hone your skills on the Keras API, try to fine-tune a model on the GLUE SST-2 dataset, using the data processing you did in section 2.", "In all of the above examples, you will need to make sure that you either already have or can easily obtain a large enough \ndomain-specific dataset at a reasonable cost to fine-tune a model. You will also need to have enough time and resources \nto fine-tune a model.\n\nIf the above examples are not the case for you, optimizing prompts can prove to be more beneficial.", "| [How to fine-tune a model on image classification (Torchvision)](https://github.com/huggingface/notebooks/blob/main/examples/image_classification.ipynb)                   | Show how to preprocess the data using Torchvision and fine-tune any pretrained Vision model on Image Classification    | [![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/image_classification.ipynb)                 | [![Open in AWS Studio](https://studiolab.sagemaker.aws/studiolab.svg)](https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/main/examples/image_classification.ipynb)|\n| [How to fine-tune a model on image classification (Albumentations)](https://github.com/huggingface/notebooks/blob/main/examples/image_classification_albumentations.ipynb) | Show how to preprocess the data using Albumentations and fine-tune any pretrained Vision model on Image Classification | [![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/image_classification_albumentations.ipynb)  | [![Open in AWS Studio](https://studiolab.sagemaker.aws/studiolab.svg)](https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/main/examples/image_classification_albumentations.ipynb)|", "| [How to fine-tune a model on image classification (Torchvision)](https://github.com/huggingface/notebooks/blob/main/examples/image_classification.ipynb)                   | Show how to preprocess the data using Torchvision and fine-tune any pretrained Vision model on Image Classification    | [![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/image_classification.ipynb)                 | [![Open in AWS Studio](https://studiolab.sagemaker.aws/studiolab.svg)](https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/main/examples/image_classification.ipynb)|\n| [How to fine-tune a model on image classification (Albumentations)](https://github.com/huggingface/notebooks/blob/main/examples/image_classification_albumentations.ipynb) | Show how to preprocess the data using Albumentations and fine-tune any pretrained Vision model on Image Classification | [![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/image_classification_albumentations.ipynb)  | [![Open in AWS Studio](https://studiolab.sagemaker.aws/studiolab.svg)](https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/main/examples/image_classification_albumentations.ipynb)|", "</Tip>\n\nTo fine-tune a model in TensorFlow, follow these steps:\n1. Define the training hyperparameters, and set up an optimizer and a learning rate schedule.\n2. Instantiate a pre-trained model.\n3. Convert a \ud83e\udd17 Dataset to a `tf.data.Dataset`.\n4. Compile your model.\n5. Add callbacks and use the `fit()` method to run the training.\n6. Upload your model to \ud83e\udd17 Hub to share with the community.\n\nStart by defining the hyperparameters, optimizer and learning rate schedule:\n\n```py\n>>> from transformers import create_optimizer\n\n>>> batch_size = 16\n>>> num_epochs = 5\n>>> num_train_steps = len(food[\"train\"]) * num_epochs\n>>> learning_rate = 3e-5\n>>> weight_decay_rate = 0.01\n\n>>> optimizer, lr_schedule = create_optimizer(\n...     init_lr=learning_rate,\n...     num_train_steps=num_train_steps,\n...     weight_decay_rate=weight_decay_rate,\n...     num_warmup_steps=0,\n... )\n```\n\nThen, load ViT with [`TFAutoModelForImageClassification`] along with the label mappings:\n\n```py\n>>> from transformers import TFAutoModelForImageClassification\n\n>>> model = TFAutoModelForImageClassification.from_pretrained(\n...     checkpoint,\n...     id2label=id2label,\n...     label2id=label2id,\n... )\n```\n\nConvert your datasets to the `tf.data.Dataset` format using the [`~datasets.Dataset.to_tf_dataset`] and your `data_collator`:", "!--Copyright 2022 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\n\nhttp://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License.\n\n\u26a0\ufe0f Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\nrendered properly in your Markdown viewer.\n\n-->\n\n# Fine-tune a pretrained model\n\n[[open-in-colab]]\n\nThere are significant benefits to using a pretrained model. It reduces computation costs, your carbon footprint, and allows you to use state-of-the-art models without having to train one from scratch. \ud83e\udd17 Transformers provides access to thousands of pretrained models for a wide range of tasks. When you use a pretrained model, you train it on a dataset specific to your task. This is known as fine-tuning, an incredibly powerful training technique. In this tutorial, you will fine-tune a pretrained model with a deep learning framework of your choice:\n\n* Fine-tune a pretrained model with \ud83e\udd17 Transformers [`Trainer`].\n* Fine-tune a pretrained model in TensorFlow with Keras.\n* Fine-tune a pretrained model in native PyTorch.\n\n<a id='data-processing'></a>\n\n## Prepare a dataset\n\n<Youtube id=\"_BZearw7f0w\"/>\n\nBefore you can fine-tune a pretrained model, download a dataset and prepare it for training. The previous tutorial showed you how to process data for training, and now you get an opportunity to put those skills to the test!"], "eval_score_GPT4": "5", "eval_feedback_GPT4": "Feedback: The response provided is completely correct, accurate, and factual. It matches the reference answer by listing the same three main steps for fine-tuning a model with the \ud83e\udd17 Datasets library: loading a dataset, preprocessing the data, and loading and computing metrics. The response also correctly uses the `Dataset.map()` function as the method for preprocessing, which aligns with the reference answer."}, {"question": "What is the maximum improvement in throughput achieved by Hugging Face Infinity compared to vanilla transformers?\n", "true_answer": "+800%", "source_doc": "huggingface/blog/blob/main/infinity-cpu-performance.md", "generated_answer": "The maximum improvement in throughput achieved by Hugging Face Infinity compared to vanilla transformers is 800% as shown in the \"Throughput\" comparison in Figure 3 in the context provided. This comparison is for running Infinity on 2 physical cores with batch size 1, compared with vanilla transformers, and is presented in the \"Conclusion\" section of the context.", "retrieved_docs": ["<br>\n<figure class=\"image table text-center m-0 w-full\">\n  <medium-zoom background=\"rgba(0,0,0,.7)\" alt=\"Latency\" src=\"assets/46_infinity_cpu_performance/latency.png\"></medium-zoom>\n  <figcaption>Figure 4. Latency (Batch=1, Physical Cores=2)</figcaption>\n</figure>\n<br>\n\n---\n\n## Conclusion\n\nIn this post, we showed how Hugging Face Infinity performs on the new Intel Ice Lake Xeon CPU. We created a detailed benchmark with over 190 different configurations sharing the results you can expect when using Hugging Face Infinity on CPU, what would be the best configuration to optimize your Infinity Container for latency, and what would be the best configuration to maximize throughput.\n\nHugging Face Infinity can deliver up to 800% higher throughput compared to vanilla transformers, and down to 1-4ms latency for sequence lengths up to 64 tokens.\n\nThe flexibility to optimize transformer models for throughput, latency, or both enables businesses to either reduce the amount of infrastructure cost for the same workload or to enable real-time use cases that were not possible before. \n\nIf you are interested in trying out Hugging Face Infinity sign up for your trial at [hf.co/infinity-trial](https://hf.co/infinity-trial) \n\n\n## Resources", "### Throughput\n\nBelow you can find the throughput comparison for running infinity on 2 physical cores with batch size 1, compared with vanilla transformers.\n\n<br>\n<figure class=\"image table text-center m-0 w-full\">\n  <medium-zoom background=\"rgba(0,0,0,.7)\" alt=\"Throughput\" src=\"assets/46_infinity_cpu_performance/throughput.png\"></medium-zoom>\n  <figcaption>Figure 3. Throughput: Infinity vs Transformers</figcaption>\n</figure>\n<br>\n\n\n| Sequence Length | Infinity    | Transformers | improvement |\n|-----------------|-------------|--------------|-------------|\n| 8               | 248 req/sec | 49 req/sec   | +506%       |\n| 16              | 212 req/sec | 50 req/sec   | +424%       |\n| 32              | 150 req/sec | 40 req/sec   | +375%       |\n| 64              | 97 req/sec  | 28 req/sec   | +346%       |\n| 128             | 55 req/sec  | 18 req/sec   | +305%       |\n| 256             | 27 req/sec  | 9 req/sec    | +300%       |\n| 384             | 17 req/sec  | 5 req/sec    | +340%       |\n| 512             | 12 req/sec  | 4 req/sec    | +300%       |\n\n\n### Latency \n\nBelow, you can find the latency results for an experiment running Hugging Face Infinity on 2 Physical Cores with Batch Size 1. It is remarkable to see how robust and constant Infinity is, with minimal deviation for p95, p99, or p100 (max latency). This result is confirmed for other experiments as well in the benchmark.", "--\ntitle: \"Case Study: Millisecond Latency using Hugging Face Infinity and modern CPUs\"\nthumbnail: /blog/assets/46_infinity_cpu_performance/thumbnail.png\nauthors:\n- user: philschmid\n- user: jeffboudier\n- user: mfuntowicz\n---\n# Case Study: Millisecond Latency using Hugging Face Infinity and modern CPUs\n\n\n<script async defer src=\"https://unpkg.com/medium-zoom-element@0/dist/medium-zoom-element.min.js\"></script>\n\n<br>\n<div style=\"background-color: #e6f9e6; padding: 16px 32px; outline: 2px solid; border-radius: 10px;\">\n  December 2022 Update: Infinity is no longer offered by Hugging Face as a commercial inference solution. To deploy and accelerate your models, we recommend the following new solutions:\n\n  * [Inference Endpoints](https://huggingface.co/docs/inference-endpoints/index) to easily deploy models on dedicated infrastructure managed by Hugging Face.\n\n  * Our open-source optimization libraries, [\ud83e\udd17 Optimum Intel](https://huggingface.co/blog/openvino) and [\ud83e\udd17 Optimum ONNX Runtime](https://huggingface.co/docs/optimum/main/en/onnxruntime/overview), to get the highest efficiency out of training and running models for inference.\n\n  * Hugging Face [Expert Acceleration Program](https://huggingface.co/support), a commercial service for Hugging Face experts to work directly with your team to accelerate your Machine Learning roadmap and models.\n</div>\n\n\n## Introduction \n\nTransfer learning has changed Machine Learning by reaching new levels of accuracy from Natural Language Processing (NLP) to Audio and Computer Vision tasks. At Hugging Face, we work hard to make these new complex models and large checkpoints as easily accessible and usable as possible. But while researchers and data scientists have converted to the new world of Transformers, few companies have been able to deploy these large, complex models in production at scale.", "---\n\nThanks for reading! If you have any questions, feel free to contact me, through\u00a0[Github](https://github.com/huggingface/transformers), or on the\u00a0[forum](https://discuss.huggingface.co/c/optimum/59). You can also connect with me on\u00a0[Twitter](https://twitter.com/_philschmid)\u00a0or\u00a0[LinkedIn](https://www.linkedin.com/in/philipp-schmid-a6a2bb196/).\n </html>", "An Infinity Container is designed to serve 1 Model and 1 Task. A Task corresponds to machine learning tasks as defined in the [Transformers Pipelines documentation](https://huggingface.co/docs/transformers/master/en/main_classes/pipelines). As of the writing of this blog post, supported tasks include feature extraction/document embedding, ranking, sequence classification, and token classification.\n\nYou can find more information about Hugging Face Infinity at [hf.co/infinity](https://huggingface.co/infinity), and if you are interested in testing it for yourself, you can sign up for a free trial at [hf.co/infinity-trial](https://huggingface.co/infinity-trial).\n\n---\n\n## Benchmark \n\nInference performance benchmarks often only measure the execution of the model. In this blog post, and when discussing the performance of Infinity, we always measure the end-to-end pipeline including pre-processing, prediction, post-processing. Please keep this in mind when comparing these results with other latency measurements. \n\n<br>\n<figure class=\"image table text-center m-0 w-full\">\n  <medium-zoom background=\"rgba(0,0,0,.7)\" alt=\"Pipeline\" src=\"assets/46_infinity_cpu_performance/pipeline.png\"></medium-zoom>\n  <figcaption>Figure 2. Infinity End-to-End Pipeline</figcaption>\n</figure>\n<br>\n\n### Environment\n\nAs a benchmark environment, we are going to use the [Amazon EC2 C6i instances](https://aws.amazon.com/ec2/instance-types/c6i), which are compute-optimized instances powered by the 3rd generation of Intel Xeon Scalable processors. These new Intel-based instances are using the ice-lake Process Technology and support Intel AVX-512, Intel Turbo Boost, and Intel Deep Learning Boost.", "Test it out to achieve __lower latency, higher throughput, and larger maximum batch size__ while training models in \ud83e\udd17 Transformers!\n\n## Performance\n\nThe chart below shows impressive acceleration __from 39% to 130%__ for Hugging Face models with Optimum when __using ONNX Runtime and DeepSpeed ZeRO Stage 1__ for training.\nThe performance measurements were done on selected Hugging Face models with PyTorch as the baseline run, only ONNX Runtime for training as the second run, and ONNX\nRuntime + DeepSpeed ZeRO Stage 1 as the final run, showing maximum gains. The Optimizer used for the baseline PyTorch runs is the AdamW optimizer and the ORT Training\nruns use the Fused Adam Optimizer(available in `ORTTrainingArguments`). The runs were performed on a single Nvidia A100 node with 8 GPUs.\n\n<figure class=\"image table text-center m-0 w-full\">\n  <img src=\"https://huggingface.co/datasets/optimum/documentation-images/resolve/main/onnxruntime/onnxruntime-training-benchmark.png\" alt=\"ONNX Runtime Training Benchmark\"/>\n</figure>\n\nThe version information used for these runs is as follows:\n```\nPyTorch: 1.14.0.dev20221103+cu116; ORT: 1.14.0.dev20221103001+cu116; DeepSpeed: 0.6.6; HuggingFace: 4.24.0.dev0; Optimum: 1.4.1.dev0; Cuda: 11.6.2\n```\n\n## Start by setting up the environment\n\nTo use ONNX Runtime for training, you need a machine with at least one NVIDIA or AMD GPU.\n\nTo use `ORTTrainer` or `ORTSeq2SeqTrainer`, you need to install ONNX Runtime Training module and Optimum.\n\n### Install ONNX Runtime", "![spaCy NER example](assets/27_summer_at_huggingface/spacy_ner.jpeg)\n\nAnother exciting integration is Sentence Transformers. You can read more about it in the [blog announcement](https://huggingface.co/blog/sentence-transformers-in-the-hub): you can find over 200 [models](https://huggingface.co/models?library=sentence-transformers) in the Hub, easily share your models with the rest of the community and reuse models from the community.\n\nBut that's not all! You can now find over 100 Adapter Transformers in the Hub and try out Speechbrain models with widgets directly in the browser for different tasks such as audio classification. If you're interested in our collaborations to integrate new ML libraries to the Hub, you can read more about them [here](https://huggingface.co/docs/hub/libraries).\n\n![Filter of all libraries](assets/27_summer_at_huggingface/filters.png)\n\n\n## Solutions\n\n### **Coming soon: Infinity**\n\nTransformers latency down to 1ms? \ud83e\udd2f\ud83e\udd2f\ud83e\udd2f\n\nWe have been working on a really sleek solution to achieve unmatched efficiency for state-of-the-art Transformer models, for companies to deploy in their own infrastructure.\n\n- Infinity comes as a single-container and can be deployed in any production environment.\n- It can achieve 1ms latency for BERT-like models on GPU and 4-10ms on CPU \ud83e\udd2f\ud83e\udd2f\ud83e\udd2f\n- Infinity meets the highest security requirements and can be integrated into your system without the need for internet access. You have control over all incoming and outgoing traffic.\n\n\u26a0\ufe0f  Join us for a [live announcement and demo on Sep 28](https://app.livestorm.co/hugging-face/hugging-face-infinity-launch?type=detailed), where we will be showcasing Infinity for the first time in public!\n\n### **NEW: Hardware Acceleration**"], "eval_score_GPT4": "5", "eval_feedback_GPT4": "The response provided is completely correct, accurate, and factual. It matches the reference answer by stating the improvement in throughput as 800%, and it also adds context by mentioning the specific conditions under which this improvement was observed, which is consistent with the kind of detail expected in a thorough and factual response."}, {"question": "What is the command to upload a spaCy pipeline to the Hugging Face Hub?\n", "true_answer": "python -m spacy huggingface-hub push en_ner_fashion-0.0.0-py3-none-any.whl", "source_doc": "huggingface/blog/blob/main/spacy.md", "generated_answer": "The command to upload a spaCy pipeline to the Hugging Face Hub is:\n\n```bash\npython -m spacy huggingface-hub push [whl_path] [--org] [--msg] [--local-repo] [--verbose]\n```\n\nThis command registers the `huggingface-hub push` command with the `spacy` module, and then uploads the specified spaCy pipeline packaged", "retrieved_docs": ["You can install spacy-huggingface-hub from pip:\n\n```bash\npip install spacy-huggingface-hub\n```\n\nYou can then check if the command has been registered successfully\n\n```bash\npython -m spacy huggingface-hub --help\n```\n\nTo push with the CLI, you can use the `huggingface-hub push` command as seen below.\n\n```bash\npython -m spacy huggingface-hub push [whl_path] [--org] [--msg] [--local-repo] [--verbose]\n```", "You can then upload any pipeline packaged with [`spacy package`](https://spacy.io/api/cli#package). Make sure to set `--build wheel` to output a binary .whl file. The uploader will read all metadata from the pipeline package, including the auto-generated pretty `README.md` and the model details available in the `meta.json`.\n\n```bash\nhuggingface-cli login\npython -m spacy package ./en_ner_fashion ./output --build wheel\ncd ./output/en_ner_fashion-0.0.0/dist\npython -m spacy huggingface-hub push en_ner_fashion-0.0.0-py3-none-any.whl\n```\n\nIn just a minute, you can get your packaged model in the Hub, try it out directly in the browser, and share it with the rest of the community. All the required metadata will be uploaded for you and you even get a cool model card.\n\nThe command will output two things:\n\n* Where to find your repo in the Hub! For example, https://huggingface.co/spacy/en_core_web_sm\n* And how to install the pipeline directly from the Hub!\n\n\n### From a Python script\n\nYou can use the `push` function from Python. It returns a dictionary containing the `\"url\"` and \"`whl_url`\" of the published model and the wheel file, which you can later install with `pip install`.\n\n```py\nfrom spacy_huggingface_hub import push\n\nresult = push(\"./en_ner_fashion-0.0.0-py3-none-any.whl\")\nprint(result[\"url\"])\n```\n\n## Additional resources", "--\ntitle: \"Welcome spaCy to the Hugging Face Hub\"\nthumbnail: /blog/assets/23_spacy/thumbnail.png\n\nauthors:\n- user: osanseviero\n- user: ines\n---\n\n# Welcome spaCy to the Hugging Face Hub\n\n\n[spaCy](https://github.com/explosion/spaCy) is a popular library for advanced Natural Language Processing used widely across industry. spaCy makes it easy to use and train pipelines for tasks like named entity recognition, text classification, part of speech tagging and more, and lets you build powerful applications to process and analyze large volumes of text.\n\nHugging Face makes it really easy to share your spaCy pipelines with the community! With a single command, you can upload any pipeline package, with a pretty model card and all required metadata auto-generated for you. The inference API currently supports NER out-of-the-box, and you can try out your pipeline interactively in your browser. You'll also get a live URL for your package that you can `pip install` from anywhere for a smooth path from prototype all the way to production!\n\n### Finding models\n\nOver 60 canonical models can be found in the [spaCy](https://hf.co/spacy) org. These models are from the [latest 3.1 release](https://explosion.ai/blog/spacy-v3-1), so you can try the latest realesed models right now! On top of this, you can find all spaCy models from the community here https://huggingface.co/models?filter=spacy.\n\n\n### Widgets\n\nThis integration includes support for NER widgets, so all models with a NER component will have this out of the box! Coming soon there will be support for text classification and POS.", "## Additional resources\n\n* spacy-huggingface-hub [library](https://github.com/explosion/spacy-huggingface-hub).\n* Launch [blog post](https://huggingface.co/blog/spacy)\n* spaCy v 3.1 [Announcement](https://explosion.ai/blog/spacy-v3-1#huggingface-hub)\n* spaCy [documentation](https://spacy.io/universe/project/spacy-huggingface-hub/)", "```bash\npython -m spacy huggingface-hub push [whl_path] [--org] [--msg] [--local-repo] [--verbose]\n```\n\n| Argument             | Type         | Description                                                                                                                   |\n| -------------------- | ------------ | ----------------------------------------------------------------------------------------------------------------------------- |\n| `whl_path`           | str / `Path` | The path to the `.whl` file packaged with [`spacy package`](https://spacy.io/api/cli#package).                                |\n| `--org`, `-o`        | str          | Optional name of organization to which the pipeline should be uploaded.                                                       |\n| `--msg`, `-m`        | str          | Commit message to use for update. Defaults to `\"Update spaCy pipeline\"`.                                                      |\n| `--local-repo`, `-l` | str / `Path` | Local path to the model repository (will be created if it doesn't exist). Defaults to `hub` in the current working directory. |\n| `--verbose`, `-V`    | bool         | Output additional info for debugging, e.g. the full generated hub metadata.                                                   |", "```bash\n>>> huggingface-cli download gpt2 config.json --token=hf_****\n/home/wauplin/.cache/huggingface/hub/models--gpt2/snapshots/11c5a3d5811f50298f278a704980280950aedb10/config.json\n```\n\n### Quiet mode\n\nBy default, the `huggingface-cli download` command will be verbose. It will print details such as warning messages, information about the downloaded files, and progress bars. If you want to silence all of this, use the `--quiet` option. Only the last line (i.e. the path to the downloaded files) is printed. This can prove useful if you want to pass the output to another command in a script.\n\n```bash\n>>> huggingface-cli download gpt2 --quiet\n/home/wauplin/.cache/huggingface/hub/models--gpt2/snapshots/11c5a3d5811f50298f278a704980280950aedb10\n```\n\n## huggingface-cli upload\n\nUse the `huggingface-cli upload` command to upload files to the Hub directly. Internally, it uses the same [`upload_file`] and [`upload_folder`] helpers described in the [Upload](./upload) guide. In the examples below, we will walk through the most common use cases. For a full list of available options, you can run:\n\n```bash\n>>> huggingface-cli upload --help\n```\n\n### Upload an entire folder\n\nThe default usage for this command is:\n\n```bash\n# Usage:  huggingface-cli upload [repo_id] [local_path] [path_in_repo]\n```\n\nTo upload the current directory at the root of the repo, use:", "Integrate your library with the Hub\n\nThe Hugging Face Hub aims to facilitate sharing machine learning models, checkpoints, and artifacts. This endeavor includes integrating the Hub into many of the amazing third-party libraries in the community. Some of the ones already integrated include [spaCy](https://spacy.io/usage/projects#huggingface_hub), [AllenNLP](https://allennlp.org/), and [timm](https://rwightman.github.io/pytorch-image-models/), among many others. Integration means users can download and upload files to the Hub directly from your library. We hope you will integrate your library and join us in democratizing artificial intelligence for everyone.\n\nIntegrating the Hub with your library provides many benefits, including:\n\n- Free model hosting for you and your users.\n- Built-in file versioning - even for huge files - made possible by [Git-LFS](https://git-lfs.github.com/).\n- All public models are powered by the [Inference API](https://huggingface.co/docs/api-inference/index).\n- In-browser widgets allow users to interact with your hosted models directly.\n\nThis tutorial will help you integrate the Hub into your library so your users can benefit from all the features offered by the Hub.\n\nBefore you begin, we recommend you create a [Hugging Face account](https://huggingface.co/join) from which you can manage your repositories and files. \n\nIf you need help with the integration, feel free to open an [issue](https://github.com/huggingface/huggingface_hub/issues/new/choose), and we would be more than happy to help you.\n\n## Installation\n\n1. Install the `huggingface_hub` library with pip in your environment:\n\n   ```bash\n   python -m pip install huggingface_hub\n   ```\n\n2. Once you have successfully installed the `huggingface_hub` library, log in to your Hugging Face account:"], "eval_score_GPT4": "4", "eval_feedback_GPT4": "The response provides the correct command structure for uploading a spaCy pipeline to the Hugging Face Hub, which aligns with the reference answer. However, the response includes placeholders and optional flags that were not present in the reference answer. The reference answer provides a specific example of a wheel file, which is not included in the response to evaluate. Despite this, the core command is accurate and factual, indicating that the response is mostly correct but lacks the specificity of the reference answer."}, {"question": "What is the time and memory complexity of the Nystr\u00f6mformer's approximation of self-attention?\n", "true_answer": "O(n)", "source_doc": "huggingface/blog/blob/main/nystromformer.md", "generated_answer": "The time complexity of the Nystr\u00f6mformer's approximation of self-attention is O(n), which is linear in the sequence length n, as opposed to the O(n^2) complexity of standard self-attention. This is achieved by using the Nystr\u00f6m method to approximate the softmax matrix in self-attention, which involves sampling landmarks from the queries and keys and constructing three matrices using these landmarks. The memory complexity of Nystr", "retrieved_docs": ["!--Copyright 2022 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\n\nhttp://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License.\n\n\u26a0\ufe0f Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\nrendered properly in your Markdown viewer.\n\n-->\n\n# Nystr\u00f6mformer\n\n## Overview\n\nThe Nystr\u00f6mformer model was proposed in [*Nystr\u00f6mformer: A Nystr\u00f6m-Based Algorithm for Approximating Self-Attention*](https://arxiv.org/abs/2102.03902) by Yunyang Xiong, Zhanpeng Zeng, Rudrasis Chakraborty, Mingxing Tan, Glenn\nFung, Yin Li, and Vikas Singh.\n\nThe abstract from the paper is the following:", "--\ntitle: \"Nystr\u00f6mformer: Approximating self-attention in linear time and memory via the Nystr\u00f6m method\"\nthumbnail: /blog/assets/86_nystromformer/thumbnail.png\nauthors:\n- user: asi\n  guest: true\n---\n\n# Nystr\u00f6mformer: Approximating self-attention in linear time and memory via the Nystr\u00f6m method\n\n\n<script async defer src=\"https://unpkg.com/medium-zoom-element@0/dist/medium-zoom-element.min.js\"></script>\n\n## Introduction\n\nTransformers have exhibited remarkable performance on various Natural Language Processing and Computer Vision tasks. Their success can be attributed to the self-attention mechanism, which captures the pairwise interactions between all the tokens in an input. However, the standard self-attention mechanism has a time and memory complexity of \\\\(O(n^2)\\\\) (where \\\\(n\\\\) is the length of the input sequence), making it expensive to train on long input sequences. \n\nThe [Nystr\u00f6mformer](https://arxiv.org/abs/2102.03902) is one of many efficient Transformer models that approximates standard self-attention with \\\\(O(n)\\\\) complexity. Nystr\u00f6mformer exhibits competitive performance on various downstream NLP and CV tasks while improving upon the efficiency of standard self-attention. The aim of this blog post is to give readers an overview of the Nystr\u00f6m method and how it can be adapted to approximate self-attention.\n\n\n## Nystr\u00f6m method for matrix approximation\n\nAt the heart of Nystr\u00f6mformer is the Nystr\u00f6m method for matrix approximation. It allows us to approximate a matrix by sampling some of its rows and columns. Let's consider a matrix \\\\(P^{n \\times n}\\\\), which is expensive to compute in its entirety. So, instead, we approximate it using the Nystr\u00f6m method. We start by sampling \\\\(m\\\\) rows and columns from \\\\(P\\\\). We can then arrange the sampled rows and columns as follows:", "This is the Nystr\u00f6m approximation of the softmax matrix in the self-attention mechanism. We multiply this matrix with the values ( \\\\(V\\\\)) to obtain a linear approximation of self-attention. Note that we never calculated the product \\\\(QK^T\\\\), avoiding the \\\\(O(n^2)\\\\) complexity. \n\n\n## How do we select landmarks?\n\nInstead of sampling \\\\(m\\\\) rows from \\\\(Q\\\\) and \\\\(K\\\\), the authors propose to construct \\\\(\\tilde{Q}\\\\) and \\\\(\\tilde{K}\\\\)\nusing segment means. In this procedure, \\\\(n\\\\) tokens are grouped into \\\\(m\\\\) segments, and the mean of each segment is computed. Ideally, \\\\(m\\\\) is much smaller than \\\\(n\\\\). According to experiments from the paper, selecting just \\\\(32\\\\) or \\\\(64\\\\) landmarks produces competetive performance compared to standard self-attention and other efficient attention mechanisms, even for long sequences lengths ( \\\\(n=4096\\\\) or \\\\(8192\\\\)). \n\nThe overall algorithm is summarised by the following figure from the paper:\n\n<figure class=\"image table text-center m-0 w-full\">\n  <medium-zoom background=\"rgba(0,0,0,.7)\" alt=\"Efficient self-attention with the Nystr\u00f6m method\" src=\"assets/86_nystromformer/paper_figure.png\"></medium-zoom>\n  <figcaption>Efficient self-attention with the Nystr\u00f6m method</figcaption>\n</figure>\n\nThe three orange matrices above correspond to the three matrices we constructed using the key and query landmarks. Also, notice that there is a DConv box. This corresponds to a skip connection added to the values using a 1D depthwise convolution.\n\n\n## How is Nystr\u00f6mformer implemented?", "Let's take a closer look. The formula to compute the output \\\\( \\mathbf{O} \\\\) of a self-attention layer for an input \\\\( \\mathbf{X} \\\\) of length \\\\( N \\\\) is:\n\n$$ \\textbf{O} = \\text{Attn}(\\mathbf{X}) = \\mathbf{V} \\times \\text{Softmax}(\\mathbf{QK}^T) \\text{ with } \\mathbf{Q} = \\mathbf{W}_q \\mathbf{X}, \\mathbf{V} = \\mathbf{W}_v \\mathbf{X}, \\mathbf{K} = \\mathbf{W}_k \\mathbf{X} $$\n\n\\\\(  \\mathbf{X} = (\\mathbf{x}_1, ... \\mathbf{x}_{N}) \\\\) is thereby the input sequence to the attention layer. The projections \\\\( \\mathbf{Q} \\\\) and \\\\( \\mathbf{K} \\\\) will each consist of \\\\( N \\\\) vectors resulting in the \\\\( \\mathbf{QK}^T \\\\) being of size \\\\( N^2 \\\\) .\n\nLLMs usually have multiple attention heads, thus doing multiple self-attention computations in parallel.\nAssuming, the LLM has 40 attention heads and runs in bfloat16 precision, we can calculate the memory requirement to store the \\\\( \\mathbf{QK^T} \\\\) matrices to be \\\\( 40 * 2 * N^2 \\\\) bytes. For \\\\( N=1000 \\\\) only around 50 MB of VRAM are needed, however, for \\\\( N=16000 \\\\) we would need 19 GB of VRAM, and for \\\\( N=100,000 \\\\) we would need almost 1TB just to store the \\\\( \\mathbf{QK}^T \\\\) matrices.\n\nLong story short, the default self-attention algorithm quickly becomes prohibitively memory-expensive for large input contexts.", "Let's take a closer look. The formula to compute the output \\\\( \\mathbf{O} \\\\) of a self-attention layer for an input \\\\( \\mathbf{X} \\\\) of length \\\\( N \\\\) is:\n\n$$ \\textbf{O} = \\text{Attn}(\\mathbf{X}) = \\mathbf{V} \\times \\text{Softmax}(\\mathbf{QK}^T) \\text{ with } \\mathbf{Q} = \\mathbf{W}_q \\mathbf{X}, \\mathbf{V} = \\mathbf{W}_v \\mathbf{X}, \\mathbf{K} = \\mathbf{W}_k \\mathbf{X} $$\n\n\\\\(  \\mathbf{X} = (\\mathbf{x}_1, ... \\mathbf{x}_{N}) \\\\) is thereby the input sequence to the attention layer. The projections \\\\( \\mathbf{Q} \\\\) and \\\\( \\mathbf{K} \\\\) will each consist of \\\\( N \\\\) vectors resulting in the \\\\( \\mathbf{QK}^T \\\\) being of size \\\\( N^2 \\\\) .\n\nLLMs usually have multiple attention heads, thus doing multiple self-attention computations in parallel.\nAssuming, the LLM has 40 attention heads and runs in bfloat16 precision, we can calculate the memory requirement to store the \\\\( \\mathbf{QK^T} \\\\) matrices to be \\\\( 40 * 2 * N^2 \\\\) bytes. For \\\\( N=1000 \\\\) only around 50 MB of VRAM are needed, however, for \\\\( N=16000 \\\\) we would need 19 GB of VRAM, and for \\\\( N=100,000 \\\\) we would need almost 1TB just to store the \\\\( \\mathbf{QK}^T \\\\) matrices.\n\nLong story short, the default self-attention algorithm quickly becomes prohibitively memory-expensive for large input contexts.", "As shown in the second line, \\\\(\\hat{P}\\\\) can be expressed as a product of three matrices. The reason for doing so will become clear later.\n\n\n## Can we approximate self-attention with the Nystr\u00f6m method?\n\nOur goal is to ultimately approximate the softmax matrix in standard self attention: S = softmax \\\\( \\frac{QK^T}{\\sqrt{d}} \\\\)\n\nHere, \\\\(Q\\\\) and \\\\(K\\\\) denote the queries and keys respectively. Following the procedure discussed above, we would sample \\\\(m\\\\) rows and columns from \\\\(S\\\\), form four submatrices, and obtain \\\\(\\hat{S}\\\\):\n\n<figure class=\"image table text-center m-0 w-full\">\n  <medium-zoom background=\"rgba(0,0,0,.7)\" alt=\"Nystr\u00f6m approximation of S\" src=\"assets/86_nystromformer/s_hat.png\"></medium-zoom>\n  <figcaption>Nystr\u00f6m approximation of S</figcaption>\n</figure>\n\nBut, what does it mean to sample a column from \\\\(S\\\\)? It means we select one element from each row. Recall how S is calculated: the final operation is a row-wise softmax. To find a single entry in a row, we must access all other entries (for the denominator in softmax). So, sampling one column requires us to know all other columns in the matrix. Therefore, we cannot directly apply the Nystr\u00f6m method to approximate the softmax matrix.\n\n\n## How can we adapt the Nystr\u00f6m method to approximate self-attention?", "## How can we adapt the Nystr\u00f6m method to approximate self-attention?\n\nInstead of sampling from \\\\(S\\\\), the authors propose to sample landmarks (or Nystr\u00f6m points) from queries and keys. We denote the query landmarks and key landmarks as \\\\(\\tilde{Q}\\\\) and \\\\(\\tilde{K}\\\\) respectively. \\\\(\\tilde{Q}\\\\) and \\\\(\\tilde{K}\\\\) can be used to construct three matrices corresponding to those in the Nystr\u00f6m approximation of \\\\(S\\\\). We define the following matrices:\n\n$$\\tilde{F} = softmax(\\frac{Q\\tilde{K}^T}{\\sqrt{d}}) \\hspace{40pt} \\tilde{A} = softmax(\\frac{\\tilde{Q}\\tilde{K}^T}{\\sqrt{d}})^+ \\hspace{40pt} \\tilde{B} = softmax(\\frac{\\tilde{Q}K^T}{\\sqrt{d}})$$\n\nThe sizes of \\\\(\\tilde{F}\\\\), \\\\(\\tilde{A}\\\\), and \\\\(\\tilde{B}) are \\\\(n \\times m, m \\times m,\\\\) and \\\\(m \\times n\\\\) respectively. \nWe replace the three matrices in the Nystr\u00f6m approximation of \\\\(S\\\\) with the new matrices we have defined to obtain an alternative Nystr\u00f6m approximation:\n\n$$\\begin{aligned}\\hat{S} &= \\tilde{F} \\tilde{A} \\tilde{B} \\\\ &= softmax(\\frac{Q\\tilde{K}^T}{\\sqrt{d}}) softmax(\\frac{\\tilde{Q}\\tilde{K}^T}{\\sqrt{d}})^+  softmax(\\frac{\\tilde{Q}K^T}{\\sqrt{d}}) \\end{aligned}$$"], "eval_score_GPT4": "4", "eval_feedback_GPT4": "Feedback: The response correctly identifies the time complexity of the Nystr\u00f6mformer as O(n), which aligns with the reference answer. However, the response is incomplete regarding the memory complexity, as it does not provide the full complexity information. Based on the score rubric, the response is mostly correct, accurate, and factual, but it is not complete."}, {"question": "What is the goal of the Named Entity Recognition task in token classification?\n", "true_answer": "The goal of the Named Entity Recognition task is to find the entities in a piece of text, such as person, location, or organization.", "source_doc": "huggingface/course/blob/main/subtitles/en/raw/tasks.md", "generated_answer": "The goal of the Named Entity Recognition task in token classification is to assign a label to each token in a sentence, indicating whether it represents a named entity (such as a person, location, or organization) or not. This task is also known as named entity recognition, token classification, or text tagging. The labels assigned to each token can be used for various downstream tasks, such as entity typing, relation classification, and named entity recognition.", "retrieved_docs": ["ote: the following transcripts are associated with Merve Noyan's videos in the Hugging Face Tasks playlist: https://www.youtube.com/playlist?list=PLo2EIpI_JMQtyEr-sLJSy5_SnLCb4vtQf\n\nToken Classification video\n\nWelcome to the Hugging Face tasks series! In this video we\u2019ll take a look at the token classification task.\nToken classification is the task of assigning a label to each token in a sentence. There are various token classification tasks and the most common are Named Entity Recognition and Part-of-Speech Tagging.\nLet\u2019s take a quick look at the Named Entity Recognition task. The goal of this task is to find the entities in a piece of text, such as person, location, or organization. This task is formulated as labelling each token with one class for each entity, and another class for tokens that have no entity.\nAnother token classification task is part-of-speech tagging. The goal of this task is to label the words for a particular part of a speech, such as noun, pronoun, adjective, verb and so on. This task is formulated as labelling each token with parts of speech.\nToken classification models are evaluated on Accuracy, Recall, Precision and F1-Score. The metrics are calculated for each of the classes. We calculate true positive, true negative and false positives to calculate precision and recall, and take their harmonic mean to get F1-Score. Then we calculate it for every class and take the overall average to evaluate our model.\nAn example dataset used for this task is ConLL2003. Here, each token belongs to a certain named entity class, denoted as the indices of the list containing the labels.\nYou can extract important information from invoices using named entity recognition models, such as date, organization name or address.\nFor more information about the Token classification task, check out the Hugging Face course.\n\n\nQuestion Answering video", "{/if}\n\nThe first application we'll explore is token classification. This generic task encompasses any problem that can be formulated as \"attributing a label to each token in a sentence,\" such as:\n\n- **Named entity recognition (NER)**: Find the entities (such as persons, locations, or organizations) in a sentence. This can be formulated as attributing a label to each token by having one class per entity and one class for \"no entity.\"\n- **Part-of-speech tagging (POS)**: Mark each word in a sentence as corresponding to a particular part of speech (such as noun, verb, adjective, etc.).\n- **Chunking**: Find the tokens that belong to the same entity. This task (which can be combined with POS or NER) can be formulated as attributing one label (usually `B-`) to any tokens that are at the beginning of a chunk, another label (usually `I-`) to tokens that are inside a chunk, and a third label (usually `O`) to tokens that don't belong to any chunk.\n\n<Youtube id=\"wVHdVlPScxA\"/>\n\nOf course, there are many other types of token classification problem; those are just a few representative examples. In this section, we will fine-tune a model (BERT) on a NER task, which will then be able to compute predictions like this one:", "et's study how to preprocess a dataset for token classification! Token classification regroups any task that can be framed as labelling each word (or token) in a sentence, like identifying the persons, organizations and locations for instance. For our example, we will use the Conll dataset, in which we remove columns we won't use and rename the other ones to get to a dataset with just two columns: words and labels. If you have your own dataset for token classification, just make sure you clean your data to get to the same point, with one column containing words (as list of strings) and another containing labels (as integers spanning from to to your number of labels -1).() Make sure you have your label names stored somewhere - here we get them from the dataset features - so you are able to map the integers to some real labels when inspecting your data! Here we are doing named entity recognitions, so ours labels are either O for words that do not belong to any entity, LOC, for location, PER, for person, ORG for organization and MISC for miscellaneous. Each label has two versions: the B- labels indicate a word that begins an entity while the I- labels indicate a word that is inside an entity. The first step to preprocess our data is to tokenize the words. This is very easily done with a tokenizer, we just have to tell it we have pre-tokenized the data with the flag is_split_into_words. Then comes the hard part. Since we have added special tokens and each word may have been split into several tokens, our labels won't match the tokens anymore. This is where the word IDs our fast tokenizer provide come to the rescue. They match each token to the word it belongs to which allows us to map each token to its label. We just have to make sure we change the B- labels to their I- counterparts for tokens that are inside (but not at the beginning) of a word. The special tokens get a label of -100, which is how we tell the Transformer loss functions to ignore them when computing the loss", "Two common types of token classification are:\n\n* named entity recognition (NER): label a token according to an entity category like organization, person, location or date. NER is especially popular in biomedical settings, where it can label genes, proteins, and drug names.\n* part-of-speech tagging (POS): label a token according to its part-of-speech like noun, verb, or adjective. POS is useful for helping translation systems understand how two identical words are grammatically different (bank as a noun versus bank as a verb).\n\n```py\n>>> from transformers import pipeline", "!--Copyright 2022 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\n\nhttp://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License.\n\n\u26a0\ufe0f Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\nrendered properly in your Markdown viewer.\n\n-->\n\n# Token classification\n\n[[open-in-colab]]\n\n<Youtube id=\"wVHdVlPScxA\"/>\n\nToken classification assigns a label to individual tokens in a sentence. One of the most common token classification tasks is Named Entity Recognition (NER). NER attempts to find a label for each entity in a sentence, such as a person, location, or organization.\n\nThis guide will show you how to:\n\n1. Finetune [DistilBERT](https://huggingface.co/distilbert-base-uncased) on the [WNUT 17](https://huggingface.co/datasets/wnut_17) dataset to detect new entities.\n2. Use your finetuned model for inference.\n\n<Tip>\nThe task illustrated in this tutorial is supported by the following model architectures:\n\n<!--This tip is automatically generated by `make fix-copies`, do not fill manually!-->", "Named-Entity Recognition\n\nRelated spaces: https://huggingface.co/spaces/rajistics/biobert_ner_demo, https://huggingface.co/spaces/abidlabs/ner, https://huggingface.co/spaces/rajistics/Financial_Analyst_AI\nTags: NER, TEXT, HIGHLIGHT\n\n## Introduction\n\nNamed-entity recognition (NER), also known as token classification or text tagging, is the task of taking a sentence and classifying every word (or \"token\") into different categories, such as names of people or names of locations, or different parts of speech.\n\nFor example, given the sentence:\n\n> Does Chicago have any Pakistani restaurants?\n\nA named-entity recognition algorithm may identify:\n\n- \"Chicago\" as a **location**\n- \"Pakistani\" as an **ethnicity**\n\nand so on.\n\nUsing `gradio` (specifically the `HighlightedText` component), you can easily build a web demo of your NER model and share that with the rest of your team.\n\nHere is an example of a demo that you'll be able to build:\n\n$demo_ner_pipeline\n\nThis tutorial will show how to take a pretrained NER model and deploy it with a Gradio interface. We will show two different ways to use the `HighlightedText` component -- depending on your NER model, either of these two ways may be easier to learn!\n\n### Prerequisites\n\nMake sure you have the `gradio` Python package already [installed](/getting_started). You will also need a pretrained named-entity recognition model. You can use your own, while in this tutorial, we will use one from the `transformers` library.\n\n### Approach 1: List of Entity Dictionaries\n\nMany named-entity recognition models output a list of dictionaries. Each dictionary consists of an _entity_, a \"start\" index, and an \"end\" index. This is, for example, how NER models in the `transformers` library operate:", "## Usage tips\n\n- This implementation is the same as [`RobertaModel`] with the addition of entity embeddings as well\n  as an entity-aware self-attention mechanism, which improves performance on tasks involving reasoning about entities.\n- LUKE treats entities as input tokens; therefore, it takes `entity_ids`, `entity_attention_mask`,\n  `entity_token_type_ids` and `entity_position_ids` as extra input. You can obtain those using\n  [`LukeTokenizer`].\n- [`LukeTokenizer`] takes `entities` and `entity_spans` (character-based start and end\n  positions of the entities in the input text) as extra input. `entities` typically consist of [MASK] entities or\n  Wikipedia entities. The brief description when inputting these entities are as follows:\n\n  - *Inputting [MASK] entities to compute entity representations*: The [MASK] entity is used to mask entities to be\n    predicted during pretraining. When LUKE receives the [MASK] entity, it tries to predict the original entity by\n    gathering the information about the entity from the input text. Therefore, the [MASK] entity can be used to address\n    downstream tasks requiring the information of entities in text such as entity typing, relation classification, and\n    named entity recognition.\n  - *Inputting Wikipedia entities to compute knowledge-enhanced token representations*: LUKE learns rich information\n    (or knowledge) about Wikipedia entities during pretraining and stores the information in its entity embedding. By\n    using Wikipedia entities as input tokens, LUKE outputs token representations enriched by the information stored in\n    the embeddings of these entities. This is particularly effective for tasks requiring real-world knowledge, such as\n    question answering.\n\n- There are three head models for the former use case:"], "eval_score_GPT4": "4", "eval_feedback_GPT4": "Feedback: The response provided is mostly correct and factual. It accurately describes the goal of the Named Entity Recognition (NER) task as assigning labels to tokens to identify named entities such as persons, locations, or organizations. However, the response includes additional information about the use of these labels for downstream tasks, which, while related, is not strictly required to answer the question about the goal of NER. This additional information does not detract from the correctness of the response but goes beyond the scope of the question. Based on the score rubric, the response aligns well with the reference answer, although it is not as concise."}, {"question": "What is the resolution of images used by the CLIPSeg model?\n", "true_answer": "352 x 352 pixels", "source_doc": "huggingface/blog/blob/main/clipseg-zero-shot.md", "generated_answer": "The resolution of images used by the CLIPSeg model is not explicitly stated in the provided context. However, the authors of the CLIPSeg paper, Timo L\u00fcddecke and Alexander Ecker, mention in their paper that they experimented with various augmentations to expand the size of the PhraseCut dataset, which contains over 340,000 phrases with corresponding image segmentation masks. They also note that the goal of their model is not only to segment categories present", "retrieved_docs": ["## Usage tips\n\n- [`CLIPSegForImageSegmentation`] adds a decoder on top of [`CLIPSegModel`]. The latter is identical to [`CLIPModel`].\n- [`CLIPSegForImageSegmentation`] can generate image segmentations based on arbitrary prompts at test time. A prompt can be either a text\n(provided to the model as `input_ids`) or an image (provided to the model as `conditional_pixel_values`). One can also provide custom\nconditional embeddings (provided to the model as `conditional_embeddings`).\n\n## Resources\n\nA list of official Hugging Face and community (indicated by \ud83c\udf0e) resources to help you get started with CLIPSeg. If you're interested in submitting a resource to be included here, please feel free to open a Pull Request and we'll review it! The resource should ideally demonstrate something new instead of duplicating an existing resource.\n\n<PipelineTag pipeline=\"image-segmentation\"/>\n\n- A notebook that illustrates [zero-shot image segmentation with CLIPSeg](https://github.com/NielsRogge/Transformers-Tutorials/blob/master/CLIPSeg/Zero_shot_image_segmentation_with_CLIPSeg.ipynb).\n\n## CLIPSegConfig\n\n[[autodoc]] CLIPSegConfig\n    - from_text_vision_configs\n\n## CLIPSegTextConfig\n\n[[autodoc]] CLIPSegTextConfig\n\n## CLIPSegVisionConfig\n\n[[autodoc]] CLIPSegVisionConfig\n\n## CLIPSegProcessor\n\n[[autodoc]] CLIPSegProcessor\n\n## CLIPSegModel\n\n[[autodoc]] CLIPSegModel\n    - forward\n    - get_text_features\n    - get_image_features\n\n## CLIPSegTextModel\n\n[[autodoc]] CLIPSegTextModel\n    - forward\n\n## CLIPSegVisionModel", "## CLIPSeg: image segmentation with CLIP\n\n[CLIPSeg](https://arxiv.org/abs/2112.10003) is a model that uses CLIP representations to create image segmentation masks. It was published by Timo L\u00fcddecke and Alexander Ecker. They achieved zero-shot image segmentation by training a Transformer-based decoder on top of the CLIP model, which is kept frozen. The decoder takes in the CLIP representation of an image, and the CLIP representation of the thing you want to segment. Using these two inputs, the CLIPSeg decoder creates a binary segmentation mask. To be more precise, the decoder doesn\u2019t only use the final CLIP representation of the image we want to segment, but it also uses the outputs of some of the layers of CLIP. \n\n<figure class=\"image table text-center m-0 w-full\">\n  <medium-zoom background=\"rgba(0,0,0,.7)\" alt=\"Overview of the CLIPSeg model\" src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/123_clipseg-zero-shot/clipseg-overview.png\"></medium-zoom>\n  <figcaption><a href=\"https://arxiv.org/abs/2112.10003\">Source</a></figcaption>\n</figure>\n\nThe decoder is trained on the [PhraseCut dataset](https://arxiv.org/abs/2008.01187), which contains over 340,000 phrases with corresponding image segmentation masks. The authors also experimented with various augmentations to expand the size of the dataset. The goal here is not only to be able to segment the categories that are present in the dataset, but also to segment unseen categories. Experiments indeed show that the decoder can generalize to unseen categories.", "!--Copyright 2022 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\n\nhttp://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License.\n\n\u26a0\ufe0f Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\nrendered properly in your Markdown viewer.\n\n-->\n\n# CLIPSeg\n\n## Overview\n\nThe CLIPSeg model was proposed in [Image Segmentation Using Text and Image Prompts](https://arxiv.org/abs/2112.10003) by Timo L\u00fcddecke\nand Alexander Ecker. CLIPSeg adds a minimal decoder on top of a frozen [CLIP](clip) model for zero- and one-shot image segmentation.\n\nThe abstract from the paper is the following:", "## CLIPSegTextModel\n\n[[autodoc]] CLIPSegTextModel\n    - forward\n\n## CLIPSegVisionModel\n\n[[autodoc]] CLIPSegVisionModel\n    - forward\n\n## CLIPSegForImageSegmentation\n\n[[autodoc]] CLIPSegForImageSegmentation\n    - forward", "## CLIP: the magic model behind CLIPSeg\n\n[CLIP](https://huggingface.co/docs/transformers/main/en/model_doc/clip), which stands for **C**ontrastive **L**anguage\u2013**I**mage **P**re-training, is a model developed by OpenAI in 2021. You can give CLIP an image or a piece of text, and CLIP will output an abstract *representation* of your input. This abstract representation, also called an *embedding*, is really just a vector (a list of numbers). You can think of this vector as a point in high-dimensional space. CLIP is trained so that the representations of similar pictures and texts are similar as well. This means that if we input an image and a text description that fits that image, the representations of the image and the text will be similar (i.e., the high-dimensional points will be close together).\n\nAt first, this might not seem very useful, but it is actually very powerful. As an example, let\u2019s take a quick look at how CLIP can be used to classify images without ever having been trained on that task. To classify an image, we input the image and the different categories we want to choose from to CLIP (e.g. we input an image and the words \u201capple\u201d, \u201corange\u201d, \u2026).  CLIP then gives us back an embedding of the image and of each category. Now, we simply have to check which category embedding is closest to the embedding of the image, et voil\u00e0! Feels like magic, doesn\u2019t it?", "--\ntitle: Zero-shot image segmentation with CLIPSeg\nthumbnail: /blog/assets/123_clipseg-zero-shot/thumb.png\nauthors:\n- user: segments-tobias\n  guest: true\n- user: nielsr\n---\n\n# Zero-shot image segmentation with CLIPSeg\n\n\n<script async defer src=\"https://unpkg.com/medium-zoom-element@0/dist/medium-zoom-element.min.js\"></script>\n\n<a target=\"_blank\" href=\"https://colab.research.google.com/github/huggingface/blog/blob/main/notebooks/123_clipseg-zero-shot.ipynb\">\n    <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n</a>\n\n**This guide shows how you can use [CLIPSeg](https://huggingface.co/docs/transformers/main/en/model_doc/clipseg), a zero-shot image segmentation model, using [`\ud83e\udd17 transformers`](https://huggingface.co/transformers). CLIPSeg creates rough segmentation masks that can be used for robot perception, image inpainting, and many other tasks. If you need more precise segmentation masks, we\u2019ll show how you can refine the results of CLIPSeg on [Segments.ai](https://segments.ai/?utm_source=hf&utm_medium=blog&utm_campaign=clipseg).**", "<figure class=\"image table text-center m-0 w-full\">\n  <medium-zoom background=\"rgba(0,0,0,.7)\" alt=\"Overview of the CLIPSeg model\" src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/123_clipseg-zero-shot/clip-tv-example.png\"></medium-zoom>\n  <figcaption>Example of image classification using CLIP (<a href=\"https://openai.com/blog/clip/\">source</a>).</figcaption>\n</figure>\n\nWhat\u2019s more, CLIP is not only useful for classification, but it can also be used for [image search](https://huggingface.co/spaces/DrishtiSharma/Text-to-Image-search-using-CLIP) (can you see how this is similar to classification?), [text-to-image models](https://huggingface.co/spaces/kamiyamai/stable-diffusion-webui) ([DALL-E 2](https://openai.com/dall-e-2/) is powered by CLIP), [object detection](https://segments.ai/zeroshot?utm_source=hf&utm_medium=blog&utm_campaign=clipseg) ([OWL-ViT](https://arxiv.org/abs/2205.06230)), and most importantly for us: image segmentation. Now you see why CLIP was truly a breakthrough in machine learning.\n\nThe reason why CLIP works so well is that the model was trained on a huge dataset of images with text captions. The dataset contained a whopping 400 million image-text pairs taken from the internet. These images contain a wide variety of objects and concepts, and CLIP is great at creating a representation for each of them.\n\n## CLIPSeg: image segmentation with CLIP"], "eval_score_GPT4": "1", "eval_feedback_GPT4": "Feedback: The response does not provide the specific resolution of images used by the CLIPSeg model, which is 352 x 352 pixels according to the reference answer. Instead, it discusses the authors' experimentation with augmentations and the goals of the model, which is not relevant to the question asked. Based on the score rubric, the response is completely incorrect as it fails to mention the correct resolution."}, {"question": "What can you use Gradio for?\n", "true_answer": "Create a demo for your machine learning model, share your machine learning model with others, and debug your model.", "source_doc": "huggingface/course/blob/main/chapters/en/chapter9/9.mdx", "generated_answer": "Gradio is an open-source Python library that allows you to quickly build customizable web apps for machine learning models, APIs, or any arbitrary Python function. It requires Python 3.8 or higher and can be installed using pip. Gradio apps can be deployed on Hugging Face Spaces for free or on your own web server using Nginx. Gradio is built on top of many open-source libraries such as FastAPI, Svelte, Vite, and T", "retrieved_docs": ["gradio\n\n## 4.11.0\n\n### Features", "gradio\n\n## 4.11.0\n\n### Features", "Or, if you already know the basics and are looking for something specific, you can search the more [technical API documentation](https://www.gradio.app/docs/).", "Running a Gradio App on your Web Server with Nginx\n\nTags: DEPLOYMENT, WEB SERVER, NGINX\n\n## Introduction\n\nGradio is a Python library that allows you to quickly create customizable web apps for your machine learning models and data processing pipelines. Gradio apps can be deployed on [Hugging Face Spaces](https://hf.space) for free.\n\nIn some cases though, you might want to deploy a Gradio app on your own web server. You might already be using [Nginx](https://www.nginx.com/), a highly performant web server, to serve your website (say `https://www.example.com`), and you want to attach Gradio to a specific subpath on your website (e.g. `https://www.example.com/gradio-demo`).\n\nIn this Guide, we will guide you through the process of running a Gradio app behind Nginx on your own web server to achieve this.\n\n**Prerequisites**\n\n1. A Linux web server with [Nginx installed](https://www.nginx.com/blog/setting-up-nginx/) and [Gradio installed](/quickstart)\n2. A working Gradio app saved as a python file on your web server\n\n## Editing your Nginx configuration file\n\n1. Start by editing the Nginx configuration file on your web server. By default, this is located at: `/etc/nginx/nginx.conf`\n\nIn the `http` block, add the following line to include server block configurations from a separate file:\n\n```bash\ninclude /etc/nginx/sites-enabled/*;\n```\n\n2. Create a new file in the `/etc/nginx/sites-available` directory (create the directory if it does not already exist), using a filename that represents your app, for example: `sudo nano /etc/nginx/sites-available/my_gradio_app`", "Quickstart\n\nGradio is an open-source Python package that allows you to quickly **build** a demo or web application for your machine learning model, API, or any arbitary Python function. You can then **share** a link to your demo or web application in just a few seconds using Gradio's built-in sharing features. *No JavaScript, CSS, or web hosting experience needed!*\n\n<img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/gradio-guides/lcm-screenshot-3.gif\" style=\"padding-bottom: 10px\">\n\nIt just takes a few lines of Python to create a beautiful demo like the one above, so let's get started \ud83d\udcab\n\n## Installation\n\n**Prerequisite**: Gradio requires [Python 3.8 or higher](https://www.python.org/downloads/)\n\n\nWe recommend installing Gradio using `pip`, which is included by default in Python. Run this in your terminal or command prompt:\n\n```bash\npip install gradio\n```\n\n\nTip: it is best to install Gradio in a virtual environment. Detailed installation instructions for all common operating systems <a href=\"https://www.gradio.app/main/guides/installing-gradio-in-a-virtual-environment\">are provided here</a>. \n\n## Building Your First Demo\n\nYou can run Gradio in your favorite code editor, Jupyter notebook, Google Colab, or anywhere else you write Python. Let's write your first Gradio app:\n\n\n$code_hello_world_4\n\n\nTip: We shorten the imported name from <code>gradio</code> to <code>gr</code> for better readability of code. This is a widely adopted convention that you should follow so that anyone working with your code can easily understand it.", "If you like Gradio, please leave us a \u2b50 on GitHub!\n\n## Open Source Stack\n\nGradio is built on top of many wonderful open-source libraries!\n\n[<img src=\"readme_files/huggingface_mini.svg\" alt=\"huggingface\" height=40>](https://huggingface.co)\n[<img src=\"readme_files/python.svg\" alt=\"python\" height=40>](https://www.python.org)\n[<img src=\"readme_files/fastapi.svg\" alt=\"fastapi\" height=40>](https://fastapi.tiangolo.com)\n[<img src=\"readme_files/encode.svg\" alt=\"encode\" height=40>](https://www.encode.io)\n[<img src=\"readme_files/svelte.svg\" alt=\"svelte\" height=40>](https://svelte.dev)\n[<img src=\"readme_files/vite.svg\" alt=\"vite\" height=40>](https://vitejs.dev)\n[<img src=\"readme_files/pnpm.svg\" alt=\"pnpm\" height=40>](https://pnpm.io)\n[<img src=\"readme_files/tailwind.svg\" alt=\"tailwind\" height=40>](https://tailwindcss.com)\n[<img src=\"readme_files/storybook.svg\" alt=\"storybook\" height=40>](https://storybook.js.org/)\n[<img src=\"readme_files/chromatic.svg\" alt=\"chromatic\" height=40>](https://www.chromatic.com/)\n\n## License", "If you like Gradio, please leave us a \u2b50 on GitHub!\n\n## Open Source Stack\n\nGradio is built on top of many wonderful open-source libraries!\n\n[<img src=\"readme_files/huggingface_mini.svg\" alt=\"huggingface\" height=40>](https://huggingface.co)\n[<img src=\"readme_files/python.svg\" alt=\"python\" height=40>](https://www.python.org)\n[<img src=\"readme_files/fastapi.svg\" alt=\"fastapi\" height=40>](https://fastapi.tiangolo.com)\n[<img src=\"readme_files/encode.svg\" alt=\"encode\" height=40>](https://www.encode.io)\n[<img src=\"readme_files/svelte.svg\" alt=\"svelte\" height=40>](https://svelte.dev)\n[<img src=\"readme_files/vite.svg\" alt=\"vite\" height=40>](https://vitejs.dev)\n[<img src=\"readme_files/pnpm.svg\" alt=\"pnpm\" height=40>](https://pnpm.io)\n[<img src=\"readme_files/tailwind.svg\" alt=\"tailwind\" height=40>](https://tailwindcss.com)\n[<img src=\"readme_files/storybook.svg\" alt=\"storybook\" height=40>](https://storybook.js.org/)\n[<img src=\"readme_files/chromatic.svg\" alt=\"chromatic\" height=40>](https://www.chromatic.com/)\n\n## License"], "eval_score_GPT4": "4", "eval_feedback_GPT4": "Feedback: The response provided is mostly correct, accurate, and factual. It gives a comprehensive overview of what Gradio is used for, including building web apps for machine learning models and deploying them. It also correctly states that Gradio is an open-source Python library and mentions the technical requirements and deployment options. However, the response could be more concise in directly addressing the uses of Gradio, such as creating demos, sharing models, and debugging, as mentioned in the reference answer. The response also does not finish the last sentence, leaving it incomplete. Based on the rubric, the response aligns well with the reference answer but includes additional, relevant information that is not strictly necessary to answer the question."}, {"question": "What TensorFlow API function is used to load a saved tensor file?\n", "true_answer": "safetensors.tensorflow.load_file", "source_doc": "huggingface/safetensors/blob/main/docs/source/api/tensorflow.mdx", "generated_answer": "The TensorFlow API function used to load a saved tensor file in the context provided is `tf.keras.models.load_model()` when loading a complete TensorFlow model saved with the `model.save()` method. However, to avoid issues with loading TensorFlow models in \ud83e\udd17 Transformers, it is recommended to save the model weights as a `h5` file using `model.save_weights()` and then reload the model with `", "retrieved_docs": ["Tensorflow API\n\n[[autodoc]] safetensors.tensorflow.load_file\n[[autodoc]] safetensors.tensorflow.load\n[[autodoc]] safetensors.tensorflow.save_file\n[[autodoc]] safetensors.tensorflow.save", "### What is a SavedModel?\n\nA SavedModel contains a standalone TensorFlow model, including its weights and its architecture. It does not require the original source of the model to be run, which makes it useful for sharing or deploying with any backend that supports reading a SavedModel such as Java, Go, C++ or JavaScript among others. The internal structure of a SavedModel is represented as such:\n```\nsavedmodel\n    /assets\n        -> here the needed assets by the model (if any)\n    /variables\n        -> here the model checkpoints that contains the weights\n   saved_model.pb -> protobuf file representing the model graph\n```\n\n### How to install TensorFlow Serving?\n\nThere are three ways to install and use TensorFlow Serving:\n- through a Docker container, \n- through an apt package,\n- or using [pip](https://pypi.org/project/pip/). \n\nTo make things easier and compliant with all the existing OS, we will use Docker in this tutorial.\n\n### How to create a SavedModel?\n\nSavedModel is the format expected by TensorFlow Serving. Since Transformers v4.2.0, creating a SavedModel has three additional features:\n\n1. The sequence length can be modified freely between runs.\n2. All model inputs are available for inference.\n3. `hidden states` or `attention` are now grouped into a single output when returning them with `output_hidden_states=True` or `output_attentions=True`.\n\nBelow, you can find the inputs and outputs representations of a `TFBertForSequenceClassification` saved as a TensorFlow SavedModel:", "## Why are shared tensors not saved in `safetensors` ?\n\nMultiple reasons for that:\n\n- *Not all frameworks support them* for instance `tensorflow` does not.\n  So if someone saves shared tensors in torch, there is no way to \n  load them in a similar fashion so we could not keep the same `Dict[str, Tensor]`\n  API.\n- *It makes lazy loading very quickly.*\n  Lazy loading is the ability to load only some tensors, or part of tensors for\n  a given file. This is trivial to do without sharing tensors but with tensor sharing\n\n  ```python\n  with safe_open(\"model.safetensors\", framework=\"pt\") as f:\n      a = f.get_tensor(\"a\")\n      b = f.get_tensor(\"b\")\n  ```\n\n  Now it's impossible with this given code to \"reshare\" buffers after the fact.\n  Once we give the `a` tensor we have no way to give back the same memory when\n  you ask for `b`. (In this particular example we could keep track of given buffers\n  but this is not the case in general, since you could do arbitrary work with `a`\n  like sending it to another device before asking for `b`)\n- *It can lead to much larger file than necessary*.\n  If you are saving a shared tensor which is only a fraction of a larger tensor,\n  then saving it with pytorch leads to saving the entire buffer instead of saving\n  just what is needed.\n\n  ```python\n  a = torch.zeros((100, 100))\n  b = a[:1, :]\n  torch.save({\"b\": b}, \"model.bin\")\n  # File is 41k instead of the expected 400 bytes\n  # In practice it could happen that you save several 10GB instead of 1GB.\n  ```\n\nNow with all those reasons being mentioned, nothing is set in stone in there.\nShared tensors do not cause unsafety, or denial of service potential, so this\ndecision could be revisited if current workarounds are not satisfactory.\n\n## How does it work ?", "## How does it work ?\n\nThe design is rather simple.\nWe're going to look for all shared tensors, then looking for all tensors\ncovering the entire buffer (there can be multiple such tensors).\nThat gives us multiple names which can be saved, we simply choose the first one\n\nDuring `load_model`, we are loading a bit like `load_state_dict` does, except\nwe're looking into the model itself, to check for shared buffers, and ignoring\nthe \"missed keys\" which were actually covered by virtue of buffer sharing (they\nwere properly loaded since there was a buffer that loaded under the hood).\nEvery other error is raised as-is\n\n**Caveat**: This means we're dropping some keys within the file. meaning if you're\nchecking for the keys saved on disk, you will see some \"missing tensors\" or if you're\nusing `load_state_dict`. Unless we start supporting shared tensors directly in\nthe format there's no real way around it.", "<Tip>\n\nRefer to the Performance [guide](performance) for more details about memory-saving techniques.\n\n</Tip>\n\n## Unable to load a saved TensorFlow model\n\nTensorFlow's [model.save](https://www.tensorflow.org/tutorials/keras/save_and_load#save_the_entire_model) method will save the entire model - architecture, weights, training configuration - in a single file. However, when you load the model file again, you may run into an error because \ud83e\udd17 Transformers may not load all the TensorFlow-related objects in the model file. To avoid issues with saving and loading TensorFlow models, we recommend you:\n\n- Save the model weights as a `h5` file extension with [`model.save_weights`](https://www.tensorflow.org/tutorials/keras/save_and_load#save_the_entire_model) and then reload the model with [`~TFPreTrainedModel.from_pretrained`]:\n\n```py\n>>> from transformers import TFPreTrainedModel\n>>> from tensorflow import keras\n\n>>> model.save_weights(\"some_folder/tf_model.h5\")\n>>> model = TFPreTrainedModel.from_pretrained(\"some_folder\")\n```\n\n- Save the model with [`~TFPretrainedModel.save_pretrained`] and load it again with [`~TFPreTrainedModel.from_pretrained`]:\n\n```py\n>>> from transformers import TFPreTrainedModel\n\n>>> model.save_pretrained(\"path_to/model\")\n>>> model = TFPreTrainedModel.from_pretrained(\"path_to/model\")\n```\n\n## ImportError\n\nAnother common error you may encounter, especially if it is a newly released model, is `ImportError`:", "Numpy API\n\n[[autodoc]] safetensors.numpy.load_file\n[[autodoc]] safetensors.numpy.load\n[[autodoc]] safetensors.numpy.save_file\n[[autodoc]] safetensors.numpy.save", "### Save tensors\n\n```python\nimport torch\nfrom safetensors.torch import save_file\n\ntensors = {\n    \"embedding\": torch.zeros((2, 2)),\n    \"attention\": torch.zeros((2, 3))\n}\nsave_file(tensors, \"model.safetensors\")\n```\n\n## Format\n\nLet's say you have safetensors file named `model.safetensors`, then `model.safetensors` will have the following internal format:\n\n<div class=\"flex justify-center\">\n    <img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/safetensors/safetensors-format.svg\"/>\n</div>\n\n## Featured Projects\n\nSafetensors is being used widely at leading AI enterprises, such as [Hugging Face](https://huggingface.co/), [EleutherAI](https://www.eleuther.ai/),\u00a0and\u00a0[StabilityAI](https://stability.ai/). Here is a non-exhaustive list of projects that are using safetensors:"], "eval_score_GPT4": "2", "eval_feedback_GPT4": "Feedback: The response provided mentions `tf.keras.models.load_model()` which is used for loading a complete TensorFlow model, not specifically a saved tensor file. The reference answer indicates that the correct function is `safetensors.tensorflow.load_file`, which is not mentioned in the response. Therefore, the response is mostly incorrect and not factual as per the reference answer provided."}, {"question": "Where can you access the logs of your Endpoints in Hugging Face Endpoints?\n", "true_answer": "In the \"Logs\" tab of your Endpoint through the UI.", "source_doc": "huggingface/hf-endpoints-documentation/blob/main/docs/source/guides/logs.mdx", "generated_answer": "Answer: You can access the logs of your Endpoints in Hugging Face Endpoints through the UI in the \"Logs\" tab of your Endpoint. Both the build logs of your Image artifacts and the Container Logs during inference are available. If your Endpoint creation is in the \"Failed\" state, you can also check the Build Logs to see the reason. (Extracted from documents 0 and 1)", "retrieved_docs": ["Access and read Logs\n\nHugging Face Endpoints provides access to the logs of your Endpoints through the UI in the \u201cLogs\u201d tab of your Endpoint. \n\nYou will have access to the build logs of your Image artifacts as well as access to the Container Logs during inference.\n\n<img src=\"https://raw.githubusercontent.com/huggingface/hf-endpoints-documentation/main/assets/9_selection.png\" alt=\"select logs\" />\n\nThe Container Logs are only available when your Endpoint is in the \u201cRunning\u201d state. \n\n_Note: If your Endpoint creation is in the \u201cFailed\u201d state, you can check the Build Logs to see what the reason was, e.g. wrong version of a dependency, etc._\n\n**Build Logs:**\n\n<img src=\"https://raw.githubusercontent.com/huggingface/hf-endpoints-documentation/main/assets/9_build_logs.png\" alt=\"build logs\" />\n\n**Container Logs:**\n\n<img src=\"https://raw.githubusercontent.com/huggingface/hf-endpoints-documentation/main/assets/9_logs.png\" alt=\"container logs\" />", "Access and view Metrics\n\nHugging Face Endpoints provides access to the metrics and analytics of your Endpoints through the UI on the detailed overview in the \u201cAnalytics\u201d tab of your Endpoints. \n\n<img src=\"https://raw.githubusercontent.com/huggingface/hf-endpoints-documentation/main/assets/10_metric.png\" alt=\"metric dashboard\" />\n\n## Access Metrics via API\n\nThe Hugging Face Inference Endpoints API exposes a [route to access the metrics](/docs/inference-endpoints/api_reference#getendpointmetrics) of your Endpoints. You can use this route to send customized Prometheus queries to your Endpoints.", "Hugging Face Inference Endpoints documentation\n\n## Setup\n\n```bash\npip install hf-doc-builder==0.4.0 watchdog --upgrade\n```\n\n## Local Development\n\n```bash\ndoc-builder preview endpoints docs/source/ --not_python_module\n```\n\n## Build Docs\n\n```bash\ndoc-builder build endpoints docs/source/ --build_dir build/ --not_python_module\n```\n\n## Add assets/Images\n\nAdding images/assets is only possible through `https://` links meaning you need to use `https://raw.githubusercontent.com/huggingface/hf-endpoints-documentation/main/assets/` prefix.\n\nexample\n\n```bash\n<img src=\"https://raw.githubusercontent.com/huggingface/hf-endpoints-documentation/main/assets/0_login.png\" alt=\"Login\" />\n```\n\n## Generate API Reference\n\n1. Copy openapi spec from `https://api.endpoints.huggingface.cloud/api-doc/openapi.json`\n2. create markdown `widdershins --environment env.json openapi.json -o myOutput.md`\n3. copy into `api_reference.mdx`", "Help & Support \n\nWe have a variety of Inference Endpoints blog posts to help you at https://huggingface.co/blog:\n\n* [Getting Started with Hugging Face Inference Endpoints](https://huggingface.co/blog/inference-endpoints)\n* [Why we\u2019re switching to Hugging Face Inference Endpoints, and maybe you should too](https://huggingface.co/blog/mantis-case-study)\n* [Deploy LLMs with Hugging Face Inference Endpoints](https://huggingface.co/blog/inference-endpoints-llm)\n* [\ud83e\udd17 LLM suggestions in Argilla with HuggingFace Inference Endpoints](https://huggingface.co/blog/alvarobartt/argilla-suggestions-via-inference-endpoints)\n* [Deploy MusicGen in no time with Inference Endpoints](https://huggingface.co/blog/run-musicgen-as-an-api)\n\nNeed more help?\n\nFeel free to ask questions on the forum so the community can also benefit from the answers:\u00a0https://discuss.huggingface.co/. If you have any other questions or issues, please contact us at <api-enterprise@huggingface.co>.", "Within minutes, you can test your endpoint and add its inference API to your application. It\u2019s never been easier! \n\n![Predicting with a Hugging Face Endpoint on Azure](assets/75_hugging_face_endpoints_on_azure/04.jpg \"Predicting with a Hugging Face Endpoint on Azure\")\n\nIf you'd like to see the service in action, you can click on the image below to launch a video walkthrough.\n\n[![Video walkthrough of Hugging Face Endpoints](assets/75_hugging_face_endpoints_on_azure/05.jpg)](https://youtu.be/cjXYjN2mNVM \"Video walkthrough of Hugging Face Endpoints\")\n\nHugging Face Model Catalog on Azure Machine Learning is available today in public preview in all Azure Regions where Azure Machine Learning is available. Give the service a try and [let us know your feedback and questions in the forum](https://discuss.huggingface.co/c/azureml/68)!", "Security & Compliance\n\n\ud83e\udd17 Inference Endpoints is built with security and secure inference at its core. Below you can find an overview of the security measures we have in place.\n\n## Data Security/Privacy\n\nHugging Face does not store any customer data in terms of payloads or tokens that are passed to the Inference Endpoint. We are storing logs for 30 days. Every Inference Endpoints uses TLS/SSL to encrypt the data in transit.\n\nWe also recommend using AWS or Azure Private Link for organizations. This allows you to access your Inference Endpoint through a private connection, without exposing it to the internet.\n\nHugging Face also offers Business Associate Addendum or GDPR data processing agreement through the Inference Endpoint enterprise plan.\n\n## Model Security/Privacy:\n\nYou can set a model repository as private if you do not want to publicly expose it. Hugging Face does not own any model or data you upload to the Hugging Face hub. Hugging Face does provide malware and pickle scans over the contents of the model repository as with all items in the Hub.\n\n## Inference Endpoints and Hub Security\n\nThe Hugging Face Hub, which Inference Endpoints is part, is also SOC2 Type 2 certified. The Hugging Face Hub offers Role Based Access Control. For more on hub security: https://huggingface.co/docs/hub/security\n\n<img width=\"150\" src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/hub/security-soc-1.jpg\">\n\n## Inference Endpoint Security level\n\nWe currently offer three types of endpoints, in order or increasing security level:\n\n- **Public**: A Public Endpoint is available from the internet, secured with TLS/SSL, and requires no authentication.\n- **Protected**: A Protected Endpoint is available from the internet, secured with TLS/SSL, and requires a valid Hugging Face token for authentication.\n- **Private** A Private Endpoint is only available through an intra-region secured AWS or Azure PrivateLink connection. Private Endpoints are not accessible from the internet.", "\ud83e\udd17 Inference Endpoints\n\n\ud83e\udd17 Inference Endpoints offers a secure production solution to easily deploy any \ud83e\udd17 Transformers, Sentence-Transformers and Diffusion models from the Hub on dedicated and autoscaling infrastructure managed by Hugging Face.\n\nA Hugging Face Endpoint is built from a [Hugging Face Model Repository](https://huggingface.co/models). When an Endpoint is created, the service creates image artifacts that are either built from the model you select or a custom-provided container image. The image artifacts are completely decoupled from the Hugging Face Hub source repositories to ensure the highest security and reliability levels.\n\n\ud83e\udd17 Inference Endpoints support all of the [\ud83e\udd17 Transformers, Sentence-Transformers and Diffusion tasks](/docs/inference-endpoints/supported_tasks) as well as [custom tasks](/docs/inference-endpoints/guides/custom_handler) not supported by \ud83e\udd17 Transformers yet like speaker diarization and diffusion.\n\nIn addition, \ud83e\udd17 Inference Endpoints gives you the option to use a custom container image managed on an external service, for instance, [Docker Hub](https://hub.docker.com/), [AWS ECR](https://aws.amazon.com/ecr/?nc1=h_ls), [Azure ACR](https://azure.microsoft.com/de-de/services/container-registry/), or [Google GCR](https://cloud.google.com/container-registry?hl=de). \n\n![creation-flow](https://raw.githubusercontent.com/huggingface/hf-endpoints-documentation/main/assets/creation_flow.png)\n\n## Documentation and Examples"], "eval_score_GPT4": "5", "eval_feedback_GPT4": "Feedback: The response provided is completely correct, accurate, and factual. It aligns with the reference answer by indicating that the logs can be accessed through the \"Logs\" tab of the Endpoint in the UI. Additionally, the response goes into further detail by explaining the types of logs available and what information can be obtained if the Endpoint creation fails, which supports the correctness and factual nature of the answer without deviating from the expected information."}, {"question": "What is the latest task added to Hugging Face AutoTrain for Computer Vision?\n", "true_answer": "Image Classification", "source_doc": "huggingface/blog/blob/main/autotrain-image-classification.md", "generated_answer": "The latest task added to Hugging Face AutoTrain for Computer Vision is image classification.", "retrieved_docs": ["elcome to the Hugging Face Course! This course has been designed to teach you all about the Hugging Face ecosystem: how to use the dataset and model hub as well as all our open source libraries. Here is the Table of Contents. As you can see, it's divided in three sections which become progressively more advanced. At this stage, the first two sections have been released. The first will teach you the basics of how to use a Transformer model, fine-tune it on your own dataset and share the result with the community. The second will dive deeper into our libraries and teach you how to tackle any NLP task. We are actively working on the last one and hope to have it ready for you for the spring of 2022. The first chapter requires no technical knowledge and is a good introduction to learn what Transformers models can do and how they could be of use to you or your company. The next chapters require a good knowledge of Python and some basic knowledge of Machine Learning and Deep Learning. If you don't know what a training and validation set is or what gradient descent means, you should look at an introductory course such as the ones published by deeplearning.ai or fast.ai. It's also best if you have some basics in one Deep Learning Framework (PyTorch or TensorFlow). Each part of the material introduced in this course has a version in both those frameworks, so you will be able to pick the one you are most comfortable with. This is the team that developed this course. I'll now let each of the speakers introduce themselves briefly.", "--\ntitle: The State of Computer Vision at Hugging Face \ud83e\udd17\nthumbnail: /blog/assets/cv_state/thumbnail.png\nauthors:\n- user: sayakpaul\n---\n\n# The State of Computer Vision at Hugging Face \ud83e\udd17\n\n\nAt Hugging Face, we pride ourselves on democratizing the field of artificial intelligence together with the community. As a part of that mission, we began focusing our efforts on computer vision over the last year. What started as a [PR for having Vision Transformers (ViT) in \ud83e\udd17 Transformers](https://github.com/huggingface/transformers/pull/10950) has now grown into something much bigger \u2013 8 core vision tasks, over 3000 models, and over 100 datasets on the Hugging Face Hub.\n\nA lot of exciting things have happened since ViTs joined the Hub. In this blog post, we\u2019ll summarize what went down and what\u2019s coming to support the continuous progress of Computer Vision from the \ud83e\udd17 ecosystem.\n\nHere is a list of things we\u2019ll cover:\n\n- [Supported vision tasks and Pipelines](#support-for-pipelines)\n- [Training your own vision models](#training-your-own-models)\n- [Integration with `timm`](#\ud83e\udd17-\ud83e\udd1d-timm)\n- [Diffusers](#\ud83e\udde8-diffusers)\n- [Support for third-party libraries](#support-for-third-party-libraries)\n- [Deployment](#deployment)\n- and much more!\n\n## Enabling the community: One task at a time \ud83d\udc41\n\nThe Hugging Face Hub is home to over 100,000 public models for different tasks such as next-word prediction, mask filling, token classification, sequence classification, and so on. As of today, we support [8 core vision tasks](https://huggingface.co/tasks) providing many model checkpoints:\n\n- Image classification\n- Image segmentation\n- (Zero-shot) object detection\n- Video classification\n- Depth estimation\n- Image-to-image synthesis\n- Unconditional image generation\n- Zero-shot image classification", "## AutoTrain with HuggingFace\n\nAutomated Machine Learning is a term for automating a Machine Learning pipeline. It also includes data cleaning, model selection, and hyper-parameter optimization too. We can use \ud83e\udd17 transformers for automated hyper-parameter searching. Hyper-parameter optimization is a difficult and time-consuming process.\n    \nWhile we can build our pipeline ourselves by using transformers and other powerful APIs, it is also possible to fully automate this with [AutoTrain](https://huggingface.co/autotrain). AutoTrain is built on many powerful APIs like transformers, [datasets](https://github.com/huggingface/datasets) and [inference-api](https://huggingface.co/docs/transformers/main_classes/trainer).\n\nCleaning the data, model selection, and hyper-parameter optimization steps are all fully automated in AutoTrain. One can fully utilize this framework to build production-ready SOTA transformer models for a specific task. Currently, AutoTrain supports binary and multi-label text classification, token classification, extractive question answering, text summarization, and text scoring. It also supports many languages like English, German, French, Spanish, Finnish, Swedish, Hindi, Dutch, and [more](https://huggingface.co/autotrain). If your language is not supported by AutoTrain, it is also possible to use custom models with custom tokenizers.\n\n## Kili\n\n[Kili](https://kili-technology.com/) is an end-to-end AI training platform for data-centric businesses. Kili provides optimized labeling features and quality management tools to manage your data. You can quickly annotate the image, video, text, pdf, and voice data while controlling the quality of the dataset. It also has powerful APIs for GraphQL and Python which eases data management a lot. \n\nIt is available either online or on-premise and it enables modern Machine Learning technics either on computer vision or on NLP and OCR. It supports text classification, named entity recognition (NER), relation extraction, and more NLP/OCR tasks. It also supports computer vision tasks like object detection, image transcription, video classification, semantic segmentation, and many more!", "It's a Good Thing in so many ways. State of the art is constantly advancing, and hardly anyone can keep up with its relentless pace. Remember that Google Vision Transformer model I mentioned earlier? Would you like to test it here and now? With Hugging Face, it's [the simplest thing](https://huggingface.co/google/vit-base-patch16-224).\n\n<kbd>\n  <img src=\"assets/31_age_of_ml_as_code/05_vision_transformer.png\">\n</kbd>\n\nHow about the latest [zero-shot text generation models](https://huggingface.co/bigscience) from the [Big Science project](https://bigscience.huggingface.co/)?\n\n<kbd>\n  <img src=\"assets/31_age_of_ml_as_code/06_big_science.png\">\n</kbd>\n\nYou can do the same with another [16,000+ models](https://huggingface.co/models) and [1,600+ datasets](https://huggingface.co/datasets), with additional tools for [inference](https://huggingface.co/inference-api), [AutoNLP](https://huggingface.co/autonlp), [latency optimization](https://huggingface.co/infinity), and [hardware acceleration](https://huggingface.co/hardware). We can also help you get your project off the ground, [from modeling to production](https://huggingface.co/support).\n\nOur mission at Hugging Face is to make Machine Learning as friendly and as productive as possible, for beginners and experts alike. \n\nWe believe in writing as little code as possible to train, optimize, and deploy models. \n\nWe believe in built-in best practices. \n\nWe believe in making infrastructure as transparent as possible. \n\nWe believe that nothing beats high quality models in production, fast.", "Webhook guide: Setup an automatic system to re-train a model when a dataset changes\n\n<Tip>\n\nWebhooks are now publicly available!\n\n</Tip>\n\nThis guide will help walk you through the setup of an automatic training pipeline on the Hugging Face platform\nusing HF Datasets, Webhooks, Spaces, and AutoTrain.\n\nWe will build a Webhook that listens to changes on an image classification dataset and triggers a fine-tuning\nof [microsoft/resnet-50](https://huggingface.co/microsoft/resnet-50) using [AutoTrain](https://huggingface.co/autotrain).\n\n\n## Prerequisite: Upload your dataset to the Hub\n\nWe will use a [simple image classification dataset](https://huggingface.co/datasets/huggingface-projects/auto-retrain-input-dataset) for the sake\nof the example. Learn more about uploading your data to the Hub [here](https://huggingface.co/docs/datasets/upload_dataset).\n\n![dataset](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/hub/webhooks-guides/002-auto-retrain/dataset.png)\n\n## Create a Webhook to react to the dataset's changes\n\nFirst, let's create a Webhook from your [settings]( https://huggingface.co/settings/webhooks).", "--\ntitle: Image Classification with AutoTrain \nthumbnail: /blog/assets/105_autotrain-image-classification/thumbnail.png\nauthors:\n- user: nimaboscarino\n---\n\n# Image Classification with AutoTrain\n\n\n<script async defer src=\"https://unpkg.com/medium-zoom-element@0/dist/medium-zoom-element.min.js\"></script>\n\nSo you\u2019ve heard all about the cool things that are happening in the machine learning world, and you want to join in. There\u2019s just one problem \u2013 you don\u2019t know how to code! \ud83d\ude31 Or maybe you\u2019re a seasoned software engineer who wants to add some ML to your side-project, but you don\u2019t have the time to pick up a whole new tech stack! For many people, the technical barriers to picking up machine learning feel insurmountable. That\u2019s why Hugging Face created [AutoTrain](https://huggingface.co/autotrain), and with the latest feature we\u2019ve just added, we\u2019re making \u201cno-code\u201d machine learning better than ever. Best of all, you can create your first project for \u2728 free! \u2728\n\n[Hugging Face AutoTrain](https://huggingface.co/autotrain) lets you train models with **zero** configuration needed. Just choose your task (translation? how about question answering?), upload your data, and let Hugging Face do the rest of the work! By letting AutoTrain experiment with number of different models, there's even a good chance that you'll end up with a model that performs better than a model that's been hand-trained by an engineer \ud83e\udd2f We\u2019ve been expanding the number of tasks that we support, and we\u2019re proud to announce that **you can now use AutoTrain for Computer Vision**! Image Classification is the latest task we\u2019ve added, with more on the way. But what does this mean for you?", "## \ud83e\udd17 AutoTrain\n\n[AutoTrain](https://huggingface.co/autotrain) provides a \u201cno-code\u201d solution to train state-of-the-art Machine Learning models for tasks like text classification, text summarization, named entity recognition, and more. For Computer Vision, we currently support [image classification](https://huggingface.co/blog/autotrain-image-classification), but one can expect more task coverage.\n\nAutoTrain also enables [automatic model evaluation](https://huggingface.co/spaces/autoevaluate/model-evaluator). This application allows you to evaluate \ud83e\udd17 Transformers [models](https://huggingface.co/models?library=transformers&sort=downloads) across a wide variety of [datasets](https://huggingface.co/datasets) on the Hub. The results of your evaluation will be displayed on the [public leaderboards](https://huggingface.co/spaces/autoevaluate/leaderboards). You can check [this blog post](https://huggingface.co/blog/eval-on-the-hub) for more details.\n\n## The technical philosophy\n\nIn this section, we wanted to share our philosophy behind adding support for Computer Vision in \ud83e\udd17\u00a0Transformers so that the community is aware of the design choices specific to this area.\n\nEven though Transformers started with NLP, we support multiple modalities today, for example \u2013 vision, audio, vision-language, and Reinforcement Learning. For all of these modalities, all the corresponding models from Transformers enjoy some common benefits:\n\n- Easy model download with a single line of code with `from_pretrained()`\n- Easy model upload with `push_to_hub()`\n- Support for loading huge checkpoints with efficient checkpoint sharding techniques\n- Optimization support (with tools like [Optimum](https://huggingface.co/docs/optimum))\n- Initialization from model configurations\n- Support for both PyTorch and TensorFlow (non-exhaustive)\n- and many more"], "eval_score_GPT4": "5", "eval_feedback_GPT4": "Feedback: The response provided matches the reference answer, indicating that it is correct. The respondent has identified \"image classification\" as the latest task added to Hugging Face AutoTrain for Computer Vision, which is in line with the reference answer. There is no inaccuracy or incorrect information in the response."}, {"question": "What is the default repository type created by the `create_repo` function on Hugging Face Hub?\n", "true_answer": "model", "source_doc": "huggingface/huggingface_hub/blob/main/docs/source/en/guides/repository.md", "generated_answer": "The default repository type created by the `create_repo` function on Hugging Face Hub is a model repository. This can be changed using the `repo_type` parameter, which accepts the values \"dataset\" and \"space\" for creating a dataset or space repository, respectively. By default, a model repository is created with the name provided as the repository ID, which is the user's namespace followed by the repository name (e.g., \"username_or_org/repo_", "retrieved_docs": ["## Creating a repository\n\nUsing the Hub's web interface you can easily create repositories, add files (even large ones!), explore models, visualize diffs, and much more. There are three kinds of repositories on the Hub, and in this guide you'll be creating a **model repository** for demonstration purposes. For information on creating and managing models, datasets, and Spaces, refer to their respective documentation.\n\n1. To create a new repository, visit [huggingface.co/new](http://huggingface.co/new):\n\n<div class=\"flex justify-center\">\n<img class=\"block dark:hidden\" src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/hub/new_repo.png\"/>\n<img class=\"hidden dark:block\" src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/hub/new_repo-dark.png\"/>\n</div>\n\n2. Specify the owner of the repository: this can be either you or any of the organizations you\u2019re affiliated with. \n\n3. Enter your model\u2019s name. This will also be the name of the repository. \n\n4. Specify whether you want your model to be public or private.\n\n5. Specify the license. You can leave the *License* field blank for now. To learn about licenses, visit the [**Licenses**](repositories-licenses) documentation.\n\nAfter creating your model repository, you should see a page like this:", "# Repository creation and management\n    create_repo,\n    delete_repo,\n    update_repo_visibility,\n\n    # And some methods to retrieve/change information about the content\n    list_models,\n    list_datasets,\n    list_metrics,\n    list_repo_files,\n    upload_file,\n    delete_file,\n)\n```\n\n\nAdditionally, it offers the very powerful `Repository` class to manage a local repository. We will explore these methods and that class in the next few section to understand how to leverage them.\n\nThe `create_repo` method can be used to create a new repository on the hub:\n\n```py\nfrom huggingface_hub import create_repo\n\ncreate_repo(\"dummy-model\")\n```\n\nThis will create the repository `dummy-model` in your namespace. If you like, you can specify which organization the repository should belong to using the `organization` argument:\n\n```py\nfrom huggingface_hub import create_repo\n\ncreate_repo(\"dummy-model\", organization=\"huggingface\")\n```\n\nThis will create the `dummy-model` repository in the `huggingface` namespace, assuming you belong to that organization.\nOther arguments which may be useful are:\n\n- `private`, in order to specify if the repository should be visible from others or not.\n- `token`, if you would like to override the token stored in your cache by a given token.\n- `repo_type`, if you would like to create a `dataset` or a `space` instead of a model. Accepted values are `\"dataset\"` and `\"space\"`.\n\nOnce the repository is created, we should add files to it! Jump to the next section to see the three ways this can be handled.\n\n\n## Using the web interface[[using-the-web-interface]]\n\nThe web interface offers tools to manage repositories directly in the Hub. Using the interface, you can easily create repositories, add files (even large ones!), explore models, visualize diffs, and much more.", "!--\u26a0\ufe0f Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\nrendered properly in your Markdown viewer.\n-->\n\n# Create and manage a repository\n\nThe Hugging Face Hub is a collection of git repositories. [Git](https://git-scm.com/) is a widely used tool in software\ndevelopment to easily version projects when working collaboratively. This guide will show you how to interact with the\nrepositories on the Hub, especially:\n\n- Create and delete a repository.\n- Manage branches and tags. \n- Rename your repository.\n- Update your repository visibility.\n- Manage a local copy of your repository.\n\n<Tip warning={true}>\n\nIf you are used to working with platforms such as GitLab/GitHub/Bitbucket, your first instinct\nmight be to use `git` CLI to clone your repo (`git clone`), commit changes (`git add, git commit`) and push them\n(`git push`). This is valid when using the Hugging Face Hub. However, software engineering and machine learning do\nnot share the same requirements and workflows. Model repositories might maintain large model weight files for different\nframeworks and tools, so cloning the repository can lead to you maintaining large local folders with massive sizes. As\na result, it may be more efficient to use our custom HTTP methods. You can read our [Git vs HTTP paradigm](../concepts/git_vs_http)\nexplanation page for more details.\n\n</Tip>\n\nIf you want to create and manage a repository on the Hub, your machine must be logged in. If you are not, please refer to\n[this section](../quick-start#authentication). In the rest of this guide, we will assume that your machine is logged in.\n\n## Repo creation and deletion\n\nThe first step is to know how to create and delete repositories. You can only manage repositories that you own (under\nyour username namespace) or from organizations in which you have write permissions.\n\n### Create a repository", "This is equivalent to `huggingface_hub.space_info(repo_id, revision)`.\n\n\n## Repo API\n\nThe following endpoints manage repository settings like creating and deleting a repository.\n### POST /api/repos/create\n\nCreate a repository. It's a model repo by default.\n\nParameters:\n- `type`: Type of repo (dataset or space; model by default).\n- `name`: Name of repo.\n- `organization`: Name of organization (optional).\n- `private`: Whether the repo is private.\n- `sdk`: When the type is `space` (streamlit, gradio, docker or static)\n\nPayload:\n\n```js\npayload = {\n    \"type\":\"model\",\n    \"name\":\"name\",\n    \"organization\": \"organization\",\n    \"private\":\"private\",\n    \"sdk\": \"sdk\"\n}\n```\n\nThis is equivalent to `huggingface_hub.create_repo()`.\n\n### DELETE /api/repos/delete\n\nDelete a repository. It's a model repo by default.\n\nParameters:\n- `type`: Type of repo (dataset or space; model by default).\n- `name`: Name of repo.\n- `organization`: Name of organization (optional).\n\nPayload:\n\n```js\npayload = {\n    \"type\": \"model\",\n    \"name\": \"name\",\n    \"organization\": \"organization\",\n}\n```\n\nThis is equivalent to `huggingface_hub.delete_repo()`.\n\n### PUT /api/repos/{repo_type}/{repo_id}/settings\n\nUpdate repo visibility.\n\nPayload:\n\n```js\npayload = {\n    \"private\": \"private\",\n}\n```\n\nThis is equivalent to `huggingface_hub.update_repo_visibility()`.", "### Advanced programmatic repository management \n\nThe `Repository` class helps manage both offline Git repositories and Hugging\nFace Hub repositories. Using the `Repository` class requires `git` and `git-lfs`\nto be installed.\n\nInstantiate a `Repository` object by calling it with a path to a local Git\nclone/repository:\n\n```python\n>>> from huggingface_hub import Repository\n>>> repo = Repository(\"<path>/<to>/<folder>\")\n```\n\nThe `Repository` takes a `clone_from` string as parameter. This can stay as\n`None` for offline management, but can also be set to any URL pointing to a Git\nrepo to clone that repository in the specified directory:\n\n```python\n>>> repo = Repository(\"huggingface-hub\", clone_from=\"https://github.com/huggingface/huggingface_hub\")\n```\n\nThe `clone_from` method can also take any Hugging Face model ID as input, and\nwill clone that repository:\n\n```python\n>>> repo = Repository(\"w2v2\", clone_from=\"facebook/wav2vec2-large-960h-lv60\")\n```\n\nIf the repository you're cloning is one of yours or one of your organisation's,\nthen having the ability to commit and push to that repository is important. In\norder to do that, you should make sure to be logged-in using `huggingface-cli\nlogin`, and to have the `token` parameter set to `True` (the default)\nwhen  instantiating the `Repository` object:\n\n```python\n>>> repo = Repository(\"my-model\", clone_from=\"<user>/<model_id>\", token=True)\n```\n\nThis works for models, datasets and spaces repositories; but you will need to\nexplicitely specify the type for the last two options:", "### Create a repository\n\nCreate an empty repository with [`create_repo`] and give it a name with the `repo_id` parameter. The `repo_id` is your namespace followed by the repository name: `username_or_org/repo_name`.\n\n```py\n>>> from huggingface_hub import create_repo\n>>> create_repo(\"lysandre/test-model\")\n'https://huggingface.co/lysandre/test-model'\n```\n\nBy default, [`create_repo`] creates a model repository. But you can use the `repo_type` parameter to specify another repository type. For example, if you want to create a dataset repository:\n\n```py\n>>> from huggingface_hub import create_repo\n>>> create_repo(\"lysandre/test-dataset\", repo_type=\"dataset\")\n'https://huggingface.co/datasets/lysandre/test-dataset'\n```\n\nWhen you create a repository, you can set your repository visibility with the `private` parameter.\n\n```py\n>>> from huggingface_hub import create_repo\n>>> create_repo(\"lysandre/test-private\", private=True)\n```\n\nIf you want to change the repository visibility at a later time, you can use the [`update_repo_visibility`] function.\n\n### Delete a repository\n\nDelete a repository with [`delete_repo`]. Make sure you want to delete a repository because this is an irreversible process!\n\nSpecify the `repo_id` of the repository you want to delete:\n\n```py\n>>> delete_repo(repo_id=\"lysandre/my-corrupted-dataset\", repo_type=\"dataset\")\n```\n\n### Duplicate a repository (only for Spaces)", "# Adding HuggingFace argument\n    parser.add_argument(\"--repo-id\", type=str, default=\"ThomasSimonini/ppo-CartPole-v1\", help=\"id of the model repository from the Hugging Face Hub {username/repo_name}\")\n\n    args = parser.parse_args()\n    args.batch_size = int(args.num_envs * args.num_steps)\n    args.minibatch_size = int(args.batch_size // args.num_minibatches)\n    # fmt: on\n    return args\n\n\ndef package_to_hub(\n    repo_id,\n    model,\n    hyperparameters,\n    eval_env,\n    video_fps=30,\n    commit_message=\"Push agent to the Hub\",\n    token=None,\n    logs=None,\n):\n    \"\"\"\n    Evaluate, Generate a video and Upload a model to Hugging Face Hub.\n    This method does the complete pipeline:\n    - It evaluates the model\n    - It generates the model card\n    - It generates a replay video of the agent\n    - It pushes everything to the hub\n    :param repo_id: id of the model repository from the Hugging Face Hub\n    :param model: trained model\n    :param eval_env: environment used to evaluate the agent\n    :param fps: number of fps for rendering the video\n    :param commit_message: commit message\n    :param logs: directory on local machine of tensorboard logs you'd like to upload\n    \"\"\"\n    msg.info(\n        \"This function will save, evaluate, generate a video of your agent, \"\n        \"create a model card and push everything to the hub. \"\n        \"It might take up to 1min. \\n \"\n        \"This is a work in progress: if you encounter a bug, please open an issue.\"\n    )\n    # Step 1: Clone or create the repo\n    repo_url = HfApi().create_repo(\n        repo_id=repo_id,\n        token=token,\n        private=False,\n        exist_ok=True,\n    )\n\n    with tempfile.TemporaryDirectory() as tmpdirname:\n        tmpdirname = Path(tmpdirname)"], "eval_score_GPT4": "5", "eval_feedback_GPT4": "Feedback: The response provided is completely correct, accurate, and factual. It aligns with the reference answer by stating that the default repository type is a model repository. Additionally, the response provides extra information that is accurate regarding the use of the `repo_type` parameter to change the type of repository and the format of the repository ID. This additional information does not detract from the correctness of the answer to the original question."}, {"question": "How many splits does the \"duorc\" dataset have?\n", "true_answer": "Six", "source_doc": "huggingface/datasets-server/blob/main/docs/source/splits.mdx", "generated_answer": "The \"duorc\" dataset has 3 splits, which are specified in the `splits` object in the endpoint response. The splits are named \"train\", \"validation\", and \"test\". The number of splits can be counted by iterating over the keys of the `splits` object, which would reveal three keys in this case. Therefore, the answer to the question is 3.\n\nHere's the code to confirm the number of splits using", "retrieved_docs": ["The endpoint response is a JSON containing the size of the dataset, as well as each of its configurations and splits. It provides the number of rows, the number of colums (where applicable) and the size in bytes for the different forms of the data: original files, size in memory (RAM) and auto-converted parquet files. For example, the [duorc](https://huggingface.co/datasets/duorc) dataset has 187.213 rows along all its configurations and splits, for a total of 97MB.", "\"answers\": {\n                \"feature\": {\n                    \"dtype\": \"string\",\n                    \"_type\": \"Value\"\n                },\n                \"_type\": \"Sequence\"\n            },\n            \"no_answer\": {\n                \"dtype\": \"bool\",\n                \"_type\": \"Value\"\n            }\n        },\n        \"builder_name\": \"duorc\",\n        \"config_name\": \"SelfRC\",\n        \"version\": {\n            \"version_str\": \"1.0.0\",\n            \"major\": 1,\n            \"minor\": 0,\n            \"patch\": 0\n        },\n        \"splits\": {\n            \"train\": {\n                \"name\": \"train\",\n                \"num_bytes\": 239852729,\n                \"num_examples\": 60721,\n                \"dataset_name\": \"duorc\"\n            },\n            \"validation\": {\n                \"name\": \"validation\",\n                \"num_bytes\": 51662519,\n                \"num_examples\": 12961,\n                \"dataset_name\": \"duorc\"\n            },\n            \"test\": {\n                \"name\": \"test\",\n                \"num_bytes\": 49142710,\n                \"num_examples\": 12559,\n                \"dataset_name\": \"duorc\"\n            }\n        },\n        \"download_checksums\": {\n            \"https://raw.githubusercontent.com/duorc/duorc/master/dataset/SelfRC_train.json\": {\n                \"num_bytes\": 24388192,", "List splits and configurations\n\nDatasets typically have splits and may also have configurations. A _split_ is a subset of the dataset, like `train` and `test`, that are used during different stages of training and evaluating a model. A _configuration_ is a sub-dataset contained within a larger dataset. Configurations are especially common in multilingual speech datasets where there may be a different configuration for each language. If you're interested in learning more about splits and configurations, check out the [Load a dataset from the Hub tutorial](https://huggingface.co/docs/datasets/main/en/load_hub)!\n\nThis guide shows you how to use Datasets Server's `/splits` endpoint to retrieve a dataset's splits and configurations programmatically. Feel free to also try it out with [Postman](https://www.postman.com/huggingface/workspace/hugging-face-apis/request/23242779-f0cde3b9-c2ee-4062-aaca-65c4cfdd96f8), [RapidAPI](https://rapidapi.com/hugging-face-hugging-face-default/api/hugging-face-datasets-api), or [ReDoc](https://redocly.github.io/redoc/?url=https://datasets-server.huggingface.co/openapi.json#operation/listSplits)\n\nThe `/splits` endpoint accepts the dataset name as its query parameter:", "## Further References \n\n- [WikiSplit dataset](https://huggingface.co/datasets/wiki_split)\n- [WikiSplit paper (Botha et al., 2018)](https://aclanthology.org/D18-1080.pdf)", "\"num_bytes\": 24388192,\n                \"checksum\": null\n            },\n            \"https://raw.githubusercontent.com/duorc/duorc/master/dataset/SelfRC_dev.json\": {\n                \"num_bytes\": 5051240,\n                \"checksum\": null\n            },\n            \"https://raw.githubusercontent.com/duorc/duorc/master/dataset/SelfRC_test.json\": {\n                \"num_bytes\": 5023228,\n                \"checksum\": null\n            }\n        },\n        \"download_size\": 34462660,\n        \"dataset_size\": 340657958,\n        \"size_in_bytes\": 375120618\n    }\n}\n```", "\"num_bytes_parquet_files\": 26005668,\n        \"num_bytes_memory\": 496682909,\n        \"num_rows\": 69524,\n        \"num_columns\": 7\n      },\n      {\n        \"dataset\": \"duorc\",\n        \"config\": \"ParaphraseRC\",\n        \"split\": \"validation\",\n        \"num_bytes_parquet_files\": 5566868,\n        \"num_bytes_memory\": 106510489,\n        \"num_rows\": 15591,\n        \"num_columns\": 7\n      },\n      {\n        \"dataset\": \"duorc\",\n        \"config\": \"ParaphraseRC\",\n        \"split\": \"test\",\n        \"num_bytes_parquet_files\": 6136591,\n        \"num_bytes_memory\": 115215760,\n        \"num_rows\": 15857,\n        \"num_columns\": 7\n      },\n      {\n        \"dataset\": \"duorc\",\n        \"config\": \"SelfRC\",\n        \"split\": \"train\",\n        \"num_bytes_parquet_files\": 14851720,\n        \"num_bytes_memory\": 239852729,\n        \"num_rows\": 60721,\n        \"num_columns\": 7\n      },\n      {\n        \"dataset\": \"duorc\",\n        \"config\": \"SelfRC\",\n        \"split\": \"validation\",\n        \"num_bytes_parquet_files\": 3114390,", "\"config\": \"SelfRC\",\n        \"split\": \"validation\",\n        \"num_bytes_parquet_files\": 3114390,\n        \"num_bytes_memory\": 51662519,\n        \"num_rows\": 12961,\n        \"num_columns\": 7\n      },\n      {\n        \"dataset\": \"duorc\",\n        \"config\": \"SelfRC\",\n        \"split\": \"test\",\n        \"num_bytes_parquet_files\": 3035736,\n        \"num_bytes_memory\": 49142710,\n        \"num_rows\": 12559,\n        \"num_columns\": 7\n      }\n    ]\n  },\n  \"pending\": [],\n  \"failed\": [],\n  \"partial\": false\n}\n```"], "eval_score_GPT4": "1", "eval_feedback_GPT4": "Feedback: The response provided is completely incorrect when compared to the reference answer. The response states that there are 3 splits in the \"duorc\" dataset, while the reference answer indicates that there are 6 splits. The evaluation is strictly based on the reference answer, and therefore, the response does not meet the criteria for correctness, accuracy, or factuality."}, {"question": "What is the purpose of Fully Sharded Data Parallel (FSDP) in distributed training?\n", "true_answer": "FSDP is developed for distributed training of large pretrained models up to 1T parameters by sharding the model parameters, gradients, and optimizer states across data parallel processes.", "source_doc": "huggingface/peft/blob/main/docs/source/accelerate/fsdp.md", "generated_answer": "Fully Sharded Data Parallel (FSDP) is a data parallel method used in distributed training to reduce memory usage and improve GPU memory-efficiency by sharding a model's parameters, gradients, and optimizer states across the available GPUs (also called workers or *rank*). Unlike DistributedDataParallel (DDP), FSDP allows for loading larger models that won't fit the available GPU memory by facilitating CPU offloading of all those tensors. During", "retrieved_docs": ["!--\u26a0\ufe0f Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\nrendered properly in your Markdown viewer.\n-->\n\n# Fully Sharded Data Parallel\n\n[Fully sharded data parallel](https://pytorch.org/docs/stable/fsdp.html) (FSDP) is developed for distributed training of large pretrained models up to 1T parameters. FSDP achieves this by sharding the model parameters, gradients, and optimizer states across data parallel processes and it can also offload sharded model parameters to a CPU. The memory efficiency afforded by FSDP allows you to scale training to larger batch or model sizes.\n\n<Tip warning={true}>\n\nCurrently, FSDP does not confer any reduction in GPU memory usage and FSDP with CPU offload actually consumes 1.65x more GPU memory during training. You can track this PyTorch [issue](https://github.com/pytorch/pytorch/issues/91165) for any updates.\n\n</Tip>\n\nFSDP is supported in \ud83e\udd17 Accelerate, and you can use it with \ud83e\udd17 PEFT. This guide will help you learn how to use our FSDP [training script](https://github.com/huggingface/peft/blob/main/examples/conditional_generation/peft_lora_seq2seq_accelerate_fsdp.py). You'll configure the script to train a large model for conditional generation.\n\n## Configuration\n\nBegin by running the following command to [create a FSDP configuration file](https://huggingface.co/docs/accelerate/main/en/usage_guides/fsdp) with \ud83e\udd17 Accelerate. Use the `--config_file` flag to save the configuration file to a specific location, otherwise it is saved as a `default_config.yaml` file in the \ud83e\udd17 Accelerate cache.\n\nThe configuration file is used to set the default options when you launch the training script.", "!--Copyright 2023 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\n\nhttp://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License.\n\n\u26a0\ufe0f Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\nrendered properly in your Markdown viewer.\n\n-->\n\n# Fully Sharded Data Parallel\n\n[Fully Sharded Data Parallel (FSDP)](https://pytorch.org/blog/introducing-pytorch-fully-sharded-data-parallel-api/) is a data parallel method that shards a model's parameters, gradients and optimizer states across the number of available GPUs (also called workers or *rank*). Unlike [DistributedDataParallel (DDP)](https://pytorch.org/docs/stable/generated/torch.nn.parallel.DistributedDataParallel.html), FSDP reduces memory-usage because a model is replicated on each GPU. This improves GPU memory-efficiency and allows you to train much larger models on fewer GPUs. FSDP is integrated with the Accelerate, a library for easily managing training in distributed environments, which means it is available for use from the [`Trainer`] class.\n\nBefore you start, make sure Accelerate is installed and at least PyTorch 2.1.0 or newer.\n\n```bash\npip install accelerate\n```\n\n## FSDP configuration", "FSDP precisely addresses this by sharding the optimizer states, gradients and model parameters across the data parallel workers. It further facilitates CPU offloading of all those tensors, thereby enabling loading large models which won't fit the available GPU memory. Similar to DDP, each worker gets a different batch of data. During the forward pass, if the CPU offload is enabled, the parameters of the local shard are first copied to the GPU/accelerator. Then, each worker performs all-gather operation for a given FSDP wrapped module/layer(s) to all get the needed parameters, computation is performed followed by releasing/emptying the parameter shards of other workers. This continues for all the FSDP modules. The loss gets computed after the forward pass and during the backward pass, again an all-gather operation is performed to get all the needed parameters for a given FSDP module, computation is performed to get local gradients followed by releasing the shards of other workers. Now, the local gradients are averaged and sharded to each relevant workers using reduce-scatter operation. This allows each worker to update the parameters of its local shard. If CPU offload is activated, the gradients are passed to CPU for updating parameters directly on CPU. \n\nPlease refer [7, 8, 9] for all the in-depth details on the workings of the PyTorch FSDP and the extensive experimentation carried out using this feature.\n\n# Issues\n\nIf you encounter any issues with the integration part of PyTorch FSDP, please open an Issue in [accelerate](https://github.com/huggingface/accelerate/issues).\n\nBut if you have problems with PyTorch FSDP configuration, and deployment - you need to ask the experts in their domains, therefore, please, open a [PyTorch Issue](https://github.com/pytorch/pytorch/issues) instead.\n\n\n# References\n\n[1] [Train Large, Then Compress: Rethinking Model Size for Efficient Training and Inference of Transformers](http://nlp.cs.berkeley.edu/pubs/Li-Wallace-Shen-Lin-Keutzer-Klein-Gonzalez_2020_Transformers_paper.pdf)", "## Next steps\n\nFSDP can be a powerful tool for training really large models and you have access to more than one GPU or TPU. By sharding the model parameters, optimizer and gradient states, and even offloading them to the CPU when they're inactive, FSDP can reduce the high cost of large-scale training. If you're interested in learning more, the following may be helpful:\n\n* Follow along with the more in-depth Accelerate guide for [FSDP](https://huggingface.co/docs/accelerate/usage_guides/fsdp).\n* Read the [Introducing PyTorch Fully Sharded Data Parallel (FSDP) API](https://pytorch.org/blog/introducing-pytorch-fully-sharded-data-parallel-api/) blog post.\n* Read the [Scaling PyTorch models on Cloud TPUs with FSDP](https://pytorch.org/blog/scaling-pytorch-models-on-cloud-tpus-with-fsdp/) blog post.", "--\ntitle: \"Accelerate Large Model Training using PyTorch Fully Sharded Data Parallel\"\nthumbnail: /blog/assets/62_pytorch_fsdp/fsdp-thumbnail.png\nauthors:\n- user: smangrul\n- user: sgugger\n---\n\n# Accelerate Large Model Training using PyTorch Fully Sharded Data Parallel\n\n\nIn this post we will look at how we can leverage **[Accelerate](https://github.com/huggingface/accelerate)** Library for training large models which enables users to leverage the latest features of **[PyTorch FullyShardedDataParallel (FSDP)](https://pytorch.org/blog/introducing-pytorch-fully-sharded-data-parallel-api/)**.\n\n# Motivation \ud83e\udd17\n\n**With the ever increasing scale, size and parameters of the Machine Learning (ML) models, ML practitioners are finding it difficult to train or even load such large models on their hardware.** On one hand, it has been found that large models learn quickly (data and compute efficient) and are significantly more performant when compared to smaller models [1]; on the other hand, it  becomes prohibitive to train such models on most of the available hardware.", "--\ntitle: \"Fine-tuning Llama 2 70B using PyTorch FSDP\" \nthumbnail: /blog/assets/160_fsdp_llama/thumbnail.jpg\nauthors:\n- user: smangrul\n- user: sgugger\n- user: lewtun\n- user: philschmid \n---\n# Fine-tuning Llama 2 70B using PyTorch FSDP\n\n\n## Introduction\n\nIn this blog post, we will look at how to fine-tune Llama 2 70B using PyTorch FSDP and related best practices. We will be leveraging Hugging Face Transformers, Accelerate and TRL. We will also learn how to use Accelerate with SLURM. \n\nFully Sharded Data Parallelism (FSDP) is a paradigm in which the optimizer states, gradients and parameters are sharded across devices. During the forward pass, each FSDP unit performs an _all-gather operation_ to get the complete weights, computation is performed followed by discarding the shards from other devices. After the forward pass, the loss is computed followed by the backward pass. In the backward pass, each FSDP unit performs an all-gather operation to get the complete weights, with computation performed to get the local gradients. These local gradients are averaged and sharded across the devices via a _reduce-scatter operation_ so that each device can update the parameters of its shard. For more information on what PyTorch FSDP is, please refer to this blog post: [Accelerate Large Model Training using PyTorch Fully Sharded Data Parallel](https://huggingface.co/blog/pytorch-fsdp).\n\n![FSDP Workflow](./assets/62_pytorch_fsdp/FSDP_workflow.png)\n\n(Source: [link](https://pytorch.org/blog/introducing-pytorch-fully-sharded-data-parallel-api/))\n\n## Hardware Used", "Distributed training is the key to enable training such large ML models. There have been major recent advances in the field of **Distributed Training at Scale**. Few the most notable advances are given below:\n1. Data Parallelism using ZeRO - Zero Redundancy Optimizer [2]\n    1. Stage 1: Shards optimizer states across data parallel workers/GPUs\n    2. Stage 2: Shards optimizer states + gradients across data parallel workers/GPUs\n    3. Stage 3: Shards optimizer states + gradients + model parameters across data parallel workers/GPUs\n    4. CPU Offload: Offloads the gradients + optimizer states to CPU building on top of ZERO Stage 2 [3]\n2. Tensor Parallelism [4]: Form of model parallelism wherein sharding parameters of individual layers with huge number of parameters across accelerators/GPUs is done in a clever manner to achieve parallel computation while avoiding expensive communication synchronization overheads. \n3. Pipeline Parallelism [5]: Form of model parallelism wherein different layers of the model are put across different accelerators/GPUs and pipelining is employed to keep all the accelerators running simultaneously. Here, for instance, the second accelerator/GPU computes on the first micro-batch while the first accelerator/GPU computes on the second micro-batch. \n4. 3D parallelism [3]: Employs Data Parallelism using ZERO + Tensor Parallelism + Pipeline Parallelism to train humongous models in the order of 100s of Billions of parameters. For instance, BigScience 176B parameters Language Model employ this [6]."], "eval_score_GPT4": "4", "eval_feedback_GPT4": "Feedback: The response provided is mostly correct, accurate, and factual. It correctly identifies FSDP as a data parallel method used in distributed training, mentions the sharding of model's parameters, gradients, and optimizer states across GPUs, and highlights the memory efficiency and the capability of handling larger models through CPU offloading. However, the response does not mention the specific capability of FSDP to handle models up to 1T parameters, which is a detail included in the reference answer. Therefore, while the response is accurate in describing the general purpose of FSDP, it lacks this specific detail about the scale of models it can handle."}, {"question": "What file format is used to save and store PyTorch model weights more securely than `.bin` files?\n", "true_answer": "`.safetensors`", "source_doc": "huggingface/safetensors/blob/main/docs/source/convert-weights.md", "generated_answer": "The file format used to save and store PyTorch model weights more securely than `.bin` files is `.safetensors`. This is recommended by Hugging Face for converting PyTorch model weights saved in `.bin` files using Python's `pickle` utility. The Convert Space provided by Hugging Face can be used to convert weights stored on the Hub, or the `convert.py` script can be run locally. Larger models may be", "retrieved_docs": ["Convert weights to safetensors\n\nPyTorch model weights are commonly saved and stored as `.bin` files with Python's [`pickle`](https://docs.python.org/3/library/pickle.html) utility. To save and store your model weights in the more secure `safetensor` format, we recommend converting your weights to `.safetensors`.\n\nThe easiest way to convert your model weights is to use the [Convert Space](https://huggingface.co/spaces/diffusers/convert), given your model weights are already stored on the Hub. The Convert Space downloads the pickled weights, converts them, and opens a Pull Request to upload the newly converted `.safetensors` file to your repository.\n\n<Tip warning={true}>\n\nFor larger models, the Space may be a bit slower because its resources are tied up in converting other models. You can also try running the [convert.py](https://github.com/huggingface/safetensors/blob/main/bindings/python/convert.py) script (this is what the Space is running) locally to convert your weights.\n\nFeel free to ping [@Narsil](https://huggingface.co/Narsil) for any issues with the Space.\n\n</Tip>", "### Models\n\nModels are designed as configurable toolboxes that are natural extensions of [PyTorch's Module class](https://pytorch.org/docs/stable/generated/torch.nn.Module.html). They only partly follow the **single-file policy**.", "### Models\n\nModels are designed as configurable toolboxes that are natural extensions of [PyTorch's Module class](https://pytorch.org/docs/stable/generated/torch.nn.Module.html). They only partly follow the **single-file policy**.", "Let's go back to Python for a bit to generate a model and tokenizer that we'd like to commit to our dummy repository:\n\n{#if fw === 'pt'}\n```py\nfrom transformers import AutoModelForMaskedLM, AutoTokenizer\n\ncheckpoint = \"camembert-base\"\n\nmodel = AutoModelForMaskedLM.from_pretrained(checkpoint)\ntokenizer = AutoTokenizer.from_pretrained(checkpoint)\n\n# Do whatever with the model, train it, fine-tune it...\n\nmodel.save_pretrained(\"<path_to_dummy_folder>\")\ntokenizer.save_pretrained(\"<path_to_dummy_folder>\")\n```\n{:else}\n```py\nfrom transformers import TFAutoModelForMaskedLM, AutoTokenizer\n\ncheckpoint = \"camembert-base\"\n\nmodel = TFAutoModelForMaskedLM.from_pretrained(checkpoint)\ntokenizer = AutoTokenizer.from_pretrained(checkpoint)\n\n# Do whatever with the model, train it, fine-tune it...\n\nmodel.save_pretrained(\"<path_to_dummy_folder>\")\ntokenizer.save_pretrained(\"<path_to_dummy_folder>\")\n```\n{/if}\n\nNow that we've saved some model and tokenizer artifacts, let's take another look at the *dummy* folder:\n\n```bash\nls\n```\n\n{#if fw === 'pt'}\n```bash\nconfig.json  pytorch_model.bin  README.md  sentencepiece.bpe.model  special_tokens_map.json tokenizer_config.json  tokenizer.json\n```\n\nIf you look at the file sizes (for example, with `ls -lh`), you should see that the model state dict file (*pytorch_model.bin*) is the only outlier, at more than 400 MB.", "In this precision, we can fit the model up to layer 21 on the GPU:\n\n```python out\n\n\n{'model.decoder.embed_tokens': 0,\n 'model.decoder.embed_positions': 0,\n 'model.decoder.final_layer_norm': 0,\n 'model.decoder.layers.0': 0,\n 'model.decoder.layers.1': 0,\n ...\n 'model.decoder.layers.21': 0,\n 'model.decoder.layers.22': 'cpu',\n ...\n 'model.decoder.layers.37': 'cpu',\n 'model.decoder.layers.38': 'disk',\n 'model.decoder.layers.39': 'disk',\n 'lm_head': 'disk'}\n```\n\nNow that we know where each weight is supposed to go, we can progressively load the pretrained weights inside the model.\n\n## Sharding state dicts\n\nTraditionally, PyTorch models are saved in a whole file containing a map from parameter name to weight. This map is often called a `state_dict`. Here is an excerpt from the [PyTorch documentation](https://pytorch.org/tutorials/beginner/basics/saveloadrun_tutorial.html) on saving on loading:\n\n```python\n# Save the model weights\ntorch.save(my_model.state_dict(), 'model_weights.pth')\n\n# Reload them\nnew_model = ModelClass()\nnew_model.load_state_dict(torch.load('model_weights.pth'))\n```\n\nThis works pretty well for models with less than 1 billion parameters, but for larger models, this is very taxing in RAM. The BLOOM model has 176 billions parameters; even with the weights saved in bfloat16 to save space, it still represents 352GB as a whole. While the super computer that trained this model might have this amount of memory available, requiring this for inference is unrealistic.", "This is it. `pytorch_model.bin` will now contain the full fp32 model weights consolidated from multiple GPUs.\n\nThe script will automatically be able to handle either a ZeRO-2 or ZeRO-3 checkpoint.\n\n`python zero_to_fp32.py -h` will give you usage details.\n\nThe script will auto-discover the deepspeed sub-folder using the contents of the file `latest`, which in the current\nexample will contain `global_step1`.\n\nNote: currently the script requires 2x general RAM of the final fp32 model weights.\n\n\n### ZeRO-3 and Infinity Nuances\n\nZeRO-3 is quite different from ZeRO-2 because of its param sharding feature.\n\nZeRO-Infinity further extends ZeRO-3 to support NVMe memory and multiple other speed and scalability improvements.\n\nWhile all the efforts were made for things to just work without needing any special changes to your models, in certain\ncircumstances you may find the following information to be needed.\n\n\n\n#### Constructing Massive Models\n\nDeepSpeed/ZeRO-3 can handle models with Trillions of parameters which may not fit onto the existing RAM. In such cases,\nbut also if you want the initialization to happen much faster, initialize the model using *deepspeed.zero.Init()*\ncontext manager (which is also a function decorator), like so:\n\n```python\nfrom transformers import T5ForConditionalGeneration, T5Config\nimport deepspeed\n\nwith deepspeed.zero.Init():\n    config = T5Config.from_pretrained(\"t5-small\")\n    model = T5ForConditionalGeneration(config)\n```\n\nAs you can see this gives you a randomly initialized model.", "class PyTorchModelHubMixin(ModelHubMixin):\n   (...)\n```\n\n2. Implement the `_save_pretrained` method:\n\n```py\nfrom huggingface_hub import ModelCard, ModelCardData\n\nclass PyTorchModelHubMixin(ModelHubMixin):\n   (...)\n\n   def _save_pretrained(self, save_directory: Path):\n      \"\"\"Generate Model Card and save weights from a Pytorch model to a local directory.\"\"\"\n      model_card = ModelCard.from_template(\n         card_data=ModelCardData(\n            license='mit',\n            library_name=\"pytorch\",\n            ...\n         ),\n         model_summary=...,\n         model_type=...,\n         ...\n      )\n      (save_directory / \"README.md\").write_text(str(model))\n      torch.save(obj=self.module.state_dict(), f=save_directory / \"pytorch_model.bin\")\n```\n\n3. Implement the `_from_pretrained` method:\n\n```python\nclass PyTorchModelHubMixin(ModelHubMixin):\n   (...)"], "eval_score_GPT4": "5", "eval_feedback_GPT4": "The response correctly identifies `.safetensors` as the file format used to save and store PyTorch model weights more securely than `.bin` files. The additional information provided about Hugging Face's recommendation and the methods for converting `.bin` files to `.safetensors` is accurate and relevant to the question, although it goes beyond the reference answer. The response is factual and aligns with the reference answer."}, {"question": "What type of security certification does Hugging Face have?\n", "true_answer": "SOC2 Type 2 certified", "source_doc": "huggingface/hub-docs/blob/main/docs/hub/security.md", "generated_answer": "Hugging Face is SOC2 Type 2 certified, which means they provide security certification to their customers and actively monitor and patch any security weaknesses. This information can be found in the \"Security\" section of the Hugging Face Hub documentation, as well as in the context provided.", "retrieved_docs": ["Security\n\nThe Hugging Face Hub offers several security features to ensure that your code and data are secure. Beyond offering [private repositories](./repositories-settings#private-repositories) for models, datasets, and Spaces, the Hub supports access tokens, commit signatures, and malware scanning.\n\nHugging Face is GDPR compliant. If a contract or specific data storage is something you'll need, we recommend taking a look at our [Expert Acceleration Program](https://huggingface.co/support). Hugging Face can also offer Business Associate Addendums or GDPR data processing agreements through an [Enterprise Plan](https://huggingface.co/pricing). \n\nHugging Face is also [SOC2 Type 2 certified](https://us.aicpa.org/interestareas/frc/assuranceadvisoryservices/aicpasoc2report.html), meaning we provide security certification to our customers and actively monitor and patch any security weaknesses.\n\n<img width=\"150\" src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/hub/security-soc-1.jpg\">\n\nFor any other security questions, please feel free to send us an email at security@huggingface.co.\n\n## Contents\n\n- [User Access Tokens](./security-tokens)\n- [Git over SSH](./security-git-ssh)\n- [Signing commits with GPG](./security-gpg)\n- [Single Sign-On (SSO)](./security-sso)\n- [Malware Scanning](./security-malware)\n- [Pickle Scanning](./security-pickle)\n- [Secrets Scanning](./security-secrets)", "Security & Compliance\n\n\ud83e\udd17 Inference Endpoints is built with security and secure inference at its core. Below you can find an overview of the security measures we have in place.\n\n## Data Security/Privacy\n\nHugging Face does not store any customer data in terms of payloads or tokens that are passed to the Inference Endpoint. We are storing logs for 30 days. Every Inference Endpoints uses TLS/SSL to encrypt the data in transit.\n\nWe also recommend using AWS or Azure Private Link for organizations. This allows you to access your Inference Endpoint through a private connection, without exposing it to the internet.\n\nHugging Face also offers Business Associate Addendum or GDPR data processing agreement through the Inference Endpoint enterprise plan.\n\n## Model Security/Privacy:\n\nYou can set a model repository as private if you do not want to publicly expose it. Hugging Face does not own any model or data you upload to the Hugging Face hub. Hugging Face does provide malware and pickle scans over the contents of the model repository as with all items in the Hub.\n\n## Inference Endpoints and Hub Security\n\nThe Hugging Face Hub, which Inference Endpoints is part, is also SOC2 Type 2 certified. The Hugging Face Hub offers Role Based Access Control. For more on hub security: https://huggingface.co/docs/hub/security\n\n<img width=\"150\" src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/hub/security-soc-1.jpg\">\n\n## Inference Endpoint Security level\n\nWe currently offer three types of endpoints, in order or increasing security level:\n\n- **Public**: A Public Endpoint is available from the internet, secured with TLS/SSL, and requires no authentication.\n- **Protected**: A Protected Endpoint is available from the internet, secured with TLS/SSL, and requires a valid Hugging Face token for authentication.\n- **Private** A Private Endpoint is only available through an intra-region secured AWS or Azure PrivateLink connection. Private Endpoints are not accessible from the internet.", "And stay tuned, as we will have a ControlNet Training event soon! Follow Hugging Face on [Twitter](https://twitter.com/huggingface) or join our [Discord]( http://hf.co/join/discord) to stay up to date on that.", "- **Does taking this course lead to a certification?**\nCurrently we do not have any certification for this course. However, we are working on a certification program for the Hugging Face ecosystem -- stay tuned!\n\n- **How much time should I spend on this course?**\nEach chapter in this course is designed to be completed in 1 week, with approximately 6-8 hours of work per week. However, you can take as much time as you need to complete the course.\n\n- **Where can I ask a question if I have one?**\nIf you have a question about any section of the course, just click on the \"*Ask a question*\" banner at the top of the page to be automatically redirected to the right section of the [Hugging Face forums](https://discuss.huggingface.co/):\n\n<img src=\"https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter1/forum-button.png\" alt=\"Link to the Hugging Face forums\" width=\"75%\">\n\nNote that a list of [project ideas](https://discuss.huggingface.co/c/course/course-event/25) is also available on the forums if you wish to practice more once you have completed the course.\n\n- **Where can I get the code for the course?**\nFor each section, click on the banner at the top of the page to run the code in either Google Colab or Amazon SageMaker Studio Lab:\n\n<img src=\"https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter1/notebook-buttons.png\" alt=\"Link to the Hugging Face course notebooks\" width=\"75%\">", "Check out [our badges](https://huggingface.co/datasets/huggingface/badges#sign-in-with-hugging-face) with explanations for integrating them in markdown or HTML.\n\n[![Sign in with Hugging Face](https://huggingface.co/datasets/huggingface/badges/resolve/main/sign-in-with-huggingface-sm.svg)](https://huggingface.co/oauth/authorize?client_id=CLIENT_ID&redirect_uri=REDIRECT_URI&scope=openid%20profile&state=STATE)\n[![Sign in with Hugging Face](https://huggingface.co/datasets/huggingface/badges/resolve/main/sign-in-with-huggingface-sm-dark.svg)](https://huggingface.co/oauth/authorize?client_id=CLIENT_ID&redirect_uri=REDIRECT_URI&scope=openid%20profile&state=STATE)\n\n[![Sign in with Hugging Face](https://huggingface.co/datasets/huggingface/badges/resolve/main/sign-in-with-huggingface-md.svg)](https://huggingface.co/oauth/authorize?client_id=CLIENT_ID&redirect_uri=REDIRECT_URI&scope=openid%20profile&state=STATE)\n[![Sign in with Hugging Face](https://huggingface.co/datasets/huggingface/badges/resolve/main/sign-in-with-huggingface-md-dark.svg)](https://huggingface.co/oauth/authorize?client_id=CLIENT_ID&redirect_uri=REDIRECT_URI&scope=openid%20profile&state=STATE)", "\ud83c\udf8e Network of peers with whom ambassadors can collaborate. \n\n\ud83e\uddd1\ud83c\udffb\u200d\ud83d\udcbb Workshops and support from the Hugging Face team! \n\n\ud83e\udd17 Insight into the latest projects, features, and more!\n\n\ud83c\udf81 Merchandise and assets. \n\n\u2728 Being officially recognized as a Hugging Face\u2019s Ambassador\n\n<br />\n\n**Eligibility Requirements for Students**\n\n- Validate your student status \n- Have taken at least one machine learning/data science course (online courses are considered as well)\n- Be enrolled in an accredited college or university\n- Be a user of the Hugging Face Hub and/or the Hugging Face\u2019s libraries\n- Acknowledge the [Code of Conduct](https://huggingface2.notion.site/huggingface2/Hugging-Face-Code-of-Conduct-45eeeafa9ef44c5e888a2952619fdfa8). Community is at the center of the Hugging Face ecosystem. Because of that, we strictly adhere to our [Code of conduct](https://huggingface2.notion.site/huggingface2/Hugging-Face-Code-of-Conduct-45eeeafa9ef44c5e888a2952619fdfa8). If any ambassador infringes it or behaves inadequately, they will be excluded from the Program.\n\n**[Apply here](https://docs.google.com/forms/d/e/1FAIpQLScY9kTi-TjZipRFRviluRCwSjFf3CCsMbKedzO1tq2S0wtbNQ/viewform?usp=sf_link) to become an ambassador!**\n\n**Timeline:**\n\n- Deadline for the end of the [application](https://docs.google.com/forms/d/e/1FAIpQLScY9kTi-TjZipRFRviluRCwSjFf3CCsMbKedzO1tq2S0wtbNQ/viewform?usp=sf_link) is June 13.\n- The Program will start on June 30, 2022.\n- The Program will end on December 31, 2022.", "!---\nCopyright 2020 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\");\nyou may not use this file except in compliance with the License.\nYou may obtain a copy of the License at\n\n    http://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software\ndistributed under the License is distributed on an \"AS IS\" BASIS,\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\nSee the License for the specific language governing permissions and\nlimitations under the License.\n-->\n\n<p align=\"center\">\n  <picture>\n    <source media=\"(prefers-color-scheme: dark)\" srcset=\"https://huggingface.co/datasets/huggingface/documentation-images/raw/main/transformers-logo-dark.svg\">\n    <source media=\"(prefers-color-scheme: light)\" srcset=\"https://huggingface.co/datasets/huggingface/documentation-images/raw/main/transformers-logo-light.svg\">\n    <img alt=\"Hugging Face Transformers Library\" src=\"https://huggingface.co/datasets/huggingface/documentation-images/raw/main/transformers-logo-light.svg\" width=\"352\" height=\"59\" style=\"max-width: 100%;\">\n  </picture>\n  <br/>\n  <br/>\n</p>"], "eval_score_GPT4": "5", "eval_feedback_GPT4": "Feedback: The response correctly identifies Hugging Face as having SOC2 Type 2 certification, which is in line with the reference answer. The additional information provided about the certification's implications and where to find more details is accurate and enhances the factual content of the response without deviating from the correct answer."}, {"question": "What do RAG models combine to generate outputs?\n", "true_answer": "Pretrained dense retrieval (DPR) and sequence-to-sequence models.", "source_doc": "huggingface/transformers/blob/main/docs/source/en/model_doc/rag.md", "generated_answer": "The question asks what RAG models combine to generate outputs.\n\nAnswer: RAG models combine a question encoder, a context encoder (retriever component), and a generator to generate outputs. In the original RAG implementation, these components were trained separately. However, this extension enables complete end-to-end training of RAG, including the context encoder in the retriever component. This allows for finetuning of the entire RAG architecture, including the D", "retrieved_docs": ["## RagConfig\n\n[[autodoc]] RagConfig\n\n## RagTokenizer\n\n[[autodoc]] RagTokenizer\n\n## Rag specific outputs\n\n[[autodoc]] models.rag.modeling_rag.RetrievAugLMMarginOutput\n\n[[autodoc]] models.rag.modeling_rag.RetrievAugLMOutput\n\n## RagRetriever\n\n[[autodoc]] RagRetriever\n\n<frameworkcontent>\n<pt>\n\n## RagModel\n\n[[autodoc]] RagModel\n    - forward\n\n## RagSequenceForGeneration\n\n[[autodoc]] RagSequenceForGeneration\n    - forward\n    - generate\n\n## RagTokenForGeneration\n\n[[autodoc]] RagTokenForGeneration\n    - forward\n    - generate\n\n</pt>\n<tf>\n\n## TFRagModel\n\n[[autodoc]] TFRagModel\n    - call\n\n## TFRagSequenceForGeneration\n\n[[autodoc]] TFRagSequenceForGeneration\n    - call\n    - generate\n\n## TFRagTokenForGeneration\n\n[[autodoc]] TFRagTokenForGeneration\n    - call\n    - generate\n\n</tf>\n</frameworkcontent>", "python examples/rag/finetune_rag.py \\\n    --data_dir $DATA_DIR \\\n    --output_dir $OUTPUT_DIR \\\n    --model_name_or_path $MODEL_NAME_OR_PATH \\\n    --model_type rag_sequence \\\n    --fp16 \\\n    --gpus 8 \\\n    --profile \\\n    --do_train \\\n    --do_predict \\\n    --n_val -1 \\\n    --train_batch_size 8 \\\n    --eval_batch_size 1 \\\n    --max_source_length 128 \\\n    --max_target_length 25 \\\n    --val_max_target_length 25 \\\n    --test_max_target_length 25 \\\n    --label_smoothing 0.1 \\\n    --dropout 0.1 \\\n    --attention_dropout 0.1 \\\n    --weight_decay 0.001 \\\n    --adam_epsilon 1e-08 \\\n    --max_grad_norm 0.1 \\\n    --lr_scheduler polynomial \\\n    --learning_rate 3e-05 \\\n    --num_train_epochs 100 \\\n    --warmup_steps 500 \\\n    --gradient_accumulation_steps 1 \\\n    --distributed_retriever ray \\\n    --num_retrieval_workers 4\n\n# Stop the Ray cluster.\nray stop\n```\n\n## What\u2019s next?\n\nUsing RAG with [Huggingface transformers](https://github.com/huggingface/transformers/tree/master/examples/research_projects/rag) and the [Ray retrieval implementation](https://github.com/huggingface/transformers/blob/master/examples/research_projects/rag/finetune_rag_ray.sh) for faster distributed fine-tuning, you can leverage RAG for retrieval-based generation on your own knowledge-intensive tasks.", "[[autodoc]] modeling_outputs.Wav2Vec2BaseModelOutput\n\n## XVectorOutput\n\n[[autodoc]] modeling_outputs.XVectorOutput\n\n## Seq2SeqTSModelOutput\n\n[[autodoc]] modeling_outputs.Seq2SeqTSModelOutput\n\n## Seq2SeqTSPredictionOutput\n\n[[autodoc]] modeling_outputs.Seq2SeqTSPredictionOutput\n\n## SampleTSPredictionOutput\n\n[[autodoc]] modeling_outputs.SampleTSPredictionOutput\n\n## TFBaseModelOutput\n\n[[autodoc]] modeling_tf_outputs.TFBaseModelOutput\n\n## TFBaseModelOutputWithPooling\n\n[[autodoc]] modeling_tf_outputs.TFBaseModelOutputWithPooling\n\n## TFBaseModelOutputWithPoolingAndCrossAttentions\n\n[[autodoc]] modeling_tf_outputs.TFBaseModelOutputWithPoolingAndCrossAttentions\n\n## TFBaseModelOutputWithPast\n\n[[autodoc]] modeling_tf_outputs.TFBaseModelOutputWithPast\n\n## TFBaseModelOutputWithPastAndCrossAttentions\n\n[[autodoc]] modeling_tf_outputs.TFBaseModelOutputWithPastAndCrossAttentions\n\n## TFSeq2SeqModelOutput\n\n[[autodoc]] modeling_tf_outputs.TFSeq2SeqModelOutput\n\n## TFCausalLMOutput\n\n[[autodoc]] modeling_tf_outputs.TFCausalLMOutput\n\n## TFCausalLMOutputWithCrossAttentions", "A sample finetuning command (run ` ./examples/research_projects/rag/finetune_rag.py --help` to list all available options):\n\n```bash\npython examples/research_projects/rag/finetune_rag.py \\\n    --data_dir $DATA_DIR \\\n    --output_dir $OUTPUT_DIR \\\n    --model_name_or_path $MODEL_NAME_OR_PATH \\\n    --model_type rag_sequence \\\n    --fp16 \\\n    --gpus 8\n```\nWe publish two `base` models which can serve as a starting point for finetuning on downstream tasks (use them as `model_name_or_path`):\n- [`facebook/rag-sequence-base`](https://huggingface.co/facebook/rag-sequence-base) - a base for finetuning `RagSequenceForGeneration` models,\n- [`facebook/rag-token-base`](https://huggingface.co/facebook/rag-token-base) - a base for finetuning `RagTokenForGeneration` models.\n\nThe `base` models initialize the question encoder with [`facebook/dpr-question_encoder-single-nq-base`](https://huggingface.co/facebook/dpr-question_encoder-single-nq-base) and the generator with [`facebook/bart-large`](https://huggingface.co/facebook/bart-large).", "End-to-End finetuning of RAG (including DPR retriever) for Question Answering.\n\nThis finetuning script is actively maintained by [Shamane Siri](https://github.com/shamanez). Feel free to ask questions on the [Forum](https://discuss.huggingface.co/) or post an issue on [GitHub](https://github.com/huggingface/transformers/issues/new/choose) and tag @shamanez.\n\nOthers that helped out: Patrick von Platen (@patrickvonplaten), Quentin Lhoest (@lhoestq), and Rivindu Weerasekera (@rivinduw)\n\nThe original RAG implementation is able to train the question encoder and generator end-to-end.\nThis extension enables complete end-to-end training of RAG including the context encoder in the retriever component.\nPlease read the [accompanying blog post](https://shamanesiri.medium.com/how-to-finetune-the-entire-rag-architecture-including-dpr-retriever-4b4385322552) for details on this implementation.\n\nThe original RAG code has also been modified to work with the latest versions of pytorch lightning (version 1.2.10) and RAY (version 1.3.0). All other implementation details remain the same as the [original RAG code](https://github.com/huggingface/transformers/tree/main/examples/research_projects/rag).\nRead more about RAG  at https://arxiv.org/abs/2005.11401.\n\nThis code can be modified to experiment with other research on retrival augmented models which include training of the retriever (e.g. [REALM](https://arxiv.org/abs/2002.08909) and [MARGE](https://arxiv.org/abs/2006.15020)).\n\nTo start training, use the bash script (finetune_rag_ray_end2end.sh) in this folder. This script also includes descriptions on each command-line argument used.\n\n# Latest Update", "[[autodoc]] modeling_tf_outputs.TFCausalLMOutput\n\n## TFCausalLMOutputWithCrossAttentions\n\n[[autodoc]] modeling_tf_outputs.TFCausalLMOutputWithCrossAttentions\n\n## TFCausalLMOutputWithPast\n\n[[autodoc]] modeling_tf_outputs.TFCausalLMOutputWithPast\n\n## TFMaskedLMOutput\n\n[[autodoc]] modeling_tf_outputs.TFMaskedLMOutput\n\n## TFSeq2SeqLMOutput\n\n[[autodoc]] modeling_tf_outputs.TFSeq2SeqLMOutput\n\n## TFNextSentencePredictorOutput\n\n[[autodoc]] modeling_tf_outputs.TFNextSentencePredictorOutput\n\n## TFSequenceClassifierOutput\n\n[[autodoc]] modeling_tf_outputs.TFSequenceClassifierOutput\n\n## TFSeq2SeqSequenceClassifierOutput\n\n[[autodoc]] modeling_tf_outputs.TFSeq2SeqSequenceClassifierOutput\n\n## TFMultipleChoiceModelOutput\n\n[[autodoc]] modeling_tf_outputs.TFMultipleChoiceModelOutput\n\n## TFTokenClassifierOutput\n\n[[autodoc]] modeling_tf_outputs.TFTokenClassifierOutput\n\n## TFQuestionAnsweringModelOutput\n\n[[autodoc]] modeling_tf_outputs.TFQuestionAnsweringModelOutput\n\n## TFSeq2SeqQuestionAnsweringModelOutput", "To start training, use the bash script (finetune_rag_ray_end2end.sh) in this folder. This script also includes descriptions on each command-line argument used.\n\n# Latest Update\n\n\u26a0\ufe0f Updated the rag-end2end-retriever to be compatible with PL==1.6.4 and RAY==1.13.0 (latest versions to the date 2022-June-11)\n\n# Note\n\n\u26a0\ufe0f This project should be run with pytorch-lightning==1.3.1 which has a potential security vulnerability\n\n# Testing\n\nThe following two bash scripts can be used to quickly test the implementation.\n1. sh ./test_run/test_finetune.sh script\n    - Tests the full end-to-end fine-tuning ability with a dummy knowlendge-base and dummy training dataset (check test_dir directory).\n    - Users can replace the dummy dataset and knowledge-base with their own to do their own finetuning.\n    - Please read the comments in the test_finetune.sh file.\n2. sh ./test_run/test_rag_new_features.sh\n    - Tests the newly added functions (set_context_encoder and set_context_encoder_tokenizer) related to modeling rag.\n    - This is sufficient to check the model's ability to use the set functions correctly.\n\n\n\n# Comparison of end2end RAG (including DPR finetuning)  VS original-RAG\n\nWe conducted a simple experiment to investigate the effectiveness of this end2end training extension using the SQuAD dataset. Please execute the following steps to reproduce the results."], "eval_score_GPT4": "4", "eval_feedback_GPT4": "Feedback: The response correctly identifies the components of RAG models by mentioning a question encoder, a context encoder (retriever component), and a generator. This aligns with the reference answer, which specifies pretrained dense retrieval (DPR) and sequence-to-sequence models, as the context encoder and generator are part of these systems. However, the response goes into detail about the training process, which, while informative, is not required by the question. The question specifically asks about the combination of components in RAG models, not the training process. Despite this, the response does not stray from accuracy and remains factual."}, {"question": "What library does MarkupLMFeatureExtractor use to extract data from HTML and XML files?\n", "true_answer": "Beautiful Soup", "source_doc": "huggingface/transformers/blob/main/docs/source/en/model_doc/markuplm.md", "generated_answer": "The extractor used by `MarkupLMFeatureExtractor` to pull data out of HTML and XML files is Beautiful Soup, a Python library for parsing and extracting data from HTML and XML documents. This information can be found in the context provided.\n\nTo answer the question, you can follow these steps:\n1. Read through the context provided to understand the concept of MarkupLM and its components.\n2. Look for information about the feature extractor used by `", "retrieved_docs": ["[`MarkupLMFeatureExtractor`] uses [Beautiful Soup](https://www.crummy.com/software/BeautifulSoup/bs4/doc/), a Python library for\npulling data out of HTML and XML files, under the hood. Note that you can still use your own parsing solution of\nchoice, and provide the nodes and xpaths yourself to [`MarkupLMTokenizer`] or [`MarkupLMTokenizerFast`].\n\nIn total, there are 5 use cases that are supported by the processor. Below, we list them all. Note that each of these\nuse cases work for both batched and non-batched inputs (we illustrate them for non-batched inputs).\n\n**Use case 1: web page classification (training, inference) + token classification (inference), parse_html = True**\n\nThis is the simplest case, in which the processor will use the feature extractor to get all nodes and xpaths from the HTML.\n\n```python\n>>> from transformers import MarkupLMProcessor\n\n>>> processor = MarkupLMProcessor.from_pretrained(\"microsoft/markuplm-base\")\n\n>>> html_string = \"\"\"\n...  <!DOCTYPE html>\n...  <html>\n...  <head>\n...  <title>Hello world</title>\n...  </head>\n...  <body>\n...  <h1>Welcome</h1>\n...  <p>Here is my website.</p>\n...  </body>\n...  </html>\"\"\"\n\n>>> # note that you can also add provide all tokenizer parameters here such as padding, truncation\n>>> encoding = processor(html_string, return_tensors=\"pt\")\n>>> print(encoding.keys())\ndict_keys(['input_ids', 'token_type_ids', 'attention_mask', 'xpath_tags_seq', 'xpath_subs_seq'])\n```", "## Usage: MarkupLMProcessor\n\nThe easiest way to prepare data for the model is to use [`MarkupLMProcessor`], which internally combines a feature extractor\n([`MarkupLMFeatureExtractor`]) and a tokenizer ([`MarkupLMTokenizer`] or [`MarkupLMTokenizerFast`]). The feature extractor is\nused to extract all nodes and xpaths from the HTML strings, which are then provided to the tokenizer, which turns them into the\ntoken-level inputs of the model (`input_ids` etc.). Note that you can still use the feature extractor and tokenizer separately,\nif you only want to handle one of the two tasks.\n\n```python\nfrom transformers import MarkupLMFeatureExtractor, MarkupLMTokenizerFast, MarkupLMProcessor\n\nfeature_extractor = MarkupLMFeatureExtractor()\ntokenizer = MarkupLMTokenizerFast.from_pretrained(\"microsoft/markuplm-base\")\nprocessor = MarkupLMProcessor(feature_extractor, tokenizer)\n```\n\nIn short, one can provide HTML strings (and possibly additional data) to [`MarkupLMProcessor`],\nand it will create the inputs expected by the model. Internally, the processor first uses\n[`MarkupLMFeatureExtractor`] to get a list of nodes and corresponding xpaths. The nodes and\nxpaths are then provided to [`MarkupLMTokenizer`] or [`MarkupLMTokenizerFast`], which converts them\nto token-level `input_ids`, `attention_mask`, `token_type_ids`, `xpath_subs_seq`, `xpath_tags_seq`.\nOptionally, one can provide node labels to the processor, which are turned into token-level `labels`.", "```python\n>>> from transformers import MarkupLMProcessor\n\n>>> processor = MarkupLMProcessor.from_pretrained(\"microsoft/markuplm-base\")\n>>> processor.parse_html = False\n\n>>> nodes = [\"hello\", \"world\", \"how\", \"are\"]\n>>> xpaths = [\"/html/body/div/li[1]/div/span\", \"/html/body/div/li[1]/div/span\", \"html/body\", \"html/body/div\"]\n>>> node_labels = [1, 2, 2, 1]\n>>> encoding = processor(nodes=nodes, xpaths=xpaths, node_labels=node_labels, return_tensors=\"pt\")\n>>> print(encoding.keys())\ndict_keys(['input_ids', 'token_type_ids', 'attention_mask', 'xpath_tags_seq', 'xpath_subs_seq', 'labels'])\n```\n\n**Use case 4: web page question answering (inference), parse_html=True**\n\nFor question answering tasks on web pages, you can provide a question to the processor. By default, the\nprocessor will use the feature extractor to get all nodes and xpaths, and create [CLS] question tokens [SEP] word tokens [SEP].\n\n```python\n>>> from transformers import MarkupLMProcessor\n\n>>> processor = MarkupLMProcessor.from_pretrained(\"microsoft/markuplm-base\")\n\n>>> html_string = \"\"\"\n...  <!DOCTYPE html>\n...  <html>\n...  <head>\n...  <title>Hello world</title>\n...  </head>\n...  <body>\n...  <h1>Welcome</h1>\n...  <p>My name is Niels.</p>\n...  </body>\n...  </html>\"\"\"", "The abstract from the paper is the following:\n\n*Multimodal pre-training with text, layout, and image has made significant progress for Visually-rich Document\nUnderstanding (VrDU), especially the fixed-layout documents such as scanned document images. While, there are still a\nlarge number of digital documents where the layout information is not fixed and needs to be interactively and\ndynamically rendered for visualization, making existing layout-based pre-training approaches not easy to apply. In this\npaper, we propose MarkupLM for document understanding tasks with markup languages as the backbone such as\nHTML/XML-based documents, where text and markup information is jointly pre-trained. Experiment results show that the\npre-trained MarkupLM significantly outperforms the existing strong baseline models on several document understanding\ntasks. The pre-trained model and code will be publicly available.*\n\nThis model was contributed by [nielsr](https://huggingface.co/nielsr). The original code can be found [here](https://github.com/microsoft/unilm/tree/master/markuplm).\n\n## Usage tips\n\n- In addition to `input_ids`, [`~MarkupLMModel.forward`] expects 2 additional inputs, namely `xpath_tags_seq` and `xpath_subs_seq`.\nThese are the XPATH tags and subscripts respectively for each token in the input sequence.\n- One can use [`MarkupLMProcessor`] to prepare all data for the model. Refer to the [usage guide](#usage-markuplmprocessor) for more info.\n\n<img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/model_doc/markuplm_architecture.jpg\"\nalt=\"drawing\" width=\"600\"/> \n\n<small> MarkupLM architecture. Taken from the <a href=\"https://arxiv.org/abs/2110.08518\">original paper.</a> </small>\n\n## Usage: MarkupLMProcessor", "**Use case 2: web page classification (training, inference) + token classification (inference), parse_html=False**\n\nIn case one already has obtained all nodes and xpaths, one doesn't need the feature extractor. In that case, one should\nprovide the nodes and corresponding xpaths themselves to the processor, and make sure to set `parse_html` to `False`.\n\n```python\n>>> from transformers import MarkupLMProcessor\n\n>>> processor = MarkupLMProcessor.from_pretrained(\"microsoft/markuplm-base\")\n>>> processor.parse_html = False\n\n>>> nodes = [\"hello\", \"world\", \"how\", \"are\"]\n>>> xpaths = [\"/html/body/div/li[1]/div/span\", \"/html/body/div/li[1]/div/span\", \"html/body\", \"html/body/div\"]\n>>> encoding = processor(nodes=nodes, xpaths=xpaths, return_tensors=\"pt\")\n>>> print(encoding.keys())\ndict_keys(['input_ids', 'token_type_ids', 'attention_mask', 'xpath_tags_seq', 'xpath_subs_seq'])\n```\n\n**Use case 3: token classification (training), parse_html=False**\n\nFor token classification tasks (such as [SWDE](https://paperswithcode.com/dataset/swde)), one can also provide the\ncorresponding node labels in order to train a model. The processor will then convert these into token-level `labels`.\nBy default, it will only label the first wordpiece of a word, and label the remaining wordpieces with -100, which is the\n`ignore_index` of PyTorch's CrossEntropyLoss. In case you want all wordpieces of a word to be labeled, you can\ninitialize the tokenizer with `only_label_first_subword` set to `False`.", "Gradio Demo: markdown_example\n\n\n```\n!pip install -q gradio \n```\n\n\n```\nimport gradio as gr\n\ncss = (\n    \"footer {display: none !important;} .gradio-container {min-height: 0px !important;}\"\n)\n\n# sample md stolen from https://dillinger.io/\n\nmd = \"\"\"# Dillinger\n## _The Last Markdown Editor, Ever_\n\nThis is some `inline code`, it is good.\n\n[![Build Status](https://travis-ci.org/joemccann/dillinger.svg?branch=master)](https://travis-ci.org/joemccann/dillinger)\n\nDillinger is a cloud-enabled, mobile-ready, offline-storage compatible,\nAngularJS-powered HTML5 Markdown editor.\n\n- Type some Markdown on the left\n- See HTML in the right\n- \u2728Magic \u2728\n\n## Features\n\n- Import a HTML file and watch it magically convert to Markdown\n- Drag and drop images (requires your Dropbox account be linked)\n- Import and save files from GitHub, Dropbox, Google Drive and One Drive\n- Drag and drop markdown and HTML files into Dillinger\n- Export documents as Markdown, HTML and PDF\n\nMarkdown is a lightweight markup language based on the formatting conventions\nthat people naturally use in email.\nAs [John Gruber] writes on the [Markdown site][df1]\n\n> The overriding design goal for Markdown's\n> formatting syntax is to make it as readable\n> as possible. The idea is that a\n> Markdown-formatted document should be\n> publishable as-is, as plain text, without\n> looking like it's been marked up with tags\n> or formatting instructions.\n\nThis text you see here is *actually- written in Markdown! To get a feel\nfor Markdown's syntax, type some text into the left window and\nwatch the results in the right.\n\n## Tech\n\nDillinger uses a number of open source projects to work properly:", "!--Copyright 2022 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\n\nhttp://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License.\n\n\u26a0\ufe0f Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\nrendered properly in your Markdown viewer.\n\n-->\n\n# MarkupLM\n\n## Overview\n\nThe MarkupLM model was proposed in [MarkupLM: Pre-training of Text and Markup Language for Visually-rich Document\nUnderstanding](https://arxiv.org/abs/2110.08518) by Junlong Li, Yiheng Xu, Lei Cui, Furu Wei. MarkupLM is BERT, but\napplied to HTML pages instead of raw text documents. The model incorporates additional embedding layers to improve\nperformance, similar to [LayoutLM](layoutlm).\n\nThe model can be used for tasks like question answering on web pages or information extraction from web pages. It obtains\nstate-of-the-art results on 2 important benchmarks:\n- [WebSRC](https://x-lance.github.io/WebSRC/), a dataset for Web-Based Structural Reading Comprehension (a bit like SQuAD but for web pages)\n- [SWDE](https://www.researchgate.net/publication/221299838_From_one_tree_to_a_forest_a_unified_solution_for_structured_web_data_extraction), a dataset\nfor information extraction from web pages (basically named-entity recogntion on web pages)\n\nThe abstract from the paper is the following:"], "eval_score_GPT4": "5", "eval_feedback_GPT4": "Feedback: The response correctly identifies Beautiful Soup as the library used by `MarkupLMFeatureExtractor` to extract data from HTML and XML files, which aligns with the reference answer. The additional information provided about the steps to find this information is not necessary for the evaluation but does not detract from the fact that the correct library is mentioned."}, {"question": "What is the file size limit for syncing to HF Spaces without using Git-LFS?\n", "true_answer": "10MB", "source_doc": "huggingface/hub-docs/blob/main/docs/hub/spaces-github-actions.md", "generated_answer": "According to the context provided, there is a file size limit of 10MB for syncing to HF Spaces without using Git-LFS. If files are larger than 10MB, it is recommended to use Git-LFS to handle files larger than this limit. This is mentioned in the section titled \"Managing Spaces with Github Actions\" in the extracted document with the source number 0. The recommendation is also mentioned in the section titled \"Hard", "retrieved_docs": ["Managing Spaces with Github Actions\n\nYou can keep your app in sync with your GitHub repository with **Github Actions**. Remember that for files larger than 10MB, Spaces requires Git-LFS. If you don't want to use Git-LFS, you may need to review your files and check your history. Use a tool like [BFG Repo-Cleaner](https://rtyley.github.io/bfg-repo-cleaner/) to remove any large files from your history. BFG Repo-Cleaner will keep a local copy of your repository as a backup.\n\nFirst, you should set up your GitHub repository and Spaces app together. Add your Spaces app as an additional remote to your existing Git repository.\n\n```bash\ngit remote add space https://huggingface.co/spaces/HF_USERNAME/SPACE_NAME\n```\n\nThen force push to sync everything for the first time:\n\n```bash\ngit push --force space main\n```\n\nNext, set up a GitHub Action to push your main branch to Spaces. In the example below:\n\n* Replace `HF_USERNAME` with your username and `SPACE_NAME` with your Space name. \n* Create a [Github secret](https://docs.github.com/en/actions/security-guides/encrypted-secrets#creating-encrypted-secrets-for-an-environment) with your `HF_TOKEN`. You can find your Hugging Face API token under **API Tokens** on your Hugging Face profile.\n\n```yaml\nname: Sync to Hugging Face hub\non:\n  push:\n    branches: [main]\n\n  # to run this workflow manually from the Actions tab\n  workflow_dispatch:", "</Tip>\n\n## (legacy) Upload files with Git LFS\n\nAll the methods described above use the Hub's API to upload files. This is the recommended way to upload files to the Hub.\nHowever, we also provide [`Repository`], a wrapper around the git tool to manage a local repository.\n\n<Tip warning={true}>\n\nAlthough [`Repository`] is not formally deprecated, we recommend using the HTTP-based methods described above instead.\nFor more details about this recommendation, please have a look at [this guide](../concepts/git_vs_http) explaining the\ncore differences between HTTP-based and Git-based approaches.\n\n</Tip>\n\nGit LFS automatically handles files larger than 10MB. But for very large files (>5GB), you need to install a custom transfer agent for Git LFS:\n\n```bash\nhuggingface-cli lfs-enable-largefiles\n```\n\nYou should install this for each repository that has a very large file. Once installed, you'll be able to push files larger than 5GB.\n\n### commit context manager\n\nThe `commit` context manager handles four of the most common Git commands: pull, add, commit, and push. `git-lfs` automatically tracks any file larger than 10MB. In the following example, the `commit` context manager:\n\n1. Pulls from the `text-files` repository.\n2. Adds a change made to `file.txt`.\n3. Commits the change.\n4. Pushes the change to the `text-files` repository.\n\n```python\n>>> from huggingface_hub import Repository\n>>> with Repository(local_dir=\"text-files\", clone_from=\"<user>/text-files\").commit(commit_message=\"My first file :)\"):\n...     with open(\"file.txt\", \"w+\") as f:\n...         f.write(json.dumps({\"hey\": 8}))\n```", "- **Repository size**: The total size of the data you're planning to upload. There is no hard limit on a Hub repository size. However, if you plan to upload hundreds of GBs or even TBs of data, we would appreciate it if you could let us know in advance so we can better help you if you have any questions during the process. You can contact us at datasets@huggingface.co or on [our Discord](http://hf.co/join/discord).\n- **Number of files**:\n    - For optimal experience, we recommend keeping the total number of files under 100k. Try merging the data into fewer files if you have more.\n      For example, json files can be merged into a single jsonl file, or large datasets can be exported as Parquet files.\n    - The maximum number of files per folder cannot exceed 10k files per folder. A simple solution is to\n      create a repository structure that uses subdirectories. For example, a repo with 1k folders from `000/` to `999/`, each containing at most 1000 files, is already enough.\n- **File size**: In the case of uploading large files (e.g. model weights), we strongly recommend splitting them **into chunks of around 5GB each**.\nThere are a few reasons for this:\n    - Uploading and downloading smaller files is much easier both for you and the other users. Connection issues can always\n      happen when streaming data and smaller files avoid resuming from the beginning in case of errors.\n    - Files are served to the users using CloudFront. From our experience, huge files are not cached by this service\n      leading to a slower download speed.\nIn all cases no single LFS file will be able to be >50GB. I.e. 50GB is the hard limit for single file size.\n- **Number of commits**: There is no hard limit for the total number of commits on your repo history. However, from", "```yaml\nname: Sync to Hugging Face hub\non:\n  push:\n    branches: [main]\n\n  # to run this workflow manually from the Actions tab\n  workflow_dispatch:\n\njobs:\n  sync-to-hub:\n    runs-on: ubuntu-latest\n    steps:\n      - uses: actions/checkout@v3\n        with:\n          fetch-depth: 0\n          lfs: true\n      - name: Push to hub\n        env:\n          HF_TOKEN: ${{ secrets.HF_TOKEN }}\n        run: git push https://HF_USERNAME:$HF_TOKEN@huggingface.co/spaces/HF_USERNAME/SPACE_NAME main\n```\n\nFinally, create an Action that automatically checks the file size of any new pull request:\n\n\n```yaml\nname: Check file size\non:               # or directly `on: [push]` to run the action on every push on any branch\n  pull_request:\n    branches: [main]\n\n  # to run this workflow manually from the Actions tab\n  workflow_dispatch:\n\njobs:\n  sync-to-hub:\n    runs-on: ubuntu-latest\n    steps:\n      - name: Check large files\n        uses: ActionsDesk/lfs-warning@v2.0\n        with:\n          filesizelimit: 10485760 # this is 10MB so we can sync to HF Spaces\n```", "- **Number of commits**: There is no hard limit for the total number of commits on your repo history. However, from\nour experience, the user experience on the Hub starts to degrade after a few thousand commits. We are constantly working to\nimprove the service, but one must always remember that a git repository is not meant to work as a database with a lot of\nwrites. If your repo's history gets very large, it is always possible to squash all the commits to get a\nfresh start using `huggingface_hub`'s [`super_squash_history`](https://huggingface.co/docs/huggingface_hub/main/en/package_reference/hf_api#huggingface_hub.HfApi.super_squash_history). Be aware that this is a non-revertible operation.\n- **Number of operations per commit**: Once again, there is no hard limit here. When a commit is uploaded on the Hub, each\ngit operation (addition or delete) is checked by the server. When a hundred LFS files are committed at once,\neach file is checked individually to ensure it's been correctly uploaded. When pushing data through HTTP,\na timeout of 60s is set on the request, meaning that if the process takes more time, an error is raised. However, it can\nhappen (in rare cases) that even if the timeout is raised client-side, the process is still\ncompleted server-side. This can be checked manually by browsing the repo on the Hub. To prevent this timeout, we recommend\nadding around 50-100 files per commit.", "Under the hood, Spaces stores your code inside a git repository, just like the model and dataset repositories. Thanks to this, the same tools we use for all the [other repositories on the Hub](./repositories) (`git` and `git-lfs`) also work for Spaces. Follow the same flow as in [Getting Started with Repositories](./repositories-getting-started) to add files to your Space. Each time a new commit is pushed, the Space will automatically rebuild and restart.\n\nFor step-by-step tutorials to creating your first Space, see the guides below:\n* [Creating a Gradio Space](./spaces-sdks-gradio)\n* [Creating a Streamlit Space](./spaces-sdks-streamlit)\n* [Creating a Docker Space](./spaces-sdks-docker-first-demo)\n\n## Hardware resources\n\nEach Spaces environment is limited to 16GB RAM, 2 CPU cores and 50GB of (not persistent) disk space by default, which you can use free of charge. You can upgrade to better hardware, including a variety of GPU accelerators and persistent storage, for a [competitive price](https://huggingface.co/pricing#spaces). To request an upgrade, please click the _Settings_ button in your Space and select your preferred hardware environment.", "Repository limitations and recommendations\n\nThere are some limitations to be aware of when dealing with a large amount of data in your repo. Given the time it takes to stream the data,\ngetting an upload/push to fail at the end of the process or encountering a degraded experience, be it on hf.co or when working locally, can be very annoying.\n\n## Recommendations\n\nWe gathered a list of tips and recommendations for structuring your repo. If you are looking for more practical tips, check out [this guide](https://huggingface.co/docs/huggingface_hub/main/en/guides/upload#practical-tips) on how to upload large amount of data using the Python library.\n\n\n| Characteristic     | Recommended        | Tips                                                   |\n| ----------------   | ------------------ | ------------------------------------------------------ |\n| Repo size          | -                  | contact us for large repos (TBs of data)               |\n| Files per repo     | <100k              | merge data into fewer files                            |\n| Entries per folder | <10k               | use subdirectories in repo                             |\n| File size          | <5GB               | split data into chunked files                          |\n| Commit size        | <100 files*        | upload files in multiple commits                       |\n| Commits per repo   | -                  | upload multiple files per commit and/or squash history |\n\n_* Not relevant when using `git` CLI directly_\n\nPlease read the next section to understand better those limits and how to deal with them.\n\n## Explanations\n\nWhat are we talking about when we say \"large uploads\", and what are their associated limitations? Large uploads can be\nvery diverse, from repositories with a few huge files (e.g. model weights) to repositories with thousands of small files\n(e.g. an image dataset)."], "eval_score_GPT4": "5", "eval_feedback_GPT4": "Feedback: The response correctly identifies the file size limit for syncing to HF Spaces without using Git-LFS as 10MB, which is in line with the reference answer. The additional context provided about the recommendation to use Git-LFS for larger files and the source of the information does not detract from the accuracy of the core fact provided."}, {"question": "What is the title of the paper introducing the ByT5 model?\n", "true_answer": "ByT5: Towards a token-free future with pre-trained byte-to-byte models", "source_doc": "huggingface/transformers/blob/main/docs/source/en/model_doc/byt5.md", "generated_answer": "The title of the paper introducing the ByT5 model is \"ByT5: Towards a token-free future with pre-trained byte-to-byte models\" (source: 1).", "retrieved_docs": ["!--Copyright 2021 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\n\nhttp://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License.\n\n\u26a0\ufe0f Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\nrendered properly in your Markdown viewer.\n\n-->\n\n# ByT5\n\n## Overview\n\nThe ByT5 model was presented in [ByT5: Towards a token-free future with pre-trained byte-to-byte models](https://arxiv.org/abs/2105.13626) by Linting Xue, Aditya Barua, Noah Constant, Rami Al-Rfou, Sharan Narang, Mihir\nKale, Adam Roberts, Colin Raffel.\n\nThe abstract from the paper is the following:", "- [t5-11b](https://huggingface.co/t5-11b).\n\nBased on the original T5 model, Google has released some follow-up works:\n\n- **T5v1.1**: T5v1.1 is an improved version of T5 with some architectural tweaks, and is pre-trained on C4 only without\n  mixing in the supervised tasks. Refer to the documentation of T5v1.1 which can be found [here](t5v1.1).\n\n- **mT5**: mT5 is a multilingual T5 model. It is pre-trained on the mC4 corpus, which includes 101 languages. Refer to\n  the documentation of mT5 which can be found [here](mt5).\n\n- **byT5**: byT5 is a T5 model pre-trained on byte sequences rather than SentencePiece subword token sequences. Refer\n  to the documentation of byT5 which can be found [here](byt5).\n\n- **UL2**: UL2 is a T5 like model pretrained on various denoising objectives\n\n- **Flan-T5**: Flan is a pretraining methods that is based on prompting. The Flan-T5 are T5 models trained on the Flan collection of \n    datasets which include: `taskmaster2`, `djaym7/wiki_dialog`, `deepmind/code_contests`, `lambada`, `gsm8k`, `aqua_rat`, `esnli`, `quasc` and `qed`.\n\n- **FLan-UL2** : the UL2 model finetuned using the \"Flan\" prompt tuning and dataset collection.\n\n- **UMT5**: UmT5 is a multilingual T5 model trained on an improved and refreshed mC4 multilingual corpus,  29 trillion characters across 107 language, using a new sampling method, UniMax. Refer to\n the documentation of mT5 which can be found [here](umt5).\n\n## Training", "!--Copyright 2022 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\n\nhttp://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License.\n\n\u26a0\ufe0f Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\nrendered properly in your Markdown viewer.\n\n-->\n\n# UL2\n\n## Overview\n\nThe T5 model was presented in [Unifying Language Learning Paradigms](https://arxiv.org/pdf/2205.05131v1.pdf) by Yi Tay, Mostafa Dehghani, Vinh Q. Tran, Xavier Garcia, Dara Bahri, Tal Schuster, Huaixiu Steven Zheng, Neil Houlsby, Donald Metzler.\n\nThe abstract from the paper is the following:", "1. **[ByT5](https://huggingface.co/docs/transformers/model_doc/byt5)** (from Google Research) released with the paper [ByT5: Towards a token-free future with pre-trained byte-to-byte models](https://arxiv.org/abs/2105.13626) by Linting Xue, Aditya Barua, Noah Constant, Rami Al-Rfou, Sharan Narang, Mihir Kale, Adam Roberts, Colin Raffel.\n1. **[CamemBERT](https://huggingface.co/docs/transformers/model_doc/camembert)** (from Inria/Facebook/Sorbonne) released with the paper [CamemBERT: a Tasty French Language Model](https://arxiv.org/abs/1911.03894) by Louis Martin*, Benjamin Muller*, Pedro Javier Ortiz Su\u00e1rez*, Yoann Dupont, Laurent Romary, \u00c9ric Villemonte de la Clergerie, Djam\u00e9 Seddah and Beno\u00eet Sagot.\n1. **[CANINE](https://huggingface.co/docs/transformers/model_doc/canine)** (from Google Research) released with the paper [CANINE: Pre-training an Efficient Tokenization-Free Encoder for Language Representation](https://arxiv.org/abs/2103.06874) by Jonathan H. Clark, Dan Garrette, Iulia Turc, John Wieting.\n1. **[Chinese-CLIP](https://huggingface.co/docs/transformers/model_doc/chinese_clip)** (from OFA-Sys) released with the paper [Chinese CLIP: Contrastive Vision-Language Pretraining in Chinese](https://arxiv.org/abs/2211.01335) by An Yang, Junshu Pan, Junyang Lin, Rui Men, Yichang Zhang, Jingren Zhou, Chang Zhou.", "1. **[ByT5](https://huggingface.co/docs/transformers/model_doc/byt5)** (from Google Research) released with the paper [ByT5: Towards a token-free future with pre-trained byte-to-byte models](https://arxiv.org/abs/2105.13626) by Linting Xue, Aditya Barua, Noah Constant, Rami Al-Rfou, Sharan Narang, Mihir Kale, Adam Roberts, Colin Raffel.\n1. **[CamemBERT](https://huggingface.co/docs/transformers/model_doc/camembert)** (from Inria/Facebook/Sorbonne) released with the paper [CamemBERT: a Tasty French Language Model](https://arxiv.org/abs/1911.03894) by Louis Martin*, Benjamin Muller*, Pedro Javier Ortiz Su\u00e1rez*, Yoann Dupont, Laurent Romary, \u00c9ric Villemonte de la Clergerie, Djam\u00e9 Seddah and Beno\u00eet Sagot.\n1. **[CANINE](https://huggingface.co/docs/transformers/model_doc/canine)** (from Google Research) released with the paper [CANINE: Pre-training an Efficient Tokenization-Free Encoder for Language Representation](https://arxiv.org/abs/2103.06874) by Jonathan H. Clark, Dan Garrette, Iulia Turc, John Wieting.\n1. **[Chinese-CLIP](https://huggingface.co/docs/transformers/model_doc/chinese_clip)** (from OFA-Sys) released with the paper [Chinese CLIP: Contrastive Vision-Language Pretraining in Chinese](https://arxiv.org/abs/2211.01335) by An Yang, Junshu Pan, Junyang Lin, Rui Men, Yichang Zhang, Jingren Zhou, Chang Zhou.", "1. **[ByT5](https://huggingface.co/docs/transformers/model_doc/byt5)** (from Google Research) released with the paper [ByT5: Towards a token-free future with pre-trained byte-to-byte models](https://arxiv.org/abs/2105.13626) by Linting Xue, Aditya Barua, Noah Constant, Rami Al-Rfou, Sharan Narang, Mihir Kale, Adam Roberts, Colin Raffel.\n1. **[CamemBERT](https://huggingface.co/docs/transformers/model_doc/camembert)** (from Inria/Facebook/Sorbonne) released with the paper [CamemBERT: a Tasty French Language Model](https://arxiv.org/abs/1911.03894) by Louis Martin*, Benjamin Muller*, Pedro Javier Ortiz Su\u00e1rez*, Yoann Dupont, Laurent Romary, \u00c9ric Villemonte de la Clergerie, Djam\u00e9 Seddah and Beno\u00eet Sagot.\n1. **[CANINE](https://huggingface.co/docs/transformers/model_doc/canine)** (from Google Research) released with the paper [CANINE: Pre-training an Efficient Tokenization-Free Encoder for Language Representation](https://arxiv.org/abs/2103.06874) by Jonathan H. Clark, Dan Garrette, Iulia Turc, John Wieting.\n1. **[Chinese-CLIP](https://huggingface.co/docs/transformers/model_doc/chinese_clip)** (from OFA-Sys) released with the paper [Chinese CLIP: Contrastive Vision-Language Pretraining in Chinese](https://arxiv.org/abs/2211.01335) by An Yang, Junshu Pan, Junyang Lin, Rui Men, Yichang Zhang, Jingren Zhou, Chang Zhou.", "1. **[ByT5](https://huggingface.co/docs/transformers/model_doc/byt5)** (from Google Research) released with the paper [ByT5: Towards a token-free future with pre-trained byte-to-byte models](https://arxiv.org/abs/2105.13626) by Linting Xue, Aditya Barua, Noah Constant, Rami Al-Rfou, Sharan Narang, Mihir Kale, Adam Roberts, Colin Raffel.\n1. **[CamemBERT](https://huggingface.co/docs/transformers/model_doc/camembert)** (from Inria/Facebook/Sorbonne) released with the paper [CamemBERT: a Tasty French Language Model](https://arxiv.org/abs/1911.03894) by Louis Martin*, Benjamin Muller*, Pedro Javier Ortiz Su\u00e1rez*, Yoann Dupont, Laurent Romary, \u00c9ric Villemonte de la Clergerie, Djam\u00e9 Seddah and Beno\u00eet Sagot.\n1. **[CANINE](https://huggingface.co/docs/transformers/model_doc/canine)** (from Google Research) released with the paper [CANINE: Pre-training an Efficient Tokenization-Free Encoder for Language Representation](https://arxiv.org/abs/2103.06874) by Jonathan H. Clark, Dan Garrette, Iulia Turc, John Wieting.\n1. **[Chinese-CLIP](https://huggingface.co/docs/transformers/model_doc/chinese_clip)** (from OFA-Sys) released with the paper [Chinese CLIP: Contrastive Vision-Language Pretraining in Chinese](https://arxiv.org/abs/2211.01335) by An Yang, Junshu Pan, Junyang Lin, Rui Men, Yichang Zhang, Jingren Zhou, Chang Zhou."], "eval_score_GPT4": "5", "eval_feedback_GPT4": "Feedback: The response provided is completely correct, accurate, and factual. It matches the reference answer exactly, including the title of the paper in quotation marks and even adds a citation note, which is not required but shows diligence in providing a source for the information."}, {"question": "What is the dimension of the feature vector for the base BERT model?\n", "true_answer": "768", "source_doc": "huggingface/course/blob/main/subtitles/en/raw/chapter1/05_encoders.md", "generated_answer": "The dimension of the feature vector for the base BERT model is 768, as stated in the context provided. This information can be found in the first extract (0) where it is mentioned that each vector produced by the encoder in BERT represents a word and has a dimension defined by the architecture of the model. For the base BERT model, this dimension is 768.", "retrieved_docs": ["Let's dive in this representation. It contains one vector per word that was passed through the encoder. Each of these vector is a numerical representation of the word in question. The dimension of that vector is defined by the architecture of the model, for the base BERT model, it is 768. These representations contain the value of a word; but contextualized. For example, the vector attributed to the word \"to\", isn't the representation of only the \"to\" word. It also takes into account the words around it, which we call the \u201ccontext\u201d.As in, it looks to the left context, the word on the left of the one we're studying (here the word \"Welcome\") and the context on the right (here the word \"NYC\") and outputs a value for the word, within its context. It is therefore a contextualized value. One could say that the vector of 768 values holds the \"meaning\" of that word in the text. How it does this is thanks to the self-attention mechanism. The self-attention mechanism relates to different positions (or different words) in a single sequence, in order to compute a representation of that sequence. As we've seen before, this means that the resulting representation of a word has been affected by other words in the sequence. We won't dive into the specifics here, but we'll offer some further readings if you want to get a better understanding at what happens under the hood. So when should one use an encoder? Encoders can be used as standalone models in a wide variety of tasks. For example BERT, arguably the most famous transformer model, is a standalone encoder model and at the time of release, beat the state of the art in many sequence classification tasks, question answering tasks, and masked language modeling, to only cite a few. The idea is that encoders are very powerful at extracting vectors that carry meaningful information about a sequence. This vector can then be handled down the road by additional layers of neurons to make sense of them", "Here\u2019s how many of the above ML architecture parts BERTbase and BERTlarge has:\n\n\n|           | Transformer Layers | Hidden Size | Attention Heads | Parameters | Processing | Length of Training |\n|-----------|--------------------|-------------|-----------------|------------|------------|--------------------|\n| BERTbase  | 12                 | 768         | 12              | 110M       | 4 TPUs     | 4 days             |\n| BERTlarge | 24                 | 1024        | 16              | 340M       | 16 TPUs    | 4 days             |\n\n\n\nLet\u2019s take a look at how BERTlarge\u2019s  additional layers, attention heads, and parameters have increased its performance across NLP tasks.\n\n## 4. BERT's performance on common language tasks\n\nBERT has successfully achieved state-of-the-art accuracy on 11 common NLP tasks, outperforming previous top NLP models, and is the first to outperform humans! \nBut, how are these achievements measured?\n\n### NLP Evaluation Methods: \n\n#### 4.1 SQuAD v1.1 & v2.0\n[SQuAD](https://huggingface.co/datasets/squad) (Stanford Question Answering Dataset) is a reading comprehension dataset of around 108k questions that can be answered via a corresponding paragraph of Wikipedia text. BERT\u2019s performance on this evaluation method was a big achievement beating previous state-of-the-art models and human-level performance:\n\n<figure class=\"image table text-center m-0 w-full\">\n  <medium-zoom background=\"rgba(0,0,0,.7)\" alt=\"BERT's performance on SQuAD v1.1\" src=\"assets/52_bert_101/BERTs-performance-on-SQuAD1.1.png\"></medium-zoom>\n</figure>", "bert-base-cased             model     a8d257ba9925ef39f3036bfc338acf5283c512d9         1.4G        9 3 days ago    main        /home/wauplin/.cache/huggingface/hub/models--bert-base-cased/snapshots/a8d257ba9925ef39f3036bfc338acf5283c512d9\nt5-base                     model     23aa4f41cb7c08d4b05c8f327b22bfa0eb8c7ad9        10.1K        3 1 week ago    main        /home/wauplin/.cache/huggingface/hub/models--t5-base/snapshots/23aa4f41cb7c08d4b05c8f327b22bfa0eb8c7ad9\nt5-small                    model     98ffebbb27340ec1b1abd7c45da12c253ee1882a       726.2M        6 1 week ago    refs/pr/1   /home/wauplin/.cache/huggingface/hub/models--t5-small/snapshots/98ffebbb27340ec1b1abd7c45da12c253ee1882a\nt5-small                    model     d0a119eedb3718e34c648e594394474cf95e0617       485.8M        6 4 weeks ago               /home/wauplin/.cache/huggingface/hub/models--t5-small/snapshots/d0a119eedb3718e34c648e594394474cf95e0617", "- `distilbert-base-uncased`: DistilBERT English language model pretrained on the same data used to pretrain Bert (concatenation of the Toronto Book Corpus and full English Wikipedia) using distillation with the supervision of the `bert-base-uncased` version of Bert. The model has 6 layers, 768 dimension and 12 heads, totalizing 66M parameters.\n- `distilbert-base-uncased-distilled-squad`: A finetuned version of `distilbert-base-uncased` finetuned using (a second step of) knowledge distillation on SQuAD 1.0. This model reaches a F1 score of 86.9 on the dev set (for comparison, Bert `bert-base-uncased` version reaches a 88.5 F1 score).\n- `distilbert-base-cased`: DistilBERT English language model pretrained on the same data used to pretrain Bert (concatenation of the Toronto Book Corpus and full English Wikipedia) using distillation with the supervision of the `bert-base-cased` version of Bert. The model has 6 layers, 768 dimension and 12 heads, totalizing 65M parameters.\n- `distilbert-base-cased-distilled-squad`: A finetuned version of `distilbert-base-cased` finetuned using (a second step of) knowledge distillation on SQuAD 1.0. This model reaches a F1 score of 87.1 on the dev set (for comparison, Bert `bert-base-cased` version reaches a 88.7 F1 score).", "!--Copyright 2020 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\n\nhttp://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License.\n\n\u26a0\ufe0f Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\nrendered properly in your Markdown viewer.\n\n-->\n\n# I-BERT\n\n## Overview\n\nThe I-BERT model was proposed in [I-BERT: Integer-only BERT Quantization](https://arxiv.org/abs/2101.01321) by\nSehoon Kim, Amir Gholami, Zhewei Yao, Michael W. Mahoney and Kurt Keutzer. It's a quantized version of RoBERTa running\ninference up to four times faster.\n\nThe abstract from the paper is the following:", "Finally, calculating the BERTScore metric involves downloading the BERT model that is used to compute the score-- the default model for `en`, `roberta-large`, takes over 1.4GB of storage space and downloading it can take a significant amount of time depending on the speed of your internet connection. If this is an issue, choose a smaller model; for instance `distilbert-base-uncased` is 268MB. A full list of compatible models can be found [here](https://docs.google.com/spreadsheets/d/1RKOVpselB98Nnh_EOC4A2BYn8_201tmPODpNWu4w7xI/edit#gid=0).  \n\n\n## Citation\n\n```bibtex\n@inproceedings{bert-score,\n  title={BERTScore: Evaluating Text Generation with BERT},\n  author={Tianyi Zhang* and Varsha Kishore* and Felix Wu* and Kilian Q. Weinberger and Yoav Artzi},\n  booktitle={International Conference on Learning Representations},\n  year={2020},\n  url={https://openreview.net/forum?id=SkeHuCVFDr}\n}\n```\n    \n## Further References \n\n- [BERTScore Project README](https://github.com/Tiiiger/bert_score#readme)\n- [BERTScore ICLR 2020 Poster Presentation](https://iclr.cc/virtual_2020/poster_SkeHuCVFDr.html)", "Finally, calculating the BERTScore metric involves downloading the BERT model that is used to compute the score-- the default model for `en`, `roberta-large`, takes over 1.4GB of storage space and downloading it can take a significant amount of time depending on the speed of your internet connection. If this is an issue, choose a smaller model; for instance `distilbert-base-uncased` is 268MB. A full list of compatible models can be found [here](https://docs.google.com/spreadsheets/d/1RKOVpselB98Nnh_EOC4A2BYn8_201tmPODpNWu4w7xI/edit#gid=0).  \n\n\n## Citation\n\n```bibtex\n@inproceedings{bert-score,\n  title={BERTScore: Evaluating Text Generation with BERT},\n  author={Tianyi Zhang* and Varsha Kishore* and Felix Wu* and Kilian Q. Weinberger and Yoav Artzi},\n  booktitle={International Conference on Learning Representations},\n  year={2020},\n  url={https://openreview.net/forum?id=SkeHuCVFDr}\n}\n```\n    \n## Further References \n\n- [BERTScore Project README](https://github.com/Tiiiger/bert_score#readme)\n- [BERTScore ICLR 2020 Poster Presentation](https://iclr.cc/virtual_2020/poster_SkeHuCVFDr.html)"], "eval_score_GPT4": "5", "eval_feedback_GPT4": "The response correctly identifies the dimension of the feature vector for the base BERT model as 768, which is in agreement with the reference answer. The additional context provided in the response about the source of the information and its relation to the encoder in BERT is accurate and supports the correctness of the answer."}, {"question": "What special identifier does the WordPiece Model use for continuing subwords?\n", "true_answer": "##", "source_doc": "huggingface/tokenizers/blob/main/docs/source-doc-builder/components.mdx", "generated_answer": "The WordPiece model uses the prefix \"##\" to continue subwords. This prefix is added to all the characters inside a word during the initial tokenization step. During training, WordPiece learns merge rules by selecting the pair with the highest score, which is calculated by dividing the frequency of the pair by the product of the frequencies of each of its parts. This prioritizes the merging of pairs where the individual parts are less frequent in the vocabulary. The resulting sub", "retrieved_docs": ["WordPiece tokenization[[wordpiece-tokenization]]\n\n<CourseFloatingBanner chapter={6}\n  classNames=\"absolute z-10 right-0 top-0\"\n  notebooks={[\n    {label: \"Google Colab\", value: \"https://colab.research.google.com/github/huggingface/notebooks/blob/master/course/en/chapter6/section6.ipynb\"},\n    {label: \"Aws Studio\", value: \"https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/master/course/en/chapter6/section6.ipynb\"},\n]} />\n\nWordPiece is the tokenization algorithm Google developed to pretrain BERT. It has since been reused in quite a few Transformer models based on BERT, such as DistilBERT, MobileBERT, Funnel Transformers, and MPNET. It's very similar to BPE in terms of the training, but the actual tokenization is done differently.\n\n<Youtube id=\"qpv6ms_t_1A\"/>\n\n<Tip>\n\n\ud83d\udca1 This section covers WordPiece in depth, going as far as showing a full implementation. You can skip to the end if you just want a general overview of the tokenization algorithm.\n\n</Tip>\n\n## Training algorithm[[training-algorithm]]\n\n<Tip warning={true}>\n\n\u26a0\ufe0f Google never open-sourced its implementation of the training algorithm of WordPiece, so what follows is our best guess based on the published literature. It may not be 100% accurate.\n\n</Tip>\n\nLike BPE, WordPiece starts from a small vocabulary including the special tokens used by the model and the initial alphabet. Since it identifies subwords by adding a prefix (like `##` for BERT), each word is initially split by adding that prefix to all the characters inside the word. So, for instance, `\"word\"` gets split like this:\n\n```\nw ##o ##r ##d\n```", "et's see together what is the training strategy of the WordPiece algorithm and how it performs the tokenization of a text once trained WordPiece is a tokenization algorithm introduced by Google. It is used for example by Bert. To our knowledge, the code of Word Pieces has not been open sourced, so we base our explanations on our own interpretation of the published literature. What is the training strategy of WordPiece? Similarly to the BPE algorithm, WordPiece starts by establishing an initial vocabulary composed of elementary units and then increases this vocabulary to the desired size. To build the initial vocabulary, we divide each word in the training corpus into the sequence of letters that make it up. As you can see, there is a small subtlety: we add a 2 hashtags in front of the letters that do not start a word. By keeping only one occurrence per elementary unit we now have our initial vocabulary. We will list all the existing pairs in our corpus. Once we have this list, we will calculate a score for each of these pairs. As for the BPE algorithm, we will select the pair with the highest score. Taking for example the first pair composed of H and U. The score of a pair is simply equal to the frequency of appearance of the pair divided by the product of the frequency of appearance of the first token by the frequency of appearance of the second token. Thus at a fixed frequency of appearance of the pair, if the subparts of the pair are very frequent in the corpus then this score will be decreased. In our example, the pair \"hu\" appears 4 times, the letter \"h\" 4 times and the letter u 4 times. This gives us a score of 0.25. Now that we know how to calculate this score, we can do it for all pairs. We can now add to the vocabulary the pair with the highest score, after merging it of course! And now we can apply this same fusion to our split corpus", "### 9. Select the sentences that apply to the WordPiece model of tokenization.\n\n<Question\n\tchoices={[\n\t\t{\n\t\t\ttext: \"WordPiece is a subword tokenization algorithm that starts with a small vocabulary and learns merge rules.\",\n\t\t\texplain: \"That's the case indeed!\",\n\t\t\tcorrect: true\n\t\t},\n\t\t{\n\t\t\ttext: \"WordPiece is a subword tokenization algorithm that starts with a big vocabulary and progressively removes tokens from it.\",\n\t\t\texplain: \"No, that's the approach taken by a different tokenization algorithm.\"\n\t\t},\n\t\t{\n\t\t\ttext: \"WordPiece tokenizers learn merge rules by merging the pair of tokens that is the most frequent.\",\n\t\t\texplain: \"No, that's the strategy applied by another tokenization algorithm.\"\n\t\t},\n\t\t{\n\t\t\ttext: \"A WordPiece tokenizer learns a merge rule by merging the pair of tokens that maximizes a score that privileges frequent pairs with less frequent individual parts.\",\n\t\t\texplain: \"That's correct!\",\n\t\t\tcorrect: true\n\t\t},\n\t\t{\n\t\t\ttext: \"WordPiece tokenizes words into subwords by finding the most likely segmentation into tokens, according to the model.\",\n\t\t\texplain: \"No, that's how another tokenization algorithm works.\"\n\t\t},\n\t\t{\n\t\t\ttext: \"WordPiece tokenizes words into subwords by finding the longest subword starting from the beginning that is in the vocabulary, then repeating the process for the rest of the text.\",\n\t\t\texplain: \"Yes, this is how WordPiece proceeds for the encoding.\",\n\t\t\tcorrect: true\n\t\t},\n\t]}\n/>\n\n### 10. Select the sentences that apply to the Unigram model of tokenization.", ". We can now add to the vocabulary the pair with the highest score, after merging it of course! And now we can apply this same fusion to our split corpus. As you can imagine, we just have to repeat the same operations until we have the vocabulary at the desired size! Let's look at a few more steps to see the evolution of our vocabulary and the length of the splits getting shorter. Now that we are happy with our vocabulary, you are probably wondering how to use it to tokenize a text.  Let's say we want to tokenize the word \"huggingface\".  WordPiece follows these rules: We will look for the longest possible token at the beginning of our word. Then we start again on the remaining part of our word. And so on until we reach the end! And that's it, huggingface is divided into 4 sub-tokens. \u00dfThis video is about to end, I hope it helped you to understand better what is behind the word WordPiece!", "#### Byte-level BPE\n\nA base vocabulary that includes all possible base characters can be quite large if *e.g.* all unicode characters are\nconsidered as base characters. To have a better base vocabulary, [GPT-2](https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf) uses bytes\nas the base vocabulary, which is a clever trick to force the base vocabulary to be of size 256 while ensuring that\nevery base character is included in the vocabulary. With some additional rules to deal with punctuation, the GPT2's\ntokenizer can tokenize every text without the need for the <unk> symbol. [GPT-2](model_doc/gpt) has a vocabulary\nsize of 50,257, which corresponds to the 256 bytes base tokens, a special end-of-text token and the symbols learned\nwith 50,000 merges.\n\n<a id='wordpiece'></a>\n\n### WordPiece\n\nWordPiece is the subword tokenization algorithm used for [BERT](model_doc/bert), [DistilBERT](model_doc/distilbert), and [Electra](model_doc/electra). The algorithm was outlined in [Japanese and Korean\nVoice Search (Schuster et al., 2012)](https://static.googleusercontent.com/media/research.google.com/ja//pubs/archive/37842.pdf) and is very similar to\nBPE. WordPiece first initializes the vocabulary to include every character present in the training data and\nprogressively learns a given number of merge rules. In contrast to BPE, WordPiece does not choose the most frequent\nsymbol pair, but the one that maximizes the likelihood of the training data once added to the vocabulary.", "Now that we've seen a little of how some different tokenizers process text, we can start to explore the underlying algorithms themselves. We'll begin with a quick look at the broadly widely applicable SentencePiece; then, over the next three sections, we'll examine how the three main algorithms used for subword tokenization work.\n\n## SentencePiece[[sentencepiece]]\n\n[SentencePiece](https://github.com/google/sentencepiece) is a tokenization algorithm for the preprocessing of text that you can use with any of the models we will see in the next three sections. It considers the text as a sequence of Unicode characters, and replaces spaces with a special character, `\u2581`. Used in conjunction with the Unigram algorithm (see [section 7](/course/chapter7/7)), it doesn't even require a pre-tokenization step, which is very useful for languages where the space character is not used (like Chinese or Japanese).\n\nThe other main feature of SentencePiece is *reversible tokenization*: since there is no special treatment of spaces, decoding the tokens is done simply by concatenating them and replacing the `_`s with spaces -- this results in the normalized text. As we saw earlier, the BERT tokenizer removes repeating spaces, so its tokenization is not reversible.\n\n## Algorithm overview[[algorithm-overview]]\n\nIn the following sections, we'll dive into the three main subword tokenization algorithms: BPE (used by GPT-2 and others), WordPiece (used for example by BERT), and Unigram (used by T5 and others). Before we get started, here's a quick overview of how they each work. Don't hesitate to come back to this table after reading each of the next sections if it doesn't make sense to you yet.", "```\nw ##o ##r ##d\n```\n\nThus, the initial alphabet contains all the characters present at the beginning of a word and the characters present inside a word preceded by the WordPiece prefix.\n\nThen, again like BPE, WordPiece learns merge rules. The main difference is the way the pair to be merged is selected. Instead of selecting the most frequent pair, WordPiece computes a score for each pair, using the following formula:\n\n$$\\mathrm{score} = (\\mathrm{freq\\_of\\_pair}) / (\\mathrm{freq\\_of\\_first\\_element} \\times \\mathrm{freq\\_of\\_second\\_element})$$\n\nBy dividing the frequency of the pair by the product of the frequencies of each of its parts, the algorithm prioritizes the merging of pairs where the individual parts are less frequent in the vocabulary. For instance, it won't necessarily merge `(\"un\", \"##able\")` even if that pair occurs very frequently in the vocabulary, because the two pairs `\"un\"` and `\"##able\"` will likely each appear in a lot of other words and have a high frequency. In contrast, a pair like `(\"hu\", \"##gging\")` will probably be merged faster (assuming the word \"hugging\" appears often in the vocabulary) since `\"hu\"` and `\"##gging\"` are likely to be less frequent individually.\n\nLet's look at the same vocabulary we used in the BPE training example:\n\n```\n(\"hug\", 10), (\"pug\", 5), (\"pun\", 12), (\"bun\", 4), (\"hugs\", 5)\n```\n\nThe splits here will be:"], "eval_score_GPT4": "4", "eval_feedback_GPT4": "The response correctly identifies the \"##\" prefix as the special identifier used by the WordPiece model for continuing subwords. However, the additional information provided about the initial tokenization step and the training process of WordPiece, while related, is not necessary to answer the specific question asked. The question only asks for the identifier, not for an explanation of the tokenization process or training methodology. Therefore, the response contains more information than required, but the core answer to the question is correct and factual."}, {"question": "What is the purpose of the \ud83e\udde8 Diffusers tutorials?\n", "true_answer": "To provide a gentle introduction to diffusion models and help understand the library fundamentals.", "source_doc": "huggingface/diffusers/blob/main/docs/source/en/tutorials/tutorial_overview.md", "generated_answer": "The purpose of the \ud83e\udde8 Diffusers tutorials is to demonstrate how to effectively use the Diffusers library, which is a collection of scripts for training or fine-tuning state-of-the-art pretrained diffusion models for generating images, audio, and even 3D structures of molecules. These tutorials aspire to be self-contained, easy-to-tweak, beginner-friendly, and for one-purpose-only. They are", "retrieved_docs": ["Using \ud83e\udde8 `diffusers` at Hugging Face\n\nDiffusers is the go-to library for state-of-the-art pretrained diffusion models for generating images, audio, and even 3D structures of molecules. Whether you\u2019re looking for a simple inference solution or want to train your own diffusion model, Diffusers is a modular toolbox that supports both. The library is designed with a focus on usability over performance, simple over easy, and customizability over abstractions.\n\n## Exploring Diffusers in the Hub\n\nThere are over 10,000 `diffusers` compatible pipelines on the Hub which you can find by filtering at the left of [the models page](https://huggingface.co/models?library=diffusers&sort=downloads). Diffusion systems are typically composed of multiple components such as text encoder, UNet, VAE, and scheduler. Even though they are not standalone models, the pipeline abstraction makes it easy to use them for inference or training.\n\nYou can find diffusion pipelines for many different tasks:\n\n* Generating images from natural language text prompts ([text-to-image](https://huggingface.co/models?library=diffusers&pipeline_tag=text-to-image&sort=downloads)).\n* Transforming images using natural language text prompts ([image-to-image](https://huggingface.co/models?library=diffusers&pipeline_tag=image-to-image&sort=downloads)).\n* Generating videos from natural language descriptions ([text-to-video](https://huggingface.co/models?library=diffusers&pipeline_tag=text-to-video&sort=downloads)).\n\n\nYou can try out the models directly in the browser if you want to test them out without downloading them, thanks to the in-browser widgets! \n\n<div class=\"flex justify-center\">\n<img class=\"hidden dark:block\" src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/refs%2Fpr%2F35/hub/libraries-diffusers_widget.png\"/>\n</div>", "!---\nCopyright 2023 The HuggingFace Team. All rights reserved.\nLicensed under the Apache License, Version 2.0 (the \"License\");\nyou may not use this file except in compliance with the License.\nYou may obtain a copy of the License at\n\n    http://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software\ndistributed under the License is distributed on an \"AS IS\" BASIS,\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\nSee the License for the specific language governing permissions and\nlimitations under the License.\n-->\n\n# \ud83e\udde8 Diffusers Examples\n\nDiffusers examples are a collection of scripts to demonstrate how to effectively use the `diffusers` library\nfor a variety of use cases involving training or fine-tuning.\n\n**Note**: If you are looking for **official** examples on how to use `diffusers` for inference, please have a look at [src/diffusers/pipelines](https://github.com/huggingface/diffusers/tree/main/src/diffusers/pipelines).\n\nOur examples aspire to be **self-contained**, **easy-to-tweak**, **beginner-friendly** and for **one-purpose-only**.\nMore specifically, this means:", "\ud83e\udde8 Diffusers Experimental\n\nWe are adding experimental code to support novel applications and usages of the Diffusers library.\nCurrently, the following experiments are supported:\n* Reinforcement learning via an implementation of the [Diffuser](https://arxiv.org/abs/2205.09991) model.", "\ud83e\udd17 Diffusers is the go-to library for state-of-the-art pretrained diffusion models for generating images, audio, and even 3D structures of molecules. Whether you're looking for a simple inference solution or training your own diffusion models, \ud83e\udd17 Diffusers is a modular toolbox that supports both. Our library is designed with a focus on [usability over performance](https://huggingface.co/docs/diffusers/conceptual/philosophy#usability-over-performance), [simple over easy](https://huggingface.co/docs/diffusers/conceptual/philosophy#simple-over-easy), and [customizability over abstractions](https://huggingface.co/docs/diffusers/conceptual/philosophy#tweakable-contributorfriendly-over-abstraction).\n\n\ud83e\udd17 Diffusers offers three core components:\n\n- State-of-the-art [diffusion pipelines](https://huggingface.co/docs/diffusers/api/pipelines/overview) that can be run in inference with just a few lines of code.\n- Interchangeable noise [schedulers](https://huggingface.co/docs/diffusers/api/schedulers/overview) for different diffusion speeds and output quality.\n- Pretrained [models](https://huggingface.co/docs/diffusers/api/models/overview) that can be used as building blocks, and combined with schedulers, for creating your own end-to-end diffusion systems.\n\n## Installation\n\nWe recommend installing \ud83e\udd17 Diffusers in a virtual environment from PyPI or Conda. For more details about installing [PyTorch](https://pytorch.org/get-started/locally/) and [Flax](https://flax.readthedocs.io/en/latest/#installation), please refer to their official documentation.\n\n### PyTorch\n\nWith `pip` (official package):\n\n```bash\npip install --upgrade diffusers[torch]\n```\n\nWith `conda` (maintained by the community):", "## Usability over Performance\n\n- While Diffusers has many built-in performance-enhancing features (see [Memory and Speed](https://huggingface.co/docs/diffusers/optimization/fp16)), models are always loaded with the highest precision and lowest optimization. Therefore, by default diffusion pipelines are always instantiated on CPU with float32 precision if not otherwise defined by the user. This ensures usability across different platforms and accelerators and means that no complex installations are required to run the library.\n- Diffusers aims to be a **light-weight** package and therefore has very few required dependencies, but many soft dependencies that can improve performance (such as `accelerate`, `safetensors`, `onnx`, etc...). We strive to keep the library as lightweight as possible so that it can be added without much concern as a dependency on other packages.\n- Diffusers prefers simple, self-explainable code over condensed, magic code. This means that short-hand code syntaxes such as lambda functions, and advanced PyTorch operators are often not desired.\n\n## Simple over easy", "## Usability over Performance\n\n- While Diffusers has many built-in performance-enhancing features (see [Memory and Speed](https://huggingface.co/docs/diffusers/optimization/fp16)), models are always loaded with the highest precision and lowest optimization. Therefore, by default diffusion pipelines are always instantiated on CPU with float32 precision if not otherwise defined by the user. This ensures usability across different platforms and accelerators and means that no complex installations are required to run the library.\n- Diffusers aims to be a **light-weight** package and therefore has very few required dependencies, but many soft dependencies that can improve performance (such as `accelerate`, `safetensors`, `onnx`, etc...). We strive to keep the library as lightweight as possible so that it can be added without much concern as a dependency on other packages.\n- Diffusers prefers simple, self-explainable code over condensed, magic code. This means that short-hand code syntaxes such as lambda functions, and advanced PyTorch operators are often not desired.\n\n## Simple over easy", "| [ControlNet](https://github.com/huggingface/diffusers/tree/main/examples/controlnet) | \ud83d\udc4d |  | \ud83d\udc4d |\n| [InstructPix2Pix](https://github.com/huggingface/diffusers/tree/main/examples/instruct_pix2pix) | \ud83d\udc4d |  |  |\n| [Custom Diffusion](https://github.com/huggingface/diffusers/tree/main/examples/custom_diffusion) |  |  |  |\n| [T2I-Adapters](https://github.com/huggingface/diffusers/tree/main/examples/t2i_adapter) | \ud83d\udc4d |  |  |\n| [Kandinsky 2.2](https://github.com/huggingface/diffusers/tree/main/examples/kandinsky2_2/text_to_image) |  | \ud83d\udc4d |  |\n| [Wuerstchen](https://github.com/huggingface/diffusers/tree/main/examples/wuerstchen/text_to_image) |  | \ud83d\udc4d |  |"], "eval_score_GPT4": "4", "eval_feedback_GPT4": "Feedback: The response provided is mostly correct and factual. It accurately describes the purpose of the \ud83e\udde8 Diffusers tutorials by mentioning that they are designed to demonstrate the effective use of the Diffusers library and that they cover a range of applications such as generating images, audio, and 3D structures of molecules. The response also touches on the tutorials being self-contained, easy-to-tweak, beginner-friendly, and for one-purpose-only, which aligns with the idea of providing a gentle introduction to diffusion models and understanding library fundamentals as stated in the reference answer. However, the response is incomplete as it ends abruptly with \"They are,\" suggesting that there was more to be said. This incomplete sentence leaves the reader without a full understanding of the tutorials' purpose. Despite this, the information provided before the abrupt ending is in line with the reference answer's intent."}, {"question": "What is the default setting for the `allow_flagging` parameter in Gradio's `Interface`?\n", "true_answer": "\"manual\"", "source_doc": "gradio-app/gradio/blob/main/guides/cn/07_other-tutorials/using-flagging.md", "generated_answer": "The default setting for the `allow_flagging` parameter in Gradio's `Interface` is \"manual\". This means that users will see a flag button and samples will only be flagged when the button is clicked.", "retrieved_docs": ["There are [four parameters](https://gradio.app/docs/#interface-header) in `gradio.Interface` that control how flagging works. We will go over them in greater detail.\n\n- `allow_flagging`: this parameter can be set to either `\"manual\"` (default), `\"auto\"`, or `\"never\"`.\n  - `manual`: users will see a button to flag, and samples are only flagged when the button is clicked.\n  - `auto`: users will not see a button to flag, but every sample will be flagged automatically.\n  - `never`: users will not see a button to flag, and no sample will be flagged.\n- `flagging_options`: this parameter can be either `None` (default) or a list of strings.\n  - If `None`, then the user simply clicks on the **Flag** button and no additional options are shown.\n  - If a list of strings are provided, then the user sees several buttons, corresponding to each of the strings that are provided. For example, if the value of this parameter is `[\"Incorrect\", \"Ambiguous\"]`, then buttons labeled **Flag as Incorrect** and **Flag as Ambiguous** appear. This only applies if `allow_flagging` is `\"manual\"`.\n  - The chosen option is then logged along with the input and output.\n- `flagging_dir`: this parameter takes a string.\n  - It represents what to name the directory where flagged data is stored.\n- `flagging_callback`: this parameter takes an instance of a subclass of the `FlaggingCallback` class\n  - Using this parameter allows you to write custom code that gets run when the flag button is clicked\n  - By default, this is set to an instance of `gr.CSVLogger`\n  - One example is setting it to an instance of `gr.HuggingFaceDatasetSaver` which can allow you to pipe any flagged data into a HuggingFace Dataset. (See more below.)\n\n## What happens to flagged data?\n\nWithin the directory provided by the `flagging_dir` argument, a CSV file will log the flagged data.", "Using Flagging\n\nRelated spaces: https://huggingface.co/spaces/gradio/calculator-flagging-crowdsourced, https://huggingface.co/spaces/gradio/calculator-flagging-options, https://huggingface.co/spaces/gradio/calculator-flag-basic\nTags: FLAGGING, DATA\n\n## Introduction\n\nWhen you demo a machine learning model, you might want to collect data from users who try the model, particularly data points in which the model is not behaving as expected. Capturing these \"hard\" data points is valuable because it allows you to improve your machine learning model and make it more reliable and robust.\n\nGradio simplifies the collection of this data by including a **Flag** button with every `Interface`. This allows a user or tester to easily send data back to the machine where the demo is running. In this Guide, we discuss more about how to use the flagging feature, both with `gradio.Interface` as well as with `gradio.Blocks`.\n\n## The **Flag** button in `gradio.Interface`\n\nFlagging with Gradio's `Interface` is especially easy. By default, underneath the output components, there is a button marked **Flag**. When a user testing your model sees input with interesting output, they can click the flag button to send the input and output data back to the machine where the demo is running. The sample is saved to a CSV log file (by default). If the demo involves images, audio, video, or other types of files, these are saved separately in a parallel directory and the paths to these files are saved in the CSV file.\n\nThere are [four parameters](https://gradio.app/docs/#interface-header) in `gradio.Interface` that control how flagging works. We will go over them in greater detail.", "We created the `gradio.HuggingFaceDatasetSaver` class, but you can pass your own custom class as long as it inherits from `FLaggingCallback` defined in [this file](https://github.com/gradio-app/gradio/blob/master/gradio/flagging.py). If you create a cool callback, contribute it to the repo!\n\n## Flagging with Blocks\n\nWhat about if you are using `gradio.Blocks`? On one hand, you have even more flexibility\nwith Blocks -- you can write whatever Python code you want to run when a button is clicked,\nand assign that using the built-in events in Blocks.\n\nAt the same time, you might want to use an existing `FlaggingCallback` to avoid writing extra code.\nThis requires two steps:\n\n1. You have to run your callback's `.setup()` somewhere in the code prior to the\n   first time you flag data\n2. When the flagging button is clicked, then you trigger the callback's `.flag()` method,\n   making sure to collect the arguments correctly and disabling the typical preprocessing.\n\nHere is an example with an image sepia filter Blocks demo that lets you flag\ndata using the default `CSVLogger`:\n\n$code_blocks_flag\n$demo_blocks_flag\n\n## Privacy\n\nImportant Note: please make sure your users understand when the data they submit is being saved, and what you plan on doing with it. This is especially important when you use `allow_flagging=auto` (when all of the data submitted through the demo is being flagged)\n\n### That's all! Happy building :)", "![annotated](https://github.com/gradio-app/gradio/blob/main/guides/assets/annotated.png?raw=true)\n\nIf you're using the `Blocks` API instead, you can insert text, markdown, or HTML anywhere using the `gr.Markdown(...)` or `gr.HTML(...)` components, with descriptive content inside the `Component` constructor.\n\nAnother useful keyword argument is `label=`, which is present in every `Component`. This modifies the label text at the top of each `Component`. You can also add the `info=` keyword argument to form elements like `Textbox` or `Radio` to provide further information on their usage.\n\n```python\ngr.Number(label='Age', info='In years, must be greater than 0')\n```\n\n## Flagging\n\nBy default, an `Interface` will have \"Flag\" button. When a user testing your `Interface` sees input with interesting output, such as erroneous or unexpected model behaviour, they can flag the input for you to review. Within the directory provided by the `flagging_dir=` argument to the `Interface` constructor, a CSV file will log the flagged inputs. If the interface involves file data, such as for Image and Audio components, folders will be created to store those flagged data as well.\n\nFor example, with the calculator interface shown above, we would have the flagged data stored in the flagged directory shown below:\n\n```directory\n+-- calculator.py\n+-- flagged/\n|   +-- logs.csv\n```\n\n_flagged/logs.csv_\n\n```csv\nnum1,operation,num2,Output\n5,add,7,12\n6,subtract,1.5,4.5\n```\n\nWith the sepia interface shown earlier, we would have the flagged data stored in the flagged directory shown below:", "## What happens to flagged data?\n\nWithin the directory provided by the `flagging_dir` argument, a CSV file will log the flagged data.\n\nHere's an example: The code below creates the calculator interface embedded below it:\n\n```python\nimport gradio as gr\n\n\ndef calculator(num1, operation, num2):\n    if operation == \"add\":\n        return num1 + num2\n    elif operation == \"subtract\":\n        return num1 - num2\n    elif operation == \"multiply\":\n        return num1 * num2\n    elif operation == \"divide\":\n        return num1 / num2\n\n\niface = gr.Interface(\n    calculator,\n    [\"number\", gr.Radio([\"add\", \"subtract\", \"multiply\", \"divide\"]), \"number\"],\n    \"number\",\n    allow_flagging=\"manual\"\n)\n\niface.launch()\n```\n\n<gradio-app space=\"gradio/calculator-flag-basic/\"></gradio-app>\n\nWhen you click the flag button above, the directory where the interface was launched will include a new flagged subfolder, with a csv file inside it. This csv file includes all the data that was flagged.\n\n```directory\n+-- flagged/\n|   +-- logs.csv\n```\n\n_flagged/logs.csv_\n\n```csv\nnum1,operation,num2,Output,timestamp\n5,add,7,12,2022-01-31 11:40:51.093412\n6,subtract,1.5,4.5,2022-01-31 03:25:32.023542\n```\n\nIf the interface involves file data, such as for Image and Audio components, folders will be created to store those flagged data as well. For example an `image` input to `image` output interface will create the following structure.", "- `allow_flagging`\uff1a\u6b64\u53c2\u6570\u53ef\u4ee5\u8bbe\u7f6e\u4e3a `\"manual\"`\uff08\u9ed8\u8ba4\u503c\uff09\uff0c`\"auto\"` \u6216 `\"never\"`\u3002\n  - `manual`\uff1a\u7528\u6237\u5c06\u770b\u5230\u4e00\u4e2a\u6807\u8bb0\u6309\u94ae\uff0c\u53ea\u6709\u5728\u70b9\u51fb\u6309\u94ae\u65f6\u6837\u672c\u624d\u4f1a\u88ab\u6807\u8bb0\u3002\n  - `auto`\uff1a\u7528\u6237\u5c06\u4e0d\u4f1a\u770b\u5230\u4e00\u4e2a\u6807\u8bb0\u6309\u94ae\uff0c\u4f46\u6bcf\u4e2a\u6837\u672c\u90fd\u4f1a\u81ea\u52a8\u88ab\u6807\u8bb0\u3002\n  - `never`\uff1a\u7528\u6237\u5c06\u4e0d\u4f1a\u770b\u5230\u4e00\u4e2a\u6807\u8bb0\u6309\u94ae\uff0c\u5e76\u4e14\u4e0d\u4f1a\u6807\u8bb0\u4efb\u4f55\u6837\u672c\u3002\n- `flagging_options`\uff1a\u6b64\u53c2\u6570\u53ef\u4ee5\u662f `None`\uff08\u9ed8\u8ba4\u503c\uff09\u6216\u5b57\u7b26\u4e32\u5217\u8868\u3002\n  - \u5982\u679c\u662f `None`\uff0c\u5219\u7528\u6237\u53ea\u9700\u70b9\u51fb**\u6807\u8bb0**\u6309\u94ae\uff0c\u4e0d\u4f1a\u663e\u793a\u5176\u4ed6\u9009\u9879\u3002\n  - \u5982\u679c\u63d0\u4f9b\u4e86\u4e00\u4e2a\u5b57\u7b26\u4e32\u5217\u8868\uff0c\u5219\u7528\u6237\u4f1a\u770b\u5230\u591a\u4e2a\u6309\u94ae\uff0c\u5bf9\u5e94\u4e8e\u63d0\u4f9b\u7684\u6bcf\u4e2a\u5b57\u7b26\u4e32\u3002\u4f8b\u5982\uff0c\u5982\u679c\u6b64\u53c2\u6570\u7684\u503c\u4e3a`[\" \u9519\u8bef \", \" \u6a21\u7cca \"]`\uff0c\u5219\u4f1a\u663e\u793a\u6807\u8bb0\u4e3a**\u6807\u8bb0\u4e3a\u9519\u8bef**\u548c**\u6807\u8bb0\u4e3a\u6a21\u7cca**\u7684\u6309\u94ae\u3002\u8fd9\u4ec5\u9002\u7528\u4e8e `allow_flagging` \u4e3a `\"manual\"` \u7684\u60c5\u51b5\u3002\n  - \u6240\u9009\u9009\u9879\u5c06\u4e0e\u8f93\u5165\u548c\u8f93\u51fa\u4e00\u8d77\u8bb0\u5f55\u3002\n- `flagging_dir`\uff1a\u6b64\u53c2\u6570\u63a5\u53d7\u4e00\u4e2a\u5b57\u7b26\u4e32\u3002\n  - \u5b83\u8868\u793a\u6807\u8bb0\u6570\u636e\u5b58\u50a8\u7684\u76ee\u5f55\u540d\u79f0\u3002\n- `flagging_callback`\uff1a\u6b64\u53c2\u6570\u63a5\u53d7 `FlaggingCallback` \u7c7b\u7684\u5b50\u7c7b\u7684\u5b9e\u4f8b\n  - \u4f7f\u7528\u6b64\u53c2\u6570\u5141\u8bb8\u60a8\u7f16\u5199\u5728\u70b9\u51fb\u6807\u8bb0\u6309\u94ae\u65f6\u8fd0\u884c\u7684\u81ea\u5b9a\u4e49\u4ee3\u7801\n  - \u9ed8\u8ba4\u60c5\u51b5\u4e0b\uff0c\u5b83\u8bbe\u7f6e\u4e3a `gr.CSVLogger` \u7684\u4e00\u4e2a\u5b9e\u4f8b\n  - \u4e00\u4e2a\u793a\u4f8b\u662f\u5c06\u5176\u8bbe\u7f6e\u4e3a `gr.HuggingFaceDatasetSaver` \u7684\u4e00\u4e2a\u5b9e\u4f8b\uff0c\u8fd9\u6837\u60a8\u53ef\u4ee5\u5c06\u4efb\u4f55\u6807\u8bb0\u7684\u6570\u636e\u5bfc\u5165\u5230 HuggingFace \u6570\u636e\u96c6\u4e2d\uff08\u53c2\u89c1\u4e0b\u6587\uff09\u3002", "![flagging callback hf](/assets/guides/flagging-callback-hf.png)\n\n\u6211\u4eec\u521b\u5efa\u4e86 `gradio.HuggingFaceDatasetSaver` \u7c7b\uff0c\u4f46\u53ea\u8981\u5b83\u7ee7\u627f\u81ea[\u6b64\u6587\u4ef6](https://github.com/gradio-app/gradio/blob/master/gradio/flagging.py)\u4e2d\u5b9a\u4e49\u7684 `FlaggingCallback`\uff0c\u60a8\u53ef\u4ee5\u4f20\u9012\u81ea\u5df1\u7684\u81ea\u5b9a\u4e49\u7c7b\u3002\u5982\u679c\u60a8\u521b\u5efa\u4e86\u4e00\u4e2a\u5f88\u68d2\u7684\u56de\u8c03\uff0c\u8bf7\u5c06\u5176\u8d21\u732e\u7ed9\u8be5\u5b58\u50a8\u5e93\uff01\n\n## \u4f7f\u7528 Blocks \u8fdb\u884c\u6807\u8bb0\n\n\u5982\u679c\u60a8\u6b63\u5728\u4f7f\u7528 `gradio.Blocks`\uff0c\u53c8\u8be5\u600e\u4e48\u529e\u5462\uff1f\u4e00\u65b9\u9762\uff0c\u4f7f\u7528 Blocks \u60a8\u62e5\u6709\u66f4\u591a\u7684\u7075\u6d3b\u6027\n--\u60a8\u53ef\u4ee5\u7f16\u5199\u4efb\u4f55\u60a8\u60f3\u5728\u6309\u94ae\u88ab\u70b9\u51fb\u65f6\u8fd0\u884c\u7684 Python \u4ee3\u7801\uff0c\n\u5e76\u4f7f\u7528 Blocks \u4e2d\u7684\u5185\u7f6e\u4e8b\u4ef6\u5206\u914d\u5b83\u3002\n\n\u540c\u65f6\uff0c\u60a8\u53ef\u80fd\u5e0c\u671b\u4f7f\u7528\u73b0\u6709\u7684 `FlaggingCallback` \u6765\u907f\u514d\u7f16\u5199\u989d\u5916\u7684\u4ee3\u7801\u3002\n\u8fd9\u9700\u8981\u4e24\u4e2a\u6b65\u9aa4\uff1a\n\n1. \u60a8\u5fc5\u987b\u5728\u4ee3\u7801\u4e2d\u7684\u67d0\u4e2a\u4f4d\u7f6e\u8fd0\u884c\u60a8\u7684\u56de\u8c03\u7684 `.setup()` \u65b9\u6cd5\n   \u5728\u7b2c\u4e00\u6b21\u6807\u8bb0\u6570\u636e\u4e4b\u524d\n2. \u5f53\u70b9\u51fb\u6807\u8bb0\u6309\u94ae\u65f6\uff0c\u60a8\u89e6\u53d1\u56de\u8c03\u7684 `.flag()` \u65b9\u6cd5\uff0c\n   \u786e\u4fdd\u6b63\u786e\u6536\u96c6\u53c2\u6570\u5e76\u7981\u7528\u901a\u5e38\u7684\u9884\u5904\u7406\u3002\n\n\u4e0b\u9762\u662f\u4e00\u4e2a\u4f7f\u7528\u9ed8\u8ba4\u7684 `CSVLogger` \u6807\u8bb0\u56fe\u50cf\u6000\u65e7\u6ee4\u955c Blocks \u6f14\u793a\u7684\u793a\u4f8b\uff1a\ndata using the default `CSVLogger`:\n\n$code_blocks_flag\n$demo_blocks_flag\n\n## \u9690\u79c1"], "eval_score_GPT4": "5", "eval_feedback_GPT4": "Feedback: The response provided is completely correct, accurate, and factual. It matches the reference answer by stating that the default setting for the `allow_flagging` parameter in Gradio's `Interface` is \"manual\". Additionally, the response gives a brief explanation of what the \"manual\" setting entails, which aligns with the expected functionality of the parameter."}, {"question": "Where can the full code for the Stable Diffusion demo be found?\n", "true_answer": "https://hf.co/spaces/stabilityai/stable-diffusion/tree/main", "source_doc": "gradio-app/gradio/blob/main/demo/stable-diffusion/DESCRIPTION.md", "generated_answer": "Answer: The full code for the Stable Diffusion demo can be found at the following link: https://hf.co/spaces/stabilityai/stable-diffusion/tree/main1. This information is provided in the extracted documents, specifically in the context provided at the beginning of this conversation.", "retrieved_docs": ["ote: This is a simplified version of the code needed to create the Stable Diffusion demo. See full code here: https://hf.co/spaces/stabilityai/stable-diffusion/tree/main", "You can find the original codebase for Stable Diffusion v1.0 at [CompVis/stable-diffusion](https://github.com/CompVis/stable-diffusion) and Stable Diffusion v2.0 at [Stability-AI/stablediffusion](https://github.com/Stability-AI/stablediffusion) as well as their original scripts for various tasks. Additional official checkpoints for the different Stable Diffusion versions and tasks can be found on the [CompVis](https://huggingface.co/CompVis), [Runway](https://huggingface.co/runwayml), and [Stability AI](https://huggingface.co/stabilityai) Hub organizations. Explore these organizations to find the best checkpoint for your use-case!\n\nThe table below summarizes the available Stable Diffusion pipelines, their supported tasks, and an interactive demo:", "The original codebase can be found at [Xiang-cd/DiffEdit-stable-diffusion](https://github.com/Xiang-cd/DiffEdit-stable-diffusion), and you can try it out in this [demo](https://blog.problemsolversguild.com/technical/research/2022/11/02/DiffEdit-Implementation.html).\n\nThis pipeline was contributed by [clarencechen](https://github.com/clarencechen). \u2764\ufe0f\n\n## Tips", "!--Copyright 2023 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\n\nhttp://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License.\n-->\n\n# Load different Stable Diffusion formats\n\n[[open-in-colab]]\n\nStable Diffusion models are available in different formats depending on the framework they're trained and saved with, and where you download them from. Converting these formats for use in \ud83e\udd17 Diffusers allows you to use all the features supported by the library, such as [using different schedulers](schedulers) for inference, [building your custom pipeline](write_own_pipeline), and a variety of techniques and methods for [optimizing inference speed](../optimization/opt_overview).\n\n<Tip>\n\nWe highly recommend using the `.safetensors` format because it is more secure than traditional pickled files which are vulnerable and can be exploited to execute any code on your machine (learn more in the [Load safetensors](using_safetensors) guide).\n\n</Tip>\n\nThis guide will show you how to convert other Stable Diffusion formats to be compatible with \ud83e\udd17 Diffusers.\n\n## PyTorch .ckpt\n\nThe checkpoint - or `.ckpt` - format is commonly used to store and save models. The `.ckpt` file contains the entire model and is typically several GBs in size. While you can load and use a `.ckpt` file directly with the [`~StableDiffusionPipeline.from_single_file`] method, it is generally better to convert the `.ckpt` file to \ud83e\udd17 Diffusers so both formats are available.", "## StableDiffusionPipelineOutput\n\n[[autodoc]] pipelines.stable_diffusion.StableDiffusionPipelineOutput", "## StableDiffusionXLControlNetXSPipeline\n[[autodoc]] StableDiffusionXLControlNetXSPipeline\n\t- all\n\t- __call__\n\n## StableDiffusionPipelineOutput\n[[autodoc]] pipelines.stable_diffusion.StableDiffusionPipelineOutput", "## Stable Diffusion XL\n\nThere's an equivalent `train_instruct_pix2pix_sdxl.py` script for [Stable Diffusion XL](https://huggingface.co/papers/2307.01952). Please refer to the docs [here](./README_sdxl.md) to learn more."], "eval_score_GPT4": "4", "eval_feedback_GPT4": "Feedback: The response provided is mostly correct and factual. The link given is very close to the reference answer, but it includes an additional number \"1\" at the end of the URL, which could potentially lead to a non-existent page or an error. This small inaccuracy prevents the response from being completely correct."}, {"question": "What transformation does the FNet model use to replace the self-attention layer in a BERT model?\n", "true_answer": "Fourier transform", "source_doc": "huggingface/transformers/blob/main/docs/source/en/model_doc/fnet.md", "generated_answer": "The FNet model replaces the self-attention layer in a BERT model with a fourier transform that returns only the real parts of the transform. This results in a significantly faster and memory-efficient model that achieves about 92-97% accuracy of BERT counterparts on the GLUE benchmark and trains much faster than the BERT model. This model was contributed by [gchhablani](https://huggingface.co/gchhab", "retrieved_docs": ["!--Copyright 2021 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\n\nhttp://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License.\n\n\u26a0\ufe0f Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\nrendered properly in your Markdown viewer.\n\n-->\n\n# FNet\n\n## Overview\n\nThe FNet model was proposed in [FNet: Mixing Tokens with Fourier Transforms](https://arxiv.org/abs/2105.03824) by\nJames Lee-Thorp, Joshua Ainslie, Ilya Eckstein, Santiago Ontanon. The model replaces the self-attention layer in a BERT\nmodel with a fourier transform which returns only the real parts of the transform. The model is significantly faster\nthan the BERT model because it has fewer parameters and is more memory efficient. The model achieves about 92-97%\naccuracy of BERT counterparts on GLUE benchmark, and trains much faster than the BERT model. The abstract from the\npaper is the following:", "2.  Second, BERT\\'s *bi-directional* self-attention layers have to be\n    changed to *uni-directional* self-attention layers to comply with\n    auto-regressive generation. Because both the bi-directional and the\n    uni-directional self-attention layer are based on the same *key*,\n    *query* and *value* projection weights, the decoder\\'s\n    self-attention layer weights can be initialized with BERT\\'s\n    self-attention layer weights. *E.g.* the query, key and value weight\n    parameters of the decoder\\'s uni-directional self-attention layer\n    are initialized with those of BERT\\'s bi-directional self-attention\n    layer \\\\(\\theta_{\\text{BERT}}^{\\text{self-attn}, 2} = \\{\\mathbf{W}_{\\text{BERT}, k}^{\\text{self-attn}, 2}, \\mathbf{W}_{\\text{BERT}, v}^{\\text{self-attn}, 2}, \\mathbf{W}_{\\text{BERT}, q}^{\\text{self-attn}, 2} \\} \\to \\theta_{\\text{dec}}^{\\text{self-attn}, 2} = \\{\\mathbf{W}_{\\text{dec}, k}^{\\text{self-attn}, 2}, \\mathbf{W}_{\\text{dec}, v}^{\\text{self-attn}, 2}, \\mathbf{W}_{\\text{dec}, q}^{\\text{self-attn}, 2} \\}. \\\\) However, in *uni-directional* self-attention each token only\n    attends to all previous tokens, so that the decoder\\'s\n    self-attention layers yield different output vectors than BERT\\'s\n    self-attention layers even though they share the same weights.\n    Compare *e.g.*, the decoder\\'s causally connected graph in the right\n    box versus BERT\\'s fully connected graph in the left box.", "### 3. Machine Learning is evolving at a neck-breaking speed\nResearch in the field of machine learning, and especially neural networks, evolves extremely fast. A model that was state-of-the-art a year ago might be outdated today. We don't know which attention mechanism, position embedding, or architecture will be the best in a year. Therefore, we cannot define standard logical patterns that apply to all models. \n\nAs an example, two years ago, one might have defined BERT's self attention layer as the standard attention layer used by all Transformers models. Logically, a \"standard\" attention function could have been moved into a central `attention.py` file. But then came attention layers that added relative positional embeddings in each attention layer (T5), multiple different forms of chunked attention (Reformer, Longformer, BigBird), and separate attention mechanism for position and word embeddings (DeBERTa), etc... Every time we would have to have asked ourselves whether the \"standard\" attention function should be adapted or whether it would have been better to add a new attention function to `attention.py`. But then how do we name it? `attention_with_positional_embd`, `reformer_attention`, `deberta_attention`? \n\nIt's dangerous to give logical components of machine learning models general names because the perception of what this component stands for might change or become outdated very quickly. E.g., does chunked attention corresponds to GPTNeo's, Reformer's, or BigBird's chunked attention? Is the attention layer a self-attention layer, a cross-attentional layer, or does it include both? However, if we name attention layers by their model's name, we should directly put the attention function in the corresponding modeling file.\n\n### 4. Machine Learning models are static\nThe Transformers library is a unified and polished collection of machine learning models that different research teams have created. Every machine learning model is usually accompanied by a paper and its official GitHub repository. Once a machine learning model is published, it is rarely adapted or changed afterward.", "*We show that Transformer encoder architectures can be sped up, with limited accuracy costs, by replacing the\nself-attention sublayers with simple linear transformations that \"mix\" input tokens. These linear mixers, along with\nstandard nonlinearities in feed-forward layers, prove competent at modeling semantic relationships in several text\nclassification tasks. Most surprisingly, we find that replacing the self-attention sublayer in a Transformer encoder\nwith a standard, unparameterized Fourier Transform achieves 92-97% of the accuracy of BERT counterparts on the GLUE\nbenchmark, but trains 80% faster on GPUs and 70% faster on TPUs at standard 512 input lengths. At longer input lengths,\nour FNet model is significantly faster: when compared to the \"efficient\" Transformers on the Long Range Arena\nbenchmark, FNet matches the accuracy of the most accurate models, while outpacing the fastest models across all\nsequence lengths on GPUs (and across relatively shorter lengths on TPUs). Finally, FNet has a light memory footprint\nand is particularly efficient at smaller model sizes; for a fixed speed and accuracy budget, small FNet models\noutperform Transformer counterparts.*\n\nThis model was contributed by [gchhablani](https://huggingface.co/gchhablani). The original code can be found [here](https://github.com/google-research/google-research/tree/master/f_net).\n\n## Usage tips\n\nThe model was trained without an attention mask as it is based on Fourier Transform. The model was trained with \nmaximum sequence length 512 which includes pad tokens. Hence, it is highly recommended to use the same maximum \nsequence length for fine-tuning and inference.\n\n## Resources\n\n- [Text classification task guide](../tasks/sequence_classification)\n- [Token classification task guide](../tasks/token_classification)\n- [Question answering task guide](../tasks/question_answering)\n- [Masked language modeling task guide](../tasks/masked_language_modeling)\n- [Multiple choice task guide](../tasks/multiple_choice)\n\n## FNetConfig\n\n[[autodoc]] FNetConfig\n\n## FNetTokenizer", "--\ntitle: \"Perceiver IO: a scalable, fully-attentional model that works on any modality\"\nthumbnail: /blog/assets/41_perceiver/thumbnail.png\nauthors:\n- user: nielsr\n---\n\n# Perceiver IO: a scalable, fully-attentional model that works on any modality\n\n\n### TLDR\n\nWe've added [Perceiver IO](https://huggingface.co/docs/transformers/model_doc/perceiver) to Transformers, the first Transformer-based neural network that works on all kinds of modalities (text, images, audio, video, point clouds,...) and combinations thereof. Take a look at the following Spaces to view some examples:\n- predicting [optical flow](https://huggingface.co/spaces/nielsr/perceiver-optical-flow) between images\n- [classifying images](https://huggingface.co/spaces/nielsr/perceiver-image-classification).\n\nWe also provide [several notebooks](https://github.com/NielsRogge/Transformers-Tutorials/tree/master/Perceiver).\n\nBelow, you can find a technical explanation  of the model.\n\n### Introduction\n\nThe [Transformer](https://arxiv.org/abs/1706.03762), originally introduced by \nVaswani et al. in 2017, caused a revolution in the AI community, initially improving\nstate-of-the-art (SOTA) results in machine translation. In 2018, [BERT](https://arxiv.org/abs/1810.04805)\nwas released, a Transformer encoder-only model that crushed the benchmarks of natural language\nprocessing (NLP), most famously the [GLUE benchmark](https://gluebenchmark.com/). \n\nNot long after that, AI researchers started to apply the idea of BERT to other domains. To name a few examples:", ". GPT-2 for instance, was pretrained this way using the content of 45 millions links posted by users on Reddit. Another example of self-supervised pretraining objective is to predict the value of randomly masked words, which is similar to fill-in-the-blank tests you may have done in school. BERT was pretrained this way using the English Wikipedia and 11,000 unpublished books. In practice, transfer learning is applied on a given model by throwing away its head, that is, its last layers focused on the pretraining objective, and replacing it with a new, randomly initialized, head suitable for the task at hand. For instance, when we fine-tuned a BERT model earlier, we removed the head that classified mask words and replaced it with a classifier with 2 outputs, since our task had two labels. To be as efficient as possible, the pretrained model used should be as similar as possible to the task it\u2019s fine-tuned on. For instance, if the problem it\u2019s to classify German sentences, it\u2019s best to use a German pretrained model. But with the good comes the bad. The pretrained model does not only transfer its knowledge, but also any bias it may contain. ImageNet mostly contains images coming from the United States and Western Europe, so models fine-tuned with it usually will perform better on images from these countries. OpenAI also studied the bias in the predictions of its GPT-3 model (which was pretrained using the guess the next work objective). Changing the gender of the prompt from \"He was very\" to \"She was very\" changed the predictions from mostly neutral adjectives to almost only physical ones. In their model card of the GPT-2 model, OpenAI also acknowledges its bias and discourages its use in systems that interact with humans.", "Now you have set up a development environment to port *brand_new_bert* to \ud83e\udd17 Transformers.\n\n### 3.-4. Run a pretrained checkpoint using the original repository\n\nAt first, you will work on the original *brand_new_bert* repository. Often, the original implementation is very\n\u201cresearchy\u201d. Meaning that documentation might be lacking and the code can be difficult to understand. But this should\nbe exactly your motivation to reimplement *brand_new_bert*. At Hugging Face, one of our main goals is to *make people\nstand on the shoulders of giants* which translates here very well into taking a working model and rewriting it to make\nit as **accessible, user-friendly, and beautiful** as possible. This is the number-one motivation to re-implement\nmodels into \ud83e\udd17 Transformers - trying to make complex new NLP technology accessible to **everybody**.\n\nYou should start thereby by diving into the original repository.\n\nSuccessfully running the official pretrained model in the original repository is often **the most difficult** step.\nFrom our experience, it is very important to spend some time getting familiar with the original code-base. You need to\nfigure out the following:\n\n- Where to find the pretrained weights?\n- How to load the pretrained weights into the corresponding model?\n- How to run the tokenizer independently from the model?\n- Trace one forward pass so that you know which classes and functions are required for a simple forward pass. Usually,\n  you only have to reimplement those functions.\n- Be able to locate the important components of the model: Where is the model's class? Are there model sub-classes,\n  *e.g.* EncoderModel, DecoderModel? Where is the self-attention layer? Are there multiple different attention layers,\n  *e.g.* *self-attention*, *cross-attention*...?\n- How can you debug the model in the original environment of the repo? Do you have to add *print* statements, can you\n  work with an interactive debugger like *ipdb*, or should you use an efficient IDE to debug the model, like PyCharm?"], "eval_score_GPT4": "5", "eval_feedback_GPT4": "The response correctly identifies the Fourier transform as the replacement for the self-attention layer in the FNet model, which aligns with the reference answer. However, the response goes beyond the reference answer by providing additional details about the characteristics and performance of the FNet model. These details are not required by the instruction but are accurate and contribute to a fuller understanding of the FNet model's transformation. Based on the score rubric, the response is completely correct, accurate, and factual in the context of the transformation used in FNet."}, {"question": "What type of test should typically accompany a bug fix in Gradio's testing strategy?\n", "true_answer": "Dynamic code test", "source_doc": "gradio-app/gradio/blob/main/test-strategy.md", "generated_answer": "According to Gradio's testing strategy document (extracted document 0), every bug fix should be accompanied by a test that failed before the fix and passes afterwards. This test should typically be a dynamic code test, but it could also be a linting rule or a new type of test if appropriate. This principle is outlined in section 2: \"Managing defects\" of the document. The specific bug fixes and their accompanying tests are listed in sections 3 and ", "retrieved_docs": ["Test Strategy\n\nVery brief, mildly aspirational test strategy document. This isn't where we are but it is where we want to get to.\n\nThis document does not detail how to setup an environment or how to run the tests locally nor does it contain any best practices that we try to follow when writing tests, that information exists in the [contributing guide](https://github.com/gradio-app/gradio/blob/main/CONTRIBUTING.md).\n\n## Objectives\n\nThe purposes of all testing activities on Gradio fit one of the following objectives:\n\n1. Ensure that the Gradio library functions as we expect it to.\n2. Enable the maintenance team to quickly identify both the presence and source of defects.\n3. Prevent regressions, i.e. if we fix something it should stay fixed.\n4. Improve the quality of the codebase in order to ease maintenance efforts.\n5. Reduce the amount of manual testing required.\n\n## Scope\n\nTesting is always a tradeoff. We can't cover everything unless we want to spend all of our time writing and running tests. We should focus on a few keys areas.\n\nWe should not focus on code coverage but on test coverage following the below criteria:\n\n- The documented Gradio API (that's the bit that users interact with via python) should be tested thoroughly. (1)\n- Additional gradio elements that are both publicly available and used internally (such as the Python and JS client libraries) should be tested thoroughly. (1)\n- Additional gradio elements that are publicly available should be tested as thoroughly as is reasonable (this could be things like demos/the gradio CLI/ other tooling). The importance of each individual component, and the appropriate investment of effort, needs to be assessed on a case-by-case basis. (1)\n- Element boundaries should be tested where there is reasonable cause to do so (e.g. config generation) (1)\n- Implementation details should only be tested where there is sufficient complexity to warrant it. (1)\n- Bug fixes should be accompanied by tests wherever is reasonably possible. (3)\n\n## Types of testing\n\nOur tests will broadly fall into one of three categories:", "## Test execution\n\nTests need to be executed in a number of environments and at different stages of the development cycle in order to be useful. The requirements for tests are as follows:\n\n- **Locally**: it is important that developers can easily run most tests locally to ensure a passing suite before making a PR. There are some exceptions to this, certain tests may require access to secret values which we cannot make available to all possible contributors for practical security reasons. It is reasonable that it isn't possible to run these tests but they should be disabled by default when running locally.\n- **CI** - It is _critical_ that all tests run successfully in CI with no exceptions. Not every test is required to pass to satisfy CI checks for practical reasons but it is required that all tests should run in CI and notify us if something unexpected happens in order for the development team to take appropriate action.\n\nFor instructions on how to write and run tests see the [contributing guide](https://github.com/gradio-app/gradio/blob/main/CONTRIBUTING.md).\n\n## Managing defects\n\nAs we formalise our testing strategy and bring / keep our test up to standard, it is important that we have some principles on managing defects as they occur/ are reported. For now we can have one very simple rule:\n\n- Every bug fix should be accompanied by a test that failed before the fix and passes afterwards. This test should _typically_ be a dynamic code test but it could be a linting rule or new type if that is appropriate. There are always exceptions but we should think very carefully before ignoring this rule.", "### Fixes\n\n- [#6046](https://github.com/gradio-app/gradio/pull/6046) [`dbb7de5e0`](https://github.com/gradio-app/gradio/commit/dbb7de5e02c53fee05889d696d764d212cb96c74) - fix tests. Thanks [@pngwn](https://github.com/pngwn)!\n\n## 0.3.0-beta.6\n\n### Features", "### Fixes\n\n- [#6046](https://github.com/gradio-app/gradio/pull/6046) [`dbb7de5e0`](https://github.com/gradio-app/gradio/commit/dbb7de5e02c53fee05889d696d764d212cb96c74) - fix tests.  Thanks [@pngwn](https://github.com/pngwn)!\n\n## 0.1.0-beta.6\n\n### Features", "### Testing and Infrastructure Changes:\n\n- Fixes tests that were failing locally but passing on CI by [@abidlabs](https://github.com/abidlabs) in [PR 3411](https://github.com/gradio-app/gradio/pull/3411)\n- Remove codecov from the repo by [@aliabd](https://github.com/aliabd) in [PR 3415](https://github.com/gradio-app/gradio/pull/3415)\n\n### Breaking Changes:\n\nNo changes to highlight.\n\n### Full Changelog:", "### Testing and Infrastructure Changes:\n\n- Fixes tests that were failing locally but passing on CI by [@abidlabs](https://github.com/abidlabs) in [PR 3411](https://github.com/gradio-app/gradio/pull/3411)\n- Remove codecov from the repo by [@aliabd](https://github.com/aliabd) in [PR 3415](https://github.com/gradio-app/gradio/pull/3415)\n\n### Breaking Changes:\n\nNo changes to highlight.\n\n### Full Changelog:", "## 4.9.1\n\n### Features\n\n- [#6781](https://github.com/gradio-app/gradio/pull/6781) [`a807ede`](https://github.com/gradio-app/gradio/commit/a807ede818e0690949aca41020e75a96f0110ece) - Fix backend tests on Windows.  Thanks [@abidlabs](https://github.com/abidlabs)!\n\n### Fixes"], "eval_score_GPT4": "4", "eval_feedback_GPT4": "Feedback: The response provided is mostly correct and factual. It aligns with the reference answer by stating that a dynamic code test should typically accompany a bug fix in Gradio's testing strategy. However, the response goes beyond the reference answer by mentioning linting rules and the possibility of new types of tests, which were not part of the reference answer. This additional information is not incorrect, but it is not strictly necessary to answer the question posed. The response could be more concise by focusing solely on the dynamic code test as the reference answer does."}, {"question": "How can you force mixed precision training when initializing the Accelerator in \ud83e\udd17 Accelerate?\n", "true_answer": "By passing `fp16=True` to the Accelerator init.", "source_doc": "huggingface/blog/blob/main/accelerate-library.md", "generated_answer": "To force mixed precision training when initializing the Accelerator in \ud83e\udd17 Accelerate, you can pass `fp16=True` as an argument to the `Accelerator` constructor. This will enable mixed precision training using the bf16 format. Here's an example:\n\n```python\nfrom accelerate import Accelerator\n\naccelerator = Accelerator(fp16=True)\n```\n\nThis", "retrieved_docs": ["The full example training loop with \ud83e\udd17 Accelerate is only a handful of lines of code long:\n\n```py\nfrom accelerate import Accelerator\nfrom torch.utils.data.dataloader import DataLoader\n\ndataloader = DataLoader(ds, batch_size=training_args.per_device_train_batch_size)\n\nif training_args.gradient_checkpointing:\n    model.gradient_checkpointing_enable()\n\naccelerator = Accelerator(fp16=training_args.fp16)\nmodel, optimizer, dataloader = accelerator.prepare(model, adam_bnb_optim, dataloader)\n\nmodel.train()\nfor step, batch in enumerate(dataloader, start=1):\n    loss = model(**batch).loss\n    loss = loss / training_args.gradient_accumulation_steps\n    accelerator.backward(loss)\n    if step % training_args.gradient_accumulation_steps == 0:\n        optimizer.step()\n        optimizer.zero_grad()\n```\n\nFirst we wrap the dataset in a [`DataLoader`](https://pytorch.org/docs/stable/data.html#torch.utils.data.DataLoader). \nThen we can enable gradient checkpointing by calling the model's [`~PreTrainedModel.gradient_checkpointing_enable`] method. \nWhen we initialize the [`Accelerator`](https://huggingface.co/docs/accelerate/package_reference/accelerator#accelerate.Accelerator) \nwe can specify if we want to use mixed precision training and it will take care of it for us in the [`prepare`] call. \nDuring the [`prepare`](https://huggingface.co/docs/accelerate/package_reference/accelerator#accelerate.Accelerator.prepare) \ncall the dataloader will also be distributed across workers should we use multiple GPUs. We use the same [8-bit optimizer](#8-bit-adam) from the earlier example.", "</Tip>\n\nInitialize an \ud83e\udd17 Accelerate environment:\n\n```bash\naccelerate config\n```\n\nTo setup a default \ud83e\udd17 Accelerate environment without choosing any configurations:\n\n```bash\naccelerate config default\n```\n\nOr if your environment doesn't support an interactive shell like a notebook, you can use:\n\n```bash\nfrom accelerate.utils import write_basic_config\n\nwrite_basic_config()\n```\n\nLastly, if you want to train a model on your own dataset, take a look at the [Create a dataset for training](create_dataset) guide to learn how to create a dataset that works with the training script.\n\n## Script parameters\n\n<Tip>\n\nThe following sections highlight parts of the training script that are important for understanding how to modify it, but it doesn't cover every aspect of the script in detail. If you're interested in learning more, feel free to read through the [script](https://github.com/huggingface/diffusers/blob/main/examples/unconditional_image_generation/train_unconditional.py) and let us know if you have any questions or concerns.\n\n</Tip>\n\nThe training script provides many parameters to help you customize your training run. All of the parameters and their descriptions are found in the [`parse_args()`](https://github.com/huggingface/diffusers/blob/096f84b05f9514fae9f185cbec0a4d38fbad9919/examples/unconditional_image_generation/train_unconditional.py#L55) function. It provides default values for each parameter, such as the training batch size and learning rate, but you can also set your own values in the training command if you'd like.\n\nFor example, to speedup training with mixed precision using the bf16 format, add the `--mixed_precision` parameter to the training command:", "```bash\ncd examples/controlnet\npip install -r requirements_flax.txt\n```\n\n</hfoption>\n</hfoptions>\n\n<Tip>\n\n\ud83e\udd17 Accelerate is a library for helping you train on multiple GPUs/TPUs or with mixed-precision. It'll automatically configure your training setup based on your hardware and environment. Take a look at the \ud83e\udd17 Accelerate [Quick tour](https://huggingface.co/docs/accelerate/quicktour) to learn more.\n\n</Tip>\n\nInitialize an \ud83e\udd17 Accelerate environment:\n\n```bash\naccelerate config\n```\n\nTo setup a default \ud83e\udd17 Accelerate environment without choosing any configurations:\n\n```bash\naccelerate config default\n```\n\nOr if your environment doesn't support an interactive shell, like a notebook, you can use:\n\n```bash\nfrom accelerate.utils import write_basic_config\n\nwrite_basic_config()\n```\n\nLastly, if you want to train a model on your own dataset, take a look at the [Create a dataset for training](create_dataset) guide to learn how to create a dataset that works with the training script.\n\n<Tip>\n\nThe following sections highlight parts of the training script that are important for understanding how to modify it, but it doesn't cover every aspect of the script in detail. If you're interested in learning more, feel free to read through the [script](https://github.com/huggingface/diffusers/blob/main/examples/controlnet/train_controlnet.py) and let us know if you have any questions or concerns.\n\n</Tip>\n\n## Script parameters", "Then we will need an optimizer. As usual we use the classic `AdamW`, which is like Adam, but with a fix in the way weight decay is applied:\n\n```py\nfrom torch.optim import AdamW\n\noptimizer = AdamW(model.parameters(), lr=2e-5)\n```\n\nOnce we have all those objects, we can send them to the `accelerator.prepare()` method. Remember that if you want to train on TPUs in a Colab notebook, you will need to move all of this code into a training function, and that shouldn't execute any cell that instantiates an `Accelerator`. We can force mixed-precision training by passing `fp16=True` to the `Accelerator` (or, if you are executing the code as a script, just make sure to fill in the \ud83e\udd17 Accelerate `config` appropriately).\n\n```py\nfrom accelerate import Accelerator\n\naccelerator = Accelerator(fp16=True)\nmodel, optimizer, train_dataloader, eval_dataloader = accelerator.prepare(\n    model, optimizer, train_dataloader, eval_dataloader\n)\n```\n\nAs you should know from the previous sections, we can only use the `train_dataloader` length to compute the number of training steps after it has gone through the `accelerator.prepare()` method. We use the same linear schedule as in the previous sections:\n\n```py\nfrom transformers import get_scheduler\n\nnum_train_epochs = 3\nnum_update_steps_per_epoch = len(train_dataloader)\nnum_training_steps = num_train_epochs * num_update_steps_per_epoch\n\nlr_scheduler = get_scheduler(\n    \"linear\",\n    optimizer=optimizer,\n    num_warmup_steps=0,\n    num_training_steps=num_training_steps,\n)\n```", "```bash\ncd examples/instruct_pix2pix\npip install -r requirements.txt\n```\n\n<Tip>\n\n\ud83e\udd17 Accelerate is a library for helping you train on multiple GPUs/TPUs or with mixed-precision. It'll automatically configure your training setup based on your hardware and environment. Take a look at the \ud83e\udd17 Accelerate [Quick tour](https://huggingface.co/docs/accelerate/quicktour) to learn more.\n\n</Tip>\n\nInitialize an \ud83e\udd17 Accelerate environment:\n\n```bash\naccelerate config\n```\n\nTo setup a default \ud83e\udd17 Accelerate environment without choosing any configurations:\n\n```bash\naccelerate config default\n```\n\nOr if your environment doesn't support an interactive shell, like a notebook, you can use:\n\n```bash\nfrom accelerate.utils import write_basic_config\n\nwrite_basic_config()\n```\n\nLastly, if you want to train a model on your own dataset, take a look at the [Create a dataset for training](create_dataset) guide to learn how to create a dataset that works with the training script.\n\n<Tip>\n\nThe following sections highlight parts of the training script that are important for understanding how to modify it, but it doesn't cover every aspect of the script in detail. If you're interested in learning more, feel free to read through the [script](https://github.com/huggingface/diffusers/blob/main/examples/instruct_pix2pix/train_instruct_pix2pix.py) and let us know if you have any questions or concerns.\n\n</Tip>\n\n## Script parameters", "```bash\ncd examples/t2i_adapter\npip install -r requirements.txt\n```\n\n<Tip>\n\n\ud83e\udd17 Accelerate is a library for helping you train on multiple GPUs/TPUs or with mixed-precision. It'll automatically configure your training setup based on your hardware and environment. Take a look at the \ud83e\udd17 Accelerate [Quick tour](https://huggingface.co/docs/accelerate/quicktour) to learn more.\n\n</Tip>\n\nInitialize an \ud83e\udd17 Accelerate environment:\n\n```bash\naccelerate config\n```\n\nTo setup a default \ud83e\udd17 Accelerate environment without choosing any configurations:\n\n```bash\naccelerate config default\n```\n\nOr if your environment doesn't support an interactive shell, like a notebook, you can use:\n\n```bash\nfrom accelerate.utils import write_basic_config\n\nwrite_basic_config()\n```\n\nLastly, if you want to train a model on your own dataset, take a look at the [Create a dataset for training](create_dataset) guide to learn how to create a dataset that works with the training script.\n\n<Tip>\n\nThe following sections highlight parts of the training script that are important for understanding how to modify it, but it doesn't cover every aspect of the script in detail. If you're interested in learning more, feel free to read through the [script](https://github.com/huggingface/diffusers/blob/main/examples/t2i_adapter/train_t2i_adapter_sdxl.py) and let us know if you have any questions or concerns.\n\n</Tip>\n\n## Script parameters", "upercharge your Pytorch training loop with Hugging Face Accelerate. There are multiple setups on which you can run your training: it could be on CPU, GPUs, TPUs. Distributed on one machine with several devices, or several machines (often called nodes) each with multiple devices. On top of that there are new tweaks to make your training faster or more memory efficient, like mixed precision and DeepSpeed. Each of those setups or training tweaks, requires you to change the code of your training loop in one way or another and to learn a new API. All those setups are handled by the Trainer API, and there are several third-party libraries that can also help you with that. The problem with those is that they can feel like a black box and that it might not be easy to implement the tweak to the training loop you need. Accelerate has been designed specifically to let you retain full control over your training loop and be as non-intrusive as possible. With just four lines to add to your training loop (here shown on the code of the training loop from the \"Raw training loop\" video), Accelerate will handle all the setups and training tweaks mentioned on the first slide. It's only one API to learn and master instead of ten different ones. More specifically, you have to import and instantiate an accelerator object, that will handle all the necessary code for your  specific setup. Then you have to send it the model, optimizer and dataloaders you are using in the prepare method, which is the main method to remember. Accelerate handles device placement, so you don't need to put your batch on the specific device you are using. Finally, you have to replace the loss.backward line by accelerate.backward(loss), and that's all you need! Accelerate also handles distributed evaluation. You can still use a classic evaluation loop such as the one we saw in the \"Raw training loop\" video, in which case all processes will each perform the full evaluation"], "eval_score_GPT4": "4", "eval_feedback_GPT4": "The response correctly identifies that mixed precision training can be enabled by setting `fp16=True` when initializing the Accelerator in \ud83e\udd17 Accelerate, which aligns with the reference answer. However, the response inaccurately states that this will enable mixed precision training using the bf16 format, which is not correct as `fp16` refers to 16-bit floating-point precision, not bfloat16 (bf16). The reference answer does not mention bf16, and the inclusion of this incorrect detail affects the accuracy of the response. Based on the score rubric, the response is mostly correct but includes an inaccuracy that is not present in the reference answer."}, {"question": "What is the range of parameters for the LLaMA models?\n", "true_answer": "7B to 65B parameters", "source_doc": "huggingface/transformers/blob/main/docs/source/en/model_doc/llama.md", "generated_answer": "The LLaMA models come in three sizes with parameters ranging from 7 billion (7B) to 70 billion (70B).", "retrieved_docs": ["The first model family in this series was the [LLaMA](https://huggingface.co/papers/2302.13971) family, released by Meta AI. The explicit objective of the researchers was to train a set of models of various sizes with the best possible performances for a given computing budget. For one of the first times, the research team explicitly decided to consider not only the training budget but also the inference cost (for a given performance objective, how much does it cost to run inference with the model). In this perspective, they decided to train smaller models on even more data and for more steps than was usually done, thereby reaching higher performances at a smaller model size (the trade-off being training compute efficiency). The biggest model in the Llama 1 family is a 65B parameters model trained on 1.4T tokens, while the smaller models (resp. 6 and 13B parameters) were trained on 1T tokens. The small 13B LLaMA model outperformed GPT-3 on most benchmarks, and the biggest LLaMA model was state of the art when it came out. The weights were released with a non-commercial license though, limiting the adoption by the community. \n\nThe [Pythia](https://huggingface.co/papers/2304.01373) models were released by the open-source non-profit lab Eleuther AI, and were a [suite of LLMs](https://huggingface.co/collections/EleutherAI/pythia-scaling-suite-64fb5dfa8c21ebb3db7ad2e1) of different sizes, trained on completely public data, provided to help researchers to understand the different steps of LLM training.", "\u2697\ufe0f Optimization\n- [Fine-tune Llama 2 with DPO](https://huggingface.co/blog/dpo-trl), a guide to using the TRL library's DPO method to fine tune Llama 2 on a specific dataset.\n- [Extended Guide: Instruction-tune Llama 2](https://www.philschmid.de/instruction-tune-llama-2), a guide to training Llama 2 to generate instructions from inputs, transforming the model from instruction-following to instruction-giving.\n- A [notebook](https://colab.research.google.com/drive/1SYpgFpcmtIUzdE7pxqknrM4ArCASfkFQ?usp=sharing) on how to fine-tune the Llama 2 model on a personal computer using QLoRa and TRL. \ud83c\udf0e\n\n\u26a1\ufe0f Inference\n- A [notebook](https://colab.research.google.com/drive/1TC56ArKerXUpbgRy5vM3woRsbTEVNq7h?usp=sharing) on how to quantize the Llama 2 model using GPTQ from the AutoGPTQ library. \ud83c\udf0e\n- A [notebook](https://colab.research.google.com/drive/1X1z9Q6domMKl2CnEM0QGHNwidLfR4dW2?usp=sharing) on how to run the Llama 2 Chat Model with 4-bit quantization on a local computer or Google Colab. \ud83c\udf0e\n\n\ud83d\ude80 Deploy\n- [Fine-tune LLaMA 2 (7-70B) on Amazon SageMaker](https://www.philschmid.de/sagemaker-llama2-qlora), a complete guide from setup to QLoRA fine-tuning and deployment on Amazon SageMaker.\n- [Deploy Llama 2 7B/13B/70B on Amazon SageMaker](https://www.philschmid.de/sagemaker-llama-llm), a guide on using Hugging Face's LLM DLC container for secure and scalable deployment.\n\n\n## LlamaConfig\n\n[[autodoc]] LlamaConfig", "!--Copyright 2023 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\n\nhttp://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License.\n\n\u26a0\ufe0f Note that this file is in Markdown but contains specific syntax for our doc-builder (similar to MDX) that may not be\nrendered properly in your Markdown viewer.\n\n-->\n\n# CodeLlama\n\n## Overview\n\nThe Code Llama model was proposed in [Code Llama: Open Foundation Models for Code](https://ai.meta.com/research/publications/code-llama-open-foundation-models-for-code/) by Baptiste Rozi\u00e8re, Jonas Gehring, Fabian Gloeckle, Sten Sootla, Itai Gat, Xiaoqing Ellen Tan, Yossi Adi, Jingyu Liu, Tal Remez, J\u00e9r\u00e9my Rapin, Artyom Kozhevnikov, Ivan Evtimov, Joanna Bitton, Manish Bhatt, Cristian Canton Ferrer, Aaron Grattafiori, Wenhan Xiong, Alexandre D\u00e9fossez, Jade Copet, Faisal Azhar, Hugo Touvron, Louis Martin, Nicolas Usunier, Thomas Scialom, Gabriel Synnaeve.\n\nThe abstract from the paper is the following:", "*we\u2019re currently running evaluation of the Llama 2 70B (non chatty version). This table will be updated with the results.\n\n\n## Demo\n\nYou can easily try the Big Llama 2 Model (70 billion parameters!) in [this Space](https://huggingface.co/spaces/ysharma/Explore_llamav2_with_TGI) or in the playground embedded below:\n\n<script type=\"module\" src=\"https://gradio.s3-us-west-2.amazonaws.com/3.37.0/gradio.js\"> </script>\n<gradio-app theme_mode=\"light\" space=\"ysharma/Explore_llamav2_with_TGI\"></gradio-app>\n\nUnder the hood, this playground uses Hugging Face's [Text Generation Inference](https://github.com/huggingface/text-generation-inference), the same technology that powers [HuggingChat](https://huggingface.co/chat/), and which we'll share more in the following sections.\n\n\n## Inference\nIn this section, we\u2019ll go through different approaches to running inference of the Llama2 models. Before using these models, make sure you have requested access to one of the models in the official [Meta Llama 2](https://huggingface.co/meta-llama) repositories. \n\n**Note: Make sure to also fill the official Meta form. Users are provided access to the repository once both forms are filled after few hours.**\n\n### Using transformers\n\nWith transformers [release 4.31](https://github.com/huggingface/transformers/releases/tag/v4.31.0), one can already use Llama 2 and leverage all the tools within the HF ecosystem, such as:", "</Tip>\n\nTips:\n\n- Weights for the Llama2 models can be obtained by filling out [this form](https://ai.meta.com/resources/models-and-libraries/llama-downloads/)\n- The architecture is very similar to the first Llama, with the addition of Grouped Query Attention (GQA) following this [paper](https://arxiv.org/pdf/2305.13245.pdf)\n- Setting `config.pretraining_tp` to a value different than 1 will activate the more accurate but slower computation of the linear layers, which should better match the original logits.\n- The original model uses `pad_id = -1` which means that there is no padding token. We can't have the same logic, make sure to add a padding token using `tokenizer.add_special_tokens({\"pad_token\":\"<pad>\"})` and resize the token embedding accordingly. You should also set the `model.config.pad_token_id`. The `embed_tokens` layer of the model is initialized with `self.embed_tokens = nn.Embedding(config.vocab_size, config.hidden_size, self.config.padding_idx)`, which makes sure that encoding the padding token will output zeros, so passing it when initializing is recommended.\n- After filling out the form and gaining access to the model checkpoints, you should be able to use the already converted checkpoints. Otherwise, if you are converting your own model, feel free to use the [conversion script](https://github.com/huggingface/transformers/blob/main/src/transformers/models/llama/convert_llama_weights_to_hf.py). The script can be called with the following (example) command:\n\n```bash\npython src/transformers/models/llama/convert_llama_weights_to_hf.py \\\n    --input_dir /path/to/downloaded/llama/weights --model_size 7B --output_dir /output/path\n```", "1. **[LLaMA](https://huggingface.co/docs/transformers/model_doc/llama)** (from The FAIR team of Meta AI) released with the paper [LLaMA: Open and Efficient Foundation Language Models](https://arxiv.org/abs/2302.13971) by Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timoth\u00e9e Lacroix, Baptiste Rozi\u00e8re, Naman Goyal, Eric Hambro, Faisal Azhar, Aurelien Rodriguez, Armand Joulin, Edouard Grave, Guillaume Lample.", "### What is Llama 2?\n\nLlama 2 is a family of LLMs from Meta, trained on 2 trillion tokens. Llama 2 comes in three sizes - 7B, 13B, and 70B parameters - and introduces key improvements like longer context length, commercial licensing, and optimized chat abilities through reinforcement learning compared to Llama (1). If you want to learn more about Llama 2 check out this [blog post](https://huggingface.co/blog/llama2). \n\n### What is GPTQ?\n\nGPTQ is a post-training quantziation method to compress LLMs, like GPT. GPTQ compresses GPT (decoder) models by reducing the number of bits needed to store each weight in the model, from 32 bits down to just 3-4 bits. This means the model takes up much less memory and can run on less Hardware, e.g. Single GPU for 13B Llama2 models. GPTQ analyzes each layer of the model separately and approximates the weights to preserve the overall accuracy. If you want to learn more and how to use it, check out [Optimize open LLMs using GPTQ and Hugging Face Optimum](https://www.philschmid.de/gptq-llama).\n\n## Benchmark\n\nTo benchmark the real-world performance of Llama 2, we tested 3 model sizes (7B, 13B, 70B parameters) on four different instance types with four different load levels, resulting in 60 different configurations:\n\n- Models: We evaluated all currently available model sizes, including 7B, 13B, and 70B.\n- Concurrent Requests: We tested configurations with 1, 5, 10, and 20 concurrent requests to determine the performance on different usage scenarios.\n- Instance Types: We evaluated different GPU instances, including g5.2xlarge, g5.12xlarge, g5.48xlarge powered by NVIDIA A10G GPUs, and p4d.24xlarge powered by NVIDIA A100 40GB GPU.\n- Quantization: We compared performance with and without quantization. We used GPTQ 4-bit as a quantization technique."], "eval_score_GPT4": "4", "eval_feedback_GPT4": "Feedback: The response provided is mostly correct in stating that the LLaMA models come in different sizes and includes the correct lower bound of the range (7 billion parameters). However, it inaccurately states the upper bound as 70 billion parameters instead of the correct 65 billion parameters as per the reference answer. This indicates that while the response is on the right track, it is not completely accurate."}, {"question": "What is the purpose of tokenizers in the NLP pipeline?\n", "true_answer": "To translate text into data that can be processed by the model.", "source_doc": "huggingface/course/blob/main/chapters/en/chapter2/4.mdx", "generated_answer": "Tokenizers in the NLP pipeline serve the purpose of converting text into numerical inputs that can be fed into a neural network for processing. They achieve this by breaking down the text into smaller units called tokens, which can be words, sub-words, or characters, depending on the tokenization method. The vocabulary size of the tokenizer indicates how many different tokens it knows, and the size of a dataset is measured by the number of tokens it contains. Tokenizers also keep track of additional", "retrieved_docs": ["## What is a Tokenizer\n\nA Tokenizer works as a pipeline, it processes some raw text as input and outputs an `Encoding`.\nThe various steps of the pipeline are:\n\n1. The `Normalizer`: in charge of normalizing the text. Common examples of normalization are\n   the [unicode normalization standards](https://unicode.org/reports/tr15/#Norm_Forms), such as `NFD` or `NFKC`.\n   More details about how to use the `Normalizers` are available on the\n   [Hugging Face blog](https://huggingface.co/docs/tokenizers/components#normalizers)\n2. The `PreTokenizer`: in charge of creating initial words splits in the text. The most common way of\n   splitting text is simply on whitespace.\n3. The `Model`: in charge of doing the actual tokenization. An example of a `Model` would be\n   `BPE` or `WordPiece`.\n4. The `PostProcessor`: in charge of post-processing the `Encoding` to add anything relevant\n   that, for example, a language model would need, such as special tokens.\n\n### Loading a pretrained tokenizer from the Hub\n```rust\nuse tokenizers::tokenizer::{Result, Tokenizer};\n\nfn main() -> Result<()> {\n    # #[cfg(feature = \"http\")]\n    # {\n        let tokenizer = Tokenizer::from_pretrained(\"bert-base-cased\", None)?;\n\n        let encoding = tokenizer.encode(\"Hey there!\", false)?;\n        println!(\"{:?}\", encoding.get_tokens());\n    # }\n    Ok(())\n}\n```\n\n### Deserialization and tokenization example\n\n```rust\nuse tokenizers::tokenizer::{Result, Tokenizer, EncodeInput};\nuse tokenizers::models::bpe::BPE;", "n this video we will see how you can create your own tokenizer from scratch! To create your own tokenizer you will have to think about each of the operations involved in tokenization, namely: normalization, pre-tokenization, model, post-processing and decoding. If you don't know what normalization, pre-tokenization and the model are, I advise you to go and see the videos linked below. The post processing gathers all the modifications that we will carry out on the tokenized text. It can include the addition of special tokens, the creation of an attention mask but also the generation of a list of token ids. The decoding operation occurs at the very end and will allow passing from the sequence of ids in a sentence. For example, in our example, we can see that the hashtags have been removed and the tokens composing the word \"today\" have been grouped together. In a fast tokenizer, all these components are gathered in the backend_tokenizer attribute. As you can see with this small code snippet, it is an instance of a tokenizer from the tokenizers library. So, to create your own transformers tokenizer you will have to follow these steps: first create a training dataset; second create and train a tokenizer with the tokenizers library and third load this tokenizer into transformers tokenizer. To understand these steps, I propose that we recreate a BERT tokenizer. The first thing to do is to create a dataset. With this code snippet you can create an iterator on the dataset wikitext-2-raw-v1 which is a rather small dataset in English. We attack here the big part: the design of our tokenizer with the tokenizers library. We start by initializing a tokenizer instance with a WordPiece model because it is the model used by BERT. Then we can define our normalizer. We will define it as a succession of 2 normalizations used to clean up characters not visible in the text, 1 lowercasing normalization and 2 normalizations used to remove accents", "Then we'll look at the tokenizer API, which is the other main component of the `pipeline()` function. Tokenizers take care of the first and last processing steps, handling the conversion from text to numerical inputs for the neural network, and the conversion back to text when it is needed. Finally, we'll show you how to handle sending multiple sentences through a model in a prepared batch, then wrap it all up with a closer look at the high-level `tokenizer()` function.\n\n<Tip>\n\u26a0\ufe0f In order to benefit from all features available with the Model Hub and \ud83e\udd17 Transformers, we recommend <a href=\"https://huggingface.co/join\">creating an account</a>.\n</Tip>", "!-- DISABLE-FRONTMATTER-SECTIONS -->\n\n# Tokenizers\n\nFast State-of-the-art tokenizers, optimized for both research and\nproduction\n\n[\ud83e\udd17 Tokenizers](https://github.com/huggingface/tokenizers) provides an\nimplementation of today's most used tokenizers, with a focus on\nperformance and versatility. These tokenizers are also used in [\ud83e\udd17 Transformers](https://github.com/huggingface/transformers).\n\n# Main features:\n\n- Train new vocabularies and tokenize, using today's most used tokenizers.\n- Extremely fast (both training and tokenization), thanks to the Rust implementation. Takes less than 20 seconds to tokenize a GB of text on a server's CPU.\n- Easy to use, but also extremely versatile.\n- Designed for both research and production.\n- Full alignment tracking. Even with destructive normalization, it's always possible to get the part of the original sentence that corresponds to any token.\n- Does all the pre-processing: Truncation, Padding, add the special tokens your model needs.", ". To enable this, the fast tokenizers store additional information at each step of their internal pipeline. That internal pipeline consists of normalization, where we apply some cleaning to the text, like lowercasing or removing the accents;() pre-tokenization, which is where we split the texts into words;() then we apply the model of the tokenizer, which is where the words are splits into tokens,() before finally doing the post-processing, where special tokens are added. From the beginning to the end of the pipeline, the tokenizer keeps track of each span of text that corresponds to each word, then each token. We will see how useful it is when we tackle the following tasks: when doing masked language modeling, one variation that gets state-of-the-art results is to mask all the tokens of a given word instead of randomly chosen tokens. This will require us to use the word IDs we saw. When doing token classification, we'll need to convert the labels we have on words, to labels on each tokens. As for the offset mappings, it will be super useful when we need to convert token positions in a sentence into a span of text, which we will need to know when looking at question answering or when grouping the tokens corresponding to the same entity in token classification. To have a look at these tasks, check the videos linked below!", "he fast tokenizers of the Transformers library are fast, but they also implement features that will be super useful for data pre-processing and post-processing. Let's have a look at them! First, let's have a look at the usual output of a tokenizer. We get input IDs that correspond to tokens, but we lose a lot of information in the process. For instance, here the tokenization is the same for the two sentences, even if one has several more spaces than the other. Just having the input IDs is thus not enough if we want to match some tokens with a span of text (something we will need to do when tackling question answering for instance). It's also difficult to know when two tokens belong to the same word or not: it looks easy when you just look at the output of a BERT tokenizer, we just need to look for the ##. But other tokenizers have different ways to tokenize parts of words. For instance RoBERTa adds this special G symbol to mark the tokens at the beginning of a word, and T5 uses this special underscore symbol for the same purpose. Thankfully, the fast tokenizers keep track of the word each token comes from, with a word_ids method you can use on their outputs. The output is not necessarily clear, but assembled together in a nice table like this, we can look at the word position for each token. Even better, the fast tokenizers keep track of the span of characters each token comes from, and we can get them when calling it on one (or several) text by adding the return_offsets_mapping=True argument. In this instance, we can see how we jump positions between the ##s token and the super token, because of the multiple spaces in the initial sentence. To enable this, the fast tokenizers store additional information at each step of their internal pipeline", "A **tokenizer** defines how the text from the training dataset is converted to numbers (as a model is a mathematical function and therefore needs numbers as inputs). Tokenization is done by transforming text into sub-units called tokens (which can be words, sub-words, or characters, depending on tokenization methods). The vocabulary size of the tokenizer indicates how many different tokens it knows, typically between 32k and 200k. The size of a dataset is often measured as the **number of tokens** it contains once split in a sequence of these individual, \"atomistic\" units, and these days range from several hundred billion tokens to several trillion tokens!\n\n**Training hyperparameters** then define how the model is trained. How much should the parameters change to fit each new example? How fast should the model be updated?\n\nOnce these parameters have been selected, you only need 1) a lot of computing power to train the model and 2) competent (and kind) people to run and monitor the training. The training itself will consist in instantiating the architecture (creating the matrices on the hardware used for training) and running the training algorithm on the training dataset with the above mentioned hyperparameters. The result is a set of model **weights**. These are the model parameters after learning and what most people mean when discussing access to an open pretrained model. These weights can then be used **inference**, i.e., for prediction on new inputs, for instance, to generate text.\n\nPretrained LLMs can also be specialized or adapted for a specific task after pretraining, particularly when the weights are openly released. They are then used as a starting point for use cases and applications through a process called **fine-tuning**. Fine-tuning involves applying additional training steps on the model on a different \u2013often more specialized and smaller\u2013 dataset to optimize it for a specific application. Even though this step has a cost in terms of compute power needed, it is usually much less costly than training a model from scratch, both financially and environmentally. This is one reason high-quality open-source pretrained models are very interesting, as they can be freely used and built upon by the community even when the practitioners have only access to a limited computing budget."], "eval_score_GPT4": "4", "eval_feedback_GPT4": "The response provided is mostly correct, accurate, and factual. It explains the role of tokenizers in the NLP pipeline by describing their function of converting text into numerical inputs for neural networks. The response also correctly identifies that tokenizers break down text into smaller units known as tokens and mentions the relevance of vocabulary size and dataset measurement in tokens. However, the response includes additional details that are not present in the reference answer, such as the mention of sub-words, characters, and keeping track of additional information, which was not required to answer the question. Despite this, the core purpose of tokenizers as described aligns well with the reference answer."}, {"question": "What is the purpose of the Safety Checker in the Diffusers library?\n", "true_answer": "The Safety Checker checks and compares the class probability of a set of hard-coded harmful concepts in the embedding space against an image after it has been generated to mitigate the risk of generating harmful content.", "source_doc": "huggingface/diffusers/blob/main/docs/source/en/conceptual/ethical_guidelines.md", "generated_answer": "The Safety Checker in the Diffusers library is a mechanism designed to mitigate the potential risks associated with diffusion technology. It checks and compares the class probability of a set of hard-coded harmful concepts in the embedding space against an image after it has been generated. The purpose of the Safety Checker is to ensure that generated outputs do not contain known hardcoded NSFW content, helping to lower the entry bar for contributors who can help run the library even without technical expertise, and making", "retrieved_docs": ["</a>\n    <a class=\"!no-underline border dark:border-gray-700 p-5 rounded-lg shadow hover:shadow-lg\" href=\"./conceptual/philosophy\"\n      ><div class=\"w-full text-center bg-gradient-to-br from-pink-400 to-pink-500 rounded-lg py-1.5 font-semibold mb-5 text-white text-lg leading-relaxed\">Conceptual guides</div>\n      <p class=\"text-gray-700\">Understand why the library was designed the way it was, and learn more about the ethical guidelines and safety implementations for using the library.</p>\n   </a>\n    <a class=\"!no-underline border dark:border-gray-700 p-5 rounded-lg shadow hover:shadow-lg\" href=\"./api/models/overview\"\n      ><div class=\"w-full text-center bg-gradient-to-br from-purple-400 to-purple-500 rounded-lg py-1.5 font-semibold mb-5 text-white text-lg leading-relaxed\">Reference</div>\n      <p class=\"text-gray-700\">Technical descriptions of how \ud83e\udd17 Diffusers classes and methods work.</p>\n    </a>\n  </div>\n</div>", "To find out which schedulers are compatible for customization, you can use the `compatibles` method:\n\n```py\nfrom diffusers import DiffusionPipeline\n\nrepo_id = \"runwayml/stable-diffusion-v1-5\"\nstable_diffusion = DiffusionPipeline.from_pretrained(repo_id, use_safetensors=True)\nstable_diffusion.scheduler.compatibles\n```\n\nLet's use the [`SchedulerMixin.from_pretrained`] method to replace the default [`PNDMScheduler`] with a more performant scheduler, [`EulerDiscreteScheduler`]. The `subfolder=\"scheduler\"` argument is required to load the scheduler configuration from the correct [subfolder](https://huggingface.co/runwayml/stable-diffusion-v1-5/tree/main/scheduler) of the pipeline repository.\n\nThen you can pass the new [`EulerDiscreteScheduler`] instance to the `scheduler` argument in [`DiffusionPipeline`]:\n\n```python\nfrom diffusers import DiffusionPipeline, EulerDiscreteScheduler\n\nrepo_id = \"runwayml/stable-diffusion-v1-5\"\nscheduler = EulerDiscreteScheduler.from_pretrained(repo_id, subfolder=\"scheduler\")\nstable_diffusion = DiffusionPipeline.from_pretrained(repo_id, scheduler=scheduler, use_safetensors=True)\n```\n\n### Safety checker\n\nDiffusion models like Stable Diffusion can generate harmful content, which is why \ud83e\udde8 Diffusers has a [safety checker](https://github.com/huggingface/diffusers/blob/main/src/diffusers/pipelines/stable_diffusion/safety_checker.py) to check generated outputs against known hardcoded NSFW content. If you'd like to disable the safety checker for whatever reason, pass `None` to the `safety_checker` argument:", "- **Transparency**: we are committed to being transparent in managing PRs, explaining our choices to users, and making technical decisions.\n\n- **Consistency**: we are committed to guaranteeing our users the same level of attention in project management, keeping it technically stable and consistent.\n\n- **Simplicity**: with a desire to make it easy to use and exploit the Diffusers library, we are committed to keeping the project\u2019s goals lean and coherent.\n\n- **Accessibility**: the Diffusers project helps lower the entry bar for contributors who can help run it even without technical expertise. Doing so makes research artifacts more accessible to the community.\n\n- **Reproducibility**: we aim to be transparent about the reproducibility of upstream code, models, and datasets when made available through the Diffusers library.\n\n- **Responsibility**: as a community and through teamwork, we hold a collective responsibility to our users by anticipating and mitigating this technology's potential risks and dangers.\n\n\n## Examples of implementations: Safety features and Mechanisms\n\nThe team works daily to make the technical and non-technical tools available to deal with the potential ethical and social risks associated with diffusion technology. Moreover, the community's input is invaluable in ensuring these features' implementation and raising awareness with us.\n\n- [**Community tab**](https://huggingface.co/docs/hub/repositories-pull-requests-discussions): it enables the community to discuss and better collaborate on a project.\n\n- **Bias exploration and evaluation**: the Hugging Face team provides a [space](https://huggingface.co/spaces/society-ethics/DiffusionBiasExplorer) to demonstrate the biases in Stable Diffusion interactively. In this sense, we support and encourage bias explorers and evaluations.\n\n- **Encouraging safety in deployment**", "!--Copyright 2023 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\n\nhttp://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License.\n-->\n\n# Logging\n\n\ud83e\udd17 Diffusers has a centralized logging system to easily manage the verbosity of the library. The default verbosity is set to `WARNING`.\n\nTo change the verbosity level, use one of the direct setters. For instance, to change the verbosity to the `INFO` level.\n\n```python\nimport diffusers\n\ndiffusers.logging.set_verbosity_info()\n```\n\nYou can also use the environment variable `DIFFUSERS_VERBOSITY` to override the default verbosity. You can set it\nto one of the following: `debug`, `info`, `warning`, `error`, `critical`. For example:\n\n```bash\nDIFFUSERS_VERBOSITY=error ./myprogram.py\n```\n\nAdditionally, some `warnings` can be disabled by setting the environment variable\n`DIFFUSERS_NO_ADVISORY_WARNINGS` to a true value, like `1`. This disables any warning logged by\n[`logger.warning_advice`]. For example:\n\n```bash\nDIFFUSERS_NO_ADVISORY_WARNINGS=1 ./myprogram.py\n```\n\nHere is an example of how to use the same logger as the library in your own module or script:\n\n```python\nfrom diffusers.utils import logging", "!---\nCopyright 2023 The HuggingFace Team. All rights reserved.\nLicensed under the Apache License, Version 2.0 (the \"License\");\nyou may not use this file except in compliance with the License.\nYou may obtain a copy of the License at\n\n    http://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software\ndistributed under the License is distributed on an \"AS IS\" BASIS,\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\nSee the License for the specific language governing permissions and\nlimitations under the License.\n-->\n\n# \ud83e\udde8 Diffusers Examples\n\nDiffusers examples are a collection of scripts to demonstrate how to effectively use the `diffusers` library\nfor a variety of use cases involving training or fine-tuning.\n\n**Note**: If you are looking for **official** examples on how to use `diffusers` for inference, please have a look at [src/diffusers/pipelines](https://github.com/huggingface/diffusers/tree/main/src/diffusers/pipelines).\n\nOur examples aspire to be **self-contained**, **easy-to-tweak**, **beginner-friendly** and for **one-purpose-only**.\nMore specifically, this means:", "- **Encouraging safety in deployment**\n\n  - [**Safe Stable Diffusion**](https://huggingface.co/docs/diffusers/main/en/api/pipelines/stable_diffusion/stable_diffusion_safe): It mitigates the well-known issue that models, like Stable Diffusion, that are trained on unfiltered, web-crawled datasets tend to suffer from inappropriate degeneration. Related paper: [Safe Latent Diffusion: Mitigating Inappropriate Degeneration in Diffusion Models](https://arxiv.org/abs/2211.05105).\n\n  - [**Safety Checker**](https://github.com/huggingface/diffusers/blob/main/src/diffusers/pipelines/stable_diffusion/safety_checker.py): It checks and compares the class probability of a set of hard-coded harmful concepts in the embedding space against an image after it has been generated. The harmful concepts are intentionally hidden to prevent reverse engineering of the checker.\n\n- **Staged released on the Hub**: in particularly sensitive situations, access to some repositories should be restricted. This staged release is an intermediary step that allows the repository\u2019s authors to have more control over its use.\n\n- **Licensing**: [OpenRAILs](https://huggingface.co/blog/open_rail), a new type of licensing, allow us to ensure free access while having a set of restrictions that ensure more responsible use.", "```python\nfrom diffusers import DiffusionPipeline\n\nrepo_id = \"runwayml/stable-diffusion-v1-5\"\nstable_diffusion = DiffusionPipeline.from_pretrained(repo_id, safety_checker=None, use_safetensors=True)\n\"\"\"\nYou have disabled the safety checker for <class 'diffusers.pipelines.stable_diffusion.pipeline_stable_diffusion.StableDiffusionPipeline'> by passing `safety_checker=None`. Ensure that you abide by the conditions of the Stable Diffusion license and do not expose unfiltered results in services or applications open to the public. Both the diffusers team and Hugging Face strongly recommend keeping the safety filter enabled in all public-facing circumstances, disabling it only for use cases that involve analyzing network behavior or auditing its results. For more information, please have a look at https://github.com/huggingface/diffusers/pull/254 .\n\"\"\"\n```\n\n### Reuse components across pipelines\n\nYou can also reuse the same components in multiple pipelines to avoid loading the weights into RAM twice. Use the [`~DiffusionPipeline.components`] method to save the components:\n\n```python\nfrom diffusers import StableDiffusionPipeline, StableDiffusionImg2ImgPipeline\n\nmodel_id = \"runwayml/stable-diffusion-v1-5\"\nstable_diffusion_txt2img = StableDiffusionPipeline.from_pretrained(model_id, use_safetensors=True)\n\ncomponents = stable_diffusion_txt2img.components\n```\n\nThen you can pass the `components` to another pipeline without reloading the weights into RAM:\n\n```py\nstable_diffusion_img2img = StableDiffusionImg2ImgPipeline(**components)\n```"], "eval_score_GPT4": "4", "eval_feedback_GPT4": "The response provided aligns well with the reference answer, indicating that the Safety Checker is used to mitigate risks by checking the class probability of harmful concepts against generated images. The response adds additional context by mentioning that it helps to prevent the generation of NSFW content and lowers the entry bar for contributors. This additional information is not incorrect or misleading, but rather provides a broader understanding of the Safety Checker's purpose. Therefore, the response is mostly correct, accurate, and factual."}, {"question": "What Python class allows you to retrieve Discussions and Pull Requests from a given repository on the Hugging Face Hub?\n", "true_answer": "HfApi", "source_doc": "huggingface/huggingface_hub/blob/main/docs/source/en/guides/community.md", "generated_answer": "The `HfApi` class in the `huggingface_hub` library allows you to retrieve Discussions and Pull Requests from a given repository on the Hugging Face Hub.", "retrieved_docs": ["!--\u26a0\ufe0f Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\nrendered properly in your Markdown viewer.\n-->\n\n# Interact with Discussions and Pull Requests \n\nThe `huggingface_hub` library provides a Python interface to interact with Pull Requests and Discussions on the Hub.\nVisit [the dedicated documentation page](https://huggingface.co/docs/hub/repositories-pull-requests-discussions)\nfor a deeper view of what Discussions and Pull Requests on the Hub are, and how they work under the hood.\n\n## Retrieve Discussions and Pull Requests from the Hub\n\nThe `HfApi` class allows you to retrieve Discussions and Pull Requests on a given repo:\n\n```python\n>>> from huggingface_hub import get_repo_discussions\n>>> for discussion in get_repo_discussions(repo_id=\"bigscience/bloom\"):\n...     print(f\"{discussion.num} - {discussion.title}, pr: {discussion.is_pull_request}\")\n\n# 11 - Add Flax weights, pr: True\n# 10 - Update README.md, pr: True\n# 9 - Training languages in the model card, pr: True\n# 8 - Update tokenizer_config.json, pr: True\n# 7 - Slurm training script, pr: False\n[...]\n```\n\n`HfApi.get_repo_discussions` supports filtering by author, type (Pull Request or Discussion) and status (`open` or `closed`):\n\n```python\n>>> from huggingface_hub import get_repo_discussions\n>>> for discussion in get_repo_discussions(\n...    repo_id=\"bigscience/bloom\",\n...    author=\"ArthurZ\",\n...    discussion_type=\"pull_request\",\n...    discussion_status=\"open\",\n... ):\n...     print(f\"{discussion.num} - {discussion.title} by {discussion.author}, pr: {discussion.is_pull_request}\")", "## Create and edit a Discussion or Pull Request programmatically\n\nThe [`HfApi`] class also offers ways to create and edit Discussions and Pull Requests.\nYou will need an [access token](https://huggingface.co/docs/hub/security-tokens) to create and edit Discussions\nor Pull Requests.\n\nThe simplest way to propose changes on a repo on the Hub is via the [`create_commit`] API: just \nset the `create_pr` parameter to `True`. This parameter is also available on other methods that wrap [`create_commit`]:\n\n    * [`upload_file`]\n    * [`upload_folder`]\n    * [`delete_file`]\n    * [`delete_folder`]\n    * [`metadata_update`]\n\n```python\n>>> from huggingface_hub import metadata_update\n\n>>> metadata_update(\n...     repo_id=\"username/repo_name\",\n...     metadata={\"tags\": [\"computer-vision\", \"awesome-model\"]},\n...     create_pr=True,\n... )\n```\n\nYou can also use [`HfApi.create_discussion`] (respectively [`HfApi.create_pull_request`]) to create a Discussion (respectively a Pull Request) on a repo.\nOpening a Pull Request this way can be useful if you need to work on changes locally. Pull Requests opened this way will be in `\"draft\"` mode.\n\n```python\n>>> from huggingface_hub import create_discussion, create_pull_request\n\n>>> create_discussion(\n...     repo_id=\"username/repo-name\",\n...     title=\"Hi from the huggingface_hub library!\",\n...     token=\"<insert your access token here>\",\n... )\nDiscussionWithDetails(...)", "### 3. What can you do using the Hugging Face Hub web interface? \n\n<Question\n\tchoices={[\n\t\t{\n\t\t\ttext: \"Fork an existing repository.\",\n\t\t\texplain: \"Forking a repository is not possible on the Hugging Face Hub.\"\n\t\t},\n\t\t{\n\t\t\ttext: \"Create a new model repository.\",\n\t\t\texplain: \"Correct! That's not all you can do, though.\",\n            correct: true\n\t\t},\n\t\t{\n\t\t\ttext: \"Manage and edit files.\",\n\t\t\texplain: \"Correct! That's not the only right answer, though.\",\n            correct: true\n\t\t},\n        {\n\t\t\ttext: \"Upload files.\",\n\t\t\texplain: \"Right! But that's not all.\",\n            correct: true\n\t\t},\n        {\n\t\t\ttext: \"See diffs across versions.\",\n\t\t\texplain: \"Correct! That's not all you can do, though.\",\n            correct: true\n\t\t}\n\t]}\n/>\n\n### 4. What is a model card?\n\n<Question\n\tchoices={[\n\t\t{\n\t\t\ttext: \"A rough description of the model, therefore less important than the model and tokenizer files.\",\n\t\t\texplain: \"It is indeed a description of the model, but it's an important piece: if it's incomplete or absent the model's utility is drastically reduced.\"\n\t\t},\n\t\t{\n\t\t\ttext: \"A way to ensure reproducibility, reusability, and fairness.\",\n\t\t\texplain: \"Correct! Sharing the right information in the model card will help users leverage your model and be aware of its limits and biases. \",\n            correct: true\n\t\t},\n\t\t{\n\t\t\ttext: \"A Python file that can be run to retrieve information about the model.\",\n\t\t\texplain: \"Model cards are simple Markdown files.\"\n\t\t}\n\t]}\n/>\n\n### 5. Which of these objects of the \ud83e\udd17 Transformers library can be directly shared on the Hub with `push_to_hub()`?", "The `push_to_hub()` method is backed by the [`huggingface_hub`](https://github.com/huggingface/huggingface_hub) Python package, which offers a direct API to the Hugging Face Hub. It's integrated within \ud83e\udd17 Transformers and several other machine learning libraries, like [`allenlp`](https://github.com/allenai/allennlp). Although we focus on the \ud83e\udd17 Transformers integration in this chapter, integrating it into your own code or library is simple.\n\nJump to the last section to see how to upload files to your newly created repository!\n\n## Using the `huggingface_hub` Python library[[using-the-huggingfacehub-python-library]]\n\nThe `huggingface_hub` Python library is a package which offers a set of tools for the model and datasets hubs. It provides simple methods and classes for common tasks like \ngetting information about repositories on the hub and managing them. It provides simple APIs that work on top of git to manage those repositories' content and to integrate the Hub\nin your projects and libraries.\n\nSimilarly to using the `push_to_hub` API, this will require you to have your API token saved in your cache. In order to do this, you will need to use the `login` command from the CLI, as mentioned in the previous section (again, make sure to prepend these commands with the `!` character if running in Google Colab):\n\n```bash\nhuggingface-cli login\n```\n\nThe `huggingface_hub` package offers several methods and classes which are useful for our purpose. Firstly, there are a few methods to manage repository creation, deletion, and others:\n\n```python no-format\nfrom huggingface_hub import (\n    # User management\n    login,\n    logout,\n    whoami,\n\n    # Repository creation and management\n    create_repo,\n    delete_repo,\n    update_repo_visibility,", "```python\n>>> from huggingface_hub import get_discussion_details\n\n>>> get_discussion_details(\n...     repo_id=\"bigscience/bloom-1b3\",\n...     discussion_num=2\n... )\nDiscussionWithDetails(\n    num=2,\n    author='cakiki',\n    title='Update VRAM memory for the V100s',\n    status='open',\n    is_pull_request=True,\n    events=[\n        DiscussionComment(type='comment', author='cakiki', ...),\n        DiscussionCommit(type='commit', author='cakiki', summary='Update VRAM memory for the V100s', oid='1256f9d9a33fa8887e1c1bf0e09b4713da96773a', ...),\n    ],\n    conflicting_files=[],\n    target_branch='refs/heads/main',\n    merge_commit_oid=None,\n    diff='diff --git a/README.md b/README.md\\nindex a6ae3b9294edf8d0eda0d67c7780a10241242a7e..3a1814f212bc3f0d3cc8f74bdbd316de4ae7b9e3 100644\\n--- a/README.md\\n+++ b/README.md\\n@@ -132,7 +132,7 [...]',\n)\n```\n\n[`HfApi.get_discussion_details`] returns a [`DiscussionWithDetails`] object, which is a subclass of [`Discussion`]\nwith more detailed information about the Discussion or Pull Request. Information includes all the comments, status changes,\nand renames of the Discussion via [`DiscussionWithDetails.events`].\n\nIn case of a Pull Request, you can retrieve the raw git diff with [`DiscussionWithDetails.diff`]. All the commits of the\nPull Request are listed in [`DiscussionWithDetails.events`].", "### Advanced programmatic repository management \n\nThe `Repository` class helps manage both offline Git repositories and Hugging\nFace Hub repositories. Using the `Repository` class requires `git` and `git-lfs`\nto be installed.\n\nInstantiate a `Repository` object by calling it with a path to a local Git\nclone/repository:\n\n```python\n>>> from huggingface_hub import Repository\n>>> repo = Repository(\"<path>/<to>/<folder>\")\n```\n\nThe `Repository` takes a `clone_from` string as parameter. This can stay as\n`None` for offline management, but can also be set to any URL pointing to a Git\nrepo to clone that repository in the specified directory:\n\n```python\n>>> repo = Repository(\"huggingface-hub\", clone_from=\"https://github.com/huggingface/huggingface_hub\")\n```\n\nThe `clone_from` method can also take any Hugging Face model ID as input, and\nwill clone that repository:\n\n```python\n>>> repo = Repository(\"w2v2\", clone_from=\"facebook/wav2vec2-large-960h-lv60\")\n```\n\nIf the repository you're cloning is one of yours or one of your organisation's,\nthen having the ability to commit and push to that repository is important. In\norder to do that, you should make sure to be logged-in using `huggingface-cli\nlogin`, and to have the `token` parameter set to `True` (the default)\nwhen  instantiating the `Repository` object:\n\n```python\n>>> repo = Repository(\"my-model\", clone_from=\"<user>/<model_id>\", token=True)\n```\n\nThis works for models, datasets and spaces repositories; but you will need to\nexplicitely specify the type for the last two options:", ">>> create_pull_request(\n...     repo_id=\"username/repo-name\",\n...     title=\"Hi from the huggingface_hub library!\",\n...     token=\"<insert your access token here>\",\n... )\nDiscussionWithDetails(..., is_pull_request=True)\n```\n\nManaging Pull Requests and Discussions can be done entirely with the [`HfApi`] class. For example:\n\n    * [`comment_discussion`] to add comments\n    * [`edit_discussion_comment`] to edit comments\n    * [`rename_discussion`] to rename a Discussion or Pull Request \n    * [`change_discussion_status`] to open or close a Discussion / Pull Request \n    * [`merge_pull_request`] to merge a Pull Request \n\n\nVisit the [`HfApi`] documentation page for an exhaustive reference of all available methods.\n\n## Push changes to a Pull Request\n\n*Coming soon !*\n\n## See also\n\nFor a more detailed reference, visit the [Discussions and Pull Requests](../package_reference/community) and the [hf_api](../package_reference/hf_api) documentation page."], "eval_score_GPT4": "5", "eval_feedback_GPT4": "Feedback: The response provided is completely correct, accurate, and factual. It correctly identifies the `HfApi` class as the tool used to retrieve Discussions and Pull Requests from a given repository on the Hugging Face Hub, which aligns with the reference answer."}, {"question": "What is the name of the new library introduced by Hugging Face for hosting scikit-learn models?\n", "true_answer": "Skops", "source_doc": "huggingface/blog/blob/main/skops.md", "generated_answer": "The question asks about a new library introduced by Hugging Face for hosting scikit-learn models. However, the provided context does not mention any new library for hosting scikit-learn models. The context only discusses the integration of the Hugging Face Hub with various third-party libraries, including spaCy, AllenNLP, and timm, but it does not specifically mention scikit-learn. Therefore, there is no answer to this question based on the provided", "retrieved_docs": ["Integrate your library with the Hub\n\nThe Hugging Face Hub aims to facilitate sharing machine learning models, checkpoints, and artifacts. This endeavor includes integrating the Hub into many of the amazing third-party libraries in the community. Some of the ones already integrated include [spaCy](https://spacy.io/usage/projects#huggingface_hub), [AllenNLP](https://allennlp.org/), and [timm](https://rwightman.github.io/pytorch-image-models/), among many others. Integration means users can download and upload files to the Hub directly from your library. We hope you will integrate your library and join us in democratizing artificial intelligence for everyone.\n\nIntegrating the Hub with your library provides many benefits, including:\n\n- Free model hosting for you and your users.\n- Built-in file versioning - even for huge files - made possible by [Git-LFS](https://git-lfs.github.com/).\n- All public models are powered by the [Inference API](https://huggingface.co/docs/api-inference/index).\n- In-browser widgets allow users to interact with your hosted models directly.\n\nThis tutorial will help you integrate the Hub into your library so your users can benefit from all the features offered by the Hub.\n\nBefore you begin, we recommend you create a [Hugging Face account](https://huggingface.co/join) from which you can manage your repositories and files. \n\nIf you need help with the integration, feel free to open an [issue](https://github.com/huggingface/huggingface_hub/issues/new/choose), and we would be more than happy to help you.\n\n## Installation\n\n1. Install the `huggingface_hub` library with pip in your environment:\n\n   ```bash\n   python -m pip install huggingface_hub\n   ```\n\n2. Once you have successfully installed the `huggingface_hub` library, log in to your Hugging Face account:", "Libraries\n\nThe Hub has support for dozens of libraries in the Open Source ecosystem. Thanks to the `huggingface_hub` Python library, it's easy to enable sharing your models on the Hub. The Hub supports many libraries, and we're working on expanding this support. We're happy to welcome to the Hub a set of Open Source libraries that are pushing Machine Learning forward.\n\nThe table below summarizes the supported libraries and their level of integration. Find all our supported libraries in [the model-libraries.ts file](https://github.com/huggingface/huggingface.js/blob/main/packages/tasks/src/model-libraries.ts).", "</div>\n\n## What's the Hugging Face Hub?\n\nWe are helping the community work together towards the goal of advancing Machine Learning \ud83d\udd25.\n\nThe Hugging Face Hub is a platform with over 120k models, 20k datasets, and 50k demos in which people can easily collaborate in their ML workflows. The Hub works as a central place where anyone can share, explore, discover, and experiment with open-source Machine Learning.\n\nNo single company, including the Tech Titans, will be able to \u201csolve AI\u201d by themselves \u2013 the only way we'll achieve this is by sharing knowledge and resources in a community-centric approach. We are building the largest open-source collection of models, datasets, demos and metrics on the Hugging Face Hub to democratize and advance ML for everyone \ud83d\ude80.\n\nWe encourage you to read the [Code of Conduct](https://huggingface.co/code-of-conduct) and the [Content Guidelines](https://huggingface.co/content-guidelines) to familiarize yourself with the values that we expect our community members to uphold \ud83e\udd17.\n\n## What can you find on the Hub?\n\nThe Hugging Face Hub hosts Git-based repositories, which are version-controlled buckets that can contain all your files. \ud83d\udcbe\n\nOn it, you'll be able to upload and discover...\n\n- Models, _hosting the latest state-of-the-art models for NLP, vision, and audio tasks_\n- Datasets, _featuring a wide variety of data for different domains and modalities_..\n- Spaces, _interactive apps for demonstrating ML models directly in your browser_.\n\nThe Hub offers **versioning, commit history, diffs, branches, and over a dozen library integrations**! You can learn more about the features that all repositories share in the [**Repositories documentation**](./repositories).\n\n## Models", "---\n\n## Willkommen bei der huggingface_hub Bibliothek\n\nDie `huggingface_hub` Bibliothek erm\u00f6glicht Ihnen die Interaktion mit dem [Hugging Face Hub](https://huggingface.co/), einer Plattform, die Open-Source Machine Learning f\u00fcr Entwickler und Mitwirkende demokratisiert. Entdecken Sie vortrainierte Modelle und Datens\u00e4tze f\u00fcr Ihre Projekte oder spielen Sie mit den Tausenden von Machine-Learning-Apps, die auf dem Hub gehostet werden. Sie k\u00f6nnen auch Ihre eigenen Modelle, Datens\u00e4tze und Demos mit der Community teilen. Die `huggingface_hub` Bibliothek bietet eine einfache M\u00f6glichkeit, all dies mit Python zu tun.\n\n## Hauptmerkmale", "### Would you like to integrate your library to the Hub?\n\nThis integration is made possible by the [`huggingface_hub`](https://github.com/huggingface/huggingface_hub) library. If you want to add your library to the Hub, we have a [guide](https://huggingface.co/docs/hub/models-adding-libraries) for you! Or simply tag someone from the Hugging Face team.\n\nA shout out to the Hugging Face team for all the work on this integration, in particular [@osanseviero](https://twitter.com/osanseviero) \ud83e\udd99.\n\nThank you fastlearners and hugging learners \ud83e\udd17.", "Hugging Face Hub documentation\n\nThe Hugging Face Hub is a platform with over 350k models, 75k datasets, and 150k demo apps (Spaces), all open source and publicly available, in an online platform where people can easily collaborate and build ML together. The Hub works as a central place where anyone can explore, experiment, collaborate, and build technology with Machine Learning. Are you ready to join the path towards open source Machine Learning? \ud83e\udd17\n\n<div class=\"grid grid-cols-1 gap-4 sm:grid-cols-2 lg:grid-cols-3 md:mt-10\">", "--\ntitle: \"Jupyter X Hugging Face\" \nthumbnail: /blog/assets/135_notebooks-hub/before_after_notebook_rendering.png\nauthors:\n- user: davanstrien\n- user: reach-vb \n- user: merve\n---\n\n# Jupyter X Hugging Face \n\n\n**We\u2019re excited to announce improved support for Jupyter notebooks hosted on the Hugging Face Hub!**\n\nFrom serving as an essential learning resource to being a key tool used for model development, Jupyter notebooks have become a key component across many areas of machine learning. Notebooks' interactive and visual nature lets you get feedback quickly as you develop models, datasets, and demos. For many, their first exposure to training machine learning models is via a Jupyter notebook, and many practitioners use notebooks as a critical tool for developing and communicating their work. \n\nHugging Face is a collaborative Machine Learning platform in which the community has shared over 150,000 models, 25,000 datasets, and 30,000 ML apps. The Hub has model and dataset versioning tools, including model cards and client-side libraries to automate the versioning process. However, only including a model card with hyperparameters is not enough to provide the best reproducibility; this is where notebooks can help. Alongside these models, datasets, and demos, the Hub hosts over 7,000 notebooks. These notebooks often document the development process of a model or a dataset and can provide guidance and tutorials showing how others can use these resources. We\u2019re therefore excited about our improved support for notebook hosting on the Hub. \n\n## What have we changed? \n\nUnder the hood, Jupyter notebook files (usually shared with an `ipynb` extension) are JSON files. While viewing these files directly is possible, it's not a format intended to be read by humans. We have now added rendering support for notebooks hosted on the Hub. This means that notebooks will now be displayed in a human-readable format."], "eval_score_GPT4": "1", "eval_feedback_GPT4": "Feedback: The response does not provide the correct name of the new library introduced by Hugging Face for hosting scikit-learn models. The reference answer states that the correct name is \"Skops,\" which is not mentioned in the response. Instead, the response discusses the integration of the Hugging Face Hub with other libraries and incorrectly implies that there is no new library for scikit-learn models. Based on the score rubric, the response is completely incorrect and not factual in relation to the specific question asked."}, {"question": "What is the purpose of Textual Inversion?\n", "true_answer": "Textual Inversion is a training method for personalizing models by learning new text embeddings from a few example images.", "source_doc": "huggingface/diffusers/blob/main/docs/source/en/api/loaders/textual_inversion.md", "generated_answer": "Textual Inversion is a training method for personalizing image generation models like Stable Diffusion by learning and updating text embeddings from a few example images. It allows for the learning of new concepts using just 3-5 examples, making it a useful technique for customizing models to specific images or concepts. The resulting embeddings can be loaded into the text encoder and activated with a special token, enabling the model to better generate images based on the learned concepts. This technique can be adapted", "retrieved_docs": ["!--Copyright 2023 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\n\nhttp://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License.\n-->\n\n# Textual Inversion\n\nTextual Inversion is a training method for personalizing models by learning new text embeddings from a few example images. The file produced from training is extremely small (a few KBs) and the new embeddings can be loaded into the text encoder.\n\n[`TextualInversionLoaderMixin`] provides a function for loading Textual Inversion embeddings from Diffusers and Automatic1111 into the text encoder and loading a special token to activate the embeddings.\n\n<Tip>\n\nTo learn more about how to load Textual Inversion embeddings, see the [Textual Inversion](../../using-diffusers/loading_adapters#textual-inversion) loading guide.\n\n</Tip>\n\n## TextualInversionLoaderMixin\n\n[[autodoc]] loaders.textual_inversion.TextualInversionLoaderMixin", "Distillation for quantization on Textual Inversion models to personalize text2image\n\n[Textual inversion](https://arxiv.org/abs/2208.01618) is a method to personalize text2image models like stable diffusion on your own images._By using just 3-5 images new concepts can be taught to Stable Diffusion and the model personalized on your own images_\nThe `textual_inversion.py` script shows how to implement the training procedure and adapt it for stable diffusion.\nWe have enabled distillation for quantization in `textual_inversion.py` to do quantization aware training as well as distillation on the model generated by Textual Inversion method.\n\n## Installing the dependencies\n\nBefore running the scripts, make sure to install the library's training dependencies:\n\n```bash\npip install -r requirements.txt\n```\n\n## Prepare Datasets\n\nOne picture which is from the huggingface datasets [sd-concepts-library/dicoo2](https://huggingface.co/sd-concepts-library/dicoo2) is needed, and save it to the `./dicoo` directory. The picture is shown below:\n\n<a href=\"https://huggingface.co/sd-concepts-library/dicoo2/blob/main/concept_images/1.jpeg\">\n    <img src=\"https://huggingface.co/sd-concepts-library/dicoo2/resolve/main/concept_images/1.jpeg\" width = \"300\" height=\"300\">\n</a>\n\n## Get a FP32 Textual Inversion model\n\nUse the following command to fine-tune the Stable Diffusion model on the above dataset to obtain the FP32 Textual Inversion model.\n\n```bash\nexport MODEL_NAME=\"CompVis/stable-diffusion-v1-4\"\nexport DATA_DIR=\"./dicoo\"", "# Textual Inversion fine-tuning example\n\n[Textual inversion](https://arxiv.org/abs/2208.01618) is a method to personalize text2image models like stable diffusion on your own images using just 3-5 examples.\nThe `textual_inversion.py` script shows how to implement the training procedure and adapt it for stable diffusion.\n\n## Running on Colab\n\nColab for training\n[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/huggingface/notebooks/blob/main/diffusers/sd_textual_inversion_training.ipynb)\n\nColab for inference\n[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/huggingface/notebooks/blob/main/diffusers/stable_conceptualizer_inference.ipynb)\n\n## Running locally with PyTorch\n### Installing the dependencies\n\nBefore running the scripts, make sure to install the library's training dependencies:\n\n**Important**\n\nTo make sure you can successfully run the latest versions of the example scripts, we highly recommend **installing from source** and keeping the install up to date as we update the example scripts frequently and install some example-specific requirements. To do this, execute the following steps in a new virtual environment:\n```bash\ngit clone https://github.com/huggingface/diffusers\ncd diffusers\npip install .\n```\n\nThen cd in the example folder and run:\n```bash\npip install -r requirements.txt\n```\n\nAnd initialize an [\ud83e\udd17 Accelerate](https://github.com/huggingface/accelerate/) environment with:\n\n```bash\naccelerate config\n```\n\n### Cat toy example", "# Textual Inversion fine-tuning example\n\n[Textual inversion](https://arxiv.org/abs/2208.01618) is a method to personalize text2image models like stable diffusion on your own images using just 3-5 examples.\nThe `textual_inversion.py` script shows how to implement the training procedure and adapt it for stable diffusion.\n\n## Running on Colab\n\nColab for training\n[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/huggingface/notebooks/blob/main/diffusers/sd_textual_inversion_training.ipynb)\n\nColab for inference\n[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/huggingface/notebooks/blob/main/diffusers/stable_conceptualizer_inference.ipynb)\n\n## Running locally with PyTorch\n### Installing the dependencies\n\nBefore running the scripts, make sure to install the library's training dependencies:\n\n**Important**\n\nTo make sure you can successfully run the latest versions of the example scripts, we highly recommend **installing from source** and keeping the install up to date as we update the example scripts frequently and install some example-specific requirements. To do this, execute the following steps in a new virtual environment:\n```bash\ngit clone https://github.com/huggingface/diffusers\ncd diffusers\npip install .\n```\n\nThen cd in the example folder  and run\n```bash\npip install -r requirements.txt\n```\n\nAnd initialize an [\ud83e\udd17Accelerate](https://github.com/huggingface/accelerate/) environment with:\n\n```bash\naccelerate config\n```\n\n\n### Cat toy example", "<!--Copyright 2023 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\n\nhttp://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License.\n-->\n\n# Textual Inversion\n\n[Textual Inversion](https://hf.co/papers/2208.01618) is a training technique for personalizing image generation models with just a few example images of what you want it to learn. This technique works by learning and updating the text embeddings (the new embeddings are tied to a special word you must use in the prompt) to match the example images you provide.\n\nIf you're training on a GPU with limited vRAM, you should try enabling the `gradient_checkpointing` and `mixed_precision` parameters in the training command. You can also reduce your memory footprint by using memory-efficient attention with [xFormers](../optimization/xformers). JAX/Flax training is also supported for efficient training on TPUs and GPUs, but it doesn't support gradient checkpointing or xFormers. With the same configuration and setup as PyTorch, the Flax training script should be at least ~70% faster!\n\nThis guide will explore the [textual_inversion.py](https://github.com/huggingface/diffusers/blob/main/examples/textual_inversion/textual_inversion.py) script to help you become more familiar with it, and how you can adapt it for your own use-case.\n\nBefore running the script, make sure you install the library from source:\n\n```bash\ngit clone https://github.com/huggingface/diffusers\ncd diffusers\npip install .\n```", "[Pivotal Tuning](https://arxiv.org/abs/2106.05744) is a method that tries to combine Textual Inversion with LoRA. First, you teach the model a new concept using Textual Inversion techniques, obtaining a new token embedding to represent it. Then, you train that token embedding using LoRA to get the best of both worlds.\n\nWe haven't explored Pivotal Tuning with LoRA yet. Who's up for the challenge? \ud83e\udd17", "# [Deprecated] Multi Token Textual Inversion\n\n**IMPORTART: This research project is deprecated. Multi Token Textual Inversion is now supported natively in [the official textual inversion example](https://github.com/huggingface/diffusers/tree/main/examples/textual_inversion#running-locally-with-pytorch).**\n\nThe author of this project is [Isamu Isozaki](https://github.com/isamu-isozaki) - please make sure to tag the author for issue and PRs as well as @patrickvonplaten.\n\nWe add multi token support to textual inversion. I added\n1. num_vec_per_token for the number of used to reference that token\n2. progressive_tokens for progressively training the token from 1 token to 2 token etc\n3. progressive_tokens_max_steps for the max number of steps until we start full training\n4. vector_shuffle to shuffle vectors\n\nFeel free to add these options to your training! In practice num_vec_per_token around 10+vector shuffle works great!\n\n## Textual Inversion fine-tuning example\n\n[Textual inversion](https://arxiv.org/abs/2208.01618) is a method to personalize text2image models like stable diffusion on your own images using just 3-5 examples.\nThe `textual_inversion.py` script shows how to implement the training procedure and adapt it for stable diffusion.\n\n## Running on Colab\n\nColab for training\n[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/huggingface/notebooks/blob/main/diffusers/sd_textual_inversion_training.ipynb)"], "eval_score_GPT4": "5", "eval_feedback_GPT4": "The response provided is completely correct, accurate, and factual. It aligns well with the reference answer by explaining that Textual Inversion is a training method for personalizing image generation models. It also adds detail about how it works, mentioning the use of a few example images to learn new concepts and the use of special tokens to activate the learned embeddings. The response expands on the reference answer without introducing any inaccuracies or incorrect information."}, {"question": "What is the recommended multiple of batch size for fp16 data type on an A100 GPU?\n", "true_answer": "64", "source_doc": "huggingface/transformers/blob/main/docs/source/en/perf_train_gpu_one.md", "generated_answer": "Based on the context provided, it is recommended to use a batch size of 2 per GPU for fp16 data type on an A100 GPU. This is advised due to the size of the images determining the amount of memory being used, and it is suggested to use a batch size of 2 per GPU for images on an A100 GPU. See [this Github thread](https://github.com/facebookresearch/detr/issues/150", "retrieved_docs": ["with batch size = 1: \n\n|quantization |act_order|bits|group_size|kernel|Load time (s)|Per-token latency (ms)|Throughput (tok/s)|Peak memory (MB)|\n|-----|---------|----|----------|------|-------------|----------------------|------------------|----------------|\n|fp16|None     |None|None      |None  |26.0         |36.958                |27.058            |29152.98        |\n|gptq |False    |4   |128       |exllama|36.2         |33.711                |29.663            |10484.34        |\n|bitsandbytes|None     |4|None      |None  |37.64        |52.00                 |19.23             |11018.36       |\n\nwith batch size = 16:", "Alternatively, one can also define a custom `collate_fn` in order to batch images together, using\n  [`~transformers.DetrImageProcessor.pad_and_create_pixel_mask`].\n- The size of the images will determine the amount of memory being used, and will thus determine the `batch_size`.\n  It is advised to use a batch size of 2 per GPU. See [this Github thread](https://github.com/facebookresearch/detr/issues/150) for more info.", "Table 2: Benchmarking FSDP on GPT-2 XL (1.5B) model\n\nFrom Table 2, we can observe that DDP (w and w/o fp16) isn\u2019t even able to run with batch size of 1 and results in CUDA OOM error. FSDP with Zero-Stage 3 is able to be run on 2 GPUs with batch size of 5 (effective batch size =10 (5 X 2)). FSDP with CPU offload can further increase the max batch size to 14 per GPU when using 2 GPUs. **FSDP with CPU offload enables training GPT-2 1.5B model on a single GPU with a batch size of 10**. This enables ML practitioners with minimal compute resources to train such large models, thereby democratizing large model training.\n\n## Capabilities and limitations of the FSDP Integration\n\nLet\u2019s dive into the current support that Accelerate provides for FSDP integration and the known limitations.\n\n**Required PyTorch version for FSDP support**: PyTorch Nightly (or 1.12.0 if you read this after it has been released) as the model saving with FSDP activated is only available with recent fixes.\n\n**Configuration through CLI:**\n\n1. **Sharding Strategy**: [1] FULL_SHARD, [2] SHARD_GRAD_OP\n2. **Min Num Params**: FSDP's minimum number of parameters for Default Auto Wrapping.\n3. **Offload Params**: Decides Whether to offload parameters and gradients to CPU.\n\nFor more control, users can leverage the `FullyShardedDataParallelPlugin` wherein they can specify `auto_wrap_policy`, `backward_prefetch` and `ignored_modules`.\n\nAfter creating an instance of this class, users can pass it when creating the Accelerator object.", "BF16 has no such problem, you can easily do `10_000*10_000=100_000_000` and it's no problem.\n\nOf course, since BF16 and FP16 have the same size of 2 bytes, one doesn't get a free lunch and one pays with really bad precision when using BF16. However, if you remember the training using stochastic gradient descent and its variations is a sort of stumbling walk, so if you don't get the perfect direction immediately it's no problem, you will correct yourself in the next steps.\n\nRegardless of whether one uses BF16 or FP16 there is also a copy of weights which is always in FP32 - this is what gets updated by the optimizer. So the 16-bit formats are only used for the computation, the optimizer updates the FP32 weights with full precision and then casts them into the 16-bit format for the next iteration.\n\nAll PyTorch components have been updated to ensure that they perform any accumulation in FP32, so no loss happening there.\n\nOne crucial issue is gradient accumulation, and it's one of the main features of pipeline parallelism as the gradients from each microbatch processing get accumulated. It's crucial to implement gradient accumulation in FP32 to keep the training precise, and this is what `BF16Optimizer` does.\n\nBesides other improvements we believe that using BF16 mixed precision training turned a potential nightmare into a relatively smooth process which can be observed from the following lm loss graph:\n\n![176B-fail](assets/86_bloom_megatron_deepspeed/176b-lm-loss.png)\n\n## Fused CUDA Kernels\n\nThe GPU performs two things. It can copy data to/from memory and perform computations on that data. While the GPU is busy copying the GPU's computations units idle. If we want to efficiently utilize the GPU we want to minimize the idle time.", "Sample FSDP config after running the command `accelerate config`:\n```bash\ncompute_environment: LOCAL_MACHINE\ndeepspeed_config: {}\ndistributed_type: FSDP\nfsdp_config:\n  min_num_params: 2000\n  offload_params: false\n  sharding_strategy: 1\nmachine_rank: 0\nmain_process_ip: null\nmain_process_port: null\nmain_training_function: main\nmixed_precision: 'no'\nnum_machines: 1\nnum_processes: 2\nuse_cpu: false\n```\n\n## Multi-GPU FSDP\n\nHere, we experiment on the Single-Node Multi-GPU setting. We compare the performance of Distributed Data Parallel (DDP) and FSDP in various configurations. First, GPT-2 Large(762M) model is used wherein DDP works with certain batch sizes without throwing Out Of Memory (OOM) errors. Next, GPT-2 XL (1.5B) model is used wherein DDP fails with OOM error even on batch size of 1. We observe that FSDP enables larger batch sizes for GPT-2 Large model and it enables training the GPT-2 XL model with decent batch size unlike DDP.\n\n**Hardware setup**: 2X24GB NVIDIA Titan RTX GPUs. \n\nCommand for training GPT-2 Large Model (762M parameters):\n```bash\nexport BS=#`try with different batch sizes till you don't get OOM error,\n#i.e., start with larger batch size and go on decreasing till it fits on GPU`", "Table 2: Benchmarking DeepSpeed ZeRO Stage-2 on BlenderBot (400M) model\n\nIn our Single-Node Multi-GPU setup, the maximum batch size that DDP supports without OOM error is 100. In contrast, DeepSpeed Zero-Stage 2 enables batch size of 200 without running into OOM errors. Therefore, DeepSpeed enables to fit **2X** more data per GPU when compared to DDP. We observe ~**1.44X** speedup in training and ~**1.23X** speedup in evaluation as we are able to fit more data on the same available hardware. As this model is of medium size, the speedup isn't that exciting but this will improve with bigger models. You can chat with the Chatbot trained using the entire data at \ud83e\udd17 Space [smangrul/Chat-E](https://huggingface.co/spaces/smangrul/Chat-E). You can give bot a persona, ground conversation to a particular emotion, use to in goal-oriented tasks or in a free flow manner. Below is a fun conversation with the chatbot \ud83d\udcac. You can find snapshots of more conversations using different contexts [here](https://github.com/pacman100/accelerate-deepspeed-test/tree/main/src/chatbot_snapshots).\n\n![Chatbot](./assets/83_accelerate_deepspeed/chatbot.png)\n\n---\n## CPU/Disk Offloading to enable training humongous models that won\u2019t fit the GPU memory\n\nOn a single 24GB NVIDIA Titan RTX GPU, one cannot train GPT-XL Model (1.5B parameters) even with a batch size of 1. We will look at how we can use DeepSpeed ZeRO Stage-3 with CPU offloading of optimizer states, gradients and parameters to train GPT-XL Model.", "First of all set batch size to 1 (you can always use gradient accumulation for any desired effective batch size).\n\n1. Enable `--gradient_checkpointing 1` (HF Trainer) or directly `model.gradient_checkpointing_enable()` - if OOM then\n2. Try ZeRO stage 2 first. if OOM then\n3. Try ZeRO stage 2 + `offload_optimizer` - if OOM then\n4. Switch to ZeRO stage 3 - if OOM then\n5. Enable `offload_param` to `cpu` - if OOM then\n6. Enable `offload_optimizer` to `cpu` - if OOM then\n\n7. If you still can't fit a batch size of 1 first check various default values and lower them if you can. For example, if you use `generate` and you don't use a wide search beam make it narrower as it'd take a lot of memory.\n\n8. Definitely use mixed half-precision over fp32 - so bf16 on Ampere and higher GPUs and fp16 on older gpu architectures.\n\n9. If you still OOM you could add more hardware or enable ZeRO-Infinity - that is switch offloads `offload_param` and  `offload_optimizer` to `nvme`. You need to make sure it's a very fast nvme. As an anecdote I was able to infer BLOOM-176B on a tiny GPU using ZeRO-Infinity except it was extremely slow. But it worked!\n\nYou can, of course, work through these steps in reverse by starting with the most GPU memory efficient config and then going backwards. Or try bi-secting it.\n\nOnce you have your batch size 1 not leading to OOM, measure your effective throughput.\n\nNext try to increase the batch size to as large as you can, since the higher the batch size the more efficient the GPUs are as they perform the best when matrices they multiply are huge.\n\nNow the performance optimization game starts. You can turn off some offload features or step down in ZeRO stages and increase/decrease batch size and again measure your effective throughput. Rinse and repeat until satisfied."], "eval_score_GPT4": "1", "eval_feedback_GPT4": "The response provided is completely incorrect when compared to the reference answer. The reference answer states that the recommended multiple of batch size for fp16 data type on an A100 GPU is 64, while the response suggests using a batch size of 2 per GPU, which is not aligned with the reference answer. There is no mention of the number 64 or any justification that aligns with the recommended multiple of batch size for fp16 on an A100 GPU. Therefore, the response does not meet the criteria for correctness, accuracy, or factuality based on the reference answer."}, {"question": "How do you run a Gradio Blocks app in reload mode using a Python IDE?\n", "true_answer": "Run `gradio run.py` in the terminal.", "source_doc": "gradio-app/gradio/blob/main/guides/09_other-tutorials/developing-faster-with-reload-mode.md", "generated_answer": "To run a Gradio Blocks app in reload mode using a Python IDE, follow these steps:\n\n1. Create a Python file (e.g., `run.py`) with your Gradio Blocks app code.\n2. In the terminal, navigate to the directory where your `run.py` file is located.\n3. Run `gradio run.py` instead of `python run.py`. This will start the backend server in reload mode", "retrieved_docs": ["Developing Faster with Auto-Reloading\n\n**Prerequisite**: This Guide requires you to know about Blocks. Make sure to [read the Guide to Blocks first](https://gradio.app/guides/quickstart/#blocks-more-flexibility-and-control).\n\nThis guide covers auto reloading, reloading in a Python IDE, and using gradio with Jupyter Notebooks.\n\n## Why Auto-Reloading?\n\nWhen you are building a Gradio demo, particularly out of Blocks, you may find it cumbersome to keep re-running your code to test your changes.\n\nTo make it faster and more convenient to write your code, we've made it easier to \"reload\" your Gradio apps instantly when you are developing in a **Python IDE** (like VS Code, Sublime Text, PyCharm, or so on) or generally running your Python code from the terminal. We've also developed an analogous \"magic command\" that allows you to re-run cells faster if you use **Jupyter Notebooks** (or any similar environment like Colab).\n\nThis short Guide will cover both of these methods, so no matter how you write Python, you'll leave knowing how to build Gradio apps faster.\n\n## Python IDE Reload \ud83d\udd25\n\nIf you are building Gradio Blocks using a Python IDE, your file of code (let's name it `run.py`) might look something like this:\n\n```python\nimport gradio as gr\n\nwith gr.Blocks() as demo:\n    gr.Markdown(\"# Greetings from Gradio!\")\n    inp = gr.Textbox(placeholder=\"What is your name?\")\n    out = gr.Textbox()\n\n    inp.change(fn=lambda x: f\"Welcome, {x}!\",\n               inputs=inp,\n               outputs=out)\n\nif __name__ == \"__main__\":\n    demo.launch()\n```", "Type your name in the textbox on the left, drag the slider, and then press the Submit button. You should see a friendly greeting on the right.\n\n> [!TIP]\n > When developing locally, you can run your Gradio app in <strong>hot reload mode</strong>, which automatically reloads the Gradio app whenever you make changes to the file. To do this, simply type in <code>gradio</code> before the name of the file instead of <code>python</code>. In the example above, you would type: `gradio app.py` in your terminal. Learn more about hot reloading in the <a href=\"https://www.gradio.app/guides/developing-faster-with-reload-mode\">Hot Reloading Guide</a>.\n\n\n**Understanding the `Interface` Class**\n\nYou'll notice that in order to make your first demo, you created an instance of the `gr.Interface` class. The `Interface` class is designed to create demos for machine learning models which accept one or more inputs, and return one or more outputs. \n\nThe `Interface` class has three core arguments:\n\n- `fn`: the function to wrap a user interface (UI) around\n- `inputs`: the Gradio component(s) to use for the input. The number of components should match the number of arguments in your function.\n- `outputs`: the Gradio component(s) to use for the output. The number of components should match the number of return values from your function.\n\nThe `fn` argument is very flexible -- you can pass *any* Python function that you want to wrap with a UI. In the example above, we saw a relatively simple function, but the function could be anything from a music generator to a tax calculator to the prediction function of a pretrained machine learning model.", "Now, run your code. If you've written the Python code in a file named, for example, `app.py`, then you would run `python app.py` from the terminal.\n\nThe demo below will open in a browser on [http://localhost:7860](http://localhost:7860) if running from a file. If you are running within a notebook, the demo will appear embedded within the notebook.\n\n$demo_hello_world_4\n\nType your name in the textbox on the left, drag the slider, and then press the Submit button. You should see a friendly greeting on the right.\n\nTip: When developing locally, you can run your Gradio app in <strong>hot reload mode</strong>, which automatically reloads the Gradio app whenever you make changes to the file. To do this, simply type in <code>gradio</code> before the name of the file instead of <code>python</code>. In the example above, you would type: `gradio app.py` in your terminal. Learn more about hot reloading in the <a href=\"https://www.gradio.app/guides/developing-faster-with-reload-mode\">Hot Reloading Guide</a>.\n\n\n**Understanding the `Interface` Class**\n\nYou'll notice that in order to make your first demo, you created an instance of the `gr.Interface` class. The `Interface` class is designed to create demos for machine learning models which accept one or more inputs, and return one or more outputs. \n\nThe `Interface` class has three core arguments:\n\n- `fn`: the function to wrap a user interface (UI) around\n- `inputs`: the Gradio component(s) to use for the input. The number of components should match the number of arguments in your function.\n- `outputs`: the Gradio component(s) to use for the output. The number of components should match the number of return values from your function.", "if __name__ == \"__main__\":\n    demo.launch()\n```\n\nThe problem is that anytime that you want to make a change to your layout, events, or components, you have to close and rerun your app by writing `python run.py`.\n\nInstead of doing this, you can run your code in **reload mode** by changing 1 word: `python` to `gradio`:\n\nIn the terminal, run `gradio run.py`. That's it!\n\nNow, you'll see that after you'll see something like this:\n\n```bash\nWatching: '/Users/freddy/sources/gradio/gradio', '/Users/freddy/sources/gradio/demo/'\n\nRunning on local URL:  http://127.0.0.1:7860\n```\n\nThe important part here is the line that says `Watching...` What's happening here is that Gradio will be observing the directory where `run.py` file lives, and if the file changes, it will automatically rerun the file for you. So you can focus on writing your code, and your Gradio demo will refresh automatically \ud83e\udd73\n\n\u26a0\ufe0f Warning: the `gradio` command does not detect the parameters passed to the `launch()` methods because the `launch()` method is never called in reload mode. For example, setting `auth`, or `show_error` in `launch()` will not be reflected in the app.\n\nThere is one important thing to keep in mind when using the reload mode: Gradio specifically looks for a Gradio Blocks/Interface demo called `demo` in your code. If you have named your demo something else, you will need to pass in the name of your demo as the 2nd parameter in your code. So if your `run.py` file looked like this:\n\n```python\nimport gradio as gr", "If you're a newcomer to Gradio, we recommend getting familiar with the overall structure of the repository so that you can focus on the part of the source code you'd like to contribute to.\n\n- `/gradio`: contains the Python source code for the library\n  - `/gradio/interface.py`: contains the Python source code for the core `Interface` class\n  - `/gradio/blocks.py`: contains the Python source code for the core `Blocks` class\n  - `/gradio/components/`: the directory that contains the Python source code for all of the Gradio components.\n- `/js`: contains the HTML/JS/CSS source code for the library ([start here for frontend changes](/js/README.md))\n  - `/js/_website`: contains the code for the Gradio website (www.gradio.app). See the README in the `/js/_website` folder for more details\n- `/test`: contains Python unit tests for the library\n- `/demo`: contains demos that are used in the documentation, you can find `Gradio` examples over here.\n\n\n## \ud83d\ude80 Run a Gradio app\n\nYou can get started by creating an `app.py` file in the root:\n\n```\nimport gradio as gr\n\nwith gr.Blocks() as demo:\n   gr.Button()\n    \nif __name__ == \"__main__\":\n   demo.launch()\n```\n\nthen run:\n\n```\ngradio app.py\n```\n\nThis will start the backend server in reload mode, which will watch for changes in the `gradio` folder and reload the app if changes are made. By default, Gradio will launch on port 7860. You can also just use `python app.py`, but this won't automatically trigger updates. \n\n\nIf you're making frontend changes, start the frontend server:", "if __name__ == \"__main__\":\n    demo.launch()\n```\n\nWhich you could run like this: `gradio run.py --name Gretel`\n\nAs a small aside, this auto-reloading happens if you change your `run.py` source code or the Gradio source code. Meaning that this can be useful if you decide to [contribute to Gradio itself](https://github.com/gradio-app/gradio/blob/main/CONTRIBUTING.md) \u2705\n\n## Jupyter Notebook Magic \ud83d\udd2e\n\nWhat about if you use Jupyter Notebooks (or Colab Notebooks, etc.) to develop code? We got something for you too!\n\nWe've developed a **magic command** that will create and run a Blocks demo for you. To use this, load the gradio extension at the top of your notebook:\n\n`%load_ext gradio`\n\nThen, in the cell that you are developing your Gradio demo, simply write the magic command **`%%blocks`** at the top, and then write the layout and components like you would normally:\n\n```py\n%%blocks\n\nimport gradio as gr\n\nwith gr.Blocks() as demo:\n    gr.Markdown(f\"# Greetings {args.name}!\")\n    inp = gr.Textbox()\n    out = gr.Textbox()\n\n    inp.change(fn=lambda x: x, inputs=inp, outputs=out)\n```\n\nNotice that:\n\n- You do not need to launch your demo \u2014 Gradio does that for you automatically!\n\n- Every time you rerun the cell, Gradio will re-render your app on the same port and using the same underlying web server. This means you'll see your changes _much, much faster_ than if you were rerunning the cell normally.\n\nHere's what it looks like in a jupyter notebook:", "```python\nimport gradio as gr\n\nwith gr.Blocks() as my_demo:\n    gr.Markdown(\"# Greetings from Gradio!\")\n    inp = gr.Textbox(placeholder=\"What is your name?\")\n    out = gr.Textbox()\n\n    inp.change(fn=lambda x: f\"Welcome, {x}!\",\n               inputs=inp,\n               outputs=out)\n\nif __name__ == \"__main__\":\n    my_demo.launch()\n```\n\nThen you would launch it in reload mode like this: `gradio run.py my_demo`.\n\nBy default, the Gradio use UTF-8 encoding for scripts. **For reload mode**, If you are using encoding formats other than UTF-8 (such as cp1252), make sure you've done like this:\n\n1. Configure encoding declaration of python script, for example: `# -*- coding: cp1252 -*-`\n2. Confirm that your code editor has identified that encoding format. \n3. Run like this: `gradio run.py --encoding cp1252`\n\n\ud83d\udd25 If your application accepts command line arguments, you can pass them in as well. Here's an example:\n\n```python\nimport gradio as gr\nimport argparse\n\nparser = argparse.ArgumentParser()\nparser.add_argument(\"--name\", type=str, default=\"User\")\nargs, unknown = parser.parse_known_args()\n\nwith gr.Blocks() as demo:\n    gr.Markdown(f\"# Greetings {args.name}!\")\n    inp = gr.Textbox()\n    out = gr.Textbox()\n\n    inp.change(fn=lambda x: x, inputs=inp, outputs=out)\n\nif __name__ == \"__main__\":\n    demo.launch()\n```"], "eval_score_GPT4": "4", "eval_feedback_GPT4": "Feedback: The response provided is mostly correct and factual. It correctly identifies that the Gradio Blocks app can be run in reload mode and mentions the use of the terminal to navigate to the directory of the Python file. However, the response suggests using `gradio run.py` which is not a standard Gradio or Python command. The standard command to run a Python file is `python run.py` or `python3 run.py`. The response does not mention the correct way to enable reload mode, which is typically done by setting a flag or parameter in the code or command line, not by simply running `gradio run.py`. Therefore, the response is not completely accurate as per the reference answer and the rubric."}, {"question": "What command is used to install the development version of the \ud83e\udd17 Transformers library in a Python virtual environment?\n", "true_answer": "pip install \"transformers[sentencepiece]\"", "source_doc": "huggingface/course/blob/main/chapters/en/chapter0/1.mdx", "generated_answer": "The command to install the development version of the \ud83e\udd17 Transformers library in a Python virtual environment is:\n\n```bash\npip install \"transformers[sentencepiece]\"\n```\n\nThis command should be run inside the virtual environment created using the Python `venv` module, as described in section 6 of the provided context. The `[sentencepiece]` suffix in the command specifies that the development version of the library", "retrieved_docs": ["## Install from source\n\nInstall \ud83e\udd17 Transformers from source with the following command:\n\n```bash\npip install git+https://github.com/huggingface/transformers\n```\n\nThis command installs the bleeding edge `main` version rather than the latest `stable` version. The `main` version is useful for staying up-to-date with the latest developments. For instance, if a bug has been fixed since the last official release but a new release hasn't been rolled out yet. However, this means the `main` version may not always be stable. We strive to keep the `main` version operational, and most issues are usually resolved within a few hours or a day. If you run into a problem, please open an [Issue](https://github.com/huggingface/transformers/issues) so we can fix it even sooner!\n\nCheck if \ud83e\udd17 Transformers has been properly installed by running the following command:\n\n```bash\npython -c \"from transformers import pipeline; print(pipeline('sentiment-analysis')('I love you'))\"\n```\n\n## Editable install\n\nYou will need an editable install if you'd like to:\n\n* Use the `main` version of the source code.\n* Contribute to \ud83e\udd17 Transformers and need to test changes in the code.\n\nClone the repository and install \ud83e\udd17 Transformers with the following commands:\n\n```bash\ngit clone https://github.com/huggingface/transformers.git\ncd transformers\npip install -e .\n```\n\nThese commands will link the folder you cloned the repository to and your Python library paths. Python will now look inside the folder you cloned to in addition to the normal library paths. For example, if your Python packages are typically installed in `~/anaconda3/envs/main/lib/python3.7/site-packages/`, Python will also search the folder you cloned to: `~/transformers/`.\n\n<Tip warning={true}>\n\nYou must keep the `transformers` folder if you want to keep using the library.\n\n</Tip>", "To get the OS and software versions automatically, run the following command:\n\n```bash\ntransformers-cli env\n```\n\nYou can also run the same command from the root of the repository:\n\n```bash\npython src/transformers/commands/transformers_cli.py env\n```\n\n### Do you want a new feature?\n\nIf there is a new feature you'd like to see in \ud83e\udd17 Transformers, please open an issue and describe:\n\n1. What is the *motivation* behind this feature? Is it related to a problem or frustration with the library? Is it a feature related to something you need for a project? Is it something you worked on and think it could benefit the community?\n\n   Whatever it is, we'd love to hear about it!\n\n2. Describe your requested feature in as much detail as possible. The more you can tell us about it, the better we'll be able to help you.\n3. Provide a *code snippet* that demonstrates the features usage.\n4. If the feature is related to a paper, please include a link.\n\nIf your issue is well written we're already 80% of the way there by the time you create it.\n\nWe have added [templates](https://github.com/huggingface/transformers/tree/main/templates) to help you get started with your issue.\n\n## Do you want to implement a new model?\n\nNew models are constantly released and if you want to implement a new model, please provide the following information\n\n* A short description of the model and a link to the paper.\n* Link to the implementation if it is open-sourced.\n* Link to the model weights if they are available.\n\nIf you are willing to contribute the model yourself, let us know so we can help you add it to \ud83e\udd17 Transformers!", "To get the OS and software versions automatically, run the following command:\n\n```bash\ntransformers-cli env\n```\n\nYou can also run the same command from the root of the repository:\n\n```bash\npython src/transformers/commands/transformers_cli.py env\n```\n\n### Do you want a new feature?\n\nIf there is a new feature you'd like to see in \ud83e\udd17 Transformers, please open an issue and describe:\n\n1. What is the *motivation* behind this feature? Is it related to a problem or frustration with the library? Is it a feature related to something you need for a project? Is it something you worked on and think it could benefit the community?\n\n   Whatever it is, we'd love to hear about it!\n\n2. Describe your requested feature in as much detail as possible. The more you can tell us about it, the better we'll be able to help you.\n3. Provide a *code snippet* that demonstrates the features usage.\n4. If the feature is related to a paper, please include a link.\n\nIf your issue is well written we're already 80% of the way there by the time you create it.\n\nWe have added [templates](https://github.com/huggingface/transformers/tree/main/templates) to help you get started with your issue.\n\n## Do you want to implement a new model?\n\nNew models are constantly released and if you want to implement a new model, please provide the following information\n\n* A short description of the model and a link to the paper.\n* Link to the implementation if it is open-sourced.\n* Link to the model weights if they are available.\n\nIf you are willing to contribute the model yourself, let us know so we can help you add it to \ud83e\udd17 Transformers!", "Start by creating a virtual environment in your project directory:\n\n```bash\npython -m venv .env\n```\n\nActivate the virtual environment. On Linux and MacOs:\n\n```bash\nsource .env/bin/activate\n```\nActivate Virtual environment on Windows\n\n```bash\n.env/Scripts/activate\n```\n\nNow you're ready to install \ud83e\udd17 Transformers with the following command:\n\n```bash\npip install transformers\n```\n\nFor CPU-support only, you can conveniently install \ud83e\udd17 Transformers and a deep learning library in one line. For example, install \ud83e\udd17 Transformers and PyTorch with:\n\n```bash\npip install 'transformers[torch]'\n```\n\n\ud83e\udd17 Transformers and TensorFlow 2.0:\n\n```bash\npip install 'transformers[tf-cpu]'\n```\n\n<Tip warning={true}>\n\nM1 / ARM Users\n    \nYou will need to install the following before installing TensorFLow 2.0\n```\nbrew install cmake\nbrew install pkg-config\n```\n\n</Tip>\n\n\ud83e\udd17 Transformers and Flax:\n\n```bash\npip install 'transformers[flax]'\n```\n\nFinally, check if \ud83e\udd17 Transformers has been properly installed by running the following command. It will download a pretrained model:\n\n```bash\npython -c \"from transformers import pipeline; print(pipeline('sentiment-analysis')('we love you'))\"\n```\n\nThen print out the label and score:\n\n```bash\n[{'label': 'POSITIVE', 'score': 0.9998704791069031}]\n```\n\n## Install from source\n\nInstall \ud83e\udd17 Transformers from source with the following command:\n\n```bash\npip install git+https://github.com/huggingface/transformers\n```", "<Tip warning={true}>\n\nYou must keep the `transformers` folder if you want to keep using the library.\n\n</Tip>\n\nNow you can easily update your clone to the latest version of \ud83e\udd17 Transformers with the following command:\n\n```bash\ncd ~/transformers/\ngit pull\n```\n\nYour Python environment will find the `main` version of \ud83e\udd17 Transformers on the next run.\n\n## Install with conda\n\nInstall from the conda channel `huggingface`:\n\n```bash\nconda install -c huggingface transformers\n```\n\n## Cache setup\n\nPretrained models are downloaded and locally cached at: `~/.cache/huggingface/hub`. This is the default directory given by the shell environment variable `TRANSFORMERS_CACHE`. On Windows, the default directory is given by `C:\\Users\\username\\.cache\\huggingface\\hub`. You can change the shell environment variables shown below - in order of priority - to specify a different cache directory:\n\n1. Shell environment variable (default): `HUGGINGFACE_HUB_CACHE` or `TRANSFORMERS_CACHE`.\n2. Shell environment variable: `HF_HOME`.\n3. Shell environment variable: `XDG_CACHE_HOME` + `/huggingface`.\n\n<Tip>\n\n\ud83e\udd17 Transformers will use the shell environment variables `PYTORCH_TRANSFORMERS_CACHE` or `PYTORCH_PRETRAINED_BERT_CACHE` if you are coming from an earlier iteration of this library and have set those environment variables, unless you specify the shell environment variable `TRANSFORMERS_CACHE`.\n\n</Tip>\n\n## Offline mode\n\nRun \ud83e\udd17 Transformers in a firewalled or offline environment with locally cached files by setting the environment variable `TRANSFORMERS_OFFLINE=1`.\n\n<Tip>\n\nAdd [\ud83e\udd17 Datasets](https://huggingface.co/docs/datasets/) to your offline training workflow with the environment variable `HF_DATASETS_OFFLINE=1`.\n\n</Tip>", "From inside this directory, create a virtual environment using the Python `venv` module:\n\n```\npython -m venv .env\n```\n\nYou should now have a directory called *.env* in your otherwise empty folder:\n\n```\nls -a\n```\n\n```out\n.      ..    .env\n```\n\nYou can jump in and out of your virtual environment with the `activate` and `deactivate` scripts:\n\n```\n# Activate the virtual environment\nsource .env/bin/activate\n\n# Deactivate the virtual environment\nsource .env/bin/deactivate\n```\n\nYou can make sure that the environment is activated by running the `which python` command: if it points to the virtual environment, then you have successfully activated it!\n\n```\nwhich python\n```\n\n```out\n/home/<user>/transformers-course/.env/bin/python\n```\n\n### Installing dependencies[[installing-dependencies]]\n\nAs in the previous section on using Google Colab instances, you'll now need to install the packages required to continue. Again, you can install the development version of \ud83e\udd17 Transformers using the `pip` package manager:\n\n```\npip install \"transformers[sentencepiece]\"\n```\n\nYou're now all set up and ready to go!", "This will take a bit of time, but then you'll be ready to go for the rest of the course!\n\n## Using a Python virtual environment[[using-a-python-virtual-environment]]\n\nIf you prefer to use a Python virtual environment, the first step is to install Python on your system. We recommend following [this guide](https://realpython.com/installing-python/) to get started.\n\nOnce you have Python installed, you should be able to run Python commands in your terminal. You can start by running the following command to ensure that it is correctly installed before proceeding to the next steps: `python --version`. This should print out the Python version now available on your system.\n\nWhen running a Python command in your terminal, such as `python --version`, you should think of the program running your command as the \"main\" Python on your system. We recommend keeping this main installation free of any packages, and using it to create separate environments for each application you work on \u2014 this way, each application can have its own dependencies and packages, and you won't need to worry about potential compatibility issues with other applications.\n\nIn Python this is done with [*virtual environments*](https://docs.python.org/3/tutorial/venv.html), which are self-contained directory trees that each contain a Python installation with a particular Python version alongside all the packages the application needs. Creating such a virtual environment can be done with a number of different tools, but we'll use the official Python package for that purpose, which is called [`venv`](https://docs.python.org/3/library/venv.html#module-venv).\n\nFirst, create the directory you'd like your application to live in \u2014 for example, you might want to make a new directory called *transformers-course* at the root of your home directory:\n\n```\nmkdir ~/transformers-course\ncd ~/transformers-course\n```\n\nFrom inside this directory, create a virtual environment using the Python `venv` module:\n\n```\npython -m venv .env\n```"], "eval_score_GPT4": "3", "eval_feedback_GPT4": "The response provided is partially correct. The command `pip install \"transformers[sentencepiece]\"` is indeed used to install the Transformers library along with the sentencepiece package, which is an additional dependency. However, the response fails to specify that this command installs the development version of the library. The reference answer does not include any additional information that would indicate the development version either, but the question specifically asks for the command to install the development version. Therefore, the response does not fully meet the criteria of being completely correct, accurate, and factual in the context of the instruction given."}, {"question": "How can you install the Hugging Face Unity API in your Unity project?\n", "true_answer": "To install the Hugging Face Unity API in your Unity project, go to `Window` -> `Package Manager`, click `+` and select `Add Package from git URL`, then enter `https://github.com/huggingface/unity-api.git`.", "source_doc": "huggingface/blog/blob/main/unity-api.md", "generated_answer": "To install the Hugging Face Unity API in your Unity project, follow these steps:\n\n1. Open your Unity project.\n2. Go to `Window` > `Package Manager`.\n3. Click `+` and select `Add Package from git URL`.\n4. Enter `https://github.com/huggingface/unity-api.git` and click `Add`.\n5. Once installed, the Hugging Face API Wizard should pop", "retrieved_docs": ["--\ntitle: \"How to Install and Use the Hugging Face Unity API\"\nthumbnail: /blog/assets/124_ml-for-games/unity-api-thumbnail.png\nauthors:\n- user: dylanebert\n---\n\n# How to Install and Use the Hugging Face Unity API\n\n<!-- {authors} --> \n\nThe [Hugging Face Unity API](https://github.com/huggingface/unity-api) is an easy-to-use integration of the [Hugging Face Inference API](https://huggingface.co/inference-api), allowing developers to access and use Hugging Face AI models in their Unity projects. In this blog post, we'll walk through the steps to install and use the Hugging Face Unity API.\n\n## Installation\n\n1. Open your Unity project\n2. Go to `Window` -> `Package Manager`\n3. Click `+` and select `Add Package from git URL`\n4. Enter `https://github.com/huggingface/unity-api.git`\n5. Once installed, the Unity API wizard should pop up. If not, go to `Window` -> `Hugging Face API Wizard`\n\n<figure class=\"image text-center\">\n  <img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/124_ml-for-games/packagemanager.gif\">\n</figure>", "6. Enter your API key. Your API key can be created in your [Hugging Face account settings](https://huggingface.co/settings/tokens).\n7. Test the API key by clicking `Test API key` in the API Wizard.\n8. Optionally, change the model endpoints to change which model to use. The model endpoint for any model that supports the inference API can be found by going to the model on the Hugging Face website, clicking `Deploy` -> `Inference API`, and copying the url from the `API_URL` field.\n9. Configure advanced settings if desired. For up-to-date information, visit the project repository at `https://github.com/huggingface/unity-api`\n10. To see examples of how to use the API, click `Install Examples`. You can now close the API Wizard.\n\n<figure class=\"image text-center\">\n  <img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/124_ml-for-games/apiwizard.png\">\n</figure> \n\nNow that the API is set up, you can make calls from your scripts to the API. Let's look at an example of performing a Sentence Similarity task:\n\n```\nusing HuggingFace.API;\n\n/* other code */\n\n// Make a call to the API\nvoid Query() {\n    string inputText = \"I'm on my way to the forest.\";\n    string[] candidates = {\n        \"The player is going to the city\",\n        \"The player is going to the wilderness\",\n        \"The player is wandering aimlessly\"\n    };\n    HuggingFaceAPI.SentenceSimilarity(inputText, OnSuccess, OnError, candidates);\n}\n\n// If successful, handle the result\nvoid OnSuccess(float[] result) {\n    foreach(float value in result) {\n        Debug.Log(value);\n    }\n}\n\n// Otherwise, handle the error\nvoid OnError(string error) {\n    Debug.LogError(error);\n}\n\n/* other code */\n```", "his package provides core backend functionality for the Hugging Face Simulate project: (https://github.com/huggingface/simulate).\n\nTo use this package, add a GameObject to the scene, then add a `Simulator` component to it.\n\nCore Classes:\n- `Simulator`: Master controller singleton that tracks objects, loads and unloads plugins, etc.\n- `Node`: Component attached to every tracked object.\n- `RenderCamera`: Component attached to every tracked camera. Call `Render()` to asynchronously request a color buffer.\n- `Client`: Manages communication with the backend. Don't modify directly; instead, use plugins.\n\nPlugins:\n- `IPlugin`: For adding custom behaviour to the backend.\n- `ICommand`: For defining custom commands from the Python frontend.\n- `IGLTFExtension`: For defining custom GLTF extensions.\n\nTo add functionality to the backend:\n- Create a separate folder. For example, `RLAgents`.\n- Add an assembly definition to this folder (Right Click > Create > Assembly Definition).\n- In the assembly definition, add a reference to `Simulate`.\n- In the folder, create a script that implements `IPlugin`. For a simple example, look at `IPlugin.cs`, or for a more complex example, see `AgentsPlugin.cs`.\n\nIf this folder is in the Unity project, it will automatically be included in the build.\n\nTo add functionality post-build, a plugin can be compiled externally, then the DLL can be placed in the build's `Resources/Plugins/` folder.", "// Otherwise, handle the error\nvoid OnError(string error) {\n    Debug.LogError(error);\n}\n\n/* other code */\n```\n\n## Supported Tasks and Custom Models\n\nThe Hugging Face Unity API also currently supports the following tasks:\n\n- [Conversation](https://huggingface.co/tasks/conversational)\n- [Text Generation](https://huggingface.co/tasks/text-generation)\n- [Text to Image](https://huggingface.co/tasks/text-to-image)\n- [Text Classification](https://huggingface.co/tasks/text-classification)\n- [Question Answering](https://huggingface.co/tasks/question-answering)\n- [Translation](https://huggingface.co/tasks/translation)\n- [Summarization](https://huggingface.co/tasks/summarization)\n- [Speech Recognition](https://huggingface.co/tasks/automatic-speech-recognition)\n\nUse the corresponding methods provided by the `HuggingFaceAPI` class to perform these tasks.\n\nTo use your own custom model hosted on Hugging Face, change the model endpoint in the API Wizard.\n\n## Usage Tips\n\n1. Keep in mind that the API makes calls asynchronously, and returns a response or error via callbacks.\n2. Address slow response times or performance issues by changing model endpoints to lower resource models.\n\n## Conclusion\n\nThe Hugging Face Unity API offers a simple way to integrate AI models into your Unity projects. We hope you found this tutorial helpful. If you have any questions or would like to get more involved in using Hugging Face for Games, join the [Hugging Face Discord](https://hf.co/join/discord)!", "## Step 2: Use Git to Clone the Space\n\nClone your newly created Space to your local machine using Git. You can do this by running the following command in your terminal or command prompt:\n\n```\ngit clone https://huggingface.co/spaces/{your-username}/{your-space-name}\n```\n\n## Step 3: Open your Unity Project\n\nOpen the Unity project you want to host in your Space.\n\n<figure class=\"image text-center\">\n  <img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/124_ml-for-games/games-in-spaces/3.png\">\n</figure> \n\n## Step 4: Switch the Build Target to WebGL\n\nNavigate to `File > Build Settings` and switch the Build Target to WebGL.\n\n<figure class=\"image text-center\">\n  <img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/124_ml-for-games/games-in-spaces/4.png\">\n</figure> \n\n## Step 5: Open Player Settings\n\nIn the Build Settings window, click the \"Player Settings\" button to open the Player Settings panel.\n\n<figure class=\"image text-center\">\n  <img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/124_ml-for-games/games-in-spaces/5.png\">\n</figure> \n\n## Step 6: Optionally, Download the Hugging Face Unity WebGL Template\n\nYou can enhance your game's appearance in a Space by downloading the Hugging Face Unity WebGL template, available [here](https://github.com/huggingface/Unity-WebGL-template-for-Hugging-Face-Spaces). Just download the repository and drop it in your project files.", "5. Set up your [Camera](https://docs.unity3d.com/ScriptReference/Camera.html) **using an orthographic projection** to match the projection of the concept art.\n\n<figure class=\"image text-center\">\n  <img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/124_ml-for-games/camera.png\" alt=\"Scene with Camera\">\n</figure>\n\n6. Add some water. I'm using the [Stylized Water Shader](https://assetstore.unity.com/packages/vfx/shaders/stylized-water-shader-71207) from the Unity asset store.\n\n<figure class=\"image text-center\">\n  <img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/124_ml-for-games/water.png\" alt=\"Scene with Water\">\n</figure>\n\n7. Finally, set up [Post-processing](https://docs.unity3d.com/Packages/com.unity.render-pipelines.universal@7.1/manual/integration-with-post-processing.html). I'm using ACES tonemapping and +0.2 exposure.\n\n<figure class=\"image text-center\">\n  <img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/124_ml-for-games/post-processing.png\" alt=\"Final Result\">\n</figure>\n\nThat's it! A simple but appealing scene, made in less than a day! Have questions? Want to get more involved? Join the [Hugging Face Discord](https://t.co/1n75wi976V?amp=1)!\n\nClick [here](https://huggingface.co/blog/ml-for-games-2) to read Part 2, where we use **AI for Game Design**.", "Then, in the Player Settings panel, switch the WebGL template to Hugging Face. To do so, in Player Settings, click \"Resolution and Presentation\", then select the Hugging Face WebGL template.\n\n<figure class=\"image text-center\">\n  <img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/124_ml-for-games/games-in-spaces/6.png\">\n</figure> \n\n## Step 7: Change the Compression Format to Disabled\n\nIn the Player Settings panel, navigate to the \"Publishing Settings\" section and change the Compression Format to \"Disabled\".\n\n<figure class=\"image text-center\">\n  <img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/124_ml-for-games/games-in-spaces/7.png\">\n</figure> \n\n## Step 8: Build your Project\n\nReturn to the Build Settings window and click the \"Build\" button. Choose a location to save your build files, and Unity will build the project for WebGL.\n\n<figure class=\"image text-center\">\n  <img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/124_ml-for-games/games-in-spaces/8.png\">\n</figure> \n\n## Step 9: Copy the Contents of the Build Folder\n\nAfter the build process is finished, navigate to the folder containing your build files. Copy the files in the build folder to the repository you cloned in [Step 2](#step-2-use-git-to-clone-the-space).\n<figure class=\"image text-center\">\n  <img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/124_ml-for-games/games-in-spaces/9.png\">\n</figure> \n\n## Step 10: Enable Git-LFS for Large File Storage"], "eval_score_GPT4": "5", "eval_feedback_GPT4": "The response provided is completely correct, accurate, and factual. It follows the steps outlined in the reference answer and adds a bit more detail by mentioning the expected outcome after the package is added, which is the Hugging Face API Wizard popping up. This additional information is accurate and could be helpful to someone following the instructions, although it is not required to meet the criteria of the reference answer. The response meets the criteria for a score of 5 as per the rubric."}, {"question": "What is the pretraining objective of the Wav2Vec2 context network?\n", "true_answer": "The pretraining objective of the Wav2Vec2 context network is a contrastive task where the model has to predict the true quantized speech representation of the masked prediction from a set of false ones.", "source_doc": "huggingface/transformers/blob/main/docs/source/en/tasks_explained.md", "generated_answer": "The pretraining objective of the Wav2Vec2 context network is Wav2Vec2's contrastive loss objective, which is used to pre-train a Wav2Vec2 model from scratch on audio data alone, as explained in the script `run_speech_wav2vec2_pretraining_no_trainer.py`. This objective is based on the Connectionist Temporal Classification (CTC) algorithm, which solves the problem of alignment", "retrieved_docs": ["--\ntitle: \"Making automatic speech recognition work on large files with Wav2Vec2 in \ud83e\udd17 Transformers\"\nthumbnail: /blog/assets/49_asr_chunking/thumbnail.png\nauthors:\n- user: Narsil\n---\n\n# Making automatic speech recognition work on large files with Wav2Vec2 in \ud83e\udd17 Transformers\n\n\n```\nTl;dr: This post explains how to use the specificities of the Connectionist\nTemporal Classification (CTC) architecture in order to achieve very good\nquality automatic speech recognition (ASR) even on arbitrarily long files or \nduring live inference.\n```\n\n\n**Wav2Vec2** is a popular pre-trained model for speech recognition.\nReleased in [September 2020](https://ai.facebook.com/blog/wav2vec-20-learning-the-structure-of-speech-from-raw-audio/)\nby Meta AI Research, the novel architecture catalyzed progress in\nself-supervised pretraining for speech recognition, *e.g.* [*G. Ng et\nal.*, 2021](https://arxiv.org/pdf/2104.03416.pdf), [*Chen et al*,\n2021](https://arxiv.org/abs/2110.13900), [*Hsu et al.*,\n2021](https://arxiv.org/abs/2106.07447) and [*Babu et al.*,\n2021](https://arxiv.org/abs/2111.09296). On the Hugging Face Hub,\nWav2Vec2's most popular pre-trained checkpoint currently amounts to\nover [**250,000** monthly\ndownloads](https://huggingface.co/facebook/wav2vec2-base-960h).", "Using Connectionist Temporal Classification (CTC), pre-trained\nWav2Vec2-like checkpoints are extremely easy to fine-tune on downstream\nspeech recognition tasks. In a nutshell, fine-tuning pre-trained\nWav2Vec2 checkpoints works as follows:\n\nA single randomly initialized linear layer is stacked on top of the\npre-trained checkpoint and trained to classify raw audio input to a\nsequence of letters. It does so by:\n\n1.  extracting audio representations from the raw audio (using CNN\n    layers),\n2.  processing the sequence of audio representations with a stack of\n    transformer layers, and,\n3.  classifying the processed audio representations into a sequence of\n    output letters.\n\nPreviously audio classification models required an additional language\nmodel (LM) and a dictionary to transform the sequence of classified audio\nframes to a coherent transcription. Wav2Vec2's architecture is based on\ntransformer layers, thus giving each processed audio representation\ncontext from all other audio representations. In addition, Wav2Vec2\nleverages the [CTC algorithm](https://distill.pub/2017/ctc/) for\nfine-tuning, which solves the problem of alignment between a varying\n\"input audio length\"-to-\"output text length\" ratio.\n\nHaving contextualized audio classifications and no alignment problems,\nWav2Vec2 does not require an external language model or dictionary to\nyield acceptable audio transcriptions.\n\nAs can be seen in Appendix C of the [official\npaper](https://arxiv.org/abs/2006.11477), Wav2Vec2 gives impressive\ndownstream performances on [LibriSpeech](https://huggingface.co/datasets/librispeech_asr) without using a language model at\nall. However, from the appendix, it also becomes clear that using Wav2Vec2\nin combination with a language model can yield a significant\nimprovement, especially when the model was trained on only 10 minutes of\ntranscribed audio.", "Wav2Vec2 was pretrained on the audio data of\n[LibriSpeech](https://huggingface.co/datasets/librispeech_asr) and\nLibriVox which both were sampling with 16kHz. Our fine-tuning dataset,\n[Timit](hhtps://huggingface.co/datasets/timit_asr), was luckily also\nsampled with 16kHz. If the fine-tuning dataset would have been sampled\nwith a rate lower or higher than 16kHz, we first would have had to up or\ndownsample the speech signal to match the sampling rate of the data used\nfor pretraining.\n\nA Wav2Vec2 feature extractor object requires the following parameters to\nbe instantiated:", "### How to effectively preprocess the data\n\n\n### How to do efficiently load datasets with limited ram and hard drive space\n\nCheck out [this](https://discuss.huggingface.co/t/german-asr-fine-tuning-wav2vec2/4558/8?u=patrickvonplaten) post.\n\n\n### How to do hyperparameter tuning\n\n\n### How to preprocess and evaluate character based languages\n\n\n## Further reading material\n\nIt is recommended that take some time to read up on how Wav2vec2 works in theory. \nGetting a better understanding of the theory and the inner mechanisms of the model often helps when fine-tuning the model. \n\n**However**, if you don't like reading blog posts/papers, don't worry - it is by no means necessary to go through the theory to fine-tune Wav2Vec2 on your language of choice.\n\nIf you are interested in learning more about the model though, here are a couple of resources that are important to better understand Wav2Vec2:\n\n- [Facebook's Wav2Vec2 blog post](https://ai.facebook.com/blog/wav2vec-state-of-the-art-speech-recognition-through-self-supervision/)\n- [Official Wav2Vec2 paper](https://arxiv.org/abs/2006.11477)\n- [Official XLSR Wav2vec2 paper](https://arxiv.org/pdf/2006.13979.pdf)\n- [Hugging Face Blog](https://huggingface.co/blog/fine-tune-xlsr-wav2vec2)\n- [How does CTC (Connectionist Temporal Classification) work](https://distill.pub/2017/ctc/)\n\nIt helps to have a good understanding of the following points:", "## Usage tips\n\n- Wav2Vec2-Conformer follows the same architecture as Wav2Vec2, but replaces the *Attention*-block with a *Conformer*-block\n  as introduced in [Conformer: Convolution-augmented Transformer for Speech Recognition](https://arxiv.org/abs/2005.08100).\n- For the same number of layers, Wav2Vec2-Conformer requires more parameters than Wav2Vec2, but also yields \nan improved word error rate.\n- Wav2Vec2-Conformer uses the same tokenizer and feature extractor as Wav2Vec2.\n- Wav2Vec2-Conformer can use either no relative position embeddings, Transformer-XL-like position embeddings, or\n  rotary position embeddings by setting the correct `config.position_embeddings_type`.\n\n## Resources\n\n- [Audio classification task guide](../tasks/audio_classification)\n- [Automatic speech recognition task guide](../tasks/asr)\n\n## Wav2Vec2ConformerConfig\n\n[[autodoc]] Wav2Vec2ConformerConfig\n\n## Wav2Vec2Conformer specific outputs\n\n[[autodoc]] models.wav2vec2_conformer.modeling_wav2vec2_conformer.Wav2Vec2ConformerForPreTrainingOutput\n\n## Wav2Vec2ConformerModel\n\n[[autodoc]] Wav2Vec2ConformerModel\n    - forward\n\n## Wav2Vec2ConformerForCTC\n\n[[autodoc]] Wav2Vec2ConformerForCTC\n    - forward\n\n## Wav2Vec2ConformerForSequenceClassification\n\n[[autodoc]] Wav2Vec2ConformerForSequenceClassification\n    - forward", "!---\nCopyright 2021 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\");\nyou may not use this file except in compliance with the License.\nYou may obtain a copy of the License at\n\n    http://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software\ndistributed under the License is distributed on an \"AS IS\" BASIS,\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\nSee the License for the specific language governing permissions and\nlimitations under the License.\n-->\n\n# Speech Recognition Pre-Training\n\n\n## Wav2Vec2 Speech Pre-Training\n\nThe script [`run_speech_wav2vec2_pretraining_no_trainer.py`](https://github.com/huggingface/transformers/blob/main/examples/pytorch/speech-pretraining/run_wav2vec2_pretraining_no_trainer.py) can be used to pre-train a [Wav2Vec2](https://huggingface.co/transformers/model_doc/wav2vec2.html?highlight=wav2vec2) model from scratch.\n\nIn the script [`run_speech_wav2vec2_pretraining_no_trainer`](https://github.com/huggingface/transformers/blob/main/examples/pytorch/speech-pretraining/run_wav2vec2_pretraining_no_trainer.py), a Wav2Vec2 model is pre-trained on audio data alone using [Wav2Vec2's contrastive loss objective](https://arxiv.org/abs/2006.11477).", "Here is an example of how you can use DeepSpeed ZeRO-2 to pretrain a small Wav2Vec2 model:\n\n```\nPYTHONPATH=../../../src deepspeed --num_gpus 4 run_pretrain.py \\\n--output_dir=\"./wav2vec2-base-libri-100h\" \\\n--num_train_epochs=\"3\" \\\n--per_device_train_batch_size=\"32\" \\\n--per_device_eval_batch_size=\"32\" \\\n--gradient_accumulation_steps=\"2\" \\\n--save_total_limit=\"3\" \\\n--save_steps=\"500\" \\\n--logging_steps=\"10\" \\\n--learning_rate=\"5e-4\" \\\n--weight_decay=\"0.01\" \\\n--warmup_steps=\"3000\" \\\n--model_name_or_path=\"patrickvonplaten/wav2vec2-base-libri-100h\" \\\n--dataset_name=\"librispeech_asr\" \\\n--dataset_config_name=\"clean\" \\\n--train_split_name=\"train.100\" \\\n--preprocessing_num_workers=\"4\" \\\n--max_duration_in_seconds=\"10.0\" \\\n--group_by_length \\\n--verbose_logging \\\n--fp16 \\\n--deepspeed ds_config_wav2vec2_zero2.json \\\n```\n\n\n### Forced Alignment\n\nCharacter level forced alignment for audio and text pairs with wav2vec2 models finetuned on ASR task for a specific language.\nInspired by [this](https://pytorch.org/tutorials/intermediate/forced_alignment_with_torchaudio_tutorial.html) Pytorch tutorial.\n\n#### Input Formats"], "eval_score_GPT4": "3", "eval_feedback_GPT4": "Feedback: The response correctly identifies the pretraining objective as a contrastive loss objective, which aligns with the reference answer's mention of a contrastive task. However, the response inaccurately associates the pretraining objective with the CTC algorithm, which is not mentioned in the reference answer and is not part of the contrastive task of predicting true quantized speech representations. Therefore, the response is somewhat correct but includes an inaccuracy that is significant to the understanding of the Wav2Vec2 pretraining objective."}, {"question": "What is the default checkpoint used by the sentiment analysis pipeline in the Transformers library?\n", "true_answer": "distilbert base uncased finetuned sst2 english", "source_doc": "huggingface/course/blob/main/subtitles/en/raw/chapter2/02_inside-pipeline-pt.md", "generated_answer": "The default checkpoint used by the sentiment analysis pipeline in the Transformers library is \"distilbert-base-uncased-finetuned-sst-2-english\". This information can be found by looking at the model card provided by Hugging Face Co. (https://huggingface.co/distilbert-base-uncased-finetuned-sst-2-english). To load this checkpoint and its associated tokenizer", "retrieved_docs": ["hat happens inside the pipeline function? In this video, we will look at what actually happens when we use the pipeline function of the Transformers library. More specifically, we will look at the sentiment analysis pipeline, and how it went from the two following sentences to the positive labels with their respective scores. As we have seen in the pipeline presentation, there are three stages in the pipeline. First, we convert the raw texts to numbers the model can make sense of, using a tokenizer. Then, those numbers go through the model, which outputs logits. Finally, the post-processing steps transforms those logits into labels and scores. Let's look in detail at those three steps, and how to replicate them using the Transformers library, beginning with the first stage, tokenization. The tokenization process has several steps. First, the text is split into small chunks called tokens. They can be words, parts of words or punctuation symbols. Then the tokenizer will had some special tokens (if the model expect them). Here the model uses expects a CLS token at the beginning and a SEP token at the end of the sentence to classify. Lastly, the tokenizer matches each token to its unique ID in the vocabulary of the pretrained model. To load such a tokenizer, the Transformers library provides the AutoTokenizer API. The most important method of this class is from_pretrained, which will download and cache the configuration and the vocabulary associated to a given checkpoint. Here, the checkpoint used by default for the sentiment analysis pipeline is distilbert base uncased finetuned sst2 english. We instantiate a tokenizer associated with that checkpoint, then feed it the two sentences. Since those two sentences are not of the same size, we will need to pad the shortest one to be able to build an array. This is done by the tokenizer with the option padding=True. With truncation=True, we ensure that any sentence longer than the maximum the model can handle is truncated", "hat happens inside the pipeline function? In this video, we will look at what actually happens when we use the pipeline function of the Transformers library. More specifically, we will look at the sentiment analysis pipeline, and how it went from the two following sentences to the positive labels with their respective scores. As we have seen in the pipeline presentation, there are three stages in the pipeline. First, we convert the raw texts to numbers the model can make sense of, using a tokenizer. Then, those numbers go through the model, which outputs logits. Finally, the post-processing steps transforms those logits into labels and scores. Let's look in detail at those three steps, and how to replicate them using the Transformers library, beginning with the first stage, tokenization. The tokenization process has several steps. First, the text is split into small chunks called tokens. They can be words, parts of words or punctuation symbols. Then the tokenizer will had some special tokens (if the model expect them). Here the model uses expects a CLS token at the beginning and a SEP token at the end of the sentence to classify. Lastly, the tokenizer matches each token to its unique ID in the vocabulary of the pretrained model. To load such a tokenizer, the Transformers library provides the AutoTokenizer API. The most important method of this class is from_pretrained, which will download and cache the configuration and the vocabulary associated to a given checkpoint. Here, the checkpoint used by default for the sentiment analysis pipeline is distilbert base uncased finetuned sst2 english. We instantiate a tokenizer associated with that checkpoint, then feed it the two sentences. Since those two sentences are not of the same size, we will need to pad the shortest one to be able to build an array. This is done by the tokenizer with the option padding=True. With truncation=True, we ensure that any sentence longer than the maximum the model can handle is truncated", "he pipeline function. The pipeline function is the most high-level API of the Transformers library. It regroups together all the steps to go from raw texts to usable predictions. The model used is at the core of a pipeline, but the pipeline also include all the necessary pre-processing (since the model does not expect texts, but numbers) as well as some post-processing to make the output of the model human-readable. Let's look at a first example with the sentiment analysis pipeline. This pipeline performs text classification on a given input, and determines if it's positive or negative. Here, it attributed the positive label on the given text, with a confidence of 95%. You can pass multiple texts to the same pipeline, which will be processed and passed through the model together, as a batch. The output is a list of individual results, in the same order as the input texts. Here we find the same label and score for the first text, and the second text is judged positive with a confidence of 99.99%. The zero-shot classification pipeline is a more general text-classification pipeline: it allows you to provide the labels you want. Here we want to classify our input text along the labels \"education\", \"politics\" and \"business\". The pipeline successfully recognizes it's more about education than the other labels, with a confidence of 84%. Moving on to other tasks, the text generation pipeline will auto-complete a given prompt. The output is generated with a bit of randomness, so it changes each time you call the generator object on a given prompt. Up until now, we have used the pipeline API with the default model associated to each task, but you can use it with any model that has been pretrained or fine-tuned on this task. Going on the model hub (huggingface.co/models), you can filter the available models by task", "## \ud035 \ud22c\uc5b4\n\n\uc6d0\ud558\ub294 \ud14d\uc2a4\ud2b8\uc5d0 \ubc14\ub85c \ubaa8\ub378\uc744 \uc0ac\uc6a9\ud560 \uc218 \uc788\ub3c4\ub85d, \uc6b0\ub9ac\ub294 `pipeline` API\ub97c \uc81c\uacf5\ud569\ub2c8\ub2e4. Pipeline\uc740 \uc0ac\uc804\ud559\uc2b5 \ubaa8\ub378\uacfc \uadf8 \ubaa8\ub378\uc744 \ud559\uc2b5\ud560 \ub54c \uc801\uc6a9\ud55c \uc804\ucc98\ub9ac \ubc29\uc2dd\uc744 \ud558\ub098\ub85c \ud569\uce69\ub2c8\ub2e4. \ub2e4\uc74c\uc740 \uae0d\uc815\uc801\uc778 \ud14d\uc2a4\ud2b8\uc640 \ubd80\uc815\uc801\uc778 \ud14d\uc2a4\ud2b8\ub97c \ubd84\ub958\ud558\uae30 \uc704\ud574 pipeline\uc744 \uc0ac\uc6a9\ud55c \uac04\ub2e8\ud55c \uc608\uc2dc\uc785\ub2c8\ub2e4:\n\n```python\n>>> from transformers import pipeline\n\n# Allocate a pipeline for sentiment-analysis\n>>> classifier = pipeline('sentiment-analysis')\n>>> classifier('We are very happy to introduce pipeline to the transformers repository.')\n[{'label': 'POSITIVE', 'score': 0.9996980428695679}]\n```", "By default, this pipeline selects a particular pretrained model that has been fine-tuned for sentiment analysis in English. The model is downloaded and cached when you create the `classifier` object. If you rerun the command, the cached model will be used instead and there is no need to download the model again.\n\nThere are three main steps involved when you pass some text to a pipeline:\n\n1. The text is preprocessed into a format the model can understand.\n2. The preprocessed inputs are passed to the model.\n3. The predictions of the model are post-processed, so you can make sense of them.\n\n\nSome of the currently [available pipelines](https://huggingface.co/transformers/main_classes/pipelines.html) are:\n\n- `feature-extraction` (get the vector representation of a text)\n- `fill-mask`\n- `ner` (named entity recognition)\n- `question-answering`\n- `sentiment-analysis`\n- `summarization`\n- `text-generation`\n- `translation`\n- `zero-shot-classification`\n\nLet's have a look at a few of these!\n\n## Zero-shot classification[[zero-shot-classification]]\n\nWe'll start by tackling a more challenging task where we need to classify texts that haven't been labelled. This is a common scenario in real-world projects because annotating text is usually time-consuming and requires domain expertise. For this use case, the `zero-shot-classification` pipeline is very powerful: it allows you to specify which labels to use for the classification, so you don't have to rely on the labels of the pretrained model. You've already seen how the model can classify a sentence as positive or negative using those two labels \u2014 but it can also classify the text using any other set of labels you like.\n\n```python\nfrom transformers import pipeline\n\nclassifier = pipeline(\"zero-shot-classification\")\nclassifier(\n    \"This is a course about the Transformers library\",\n    candidate_labels=[\"education\", \"politics\", \"business\"],\n)\n```", "On the c6i (Ice Lake) instance, we only use a vanilla Transformers pipeline. \n\n```\nfrom transformers import pipeline\n\nfor model in models:\n    print(f\"Benchmarking {model}\")\n    pipe = pipeline(\"sentiment-analysis\", model=model)\n    result = benchmark(pipe, sentence_short)\n    print(f\"Transformers pipeline, short sentence: {result}\")\n    result = benchmark(pipe, sentence_long)\n    print(f\"Transformers pipeline, long sentence: {result}\")\n    result = benchmark(pipe, sentence_short_array)\n    print(f\"Transformers pipeline, short sentence array: {result}\")\n    result = benchmark(pipe, sentence_long_array)\n    print(f\"Transformers pipeline, long sentence array: {result}\")\n```\n\nOn the r7iz (Sapphire Rapids) instance, we use both a vanilla pipeline and an Optimum pipeline. In the Optimum pipeline, we enable `bfloat16` mode to leverage the AMX instructions. We also set `jit` to `True` to further optimize the model with just-in-time compilation. \n\n\n```\n   import torch\n\tfrom optimum.intel import inference_mode\n\t\n\twith inference_mode(pipe, dtype=torch.bfloat16, jit=True) as opt_pipe:\n\t    result = benchmark(opt_pipe, sentence_short)\n\t    print(f\"Optimum pipeline, short sentence: {result}\")\n\t    result = benchmark(opt_pipe, sentence_long)\n\t    print(f\"Optimum pipeline, long sentence: {result}\")\n\t    result = benchmark(opt_pipe, sentence_short_array)\n\t    print(f\"Optimum pipeline, short sentence array: {result}\")\n\t    result = benchmark(opt_pipe, sentence_long_array)\n\t    print(f\"Optimum pipeline, long sentence array: {result}\")\n```\n\nFor the sake of brevity, we'll just look at the p99 results for [distilbert-base-uncased](https://huggingface.co/distilbert-base-uncased). All times are in milliseconds. You'll find full results at the end of the post.", "Let's quickly go over each of these.\n\n## Preprocessing with a tokenizer[[preprocessing-with-a-tokenizer]]\n\nLike other neural networks, Transformer models can't process raw text directly, so the first step of our pipeline is to convert the text inputs into numbers that the model can make sense of. To do this we use a *tokenizer*, which will be responsible for:\n\n- Splitting the input into words, subwords, or symbols (like punctuation) that are called *tokens*\n- Mapping each token to an integer\n- Adding additional inputs that may be useful to the model\n\nAll this preprocessing needs to be done in exactly the same way as when the model was pretrained, so we first need to download that information from the [Model Hub](https://huggingface.co/models). To do this, we use the `AutoTokenizer` class and its `from_pretrained()` method. Using the checkpoint name of our model, it will automatically fetch the data associated with the model's tokenizer and cache it (so it's only downloaded the first time you run the code below).\n\nSince the default checkpoint of the `sentiment-analysis` pipeline is `distilbert-base-uncased-finetuned-sst-2-english` (you can see its model card [here](https://huggingface.co/distilbert-base-uncased-finetuned-sst-2-english)), we run the following:\n\n```python\nfrom transformers import AutoTokenizer\n\ncheckpoint = \"distilbert-base-uncased-finetuned-sst-2-english\"\ntokenizer = AutoTokenizer.from_pretrained(checkpoint)\n```\n\nOnce we have the tokenizer, we can directly pass our sentences to it and we'll get back a dictionary that's ready to feed to our model! The only thing left to do is to convert the list of input IDs to tensors."], "eval_score_GPT4": "5", "eval_feedback_GPT4": "The response provided is completely correct, accurate, and factual. It matches the reference answer and includes the correct default checkpoint for the sentiment analysis pipeline in the Transformers library. Additionally, the response provides a source for the information, which supports the accuracy of the statement."}, {"question": "What is the purpose of the notebook \"How to use DeepSpeed to train models with billions of parameters on Habana Gaudi\"?\n", "true_answer": "To show how to use DeepSpeed to pre-train/fine-tune the 1.6B-parameter GPT2-XL for causal language modeling on Habana Gaudi.", "source_doc": "huggingface/optimum/blob/main/docs/source/notebooks.md", "generated_answer": "The purpose of the notebook \"How to use DeepSpeed to train models with billions of parameters on Habana Gaudi\" is to demonstrate how to use the DeepSpeed framework, which is optimized for training large-scale machine learning models, in conjunction with Habana Gaudi, a second-generation AI hardware accelerator, to efficiently train models with billions of parameters for causal language modeling. The notebook provides step-by-step instructions on how to use DeepSpeed", "retrieved_docs": ["| [How to use DeepSpeed to train models with billions of parameters on Habana Gaudi](https://github.com/huggingface/optimum-habana/blob/main/notebooks/AI_HW_Summit_2022.ipynb) | Show how to use DeepSpeed to pre-train/fine-tune the 1.6B-parameter GPT2-XL for causal language modeling on Habana Gaudi. |  [![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/huggingface/optimum-habana/blob/main/notebooks/AI_HW_Summit_2022.ipynb) | [![Open in AWS Studio](https://studiolab.sagemaker.aws/studiolab.svg)](https://studiolab.sagemaker.aws/import/github/huggingface/optimum-habana/blob/main/notebooks/AI_HW_Summit_2022.ipynb) |", "| [How to use DeepSpeed to train models with billions of parameters on Habana Gaudi](https://github.com/huggingface/optimum-habana/blob/main/notebooks/AI_HW_Summit_2022.ipynb) | Show how to use DeepSpeed to pre-train/fine-tune the 1.6B-parameter GPT2-XL for causal language modeling on Habana Gaudi. |  [![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/huggingface/optimum-habana/blob/main/notebooks/AI_HW_Summit_2022.ipynb) | [![Open in AWS Studio](https://studiolab.sagemaker.aws/studiolab.svg)](https://studiolab.sagemaker.aws/import/github/huggingface/optimum-habana/blob/main/notebooks/AI_HW_Summit_2022.ipynb) |", "To go further, we are looking forward to using HPU graphs for training models even faster and to presenting how to use DeepSpeed ZeRO-3 on Gaudi2 to accelerate the training of your LLMs. Stay tuned!\n\nIf you are interested in accelerating your Machine Learning training and inference workflows using the latest AI hardware accelerators and software libraries, check out our [Expert Acceleration Program](https://huggingface.co/support). To learn more about Habana solutions, [read about our partnership and contact them here](https://huggingface.co/hardware/habana). To learn more about Hugging Face efforts to make AI hardware accelerators easy to use, check out our [Hardware Partner Program](https://huggingface.co/hardware).\n\n\n### Related Topics\n\n- [Faster Training and Inference: Habana Gaudi-2 vs Nvidia A100 80GB](https://huggingface.co/blog/habana-gaudi-2-benchmark)\n- [Fast Inference on Large Language Models: BLOOMZ on Habana Gaudi2 Accelerator](https://huggingface.co/blog/habana-gaudi-2-bloom)", "--\ntitle: \"Faster Training and Inference: Habana Gaudi\u00ae2 vs Nvidia A100 80GB\"\nthumbnail: /blog/assets/habana-gaudi-2-benchmark/thumbnail.png\nauthors:\n- user: regisss\n---\n\n# Faster Training and Inference: Habana Gaudi\u00ae-2 vs Nvidia A100 80GB\n\n\nIn this article, you will learn how to use [Habana\u00ae Gaudi\u00ae2](https://habana.ai/training/gaudi2/) to accelerate model training and inference, and train bigger models with \ud83e\udd17 [Optimum Habana](https://huggingface.co/docs/optimum/habana/index). Then, we present several benchmarks including BERT pre-training, Stable Diffusion inference and T5-3B fine-tuning, to assess the performance differences between first generation Gaudi, Gaudi2 and Nvidia A100 80GB. Spoiler alert - Gaudi2 is about twice faster than Nvidia A100 80GB for both training and inference!\n\n[Gaudi2](https://habana.ai/training/gaudi2/) is the second generation AI hardware accelerator designed by Habana Labs. A single server contains 8 accelerator devices with 96GB of memory each (versus 32GB on first generation Gaudi and 80GB on A100 80GB). The Habana SDK, [SynapseAI](https://developer.habana.ai/), is common to both first-gen Gaudi and Gaudi2.\nThat means that \ud83e\udd17 Optimum Habana, which offers a very user-friendly interface between the \ud83e\udd17 Transformers and \ud83e\udd17 Diffusers libraries and SynapseAI, **works the exact same way on Gaudi2 as on first-gen Gaudi!**\nSo if you already have ready-to-use training or inference workflows for first-gen Gaudi, we encourage you to try them on Gaudi2, as they will work without any single change.\n\n\n## How to Get Access to Gaudi2?\n\nOne of the easy, cost-efficient ways that Intel and Habana have made Gaudi2 available is on the Intel Developer Cloud. To start using Gaudi2 there, you should follow the following steps:", "- Habana's Gaudi processors\n- ONNX Runtime (optimized for GPUs)\n\n### Habana\n\nBefore you begin, make sure you have all the necessary libraries installed :\n\n```bash\npip install --upgrade-strategy eager optimum[habana]\n```\n\n```diff\n- from transformers import Trainer, TrainingArguments\n+ from optimum.habana import GaudiTrainer, GaudiTrainingArguments\n\n  # Download a pretrained model from the Hub\n  model = AutoModelForXxx.from_pretrained(\"bert-base-uncased\")\n\n  # Define the training arguments\n- training_args = TrainingArguments(\n+ training_args = GaudiTrainingArguments(\n      output_dir=\"path/to/save/folder/\",\n+     use_habana=True,\n+     use_lazy_mode=True,\n+     gaudi_config_name=\"Habana/bert-base-uncased\",\n      ...\n  )\n\n  # Initialize the trainer\n- trainer = Trainer(\n+ trainer = GaudiTrainer(\n      model=model,\n      args=training_args,\n      train_dataset=train_dataset,\n      ...\n  )\n\n  # Use Habana Gaudi processor for training!\n  trainer.train()\n```\n\nYou can find more examples in the [documentation](https://huggingface.co/docs/optimum/habana/quickstart) and in the [examples](https://github.com/huggingface/optimum-habana/tree/main/examples).\n\n### ONNX Runtime\n\n```diff\n- from transformers import Trainer, TrainingArguments\n+ from optimum.onnxruntime import ORTTrainer, ORTTrainingArguments\n\n  # Download a pretrained model from the Hub\n  model = AutoModelForSequenceClassification.from_pretrained(\"bert-base-uncased\")", "*BS* is the batch size per device. Gaudi2 and A100 runs were performed in fp32 with gradient checkpointing enabled. All runs were *distributed* runs on *8 devices*.\n\n\n## Conclusion\n\nIn this article, we discuss our first experience with Gaudi2. The transition from first generation Gaudi to Gaudi2 is completely seamless since SynapseAI, Habana's SDK, is fully compatible with both. This means that new optimizations proposed by future releases will benefit both of them.\n\nYou have seen that Habana Gaudi2 significantly improves performance over first generation Gaudi and delivers about twice the throughput speed as Nvidia A100 80GB for both training and inference.\n\nYou also know now how to setup a Gaudi2 instance through the Intel Developer Zone. Check out the [examples](https://github.com/huggingface/optimum-habana/tree/main/examples) you can easily run on it with \ud83e\udd17 Optimum Habana.\n\nIf you are interested in accelerating your Machine Learning training and inference workflows using the latest AI hardware accelerators and software libraries, check out our [Expert Acceleration Program](https://huggingface.co/support). To learn more about Habana solutions, [read about our partnership here](https://huggingface.co/hardware/habana) and [contact them](https://habana.ai/contact-us/). To learn more about Hugging Face efforts to make AI hardware accelerators easy to use, check out our [Hardware Partner Program](https://huggingface.co/hardware).\n\n\n### Related Topics\n\n- [Getting Started on Transformers with Habana Gaudi](https://huggingface.co/blog/getting-started-habana)\n- [Accelerate Transformer Model Training with Hugging Face and Habana Labs](https://developer.habana.ai/events/accelerate-transformer-model-training-with-hugging-face-and-habana-labs/)\n\n---", "Moreover, support for [HPU graphs](https://docs.habana.ai/en/latest/PyTorch/Inference_on_PyTorch/Inference_Using_HPU_Graphs.html) and [DeepSpeed-inference](https://docs.habana.ai/en/latest/PyTorch/DeepSpeed/Inference_Using_DeepSpeed.html) have just recently been introduced in SynapseAI, and these are well-suited for latency-sensitive applications as shown in our benchmark below.\n\nAll these features are integrated into the \ud83e\udd17 [Optimum Habana](https://github.com/huggingface/optimum-habana) library so that deploying your model on Gaudi is very simple. Check out the quick-start page [here](https://huggingface.co/docs/optimum/habana/quickstart).\n\nIf you would like to get access to Gaudi2, go to the [Intel Developer Cloud](https://www.intel.com/content/www/us/en/secure/developer/devcloud/cloud-launchpad.html) and follow [this guide](https://huggingface.co/blog/habana-gaudi-2-benchmark#how-to-get-access-to-gaudi2).\n\n\n## Benchmarks"], "eval_score_GPT4": "4", "eval_feedback_GPT4": "Feedback: The response correctly identifies the use of the DeepSpeed framework in conjunction with Habana Gaudi hardware for training large-scale models. It also accurately mentions the purpose of the notebook, which is to provide instructions for training models with billions of parameters. However, the response does not specify the model type (GPT2-XL) or the exact number of parameters (1.6B), nor does it mention the specific task of pre-training/fine-tuning for causal language modeling, which are details included in the reference answer. Therefore, while the response is on the right track, it lacks some of the precision and specificity found in the reference answer."}, {"question": "What command line module does PyTorch provide to run a script on multiple GPUs?\n", "true_answer": "torchrun", "source_doc": "huggingface/blog/blob/main/pytorch-ddp-accelerate-transformers.md", "generated_answer": "The command line module provided by PyTorch to run a script on multiple GPUs is `torchrun`. It can be used with the `--nproc_per_node` option to select the number of GPUs to use, and the `--nnodes` option to specify the number of nodes to use in a distributed setup. For example, `torchrun --nproc_per_node=2 --nnodes=1 example_script.py` will run the script on", "retrieved_docs": ["With this your PyTorch training loop is now setup to be ran on any distributed setup thanks to the `Accelerator` object. This code can then still be launched through the `torchrun` CLI or through Accelerate's own CLI interface, [`accelerate launch`](https://huggingface.co/docs/accelerate/v0.12.0/en/basic_tutorials/launch).\n\nAs a result its now trivialized to perform distributed training with Accelerate and keeping as much of the barebones PyTorch code the same as possible.\n\nEarlier it was mentioned that Accelerate also makes the DataLoaders more efficient. This is through custom Samplers that can send parts of the batches automatically to different devices during training allowing for a single copy of the data to be known at one time, rather than four at once into memory depending on the configuration. Along with this, there is only a single full copy of the original dataset in memory total. Subsets of this dataset are split between all of the nodes that are utilized for training, allowing for much larger datasets to be trained on a single instance without an explosion in memory utilized.\n\n### Using the `notebook_launcher`\n\nEarlier it was mentioned you can start distributed code directly out of your Jupyter Notebook. This comes from Accelerate's [`notebook_launcher`](https://huggingface.co/docs/accelerate/v0.12.0/en/basic_tutorials/notebook) utility, which allows for starting multi-gpu training based on code inside of a Jupyter Notebook.\n\nTo use it is as trivial as importing the launcher:\n\n```python\nfrom accelerate import notebook_launcher\n```\n\nAnd passing the training function we declared earlier, any arguments to be passed, and the number of processes to use (such as 8 on a TPU, or 2 for two GPUs). Both of the above training functions can be ran, but do note that after you start a single launch, the instance needs to be restarted before spawning another\n\n```python\nnotebook_launcher(train_ddp, args=(), num_processes=2)\n```\n\nOr:", "## PyTorch Distributed Data Parallelism\n\nAs the name implies, `torch.distributed` is meant to work on *distributed* setups. This can include multi-node, where you have a number of machines each with a single GPU, or multi-gpu where a single system has multiple GPUs, or some combination of both.\n\nTo convert our above code to work within a distributed setup, a few setup configurations must first be defined, detailed in the [Getting Started with DDP Tutorial](https://pytorch.org/tutorials/intermediate/ddp_tutorial.html)\n\nFirst a `setup` and a `cleanup` function must be declared. This will open up a processing group that all of the compute processes can communicate through\n\n> Note: for this section of the tutorial it should be assumed these are sent in python script files. Later on a launcher using Accelerate will be discussed that removes this necessity\n\n```python\nimport os\nimport torch.distributed as dist\n\ndef setup(rank, world_size):\n    \"Sets up the process group and configuration for PyTorch Distributed Data Parallelism\"\n    os.environ[\"MASTER_ADDR\"] = 'localhost'\n    os.environ[\"MASTER_PORT\"] = \"12355\"\n\n    # Initialize the process group\n    dist.init_process_group(\"gloo\", rank=rank, world_size=world_size)\n\ndef cleanup():\n    \"Cleans up the distributed environment\"\n    dist.destroy_process_group()\n```\n\nThe last piece of the puzzle is *how do I send my data and model to another GPU?*\n\nThis is where the `DistributedDataParallel` module comes into play. It will copy your model onto each GPU, and when `loss.backward()` is called the backpropagation is performed and the resulting gradients across all these copies of the model will be averaged/reduced. This ensures each device has the same weights post the optimizer step.\n\nBelow is an example of our training setup, refactored as a function, with this capability:", "train_loader = torch.utils.data.DataLoader(train_dset, shuffle=True, batch_size=64)\ntest_loader = torch.utils.data.DataLoader(test_dset, shuffle=False, batch_size=64)\n```\n\nMove the model to the CUDA device:\n\n```python\nmodel = BasicNet().to(device)\n```\n\nBuild a PyTorch optimizer:\n\n```python\noptimizer = optim.AdamW(model.parameters(), lr=1e-3)\n```\n\nBefore finally creating a simplistic training and evaluation loop that performs one full iteration over the dataset and calculates the test accuracy:\n\n```python\nmodel.train()\nfor batch_idx, (data, target) in enumerate(train_loader):\n    data, target = data.to(device), target.to(device)\n    output = model(data)\n    loss = F.nll_loss(output, target)\n    loss.backward()\n    optimizer.step()\n    optimizer.zero_grad()\n\nmodel.eval()\ncorrect = 0\nwith torch.no_grad():\n    for data, target in test_loader:\n        output = model(data)\n        pred = output.argmax(dim=1, keepdim=True)\n        correct += pred.eq(target.view_as(pred)).sum().item()\nprint(f'Accuracy: {100. * correct / len(test_loader.dataset)}')\n```\n\nTypically from here, one could either throw all of this into a python script or run it on a Jupyter Notebook.\n\nHowever, how would you then get this script to run on say two GPUs or on multiple machines if these resources are available, which could improve training speed through *distributed* training? Just doing `python myscript.py` will only ever run the script using a single GPU. This is where `torch.distributed` comes into play\n\n## PyTorch Distributed Data Parallelism", "Below is an example of our training setup, refactored as a function, with this capability:\n\n> Note: Here rank is the overall rank of the current GPU compared to all the other GPUs available, meaning they have a rank of `0 -> n-1`\n\n```python\nfrom torch.nn.parallel import DistributedDataParallel as DDP\n\ndef train(model, rank, world_size):\n    setup(rank, world_size)\n    model = model.to(rank)\n    ddp_model = DDP(model, device_ids=[rank])\n    optimizer = optim.AdamW(ddp_model.parameters(), lr=1e-3)\n    # Train for one epoch\n    model.train()\n    for batch_idx, (data, target) in enumerate(train_loader):\n        data, target = data.to(device), target.to(device)\n        output = model(data)\n        loss = F.nll_loss(output, target)\n        loss.backward()\n        optimizer.step()\n        optimizer.zero_grad()\n    cleanup()\n```\n\nThe optimizer needs to be declared based on the model *on the specific device* (so `ddp_model` and not `model`) for all of the gradients to properly be calculated.\n\nLastly, to run the script PyTorch has a convenient `torchrun` command line module that can help. Just pass in the number of nodes it should use as well as the script to run and you are set:\n\n```bash\ntorchrun --nproc_per_node=2 --nnodes=1 example_script.py\n```\n\nThe above will run the training script on two GPUs that live on a single machine and this is the barebones for performing only distributed training with PyTorch.\n\nNow let's talk about Accelerate, a library aimed to make this process more seameless and also help with a few best practices\n\n## \ud83e\udd17 Accelerate", "optimizer.zero_grad()\n\n          output = model(source)\n          loss = F.cross_entropy(output, targets)\n\n-         loss.backward()\n+         accelerator.backward(loss)\n\n          optimizer.step()\n```\n\nBy just adding five lines of code to any standard PyTorch training script, you can now run said script on any kind of distributed setting, as well as with or without mixed precision. \ud83e\udd17 Accelerate even handles the device placement for you, so you can simplify the training loop above even further:\n\n```diff\n  import torch\n  import torch.nn.functional as F\n  from datasets import load_dataset\n+ from accelerate import Accelerator\n\n+ accelerator = Accelerator()\n- device = 'cpu'\n\n- model = torch.nn.Transformer().to(device)\n+ model = torch.nn.Transformer()\n  optim = torch.optim.Adam(model.parameters())\n\n  dataset = load_dataset('my_dataset')\n  data = torch.utils.data.DataLoader(dataset, shuffle=True)\n\n+ model, optim, data = accelerator.prepare(model, optim, data)\n\n  model.train()\n  for epoch in range(10):\n      for source, targets in data:\n-         source = source.to(device)\n-         targets = targets.to(device)\n\n          optimizer.zero_grad()\n\n          output = model(source)\n          loss = F.cross_entropy(output, targets)\n\n-         loss.backward()\n+         accelerator.backward(loss)\n\n          optimizer.step()\n```\n\nIn contrast, here are the changes needed to have this code run with distributed training are the followings:\n\n```diff\n+ import os\n  import torch\n  import torch.nn.functional as F\n  from datasets import load_dataset\n+ from torch.utils.data import DistributedSampler\n+ from torch.nn.parallel import DistributedDataParallel", "export GPUS_PER_NODE=8\nexport MASTER_ADDR=$(scontrol show hostnames $SLURM_JOB_NODELIST | head -n 1)\nexport MASTER_PORT=9901\n\nsrun --jobid $SLURM_JOBID bash -c 'python -m torch.distributed.run \\\n --nproc_per_node $GPUS_PER_NODE --nnodes $SLURM_NNODES --node_rank $SLURM_PROCID \\\n --master_addr $MASTER_ADDR --master_port $MASTER_PORT \\\nyour_program.py <normal cl args> --deepspeed ds_config.json'\n```\n\nAll is left is to schedule it to run:\n```bash\nsbatch launch.slurm\n```\n\n`srun` will take care of launching the program simultaneously on all nodes.\n\n\n#### Use of Non-shared filesystem\n\nBy default DeepSpeed expects that a multi-node environment uses a shared storage. If this is not the case and each node can only see the local filesystem, you need to adjust the config file to include a  [`checkpoint`_section](https://www.deepspeed.ai/docs/config-json/#checkpoint-options) with the following setting:\n\n```json\n{\n  \"checkpoint\": {\n    \"use_node_local_storage\": true\n  }\n}\n```\n\nAlternatively, you can also use the [`Trainer`]'s `--save_on_each_node` argument, and the above config will be added automatically for you.\n\n\n<a id='deepspeed-notebook'></a>\n\n### Deployment in Notebooks\n\nThe problem with running notebook cells as a script is that there is no normal `deepspeed` launcher to rely on, so\nunder certain setups we have to emulate it.\n\nIf you're using only 1 GPU, here is how you'd have to adjust your training code in the notebook to use DeepSpeed.", "### Number of GPUs\n\nFor example, if you have 4 GPUs and you only want to use the first 2:\n\n<hfoptions id=\"select-gpu\">\n<hfoption id=\"torchrun\">\n\nUse the `--nproc_per_node` to select how many GPUs to use.\n\n```bash\ntorchrun --nproc_per_node=2  trainer-program.py ...\n```\n\n</hfoption>\n<hfoption id=\"Accelerate\">\n\nUse `--num_processes` to select how many GPUs to use.\n\n```bash\naccelerate launch --num_processes 2 trainer-program.py ...\n```\n\n</hfoption>\n<hfoption id=\"DeepSpeed\">\n\nUse `--num_gpus` to select how many GPUs to use.\n\n```bash\ndeepspeed --num_gpus 2 trainer-program.py ...\n```\n\n</hfoption>\n</hfoptions>\n\n### Order of GPUs\n\nNow, to select which GPUs to use and their order, you'll use the `CUDA_VISIBLE_DEVICES` environment variable. It is easiest to set the environment variable in a `~/bashrc` or another startup config file. `CUDA_VISIBLE_DEVICES` is used to map which GPUs are used. For example, if you have 4 GPUs (0, 1, 2, 3) and you only want to run GPUs 0 and 2:\n\n```bash\nCUDA_VISIBLE_DEVICES=0,2 torchrun trainer-program.py ...\n```"], "eval_score_GPT4": "5", "eval_feedback_GPT4": "The response correctly identifies `torchrun` as the command line module provided by PyTorch to run a script on multiple GPUs. It also accurately describes the use of `--nproc_per_node` to select the number of GPUs and `--nnodes` for the number of nodes in a distributed setup, providing an example command that demonstrates how to use these options. This information is factual and directly related to the instruction given."}, {"question": "What is the most popular vision transformer model on the Hugging Face Model Hub for image classification?\n", "true_answer": "google/vit-base-patch16-224", "source_doc": "gradio-app/gradio/blob/main/guides/06_integrating-other-frameworks/image-classification-with-vision-transformers.md", "generated_answer": "The most popular vision transformer model on the Hugging Face Model Hub for image classification at the time of writing is \"google/vit-base-patch16-224\". This model has been trained on ImageNet images at a resolution of 224x224 pixels and is designed for the task of image classification. It can be easily loaded and used with the Gradio library for building web demos, making it a convenient choice for developers looking to quickly prototype", "retrieved_docs": ["elcome to the Hugging Face Course! This course has been designed to teach you all about the Hugging Face ecosystem: how to use the dataset and model hub as well as all our open source libraries. Here is the Table of Contents. As you can see, it's divided in three sections which become progressively more advanced. At this stage, the first two sections have been released. The first will teach you the basics of how to use a Transformer model, fine-tune it on your own dataset and share the result with the community. The second will dive deeper into our libraries and teach you how to tackle any NLP task. We are actively working on the last one and hope to have it ready for you for the spring of 2022. The first chapter requires no technical knowledge and is a good introduction to learn what Transformers models can do and how they could be of use to you or your company. The next chapters require a good knowledge of Python and some basic knowledge of Machine Learning and Deep Learning. If you don't know what a training and validation set is or what gradient descent means, you should look at an introductory course such as the ones published by deeplearning.ai or fast.ai. It's also best if you have some basics in one Deep Learning Framework (PyTorch or TensorFlow). Each part of the material introduced in this course has a version in both those frameworks, so you will be able to pick the one you are most comfortable with. This is the team that developed this course. I'll now let each of the speakers introduce themselves briefly.", "Using \ud83e\udd17 `transformers` at Hugging Face\n\n\ud83e\udd17 `transformers` is a library maintained by Hugging Face and the community, for state-of-the-art Machine Learning for Pytorch, TensorFlow and JAX. It provides thousands of pretrained models to perform tasks on different modalities such as text, vision, and audio. We are a bit biased, but we really like \ud83e\udd17 `transformers`!\n\n## Exploring \ud83e\udd17 transformers in the Hub\n\nThere are over 25,000 `transformers` models in the Hub which you can find by filtering at the left of [the models page](https://huggingface.co/models?library=transformers&sort=downloads). \n\nYou can find models for many different tasks:", "Image Classification with Vision Transformers\n\nRelated spaces: https://huggingface.co/spaces/abidlabs/vision-transformer\nTags: VISION, TRANSFORMERS, HUB\n\n## Introduction\n\nImage classification is a central task in computer vision. Building better classifiers to classify what object is present in a picture is an active area of research, as it has applications stretching from facial recognition to manufacturing quality control.\n\nState-of-the-art image classifiers are based on the _transformers_ architectures, originally popularized for NLP tasks. Such architectures are typically called vision transformers (ViT). Such models are perfect to use with Gradio's _image_ input component, so in this tutorial we will build a web demo to classify images using Gradio. We will be able to build the whole web application in a **single line of Python**, and it will look like the demo on the bottom of the page.\n\nLet's get started!\n\n### Prerequisites\n\nMake sure you have the `gradio` Python package already [installed](/getting_started).\n\n## Step 1 \u2014 Choosing a Vision Image Classification Model\n\nFirst, we will need an image classification model. For this tutorial, we will use a model from the [Hugging Face Model Hub](https://huggingface.co/models?pipeline_tag=image-classification). The Hub contains thousands of models covering dozens of different machine learning tasks.\n\nExpand the Tasks category on the left sidebar and select \"Image Classification\" as our task of interest. You will then see all of the models on the Hub that are designed to classify images.\n\nAt the time of writing, the most popular one is `google/vit-base-patch16-224`, which has been trained on ImageNet images at a resolution of 224x224 pixels. We will use this model for our demo.\n\n## Step 2 \u2014 Loading the Vision Transformer Model with Gradio", "It's a Good Thing in so many ways. State of the art is constantly advancing, and hardly anyone can keep up with its relentless pace. Remember that Google Vision Transformer model I mentioned earlier? Would you like to test it here and now? With Hugging Face, it's [the simplest thing](https://huggingface.co/google/vit-base-patch16-224).\n\n<kbd>\n  <img src=\"assets/31_age_of_ml_as_code/05_vision_transformer.png\">\n</kbd>\n\nHow about the latest [zero-shot text generation models](https://huggingface.co/bigscience) from the [Big Science project](https://bigscience.huggingface.co/)?\n\n<kbd>\n  <img src=\"assets/31_age_of_ml_as_code/06_big_science.png\">\n</kbd>\n\nYou can do the same with another [16,000+ models](https://huggingface.co/models) and [1,600+ datasets](https://huggingface.co/datasets), with additional tools for [inference](https://huggingface.co/inference-api), [AutoNLP](https://huggingface.co/autonlp), [latency optimization](https://huggingface.co/infinity), and [hardware acceleration](https://huggingface.co/hardware). We can also help you get your project off the ground, [from modeling to production](https://huggingface.co/support).\n\nOur mission at Hugging Face is to make Machine Learning as friendly and as productive as possible, for beginners and experts alike. \n\nWe believe in writing as little code as possible to train, optimize, and deploy models. \n\nWe believe in built-in best practices. \n\nWe believe in making infrastructure as transparent as possible. \n\nWe believe that nothing beats high quality models in production, fast.", "</div>\n\n## What's the Hugging Face Hub?\n\nWe are helping the community work together towards the goal of advancing Machine Learning \ud83d\udd25.\n\nThe Hugging Face Hub is a platform with over 120k models, 20k datasets, and 50k demos in which people can easily collaborate in their ML workflows. The Hub works as a central place where anyone can share, explore, discover, and experiment with open-source Machine Learning.\n\nNo single company, including the Tech Titans, will be able to \u201csolve AI\u201d by themselves \u2013 the only way we'll achieve this is by sharing knowledge and resources in a community-centric approach. We are building the largest open-source collection of models, datasets, demos and metrics on the Hugging Face Hub to democratize and advance ML for everyone \ud83d\ude80.\n\nWe encourage you to read the [Code of Conduct](https://huggingface.co/code-of-conduct) and the [Content Guidelines](https://huggingface.co/content-guidelines) to familiarize yourself with the values that we expect our community members to uphold \ud83e\udd17.\n\n## What can you find on the Hub?\n\nThe Hugging Face Hub hosts Git-based repositories, which are version-controlled buckets that can contain all your files. \ud83d\udcbe\n\nOn it, you'll be able to upload and discover...\n\n- Models, _hosting the latest state-of-the-art models for NLP, vision, and audio tasks_\n- Datasets, _featuring a wide variety of data for different domains and modalities_..\n- Spaces, _interactive apps for demonstrating ML models directly in your browser_.\n\nThe Hub offers **versioning, commit history, diffs, branches, and over a dozen library integrations**! You can learn more about the features that all repositories share in the [**Repositories documentation**](./repositories).\n\n## Models", "- Image classification\n- Image segmentation\n- (Zero-shot) object detection\n- Video classification\n- Depth estimation\n- Image-to-image synthesis\n- Unconditional image generation\n- Zero-shot image classification\n\nEach of these tasks comes with at least 10 model checkpoints on the Hub for you to explore. Furthermore, we support [tasks](https://huggingface.co/tasks) that lie at the intersection of vision and language such as:\n\n- Image-to-text (image captioning, OCR)\n- Text-to-image\n- Document question-answering\n- Visual question-answering\n\nThese tasks entail not only state-of-the-art Transformer-based architectures such as [ViT](https://huggingface.co/docs/transformers/model_doc/vit), [Swin](https://huggingface.co/docs/transformers/model_doc/swin), [DETR](https://huggingface.co/docs/transformers/model_doc/detr) but also *pure convolutional* architectures like [ConvNeXt](https://huggingface.co/docs/transformers/model_doc/convnext), [ResNet](https://huggingface.co/docs/transformers/model_doc/resnet), [RegNet](https://huggingface.co/docs/transformers/model_doc/regnet), and more! Architectures like ResNets are still very much relevant for a myriad of industrial use cases and hence the support of these non-Transformer architectures in \ud83e\udd17 Transformers.\n\nIt\u2019s also important to note that the models on the Hub are not just from the Transformers library but also from other third-party libraries. For example, even though we support tasks like unconditional image generation on the Hub, we don\u2019t have any models supporting that task in Transformers yet (such as [this](https://huggingface.co/ceyda/butterfly_cropped_uniq1K_512)). Supporting all ML tasks, whether they are solved with Transformers or a third-party library is a part of our mission to foster a collaborative open-source Machine Learning ecosystem.", "Now that we have covered the basics, let's dive into the specific characteristics of models, datasets and spaces hosted on the Hugging Face Hub.\n\n### Models\n\n[Transfer learning](https://www.youtube.com/watch?v=BqqfQnyjmgg&ab_channel=HuggingFace) has changed the way companies approach machine learning problems. Traditionally, companies needed to train models from scratch, which requires a lot of time, data, and resources. Now machine learning teams can use a pre-trained model and [fine-tune it for their own use case](https://huggingface.co/course/chapter3/1?fw=pt) in a fast and cost-effective way. This dramatically accelerates the process of getting accurate and performant models.\n\nOn the Hub, you can find 60,000+ state-of-the-art open source pre-trained models for NLP, computer vision, speech, time-series, biology, reinforcement learning, chemistry and more. You can use the search bar or filter by tasks, libraries, licenses and other tags to find the right model for your particular use case:\n\n<figure class=\"image table text-center m-0 w-full\">\n  <medium-zoom background=\"rgba(0,0,0,.7)\" alt=\"60,000+ models available on the Hub\" src=\"assets/92_introducing_private_hub/models.png\"></medium-zoom>\n  <figcaption>60,000+ models available on the Hub</figcaption>\n</figure>\n\nThese models span 180 languages and support up to 25 ML libraries (including Transformers, Keras, spaCy, Timm and others), so there is a lot of flexibility in terms of the type of models, languages and libraries."], "eval_score_GPT4": "5", "eval_feedback_GPT4": "The response correctly identifies \"google/vit-base-patch16-224\" as the most popular vision transformer model on the Hugging Face Model Hub for image classification, which matches the reference answer. The additional information provided about the model being trained on ImageNet images and its compatibility with the Gradio library is accurate and relevant, although not required by the instruction. Based on the score rubric, the response is completely correct, accurate, and factual."}, {"question": "What is the command to upload an ESPnet model to a Hugging Face repository?\n", "true_answer": "./run.sh --stage 15 --skip_upload_hf false --hf_repo username/model_repo", "source_doc": "huggingface/hub-docs/blob/main/docs/hub/espnet.md", "generated_answer": "The command to upload an ESPnet model to a Hugging Face repository using the `run.sh` script is:\n\n```bash\n./run.sh --stage 15 --skip_upload_hf false --hf_repo username/model_repo\n```\n\nReplace `username` with your Hugging Face username and `model_repo` with the name of your repository on Hugging Face. This command will upload the model to the specified repository on Hug", "retrieved_docs": ["Uploading models\n\nTo upload models to the Hub, you'll need to create an account at [Hugging Face](https://huggingface.co/join). Models on the Hub are [Git-based repositories](./repositories), which give you versioning, branches, discoverability and sharing features, integration with over a dozen libraries, and more! You have control over what you want to upload to your repository, which could include checkpoints, configs, and any other files.\n\nYou can link repositories with an individual, such as [osanseviero/fashion_brands_patterns](https://huggingface.co/osanseviero/fashion_brands_patterns), or with an organization, such as [facebook/bart-large-xsum](https://huggingface.co/facebook/bart-large-xsum). Organizations can collect models related to a company, community, or library! If you choose an organization, the model will be featured on the organization\u2019s page, and every member of the organization will have the ability to contribute to the repository. You can create a new organization [here](https://huggingface.co/organizations/new).\n\nThere are several ways to upload models to the Hub, described below. We suggest adding a [Model Card](./model-cards) to your repo to document your model.\n\n## Using the web interface\n\nTo create a brand new model repository, visit [huggingface.co/new](http://huggingface.co/new). Then follow these steps:\n\n1. In the \"Files and versions\" tab, select \"Add File\" and specify \"Upload File\":", "Using ESPnet at Hugging Face\n\n`espnet` is an end-to-end toolkit for speech processing, including automatic speech recognition, text to speech, speech enhancement, dirarization and other tasks.\n\n## Exploring ESPnet in the Hub\n\nYou can find hundreds of `espnet` models by filtering at the left of the [models page](https://huggingface.co/models?library=espnet&sort=downloads). \n\nAll models on the Hub come up with useful features:\n1. An automatically generated model card with a description, a training configuration, licenses and more.\n2. Metadata tags that help for discoverability and contain information such as license, language and datasets.\n3. An interactive widget you can use to play out with the model directly in the browser.\n4. An Inference API that allows to make inference requests.\n\n<div class=\"flex justify-center\">\n<img class=\"block dark:hidden\" src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/hub/libraries-espnet_widget.png\"/>\n<img class=\"hidden dark:block\" src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/hub/libraries-espnet_widget-dark.png\"/>\n</div>\n\n## Using existing models\n\nFor a full guide on loading pre-trained models, we recommend checking out the [official guide](https://github.com/espnet/espnet_model_zoo)). \n\nIf you're interested in doing inference, different classes for different tasks have a `from_pretrained` method that allows loading models from the Hub. For example:\n* `Speech2Text` for Automatic Speech Recognition.\n* `Text2Speech` for Text to Speech.\n* `SeparateSpeech` for Audio Source Separation.\n\nHere is an inference example:\n\n```py\nimport soundfile\nfrom espnet2.bin.tts_inference import Text2Speech", "Here is an inference example:\n\n```py\nimport soundfile\nfrom espnet2.bin.tts_inference import Text2Speech\n\ntext2speech = Text2Speech.from_pretrained(\"model_name\")\nspeech = text2speech(\"foobar\")[\"wav\"]\nsoundfile.write(\"out.wav\", speech.numpy(), text2speech.fs, \"PCM_16\")\n```\n\nIf you want to see how to load a specific model, you can click `Use in ESPnet` and you will be given a working snippet that you can load it! \n\n<div class=\"flex justify-center\">\n<img class=\"block dark:hidden\" src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/hub/libraries-espnet_snippet.png\"/>\n<img class=\"hidden dark:block\" src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/hub/libraries-espnet_snippet-dark.png\"/>\n</div>\n\n## Sharing your models\n\n`ESPnet` outputs a `zip` file that can be uploaded to Hugging Face easily. For a full guide on sharing  models, we recommend checking out the [official guide](https://github.com/espnet/espnet_model_zoo#register-your-model)).\n\nThe `run.sh` script allows to upload a given model to a Hugging Face repository.\n\n```bash\n./run.sh --stage 15 --skip_upload_hf false --hf_repo username/model_repo\n```\n\n## Additional resources", "### Download Model Repository Directly\n\nHugging Face repositories can be downloaded directly using `git clone`:\n\n```\ngit clone git@hf.co:<Name of HuggingFace Repo> # example: git clone git@hf.co:bigscience/bloom\n```\n\n## Using Downloaded Models with Sample-Factory\n\nAfter downloading the model, you can run the models in the repo with the enjoy script corresponding to your environment. For example, if you are downloading a `mujoco-ant` model, it can be run with:\n\n```\npython -m sf_examples.mujoco.enjoy_mujoco --algo=APPO --env=mujoco_ant --experiment=<repo_name> --train_dir=./train_dir\n```\n\nNote, you may have to specify the `--train_dir` if your local train_dir has a different path than the one in the `cfg.json`\n\n## Sharing your models\n### Using push_to_hub\n\nIf you want to upload without generating evaluation metrics or a replay video, you can use the `push_to_hub` script:\n\n```\npython -m sample_factory.huggingface.push_to_hub -r <hf_username>/<hf_repo_name> -d <experiment_dir_path>\n```\n\nThe command line arguments are:\n\n- `-r`: The repo_id to save on HF Hub. This is the same as `hf_repository` in the enjoy script and must be in the form `<hf_username>/<hf_repo_name>`\n- `-d`: The full path to your experiment directory to upload\n\n\n### Using enjoy.py\n\nYou can upload your models to the Hub using your environment's `enjoy` script with the `--push_to_hub` flag. Uploading using `enjoy` can also generate evaluation metrics and a replay video.", "```bash\n./run.sh --stage 15 --skip_upload_hf false --hf_repo username/model_repo\n```\n\n## Additional resources\n\n* ESPnet [docs](https://espnet.github.io/espnet/index.html).\n* ESPnet model zoo [repository](https://github.com/espnet/espnet_model_zoo).\n* Integration [docs](https://github.com/asteroid-team/asteroid/blob/master/docs/source/readmes/pretrained_models.md).", "Doing so will also add a tag to your model so users can quickly identify models from your library.\n\n<div class=\"flex justify-center\">\n<img class=\"block dark:hidden\" src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/hub/libraries-tags.png\"/>\n<img class=\"hidden dark:block\" src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/hub/libraries-tags-dark.png\"/>\n</div>\n\n## Upload files to the Hub\n\nYou might also want to provide a method for creating model repositories and uploading files to the Hub directly from your library. The `huggingface_hub` library offers two ways to assist you with creating repositories and uploading files:\n\n- `create_repo` creates a repository on the Hub.\n- `upload_file` directly uploads files to a repository on the Hub.\n\n### `create_repo`\n\nThe `create_repo` method creates a repository on the Hub. Use the `name` parameter to provide a name for your repository:\n\n```python\n>>> from huggingface_hub import create_repo\n>>> create_repo(repo_id=\"test-model\")\n'https://huggingface.co/lysandre/test-model'\n```\n\nWhen you check your Hugging Face account, you should now see a `test-model` repository under your namespace.\n\n### `upload_file`\n\nThe `upload_file` method uploads files to the Hub. This method requires the following:\n\n- A path to the file to upload.\n- The final path in the repository.\n- The repository you wish to push the files to.\n\nFor example:", "![new_model_repo](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/new_model_repo.png)\n\nFrom here, add some information about your model:\n\n- Select the **owner** of the repository. This can be yourself or any of the organizations you belong to.\n- Pick a name for your model, which will also be the repository name.\n- Choose whether your model is public or private.\n- Specify the license usage for your model.\n\nNow click on the **Files** tab and click on the **Add file** button to upload a new file to your repository. Then drag-and-drop a file to upload and add a commit message.\n\n![upload_file](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/upload_file.png)\n\n## Add a model card\n\nTo make sure users understand your model's capabilities, limitations, potential biases and ethical considerations, please add a model card to your repository. The model card is defined in the `README.md` file. You can add a model card by:\n\n* Manually creating and uploading a `README.md` file.\n* Clicking on the **Edit model card** button in your model repository.\n\nTake a look at the DistilBert [model card](https://huggingface.co/distilbert-base-uncased) for a good example of the type of information a model card should include. For more details about other options you can control in the `README.md` file such as a model's carbon footprint or widget examples, refer to the documentation [here](https://huggingface.co/docs/hub/models-cards)."], "eval_score_GPT4": "5", "eval_feedback_GPT4": "The response provided is completely correct, accurate, and factual. It matches the reference answer and includes additional information that is helpful for understanding how to use the command, such as the need to replace `username` and `model_repo` with the user's Hugging Face username and the name of their repository. The response is fully aligned with the reference answer and adheres to the score rubric for a score of 5."}, {"question": "What file should be added to a model repository to install custom Python dependencies for Inference Endpoints?\n", "true_answer": "requirements.txt", "source_doc": "huggingface/hf-endpoints-documentation/blob/main/docs/source/guides/custom_dependencies.mdx", "generated_answer": "The file that should be added to a model repository to install custom Python dependencies for Inference Endpoints is a `requirements.txt` file. This file should be placed in the root directory of the model repository and should contain a list of the Python dependencies that are required for running the model inference. When the Inference Endpoint is created, the `requirements.txt` file is checked, and the dependencies listed within are installed. This allows for customization of the inference pipeline", "retrieved_docs": ["Add custom Dependencies\n\nInference Endpoints\u2019 base image includes all required libraries to run inference on \ud83e\udd17 Transformers models, but it also supports custom dependencies. This is useful if you want to:\n\n* [customize your inference pipeline](/docs/inference-endpoints/guides/custom_handler) and need additional Python dependencies\n* run a model which requires special dependencies like the newest or a fixed version of a library (for example, `tapas` (`torch-scatter`)).\n\nTo add custom dependencies, add a `requirements.txt` [file](https://huggingface.co/philschmid/distilbert-onnx-banking77/blob/main/requirements.txt) with the Python dependencies you want to install in your model repository on the Hugging Face Hub. When your Endpoint and Image artifacts are created, Inference Endpoints checks if the model repository contains a `requirements.txt ` file and installs the dependencies listed within.\n\n```bash\noptimum[onnxruntime]==1.2.3\nmkl-include\nmkl\n```\n\nCheck out the `requirements.txt` files in the following model repositories for examples:\n\n* [Optimum and onnxruntime](https://huggingface.co/philschmid/distilbert-onnx-banking77/blob/main/requirements.txt)\n* [diffusers](https://huggingface.co/philschmid/stable-diffusion-v1-4-endpoints/blob/main/requirements.txt)\n\nFor more information, take a look at how you can create and install dependencies when you [use your own custom container](/docs/inference-endpoints/guides/custom_container) for inference.", "Inference Endpoints\n\nInference Endpoints provides a secure production solution to easily deploy any `transformers`, `sentence-transformers`, and `diffusers` models on a dedicated and autoscaling infrastructure managed by Hugging Face. An Inference Endpoint is built from a model from the [Hub](https://huggingface.co/models). \nIn this guide, we will learn how to programmatically manage Inference Endpoints with `huggingface_hub`. For more information about the Inference Endpoints product itself, check out its [official documentation](https://huggingface.co/docs/inference-endpoints/index).\n\nThis guide assumes `huggingface_hub` is correctly installed and that your machine is logged in. Check out the [Quick Start guide](https://huggingface.co/docs/huggingface_hub/quick-start#quickstart) if that's not the case yet. The minimal version supporting Inference Endpoints API is `v0.19.0`.\n\n\n## Create an Inference Endpoint\n\nThe first step is to create an Inference Endpoint using [`create_inference_endpoint`]:\n\n```py\n>>> from huggingface_hub import create_inference_endpoint\n\n>>> endpoint = create_inference_endpoint(\n...     \"my-endpoint-name\",\n...     repository=\"gpt2\",\n...     framework=\"pytorch\",\n...     task=\"text-generation\",\n...     accelerator=\"cpu\",\n...     vendor=\"aws\",\n...     region=\"us-east-1\",\n...     type=\"protected\",\n...     instance_size=\"medium\",\n...     instance_type=\"c6i\"\n... )\n```", "## User defined code and modules\n\nThe Hugging Face Inference Toolkit allows the user to override the default methods of the `HuggingFaceHandlerService`. You will need to create a folder named `code/` with an `inference.py` file in it. See [here](#create-a-model-artifact-for-deployment) for more details on how to archive your model artifacts. For example:  \n\n```bash\nmodel.tar.gz/\n|- pytorch_model.bin\n|- ....\n|- code/\n  |- inference.py\n  |- requirements.txt \n```\n\nThe `inference.py` file contains your custom inference module, and the `requirements.txt` file contains additional dependencies that should be added. The custom module can override the following methods:", "Inference Endpoints Version\n\nHugging Face Inference Endpoints comes with a default serving container which is used for all [supported Transformers and Sentence-Transformers tasks](/docs/inference-endpoints/supported_tasks) and for [custom inference handler](/docs/inference-endpoints/guides/custom_handler) and implement batching.\nBelow you will find information about the installed packages and versions used.\n\nYou can always upgrade installed packages and a custom packages by adding a `requirements.txt` file to your model repository. Read more in [Add custom Dependencies](/docs/inference-endpoints/guides/custom_dependencies).\n\n## Installed packages & version\n\nThe installed packages are split into `general`, `CPU` & `GPU` packages. The `general` packages are installed in all containers, the `CPU` and `GPU` packages are only installed in the corresponding containers.\nThe Hugging Face Inference Runtime has separate versions for `PyTorch` and `TensorFlow` for `CPU` and `GPU`, which are used based on the selected `framework` when creating an Inference Endpoint. The `TensorFlow` and `PyTorch` flavors are grouped together in the list below.\n\n### General\n\n- `Python`: `3.9.13`\n\n### CPU\n\n- `transformers[sklearn,sentencepiece,audio,vision]`: `4.27.2`\n- `diffusers`: `0.14.0`\n- `accelerate`: `0.17.1`\n- `sentence_transformers`: `latest`\n- `pandas`: `latest`\n- `pytorch`: `1.13.1`\n- `torchvision`: `0.14.1`\n- `tensorflow`: `2.9.1`\n\n### GPU", "The `inference.py` file contains your custom inference module, and the `requirements.txt` file contains additional dependencies that should be added. The custom module can override the following methods:  \n\n* `model_fn(model_dir)` overrides the default method for loading a model. The return value `model` will be used in `predict` for predictions. `predict` receives argument the `model_dir`, the path to your unzipped `model.tar.gz`.\n* `transform_fn(model, data, content_type, accept_type)` overrides the default transform function with your custom implementation. You will need to implement your own `preprocess`, `predict` and `postprocess` steps in the `transform_fn`. This method can't be combined with `input_fn`, `predict_fn` or `output_fn` mentioned below.\n* `input_fn(input_data, content_type)` overrides the default method for preprocessing. The return value `data` will be used in `predict` for predictions. The inputs are:\n  - `input_data` is the raw body of your request.\n  - `content_type` is the content type from the request header.\n* `predict_fn(processed_data, model)` overrides the default method for predictions. The return value `predictions` will be used in `postprocess`. The input is `processed_data`, the result from `preprocess`.\n* `output_fn(prediction, accept)` overrides the default method for postprocessing. The return value `result` will be the response of your request (e.g.`JSON`). The inputs are:\n  - `predictions` is the result from `predict`.\n  - `accept` is the return accept type from the HTTP Request, e.g. `application/json`.\n\nHere is an example of a custom inference module with `model_fn`, `input_fn`, `predict_fn`, and `output_fn`:  \n\n```python\nfrom sagemaker_huggingface_inference_toolkit import decoder_encoder", "```\n# add all our new files\n!git add *\n# commit our files\n!git commit -m \"add custom handler\"\n# push the files to the hub\n!git push\n```\n\nNow, you should see your `handler.py` and `requirements.txt` in your repository in the [\u201cFiles and version\u201d](https://huggingface.co/philschmid/distilbert-base-uncased-emotion/tree/main) tab.\n\n### 6. Deploy your Custom Handler as an Inference Endpoint\n\nThe last step is to deploy your Custom Handler as an Inference Endpoint. You can deploy your Custom Handler like you would a regular Inference Endpoint. Add your repository, select your cloud and region, your instance and security setting, and deploy.\n\nWhen creating your Endpoint, the Inference Endpoint Service will check for an available and valid `handler.py`, and will use it for serving requests no matter which \u201cTask\u201d you select.\n\n_Note: In your [Inference Endpoints dashboard](https://ui.endpoints.huggingface.co/), the Task for this Endpoint should now be set to Custom_", "Once the command has finished, you should have a total of 7 new files spread across the repository:\n```\ndocs/source/model_doc/<model_name>.md\nsrc/transformers/models/<model_name>/configuration_<model_name>.py\nsrc/transformers/models/<model_name>/modeling_<model_name>.py\nsrc/transformers/models/<model_name>/modeling_tf_<model_name>.py\nsrc/transformers/models/<model_name>/tokenization_<model_name>.py\ntests/test_modeling_<model_name>.py\ntests/test_modeling_tf_<model_name>.py\n```\n\nYou can run the tests to ensure that they all pass:\n\n```\npython -m pytest ./tests/test_*<model_name>*.py\n```\n\nFeel free to modify each file to mimic the behavior of your model. \n\n\u26a0 You should be careful about the classes preceded by the following line:\ufe0f \n\n```python\n# Copied from transformers.[...]\n```\n\nThis line ensures that the copy does not diverge from the source. If it *should* diverge, because the implementation\nis different, this line needs to be deleted. If you don't delete this line and run `make fix-copies`,\nyour changes will be overwritten.\n\nOnce you have edited the files to fit your architecture, simply re-run the tests (and edit them if a change \nis needed!) afterwards to make sure everything works as expected. \n\nOnce the files are generated and you are happy with your changes, here's a checklist to ensure that your contribution\nwill be merged quickly:\n\n- You should run the `make fixup` utility to fix the style of the files and to ensure the code quality meets the\n  library's standards.\n- You should complete the documentation file (`docs/source/model_doc/<model_name>.rst`) so that your model may be\n  usable.\n\n## Add new model like command"], "eval_score_GPT4": "5", "eval_feedback_GPT4": "The response provided is completely correct, accurate, and factual. It aligns with the reference answer by stating that a `requirements.txt` file is needed in the model repository to install custom Python dependencies for Inference Endpoints. The response goes further to explain the placement of the file and its function, which supports the correctness and factual nature of the answer without deviating from the expected response."}, {"question": "How many images are needed to teach new concepts to Stable Diffusion using Textual Inversion?\n", "true_answer": "3-5 images", "source_doc": "huggingface/diffusers/blob/main/examples/research_projects/intel_opts/textual_inversion_dfq/README.md", "generated_answer": "According to the context provided, Textual Inversion allows teaching new concepts to Stable Diffusion using just 3-5 sample images. (Source: Extracted document 0)", "retrieved_docs": ["Textual Inversion lets you personalize a Stable Diffusion model on your own images with just 3-5 samples. With this tool, you can train a model on a concept, and then share the concept with the rest of the community! \n\n![image info](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/diffusers-2nd-month/textual_inversion.jpeg)\n\nIn just a couple of days, the community shared over 200 concepts! Check them out!\n\n* [Organization](https://huggingface.co/sd-concepts-library) with the concepts. \n* [Navigator Colab](https://colab.research.google.com/github/huggingface/notebooks/blob/main/diffusers/stable_diffusion_textual_inversion_library_navigator.ipynb): Browse visually and use over 150 concepts created by the community.\n* [Training Colab](https://colab.research.google.com/github/huggingface/notebooks/blob/main/diffusers/sd_textual_inversion_training.ipynb): Teach Stable Diffusion a new concept and share it with the rest of the community.\n* [Inference Colab](https://colab.research.google.com/github/huggingface/notebooks/blob/main/diffusers/stable_conceptualizer_inference.ipynb): Run Stable Diffusion with the learned concepts.\n\n\n## Experimental inpainting pipeline\n\nInpainting allows to provide an image, then select an area in the image (or provide a mask), and use Stable Diffusion to replace the mask. Here is an example:\n\n<figure class=\"image table text-center m-0 w-full\">\n  <img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/diffusers-2nd-month/inpainting.png\" alt=\"Example inpaint of owl being generated from an initial image and a prompt\"/>\n</figure>", "!--Copyright 2023 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\n\nhttp://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License.\n-->\n\n# Textual inversion\n\n[[open-in-colab]]\n\nThe [`StableDiffusionPipeline`] supports textual inversion, a technique that enables a model like Stable Diffusion to learn a new concept from just a few sample images. This gives you more control over the generated images and allows you to tailor the model towards specific concepts. You can get started quickly with a collection of community created concepts in the [Stable Diffusion Conceptualizer](https://huggingface.co/spaces/sd-concepts-library/stable-diffusion-conceptualizer).\n\nThis guide will show you how to run inference with textual inversion using a pre-learned concept from the Stable Diffusion Conceptualizer. If you're interested in teaching a model new concepts with textual inversion, take a look at the [Textual Inversion](../training/text_inversion) training guide.\n\nImport the necessary libraries:\n\n```py\nimport torch\nfrom diffusers import StableDiffusionPipeline\nfrom diffusers.utils import make_image_grid\n```\n\n## Stable Diffusion 1 and 2\n\nPick a Stable Diffusion checkpoint and a pre-learned concept from the [Stable Diffusion Conceptualizer](https://huggingface.co/spaces/sd-concepts-library/stable-diffusion-conceptualizer):\n\n```py\npretrained_model_name_or_path = \"runwayml/stable-diffusion-v1-5\"\nrepo_id_embeds = \"sd-concepts-library/cat-toy\"\n```", "## Textual Inversion\n\n[Paper](https://arxiv.org/abs/2208.01618)\n\n[Textual Inversion](../training/text_inversion) fine-tunes a model to teach it about a new concept. I.e. a few pictures of a style of artwork can be used to generate images in that style.\n\n## ControlNet\n\n[Paper](https://arxiv.org/abs/2302.05543)\n\n[ControlNet](../api/pipelines/controlnet) is an auxiliary network which adds an extra condition.\nThere are 8 canonical pre-trained ControlNets trained on different conditionings such as edge detection, scribbles,\ndepth maps, and semantic segmentations.\n\n## Prompt Weighting\n\n[Prompt weighting](../using-diffusers/weighted_prompts) is a simple technique that puts more attention weight on certain parts of the text\ninput.\n\n## Custom Diffusion\n\n[Paper](https://arxiv.org/abs/2212.04488)\n\n[Custom Diffusion](../training/custom_diffusion) only fine-tunes the cross-attention maps of a pre-trained\ntext-to-image diffusion model. It also allows for additionally performing Textual Inversion. It supports\nmulti-concept training by design. Like DreamBooth and Textual Inversion, Custom Diffusion is also used to\nteach a pre-trained text-to-image diffusion model about new concepts to generate outputs involving the\nconcept(s) of interest.\n\n## Model Editing\n\n[Paper](https://arxiv.org/abs/2303.08084)\n\nThe [text-to-image model editing pipeline](../api/pipelines/model_editing) helps you mitigate some of the incorrect implicit assumptions a pre-trained text-to-image\ndiffusion model might make about the subjects present in the input prompt. For example, if you prompt Stable Diffusion to generate images for \"A pack of roses\", the roses in the generated images\nare more likely to be red. This pipeline helps you change that assumption.\n\n## DiffEdit", "Distillation for quantization on Textual Inversion models to personalize text2image\n\n[Textual inversion](https://arxiv.org/abs/2208.01618) is a method to personalize text2image models like stable diffusion on your own images._By using just 3-5 images new concepts can be taught to Stable Diffusion and the model personalized on your own images_\nThe `textual_inversion.py` script shows how to implement the training procedure and adapt it for stable diffusion.\nWe have enabled distillation for quantization in `textual_inversion.py` to do quantization aware training as well as distillation on the model generated by Textual Inversion method.\n\n## Installing the dependencies\n\nBefore running the scripts, make sure to install the library's training dependencies:\n\n```bash\npip install -r requirements.txt\n```\n\n## Prepare Datasets\n\nOne picture which is from the huggingface datasets [sd-concepts-library/dicoo2](https://huggingface.co/sd-concepts-library/dicoo2) is needed, and save it to the `./dicoo` directory. The picture is shown below:\n\n<a href=\"https://huggingface.co/sd-concepts-library/dicoo2/blob/main/concept_images/1.jpeg\">\n    <img src=\"https://huggingface.co/sd-concepts-library/dicoo2/resolve/main/concept_images/1.jpeg\" width = \"300\" height=\"300\">\n</a>\n\n## Get a FP32 Textual Inversion model\n\nUse the following command to fine-tune the Stable Diffusion model on the above dataset to obtain the FP32 Textual Inversion model.\n\n```bash\nexport MODEL_NAME=\"CompVis/stable-diffusion-v1-4\"\nexport DATA_DIR=\"./dicoo\"", "# Textual Inversion fine-tuning example\n\n[Textual inversion](https://arxiv.org/abs/2208.01618) is a method to personalize text2image models like stable diffusion on your own images using just 3-5 examples.\nThe `textual_inversion.py` script shows how to implement the training procedure and adapt it for stable diffusion.\n\n## Training with Intel Extension for PyTorch\n\nIntel Extension for PyTorch provides the optimizations for faster training and inference on CPUs. You can leverage the training example \"textual_inversion.py\". Follow the [instructions](https://github.com/huggingface/diffusers/tree/main/examples/textual_inversion) to get the model and [dataset](https://huggingface.co/sd-concepts-library/dicoo2) before running the script.\n\nThe example supports both single node and multi-node distributed training:\n\n### Single node training\n\n```bash\nexport MODEL_NAME=\"CompVis/stable-diffusion-v1-4\"\nexport DATA_DIR=\"path-to-dir-containing-dicoo-images\"\n\npython textual_inversion.py \\\n  --pretrained_model_name_or_path=$MODEL_NAME \\\n  --train_data_dir=$DATA_DIR \\\n  --learnable_property=\"object\" \\\n  --placeholder_token=\"<dicoo>\" --initializer_token=\"toy\" \\\n  --seed=7 \\\n  --resolution=512 \\\n  --train_batch_size=1 \\\n  --gradient_accumulation_steps=1 \\\n  --max_train_steps=3000 \\\n  --learning_rate=2.5e-03 --scale_lr \\\n  --output_dir=\"textual_inversion_dicoo\"\n```\n\nNote: Bfloat16 is available on Intel Xeon Scalable Processors Cooper Lake or Sapphire Rapids. You may not get performance speedup without Bfloat16 support.\n\n### Multi-node distributed training", "# Textual Inversion fine-tuning example\n\n[Textual inversion](https://arxiv.org/abs/2208.01618) is a method to personalize text2image models like stable diffusion on your own images using just 3-5 examples.\nThe `textual_inversion.py` script shows how to implement the training procedure and adapt it for stable diffusion.\n\n## Running on Colab\n\nColab for training\n[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/huggingface/notebooks/blob/main/diffusers/sd_textual_inversion_training.ipynb)\n\nColab for inference\n[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/huggingface/notebooks/blob/main/diffusers/stable_conceptualizer_inference.ipynb)\n\n## Running locally with PyTorch\n### Installing the dependencies\n\nBefore running the scripts, make sure to install the library's training dependencies:\n\n**Important**\n\nTo make sure you can successfully run the latest versions of the example scripts, we highly recommend **installing from source** and keeping the install up to date as we update the example scripts frequently and install some example-specific requirements. To do this, execute the following steps in a new virtual environment:\n```bash\ngit clone https://github.com/huggingface/diffusers\ncd diffusers\npip install .\n```\n\nThen cd in the example folder and run:\n```bash\npip install -r requirements.txt\n```\n\nAnd initialize an [\ud83e\udd17 Accelerate](https://github.com/huggingface/accelerate/) environment with:\n\n```bash\naccelerate config\n```\n\n### Cat toy example", "# Textual Inversion fine-tuning example\n\n[Textual inversion](https://arxiv.org/abs/2208.01618) is a method to personalize text2image models like stable diffusion on your own images using just 3-5 examples.\nThe `textual_inversion.py` script shows how to implement the training procedure and adapt it for stable diffusion.\n\n## Running on Colab\n\nColab for training\n[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/huggingface/notebooks/blob/main/diffusers/sd_textual_inversion_training.ipynb)\n\nColab for inference\n[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/huggingface/notebooks/blob/main/diffusers/stable_conceptualizer_inference.ipynb)\n\n## Running locally with PyTorch\n### Installing the dependencies\n\nBefore running the scripts, make sure to install the library's training dependencies:\n\n**Important**\n\nTo make sure you can successfully run the latest versions of the example scripts, we highly recommend **installing from source** and keeping the install up to date as we update the example scripts frequently and install some example-specific requirements. To do this, execute the following steps in a new virtual environment:\n```bash\ngit clone https://github.com/huggingface/diffusers\ncd diffusers\npip install .\n```\n\nThen cd in the example folder  and run\n```bash\npip install -r requirements.txt\n```\n\nAnd initialize an [\ud83e\udd17Accelerate](https://github.com/huggingface/accelerate/) environment with:\n\n```bash\naccelerate config\n```\n\n\n### Cat toy example"], "eval_score_GPT4": "5", "eval_feedback_GPT4": "Feedback: The response provided is completely correct, accurate, and factual. It matches the reference answer by stating that 3-5 sample images are needed to teach new concepts to Stable Diffusion using Textual Inversion. The inclusion of a source reference does not detract from the accuracy of the answer."}, {"question": "What is the maximum size of a model checkpoint before it is automatically sharded in Transformers version 4.18.0?\n", "true_answer": "10GB", "source_doc": "huggingface/transformers/blob/main/docs/source/en/big_models.md", "generated_answer": "The maximum size of a model checkpoint before it is automatically sharded in Transformers version 4.18.0 is 10GB. This feature was introduced in version 4.18.0 and can be controlled with the `max_shard_size` parameter. If the checkpoint exceeds 10GB, it will be automatically sharded into smaller pieces with a maximum size of `max_shard_size`. The default value for `max", "retrieved_docs": ["</Tip>\n\nIn this guide, we explore the solutions Transformers offer to deal with this issue. Note that this is an area of active development, so the APIs explained here may change slightly in the future.\n\n## Sharded checkpoints\n\nSince version 4.18.0, model checkpoints that end up taking more than 10GB of space are automatically sharded in smaller pieces. In terms of having one single checkpoint when you do `model.save_pretrained(save_dir)`, you will end up with several partial checkpoints (each of which being of size < 10GB) and an index that maps parameter names to the files they are stored in.\n\nYou can control the maximum size before sharding with the `max_shard_size` parameter, so for the sake of an example, we'll use a normal-size models with a small shard size: let's take a traditional BERT model.\n\n```py\nfrom transformers import AutoModel\n\nmodel = AutoModel.from_pretrained(\"bert-base-cased\")\n```\n\nIf you save it using [`~PreTrainedModel.save_pretrained`], you will get a new folder with two files: the config of the model and its weights:\n\n```py\n>>> import os\n>>> import tempfile\n\n>>> with tempfile.TemporaryDirectory() as tmp_dir:\n...     model.save_pretrained(tmp_dir)\n...     print(sorted(os.listdir(tmp_dir)))\n['config.json', 'pytorch_model.bin']\n```\n\nNow let's use a maximum shard size of 200MB:", "1. **[FLAN-T5](https://huggingface.co/docs/transformers/model_doc/flan-t5)** (from Google AI) released in the repository [google-research/t5x](https://github.com/google-research/t5x/blob/main/docs/models.md#flan-t5-checkpoints) by Hyung Won Chung, Le Hou, Shayne Longpre, Barret Zoph, Yi Tay, William Fedus, Eric Li, Xuezhi Wang, Mostafa Dehghani, Siddhartha Brahma, Albert Webson, Shixiang Shane Gu, Zhuyun Dai, Mirac Suzgun, Xinyun Chen, Aakanksha Chowdhery, Sharan Narang, Gaurav Mishra, Adams Yu, Vincent Zhao, Yanping Huang, Andrew Dai, Hongkun Yu, Slav Petrov, Ed H. Chi, Jeff Dean, Jacob Devlin, Adam Roberts, Denny Zhou, Quoc V. Le, and Jason Wei\n1. **[FLAN-UL2](https://huggingface.co/docs/transformers/model_doc/flan-ul2)** (from Google AI) released in the repository [google-research/t5x](https://github.com/google-research/t5x/blob/main/docs/models.md#flan-ul2-checkpoints) by Hyung Won Chung, Le Hou, Shayne Longpre, Barret Zoph, Yi Tay, William Fedus, Eric Li, Xuezhi Wang, Mostafa Dehghani, Siddhartha Brahma, Albert Webson, Shixiang Shane Gu, Zhuyun Dai, Mirac Suzgun, Xinyun Chen, Aakanksha Chowdhery, Sharan Narang, Gaurav Mishra, Adams Yu, Vincent Zhao, Yanping Huang, Andrew Dai, Hongkun Yu, Slav Petrov, Ed H. Chi, Jeff Dean, Jacob Devlin, Adam Roberts, Denny Zhou, Quoc V. Le, and Jason Wei", "1. **[FLAN-T5](https://huggingface.co/docs/transformers/model_doc/flan-t5)** (from Google AI) released in the repository [google-research/t5x](https://github.com/google-research/t5x/blob/main/docs/models.md#flan-t5-checkpoints) by Hyung Won Chung, Le Hou, Shayne Longpre, Barret Zoph, Yi Tay, William Fedus, Eric Li, Xuezhi Wang, Mostafa Dehghani, Siddhartha Brahma, Albert Webson, Shixiang Shane Gu, Zhuyun Dai, Mirac Suzgun, Xinyun Chen, Aakanksha Chowdhery, Sharan Narang, Gaurav Mishra, Adams Yu, Vincent Zhao, Yanping Huang, Andrew Dai, Hongkun Yu, Slav Petrov, Ed H. Chi, Jeff Dean, Jacob Devlin, Adam Roberts, Denny Zhou, Quoc V. Le, and Jason Wei\n1. **[FLAN-UL2](https://huggingface.co/docs/transformers/model_doc/flan-ul2)** (from Google AI) released in the repository [google-research/t5x](https://github.com/google-research/t5x/blob/main/docs/models.md#flan-ul2-checkpoints) by Hyung Won Chung, Le Hou, Shayne Longpre, Barret Zoph, Yi Tay, William Fedus, Eric Li, Xuezhi Wang, Mostafa Dehghani, Siddhartha Brahma, Albert Webson, Shixiang Shane Gu, Zhuyun Dai, Mirac Suzgun, Xinyun Chen, Aakanksha Chowdhery, Sharan Narang, Gaurav Mishra, Adams Yu, Vincent Zhao, Yanping Huang, Andrew Dai, Hongkun Yu, Slav Petrov, Ed H. Chi, Jeff Dean, Jacob Devlin, Adam Roberts, Denny Zhou, Quoc V. Le, and Jason Wei", "1. **[FLAN-T5](https://huggingface.co/docs/transformers/model_doc/flan-t5)** (from Google AI) released in the repository [google-research/t5x](https://github.com/google-research/t5x/blob/main/docs/models.md#flan-t5-checkpoints) by Hyung Won Chung, Le Hou, Shayne Longpre, Barret Zoph, Yi Tay, William Fedus, Eric Li, Xuezhi Wang, Mostafa Dehghani, Siddhartha Brahma, Albert Webson, Shixiang Shane Gu, Zhuyun Dai, Mirac Suzgun, Xinyun Chen, Aakanksha Chowdhery, Sharan Narang, Gaurav Mishra, Adams Yu, Vincent Zhao, Yanping Huang, Andrew Dai, Hongkun Yu, Slav Petrov, Ed H. Chi, Jeff Dean, Jacob Devlin, Adam Roberts, Denny Zhou, Quoc V. Le, and Jason Wei\n1. **[FLAN-UL2](https://huggingface.co/docs/transformers/model_doc/flan-ul2)** (from Google AI) released in the repository [google-research/t5x](https://github.com/google-research/t5x/blob/main/docs/models.md#flan-ul2-checkpoints) by Hyung Won Chung, Le Hou, Shayne Longpre, Barret Zoph, Yi Tay, William Fedus, Eric Li, Xuezhi Wang, Mostafa Dehghani, Siddhartha Brahma, Albert Webson, Shixiang Shane Gu, Zhuyun Dai, Mirac Suzgun, Xinyun Chen, Aakanksha Chowdhery, Sharan Narang, Gaurav Mishra, Adams Yu, Vincent Zhao, Yanping Huang, Andrew Dai, Hongkun Yu, Slav Petrov, Ed H. Chi, Jeff Dean, Jacob Devlin, Adam Roberts, Denny Zhou, Quoc V. Le, and Jason Wei", "1. **[FLAN-T5](https://huggingface.co/docs/transformers/model_doc/flan-t5)** (from Google AI) released in the repository [google-research/t5x](https://github.com/google-research/t5x/blob/main/docs/models.md#flan-t5-checkpoints) by Hyung Won Chung, Le Hou, Shayne Longpre, Barret Zoph, Yi Tay, William Fedus, Eric Li, Xuezhi Wang, Mostafa Dehghani, Siddhartha Brahma, Albert Webson, Shixiang Shane Gu, Zhuyun Dai, Mirac Suzgun, Xinyun Chen, Aakanksha Chowdhery, Sharan Narang, Gaurav Mishra, Adams Yu, Vincent Zhao, Yanping Huang, Andrew Dai, Hongkun Yu, Slav Petrov, Ed H. Chi, Jeff Dean, Jacob Devlin, Adam Roberts, Denny Zhou, Quoc V. Le, and Jason Wei\n1. **[FLAN-UL2](https://huggingface.co/docs/transformers/model_doc/flan-ul2)** (from Google AI) released in the repository [google-research/t5x](https://github.com/google-research/t5x/blob/main/docs/models.md#flan-ul2-checkpoints) by Hyung Won Chung, Le Hou, Shayne Longpre, Barret Zoph, Yi Tay, William Fedus, Eric Li, Xuezhi Wang, Mostafa Dehghani, Siddhartha Brahma, Albert Webson, Shixiang Shane Gu, Zhuyun Dai, Mirac Suzgun, Xinyun Chen, Aakanksha Chowdhery, Sharan Narang, Gaurav Mishra, Adams Yu, Vincent Zhao, Yanping Huang, Andrew Dai, Hongkun Yu, Slav Petrov, Ed H. Chi, Jeff Dean, Jacob Devlin, Adam Roberts, Denny Zhou, Quoc V. Le, and Jason Wei", "## Model architectures\n\n**[All the model checkpoints](https://huggingface.co/models)** provided by \ud83e\udd17 Transformers are seamlessly integrated from the huggingface.co [model hub](https://huggingface.co/models), where they are uploaded directly by [users](https://huggingface.co/users) and [organizations](https://huggingface.co/organizations).\n\nCurrent number of checkpoints: ![](https://img.shields.io/endpoint?url=https://huggingface.co/api/shields/models&color=brightgreen)\n\n\ud83e\udd17 Transformers currently provides the following architectures (see [here](https://huggingface.co/docs/transformers/model_summary) for a high-level summary of each them):", "7.  [ ] Successfully ran forward pass in Transformers that gives\n    identical output to original checkpoint\n\n8.  [ ] Finished model tests in Transformers\n\n9.  [ ] Successfully added Tokenizer in Transformers\n\n10. [ ] Run end-to-end integration tests\n\n11. [ ] Finished docs\n\n12. [ ] Uploaded model weights to the hub\n\n13. [ ] Submitted the pull request for review\n\n14. [ ] (Optional) Added a demo notebook\n\nTo begin with, we usually recommend to start by getting a good\ntheoretical understanding of `[camelcase name of model]`. However, if you prefer to\nunderstand the theoretical aspects of the model *on-the-job*, then it is\ntotally fine to directly dive into the `[camelcase name of model]`'s code-base. This\noption might suit you better, if your engineering skills are better than\nyour theoretical skill, if you have trouble understanding\n`[camelcase name of model]`'s paper, or if you just enjoy programming much more than\nreading scientific papers.\n\n### 1. (Optional) Theoretical aspects of [camelcase name of model]\n\nYou should take some time to read *[camelcase name of model]'s* paper, if such\ndescriptive work exists. There might be large sections of the paper that\nare difficult to understand. If this is the case, this is fine - don't\nworry! The goal is not to get a deep theoretical understanding of the\npaper, but to extract the necessary information required to effectively\nre-implement the model in \ud83e\udd17 Transformers. That being said, you don't\nhave to spend too much time on the theoretical aspects, but rather focus\non the practical ones, namely:"], "eval_score_GPT4": "5", "eval_feedback_GPT4": "The response correctly identifies the maximum size of a model checkpoint before it is automatically sharded as 10GB, which is in line with the reference answer. The additional information provided about the `max_shard_size` parameter and the default behavior of the sharding process is accurate and relevant to the question, although it goes beyond the reference answer. Based on the score rubric, the response is completely correct, accurate, and factual."}, {"question": "What is the purpose of Weights and Biases (W&B) for data scientists and machine learning scientists?\n", "true_answer": "To track their machine learning experiments at every stage, from training to production.", "source_doc": "gradio-app/gradio/blob/main/guides/06_integrating-other-frameworks/Gradio-and-Wandb-Integration.md", "generated_answer": "Weights and Biases (W&B) is a tool for data scientists and machine learning scientists to track their machine learning experiments at every stage, from training to production. It allows any metric to be aggregated over samples and shown in customizable and searchable dashboards. The purpose of W&B is to help data scientists and machine learning scientists address bias throughout the ML development cycle by providing a platform to put bias in context, address bias throughout the ML development cycle, and communicate information flow", "retrieved_docs": ["Gradio and W&B Integration\n\nRelated spaces: https://huggingface.co/spaces/akhaliq/JoJoGAN\nTags: WANDB, SPACES\nContributed by Gradio team\n\n## Introduction\n\nIn this Guide, we'll walk you through:\n\n- Introduction of Gradio, and Hugging Face Spaces, and Wandb\n- How to setup a Gradio demo using the Wandb integration for JoJoGAN\n- How to contribute your own Gradio demos after tracking your experiments on wandb to the Wandb organization on Hugging Face\n\n\n## What is Wandb?\n\nWeights and Biases (W&B) allows data scientists and machine learning scientists to track their machine learning experiments at every stage, from training to production. Any metric can be aggregated over samples and shown in panels in a customizable and searchable dashboard, like below:\n\n<img alt=\"Screen Shot 2022-08-01 at 5 54 59 PM\" src=\"https://user-images.githubusercontent.com/81195143/182252755-4a0e1ca8-fd25-40ff-8c91-c9da38aaa9ec.png\">\n\n## What are Hugging Face Spaces & Gradio?\n\n### Gradio\n\nGradio lets users demo their machine learning models as a web app, all in a few lines of Python. Gradio wraps any Python function (such as a machine learning model's inference function) into a user interface and the demos can be launched inside jupyter notebooks, colab notebooks, as well as embedded in your own website and hosted on Hugging Face Spaces for free.\n\nGet started [here](https://gradio.app/getting_started)\n\n### Hugging Face Spaces", "**<span style=\"text-decoration:underline;\">Table of contents:</span>**\n* **<span style=\"text-decoration:underline;\">On Machine Biases</span>**\n    * [Machine Bias: from ML Systems to Risks](#machine-bias-from-ml-systems-to-personal-and-social-risks)\n    * [Putting Bias in Context](#putting-bias-in-context)\n* **<span style=\"text-decoration:underline;\">Tools and Recommendations</span>**\n    * [Addressing Bias throughout ML Development](#addressing-bias-throughout-the-ml-development-cycle)\n        * [Task Definition](#i-am-defining-the-task-of-my-ml-system-how-can-i-address-bias)\n        * [Dataset Curation](#i-am-curatingpicking-a-dataset-for-my-ml-system-how-can-i-address-bias)\n        * [Model Training](#i-am-trainingselecting-a-model-for-my-ml-system-how-can-i-address-bias)\n    * [Overview of \ud83e\udd17 Bias Tools](#conclusion-and-overview-of-bias-analysis-and-documentation-tools-from-)\n\n## _Machine Bias:_ from ML Systems to Personal and Social Risks\n\nML systems allow us to automate complex tasks at a scale never seen before as they are deployed in more sectors and use cases. When the technology works at its best, it can help smooth interactions between people and technical systems, remove the need for highly repetitive work, or unlock new ways of processing information to support research.\n\nThese same systems are also likely to reproduce discriminatory and abusive behaviors represented in their training data, especially when the data encodes human behaviors.\nThe technology then has the potential to make these issues significantly worse. Automation and deployment at scale can indeed:", "So, who\u2019s on the hook for machine biases in ML? These three cases illustrate one of the reasons why discussions about the responsibility of ML developers in addressing bias can get so complicated: depending on decisions made at other points in the ML system development process by other people, the biases in an ML dataset or model may land anywhere between being irrelevant to the application settings and directly leading to grievous harm. However, in all of these cases, **stronger biases in the model/dataset increase the risk of negative outcomes**. The European Union has started to develop frameworks that address this phenomenon in [recent regulatory efforts](https://ec.europa.eu/info/business-economy-euro/doing-business-eu/contract-rules/digital-contracts/liability-rules-artificial-intelligence_en): in short, a company that deploys an AI system based on a measurably biased model is liable for harm caused by the system.\n\nConceptualizing bias as a risk factor then allows us to better understand the **shared responsibility** for machine biases between developers at all stages. Bias can never be fully removed, not least because the definitions of social biases and the power dynamics that tie them to discrimination vary vastly across social contexts. However:\n\n\n\n1. Each stage of the development process, from task specification, dataset curation, and model training, to model integration and system deployment, can take steps to minimize the aspects of machine bias** that most directly depend on its choices** and technical decisions, and\n2. Clear communication and **information flow between the various ML development stages** can make the difference between making choices that build on top of each other to attenuate the negative potential of bias (multipronged approach to bias mitigation, as in deployment scenario 1 above) _versus_ making choices that compound this negative potential to exacerbate the risk of harm (as in deployment scenario 3).\n\nIn the next section, we review these various stages along with some of the tools that can help us address machine bias at each of them.\n\n\n## Addressing Bias throughout the ML Development Cycle\n\nReady for some practical advice yet? Here we go \ud83e\udd17", "#### Task definition: recommendations\n\nThere are as many ways for the ML task definition and deployment to affect the risk of bias-related harms as there are applications for ML systems. As in the examples above, some common steps that may help decide whether and how to apply ML in a way that minimizes bias-related risk include:\n\n\n\n* Investigate:\n    * Reports of bias in the field pre-ML\n    * At-risk demographic categories for your specific use case\n* Examine:\n    * The impact of your optimization objective on reinforcing biases\n    * Alternative objectives that favor diversity and positive long-term impacts\n\n\n### I am <span style=\"text-decoration:underline;\">curating/picking a dataset</span> for my ML system, how can I address bias?\n\nWhile training datasets are [not the sole source of bias](https://www.cell.com/patterns/fulltext/S2666-3899(21)00061-1) in the ML development cycle, they do play a significant role. Does your [dataset disproportionately associate](https://aclanthology.org/2020.emnlp-main.23/) biographies of women with life events but those of men with achievements? Those **stereotypes** are probably going to show up in your full ML system! Does your voice recognition dataset only feature specific accents? Not a good sign for [the inclusivity of technology](https://www.scientificamerican.com/article/speech-recognition-tech-is-yet-another-example-of-bias/) you build with it in terms of **disparate performance**! Whether you\u2019re curating a dataset for ML applications or selecting a dataset to train an ML model, finding out, mitigating, and [communicating](https://dl.acm.org/doi/10.1145/3479582) to what extent the data exhibits these phenomena are all necessary steps to reducing bias-related risks.", "## Addressing Bias throughout the ML Development Cycle\n\nReady for some practical advice yet? Here we go \ud83e\udd17\n\nThere is no one single way to develop ML systems; which steps happen in what order depends on a number of factors including the development setting (university, large company, startup, grassroots organization, etc\u2026), the modality (text, tabular data, images, etc\u2026), and the preeminence or scarcity of publicly available ML resources. However, we can identify three common stages of particular interest in addressing bias. These are the task definition, the data curation, and the model training. Let\u2019s have a look at how bias handling may differ across these various stages.\n\n<p align=\"center\">\n <br>\n <img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/ethics_soc_2/img_pipeline.png\" alt=\"The Bias ML Pipeline by Meg\" width=\"500\" />\n    <em>The Bias ML Pipeline by <a href=\"https://huggingface.co/meg\">Meg</a></em>\n</p>\n\n### I am <span style=\"text-decoration:underline;\">defining the task</span> of my ML system, how can I address bias?\n\nWhether and to what extent bias in the system concretely affects people ultimately depends on what the system is used for. As such, the first place developers can work to mitigate bias is when deciding how ML fits in their system, e.g., by deciding what optimization objective it will use.", "--\ntitle: \"Let's talk about biases in machine learning! Ethics and Society Newsletter #2\" \nthumbnail: /blog/assets/122_ethics_soc_2/thumbnail-solstice.png\nauthors:\n- user: yjernite\n---\n\n# Machine Learning in development: Let's talk about bias!\n\n\n_Bias in ML is ubiquitous, and Bias in ML is complex; so complex in fact that no single technical intervention is likely to meaningfully address the problems it engenders. ML models, as sociotechnical systems, amplify social trends that may exacerbate inequities and harmful biases in ways that depend on their deployment context and are constantly evolving._\n\n_This means that developing ML systems with care requires vigilance and responding to feedback from those deployment contexts, which in turn we can facilitate by sharing lessons across contexts and developing tools to analyze signs of bias at every level of ML development._\n\n_This blog post from the [Ethics and Society regulars @\ud83e\udd17](https://huggingface.co/blog/ethics-soc-1) shares some of the lessons we have learned along with tools we have developed to support ourselves and others in our community\u2019s efforts to better address bias in Machine Learning. The first part is a broader reflection on bias and its context. If you\u2019ve already read it and are coming back specifically for the tools, feel free to jump to the [datasets](#i-am-curatingpicking-a-dataset-for-my-ml-system-how-can-i-address-bias) or [models](#i-am-trainingselecting-a-model-for-my-ml-system-how-can-i-address-bias)\nsection!_\n\n<p align=\"center\">\n <br>\n <img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/ethics_soc_2/img1.jpg\" alt=\"Selection of tools developed by HF team members to address bias in ML\" />\n    <em>Selection of tools developed by \ud83e\udd17 team members to address bias in ML</em>\n</p>", "**Machine Learning & Sensing Laboratory:** A University of Florida laboratory that develops machine learning methods for autonomously analyzing and understanding sensor data.\n\n#### **1. How has ML made a positive impact on Science**\nML has made a positive impact in a number of ways from helping to automate tedious and/or slow tasks or providing new ways to examine and look at various questions. One example from my work in ML for plant science is that we have developed ML approaches to automate plant root segmentation and characterization in imagery. This task was previously a bottleneck for plant scientists looking at root imagery. By automating this step through ML we can conduct these analyses at a much higher throughput and begin to use this data to investigate plant biology research questions at scale.\n\n#### **2. What are the biggest ML challenges within Scientific research?**\nThere are many challenges. One example is when using ML for Science research, we have to think carefully through the data collection and curation protocols. In some cases, the protocols we used for non-ML analysis are not appropriate or effective. The quality of the data and how representative it is of what we expect to see in the application can make a huge impact on the performance, reliability, and trustworthiness of an ML-based system.\n\n#### **3. What\u2019s a common mistake you see people make trying to integrate ML into Science?**\nRelated to the question above, one common mistake is misinterpreting results or performance to be a function of just the ML system and not also considering the data collection, curation, calibration, and normalization protocols."], "eval_score_GPT4": "4", "eval_feedback_GPT4": "The response provided aligns well with the reference answer, stating that W&B is used to track machine learning experiments at every stage, which is correct and factual. However, the response adds additional details about the aggregation of metrics, customizable dashboards, and addressing bias that are not mentioned in the reference answer. While these details are relevant to W&B's functionality, they are not required by the reference answer. The response could be considered to have provided more information than necessary, but it does not detract from the fact that it correctly identifies the primary purpose of W&B as stated in the reference answer. Therefore, the response is mostly correct, accurate, and factual."}, {"question": "What is the name of the open-source library created by Hugging Face to simplify Transformer acceleration?\n", "true_answer": "Optimum", "source_doc": "huggingface/blog/blob/main/intel.md", "generated_answer": "The name of the open-source library created by Hugging Face to simplify Transformer acceleration is Optimum. It is being developed in collaboration with Graphcore as part of the Hugging Face Hardware Partner Program, and will provide hardware-optimized models certified by Hugging Face. These models will cover a wide range of applications, from vision and speech to translation and text generation, and will be accessible through the new open-source library and toolkit, Optimum. The first IP", "retrieved_docs": ["Making Poplar compatible with these widely used, third-party systems allows developers to easily port their models from their other compute platforms and start taking advantage of the IPU\u2019s advanced AI capabilities.\n\n## Optimising Transformers for Production\n\nTransformers have completely transformed (pun intended) the field of AI. Models such as BERT are widely used by Graphcore customers in a huge array of applications, across NLP and beyond. These multi-talented models can perform feature extraction, text generation, sentiment analysis, translation and many more functions.\n\nAlready, Hugging Face plays host to hundreds of Transformers, from the French-language CamemBERT to ViT which applies lessons learned in NLP to computer vision. The Transformers library is downloaded an average of 2 million times every month and demand is growing.\n\nWith a user base of more than 50,000 developers \u2013 Hugging Face has seen the fastest ever adoption of an open-source project.\n\nNow, with its Hardware Partner Program, Hugging Face is connecting the ultimate Transformer toolset with today's most advanced AI hardware.\n\nUsing Optimum, a new open-source library and toolkit, developers will be able to access hardware-optimized models certified by Hugging Face.\n\nThese are being developed in a collaboration between Graphcore and Hugging Face, with the first IPU-optimized models appearing on Optimum later this year. Ultimately, these will cover a wide range of applications, from vision and speech to translation and text generation.\n\nHugging Face CEO Cl\u00e9ment Delangue said: \u201cDevelopers all want access to the latest and greatest hardware \u2013 like the Graphcore IPU, but there\u2019s always that question of whether they\u2019ll have to learn new code or processes. With Optimum and the Hugging Face Hardware Program, that\u2019s just not an issue. It\u2019s essentially plug-and-play\".\n\n## SOTA Models meet SOTA Hardware\n\nPrior to the announcement of the Hugging Face Partnership, we had demonstrated the power of the IPU to accelerate state-of-the-art Transformer models with a special Graphcore-optimised implementation of Hugging Face BERT using Pytorch.", "Using \ud83e\udd17 `transformers` at Hugging Face\n\n\ud83e\udd17 `transformers` is a library maintained by Hugging Face and the community, for state-of-the-art Machine Learning for Pytorch, TensorFlow and JAX. It provides thousands of pretrained models to perform tasks on different modalities such as text, vision, and audio. We are a bit biased, but we really like \ud83e\udd17 `transformers`!\n\n## Exploring \ud83e\udd17 transformers in the Hub\n\nThere are over 25,000 `transformers` models in the Hub which you can find by filtering at the left of [the models page](https://huggingface.co/models?library=transformers&sort=downloads). \n\nYou can find models for many different tasks:", "## Tweakable, contributor-friendly over abstraction\n\nFor large parts of the library, Diffusers adopts an important design principle of the [Transformers library](https://github.com/huggingface/transformers), which is to prefer copy-pasted code over hasty abstractions. This design principle is very opinionated and stands in stark contrast to popular design principles such as [Don't repeat yourself (DRY)](https://en.wikipedia.org/wiki/Don%27t_repeat_yourself).\nIn short, just like Transformers does for modeling files, Diffusers prefers to keep an extremely low level of abstraction and very self-contained code for pipelines and schedulers.\nFunctions, long code blocks, and even classes can be copied across multiple files which at first can look like a bad, sloppy design choice that makes the library unmaintainable.\n**However**, this design has proven to be extremely successful for Transformers and makes a lot of sense for community-driven, open-source machine learning libraries because:\n- Machine Learning is an extremely fast-moving field in which paradigms, model architectures, and algorithms are changing rapidly, which therefore makes it very difficult to define long-lasting code abstractions.\n- Machine Learning practitioners like to be able to quickly tweak existing code for ideation and research and therefore prefer self-contained code over one that contains many abstractions.\n- Open-source libraries rely on community contributions and therefore must build a library that is easy to contribute to. The more abstract the code, the more dependencies, the harder to read, and the harder to contribute to. Contributors simply stop contributing to very abstract libraries out of fear of breaking vital functionality. If contributing to a library cannot break other fundamental code, not only is it more inviting for potential new contributors, but it is also easier to review and contribute to multiple parts in parallel.\n\nAt Hugging Face, we call this design the **single-file policy** which means that almost all of the code of a certain class should be written in a single, self-contained file. To read more about the philosophy, you can have a look\nat [this blog post](https://huggingface.co/blog/transformers-design-philosophy).", "## Tweakable, contributor-friendly over abstraction\n\nFor large parts of the library, Diffusers adopts an important design principle of the [Transformers library](https://github.com/huggingface/transformers), which is to prefer copy-pasted code over hasty abstractions. This design principle is very opinionated and stands in stark contrast to popular design principles such as [Don't repeat yourself (DRY)](https://en.wikipedia.org/wiki/Don%27t_repeat_yourself).\nIn short, just like Transformers does for modeling files, Diffusers prefers to keep an extremely low level of abstraction and very self-contained code for pipelines and schedulers.\nFunctions, long code blocks, and even classes can be copied across multiple files which at first can look like a bad, sloppy design choice that makes the library unmaintainable.\n**However**, this design has proven to be extremely successful for Transformers and makes a lot of sense for community-driven, open-source machine learning libraries because:\n- Machine Learning is an extremely fast-moving field in which paradigms, model architectures, and algorithms are changing rapidly, which therefore makes it very difficult to define long-lasting code abstractions.\n- Machine Learning practitioners like to be able to quickly tweak existing code for ideation and research and therefore prefer self-contained code over one that contains many abstractions.\n- Open-source libraries rely on community contributions and therefore must build a library that is easy to contribute to. The more abstract the code, the more dependencies, the harder to read, and the harder to contribute to. Contributors simply stop contributing to very abstract libraries out of fear of breaking vital functionality. If contributing to a library cannot break other fundamental code, not only is it more inviting for potential new contributors, but it is also easier to review and contribute to multiple parts in parallel.\n\nAt Hugging Face, we call this design the **single-file policy** which means that almost all of the code of a certain class should be written in a single, self-contained file. To read more about the philosophy, you can have a look\nat [this blog post](https://huggingface.co/blog/transformers-design-philosophy).", "---\n\nThanks for reading! If you are as excited as I am about accelerating Transformers, make them efficient and scale them to billions of requests. You should apply, [we are hiring](https://apply.workable.com/huggingface/#jobs).\ud83d\ude80\n\nIf you have any questions, feel free to contact me, through [Github](https://github.com/huggingface/optimum/issues), or on the [forum](https://discuss.huggingface.co/c/optimum/59). You can also connect with me on [Twitter](https://twitter.com/_philschmid) or [LinkedIn](https://www.linkedin.com/in/philipp-schmid-a6a2bb196/).", "The Hugging Face Course\n\nThis repo contains the content that's used to create the **[Hugging Face course](https://huggingface.co/course/chapter1/1)**. The course teaches you about applying Transformers to various tasks in natural language processing and beyond. Along the way, you'll learn how to use the [Hugging Face](https://huggingface.co/) ecosystem \u2014 [\ud83e\udd17 Transformers](https://github.com/huggingface/transformers), [\ud83e\udd17 Datasets](https://github.com/huggingface/datasets), [\ud83e\udd17 Tokenizers](https://github.com/huggingface/tokenizers), and [\ud83e\udd17 Accelerate](https://github.com/huggingface/accelerate) \u2014 as well as the [Hugging Face Hub](https://huggingface.co/models). It's completely free and open-source!\n\n## \ud83c\udf0e Languages and translations", "!--Copyright 2020 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\n\nhttp://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License.\n\n\u26a0\ufe0f Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\nrendered properly in your Markdown viewer.\n\n-->\n\n# Philosophy\n\n\ud83e\udd17 Transformers is an opinionated library built for:\n\n- machine learning researchers and educators seeking to use, study or extend large-scale Transformers models.\n- hands-on practitioners who want to fine-tune those models or serve them in production, or both.\n- engineers who just want to download a pretrained model and use it to solve a given machine learning task.\n\nThe library was designed with two strong goals in mind:\n\n1. Be as easy and fast to use as possible:"], "eval_score_GPT4": "5", "eval_feedback_GPT4": "The response correctly identifies \"Optimum\" as the open-source library created by Hugging Face to simplify Transformer acceleration. The additional information provided about the collaboration with Graphcore and the Hugging Face Hardware Partner Program, as well as the applications and accessibility of the models, is accurate and relevant, although not required by the instruction. The response goes beyond the reference answer but remains factual and correct in the context of the question asked."}, {"question": "What parameter is used to ensure that elements in a row have the same height in Gradio?\n", "true_answer": "equal_height", "source_doc": "gradio-app/gradio/blob/main/guides/cn/03_building-with-blocks/02_controlling-layout.md", "generated_answer": "The `min_height` parameter is used to ensure that elements in a row have the same height in Gradio. This parameter is available for components that support it, and it sets a minimum height for the component. If the row does not have sufficient space to satisfy all `min_height` values, the row will wrap.\n\nHowever, in the context provided, the focus is on dimensions and layout components, specifically Columns, Rows, Tabs, and Accordions.", "retrieved_docs": ["- `min_width` will set the minimum width the element will take. The Row will wrap if there isn't sufficient space to satisfy all `min_width` values.\n\nLearn more about Rows in the [docs](https://gradio.app/docs/#row).\n\n## Columns and Nesting\n\nComponents within a Column will be placed vertically atop each other. Since the vertical layout is the default layout for Blocks apps anyway, to be useful, Columns are usually nested within Rows. For example:\n\n$code_rows_and_columns\n$demo_rows_and_columns\n\nSee how the first column has two Textboxes arranged vertically. The second column has an Image and Button arranged vertically. Notice how the relative widths of the two columns is set by the `scale` parameter. The column with twice the `scale` value takes up twice the width.\n\nLearn more about Columns in the [docs](https://gradio.app/docs/#column).\n\n# Dimensions\n\nYou can control the height and width of various components, where the parameters are available. These parameters accept either a number (interpreted as pixels) or a string. Using a string allows the direct application of any CSS unit to the encapsulating Block element, catering to more specifc design requirements. When omitted, Gradio uses default dimensions suited for most use cases.\n\nBelow is an example illustrating the use of viewport width (vw):\n\n```python\nimport gradio as gr\n\nwith gr.Blocks() as demo:\n    im = gr.ImageEditor(\n        width=\"50vw\",\n    )\n\ndemo.launch()\n```\n\nWhen using percentage values for dimensions, you may want to define a parent component with an absolute unit (e.g. `px` or `vw`). This approach ensures that child components with relative dimensions are sized appropriately:\n\n\n```python\nimport gradio as gr\n\ncss = \"\"\"\n.container {\n    height: 100vh;\n}\n\"\"\"", "- [#5283](https://github.com/gradio-app/gradio/pull/5283) [`a7460557`](https://github.com/gradio-app/gradio/commit/a74605572dd0d6bb41df6b38b120d656370dd67d) - Add height parameter and scrolling to `gr.Dataframe`. Thanks [@abidlabs](https://github.com/abidlabs)!", "- [#5283](https://github.com/gradio-app/gradio/pull/5283) [`a7460557`](https://github.com/gradio-app/gradio/commit/a74605572dd0d6bb41df6b38b120d656370dd67d) - Add height parameter and scrolling to `gr.Dataframe`.  Thanks [@abidlabs](https://github.com/abidlabs)!\n- [#5232](https://github.com/gradio-app/gradio/pull/5232) [`c57d4c23`](https://github.com/gradio-app/gradio/commit/c57d4c232a97e03b4671f9e9edc3af456438fe89) - `gr.Radio` and `gr.CheckboxGroup` can now accept different names and values.  Thanks [@abidlabs](https://github.com/abidlabs)!\n- [#5219](https://github.com/gradio-app/gradio/pull/5219) [`e8fd4e4e`](https://github.com/gradio-app/gradio/commit/e8fd4e4ec68a6c974bc8c84b61f4a0ec50a85bc6) - Add `api_name` parameter to `gr.Interface`. Additionally, completely hide api page if show_api=False.  Thanks [@freddyaboulton](https://github.com/freddyaboulton)!", "- [#5283](https://github.com/gradio-app/gradio/pull/5283) [`a7460557`](https://github.com/gradio-app/gradio/commit/a74605572dd0d6bb41df6b38b120d656370dd67d) - Add height parameter and scrolling to `gr.Dataframe`.  Thanks [@abidlabs](https://github.com/abidlabs)!\n- [#5232](https://github.com/gradio-app/gradio/pull/5232) [`c57d4c23`](https://github.com/gradio-app/gradio/commit/c57d4c232a97e03b4671f9e9edc3af456438fe89) - `gr.Radio` and `gr.CheckboxGroup` can now accept different names and values.  Thanks [@abidlabs](https://github.com/abidlabs)!\n- [#5219](https://github.com/gradio-app/gradio/pull/5219) [`e8fd4e4e`](https://github.com/gradio-app/gradio/commit/e8fd4e4ec68a6c974bc8c84b61f4a0ec50a85bc6) - Add `api_name` parameter to `gr.Interface`. Additionally, completely hide api page if show_api=False.  Thanks [@freddyaboulton](https://github.com/freddyaboulton)!", "- ensure defaults height match for media inputs by [@pngwn](https://github.com/pngwn) in [PR 1236](https://github.com/gradio-app/gradio/pull/1236)\n- Default Label label value by [@radames](https://github.com/radames) in [PR 1239](https://github.com/gradio-app/gradio/pull/1239)\n- update-shortcut-syntax by [@FarukOzderim](https://github.com/FarukOzderim) in [PR 1234](https://github.com/gradio-app/gradio/pull/1234)\n- Update version.txt by [@FarukOzderim](https://github.com/FarukOzderim) in [PR 1244](https://github.com/gradio-app/gradio/pull/1244)\n- Layout bugs by [@pngwn](https://github.com/pngwn) in [PR 1246](https://github.com/gradio-app/gradio/pull/1246)\n- Update demo by [@FarukOzderim](https://github.com/FarukOzderim) in [PR 1253](https://github.com/gradio-app/gradio/pull/1253)\n- Button default name by [@FarukOzderim](https://github.com/FarukOzderim) in [PR 1243](https://github.com/gradio-app/gradio/pull/1243)", "- ensure defaults height match for media inputs by [@pngwn](https://github.com/pngwn) in [PR 1236](https://github.com/gradio-app/gradio/pull/1236)\n- Default Label label value by [@radames](https://github.com/radames) in [PR 1239](https://github.com/gradio-app/gradio/pull/1239)\n- update-shortcut-syntax by [@FarukOzderim](https://github.com/FarukOzderim) in [PR 1234](https://github.com/gradio-app/gradio/pull/1234)\n- Update version.txt by [@FarukOzderim](https://github.com/FarukOzderim) in [PR 1244](https://github.com/gradio-app/gradio/pull/1244)\n- Layout bugs by [@pngwn](https://github.com/pngwn) in [PR 1246](https://github.com/gradio-app/gradio/pull/1246)\n- Update demo by [@FarukOzderim](https://github.com/FarukOzderim) in [PR 1253](https://github.com/gradio-app/gradio/pull/1253)\n- Button default name by [@FarukOzderim](https://github.com/FarukOzderim) in [PR 1243](https://github.com/gradio-app/gradio/pull/1243)", "```python\nimport gradio as gr\n\ncss = \"\"\"\n.container {\n    height: 100vh;\n}\n\"\"\"\n\nwith gr.Blocks(css=css) as demo:\n    with gr.Column(elem_classes=[\"container\"]):\n        name = gr.Chatbot(value=[[\"1\", \"2\"]], height=\"70%\")\n\ndemo.launch()\n```\n\nIn this example, the Column layout component is given a height of 100% of the viewport height (100vh), and the Chatbot component inside it takes up 70% of the Column's height.\n\nYou can apply any valid CSS unit for these parameters. For a comprehensive list of CSS units, refer to [this guide](https://www.w3schools.com/cssref/css_units.php). We recommend you always consider responsiveness and test your interfaces on various screen sizes to ensure a consistent user experience.\n\n\n\n## Tabs and Accordions\n\nYou can also create Tabs using the `with gr.Tab('tab_name'):` clause. Any component created inside of a `with gr.Tab('tab_name'):` context appears in that tab. Consecutive Tab clauses are grouped together so that a single tab can be selected at one time, and only the components within that Tab's context are shown.\n\nFor example:\n\n$code_blocks_flipper\n$demo_blocks_flipper\n\nAlso note the `gr.Accordion('label')` in this example. The Accordion is a layout that can be toggled open or closed. Like `Tabs`, it is a layout element that can selectively hide or show content. Any components that are defined inside of a `with gr.Accordion('label'):` will be hidden or shown when the accordion's toggle icon is clicked.\n\nLearn more about [Tabs](https://gradio.app/docs/#tab) and [Accordions](https://gradio.app/docs/#accordion) in the docs."], "eval_score_GPT4": "1", "eval_feedback_GPT4": "Feedback: The response provided mentions the `min_height` parameter, which is not the correct parameter for ensuring elements in a row have the same height in Gradio. The correct parameter, as per the reference answer, is `equal_height`. Therefore, the response is completely incorrect as it does not match the reference answer and does not accurately represent the functionality for maintaining equal height of elements in a row in Gradio."}, {"question": "What is the command to install the latest version of Optimum with OpenVINO support?\n", "true_answer": "pip install --upgrade-strategy eager optimum[\"openvino\"]", "source_doc": "huggingface/diffusers/blob/main/docs/source/en/optimization/open_vino.md", "generated_answer": "To install the latest version of Optimum with OpenVINO support, you can use the following command:\n\n```bash\npython -m pip install optimum[openvino,nncf]\n```\n\nThis command will install the main package of Optimum as well as the required dependencies for OpenVINO and NNCF (Neural Network Compact Format). Note that you should have already installed OpenVINO separately before running this command. If", "retrieved_docs": ["## Optimum Intel\n\n### OpenVINO", "## Optimum Intel\n\n### OpenVINO", "| [How to run inference with OpenVINO](https://github.com/huggingface/optimum-intel/blob/main/notebooks/openvino/optimum_openvino_inference.ipynb) | Explains how to export your model to OpenVINO and run inference with OpenVINO Runtime on various tasks| [![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/huggingface/optimum-intel/blob/main/notebooks/openvino/optimum_openvino_inference.ipynb)| [![Open in AWS Studio](https://studiolab.sagemaker.aws/studiolab.svg)](https://studiolab.sagemaker.aws/import/github/huggingface/optimum-intel/blob/main/notebooks/openvino/optimum_openvino_inference.ipynb)|", "| [How to run inference with OpenVINO](https://github.com/huggingface/optimum-intel/blob/main/notebooks/openvino/optimum_openvino_inference.ipynb) | Explains how to export your model to OpenVINO and run inference with OpenVINO Runtime on various tasks| [![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/huggingface/optimum-intel/blob/main/notebooks/openvino/optimum_openvino_inference.ipynb)| [![Open in AWS Studio](https://studiolab.sagemaker.aws/studiolab.svg)](https://studiolab.sagemaker.aws/import/github/huggingface/optimum-intel/blob/main/notebooks/openvino/optimum_openvino_inference.ipynb)|", "# How to contribute to Optimum?\n\nOptimum is an open source project, so all contributions and suggestions are welcome.\n\nYou can contribute in many different ways: giving ideas, answering questions, reporting bugs, proposing enhancements, improving the documentation, fixing bugs,...\n\nMany thanks in advance to every contributor.\n\n## How to work on an open Issue?\nYou have the list of open Issues at: https://github.com/huggingface/optimum/issues\n\nSome of them may have the label `help wanted`: that means that any contributor is welcomed!\n\nIf you would like to work on any of the open Issues:\n\n1. Make sure it is not already assigned to someone else. You have the assignee (if any) on the top of the right column of the Issue page.\n\n2. You can self-assign it by commenting on the Issue page with one of the keywords: `#take` or `#self-assign`.\n\n3. Work on your self-assigned issue and eventually create a Pull Request.\n\n## How to create a Pull Request?\n1. Fork the [repository](https://github.com/huggingface/optimum) by clicking on the 'Fork' button on the repository's page. This creates a copy of the code under your GitHub user account.\n\n2. Clone your fork to your local disk, and add the base repository as a remote:\n\n\t```bash\n\tgit clone git@github.com:<your Github handle>/optimum.git\n\tcd optimum\n\tgit remote add upstream https://github.com/huggingface/optimum.git\n\t```\n\n3. Create a new branch to hold your development changes:\n\n\t```bash\n\tgit checkout -b a-descriptive-name-for-my-changes\n\t```\n\n\t**do not** work on the `main` branch.\n\n4. Set up a development environment by running the following command in a virtual environment:\n\n\t```bash\n\tpip install -e \".[dev]\"\n\t```", "If you'd like to use the accelerator-specific features of \ud83e\udd17 Optimum, you can install the required dependencies according to the table below:\n\n| Accelerator                                                                                                            | Installation                                      |\n|:-----------------------------------------------------------------------------------------------------------------------|:--------------------------------------------------|\n| [ONNX runtime](https://onnxruntime.ai/docs/)                                                                           | `pip install --upgrade-strategy eager install optimum[onnxruntime]`|\n| [Intel Neural Compressor (INC)](https://www.intel.com/content/www/us/en/developer/tools/oneapi/neural-compressor.html) | `pip install --upgrade-strategy eager optimum[neural-compressor]`  |\n| [Intel OpenVINO](https://docs.openvino.ai/latest/index.html)                                                           | `pip install --upgrade-strategy eager optimum[openvino,nncf]`      |\n| [AMD Instinct GPUs and Ryzen AI NPU](https://www.amd.com/en/graphics/instinct-server-accelerators)                                      | `pip install --upgrade-strategy eager optimum[amd]`                |\n| [Habana Gaudi Processor (HPU)](https://habana.ai/training/)                                                            | `pip install --upgrade-strategy eager optimum[habana]`             |\n| [FuriosaAI](https://www.furiosa.ai/)                                                                                   | `pip install --upgrade-strategy eager optimum[furiosa]`            |", "!--Copyright 2022 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\n\nhttp://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License.\n-->\n\n# Installation\n\n\ud83e\udd17 Optimum can be installed using `pip` as follows:\n\n```bash\npython -m pip install optimum\n```\n\nIf you'd like to use the accelerator-specific features of \ud83e\udd17 Optimum, you can install the required dependencies according to the table below:"], "eval_score_GPT4": "4", "eval_feedback_GPT4": "Feedback: The response provided is mostly correct, accurate, and factual. It correctly identifies the need to use pip for installing Optimum with OpenVINO support and includes the necessary extras for OpenVINO and NNCF. However, it does not specify the use of the `--upgrade-strategy eager` option, which is present in the reference answer. This option ensures that dependencies are upgraded to the latest version, which could be important for the latest support. The response also adds additional information about the need to have OpenVINO installed separately, which is not incorrect but is not specified in the reference answer. Based on the rubric, the response is not completely correct due to the missing upgrade strategy but is mostly correct."}]