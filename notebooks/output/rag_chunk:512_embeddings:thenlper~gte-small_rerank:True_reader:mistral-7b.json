[{"question": "What architecture is the `tokenizers-linux-x64-musl` binary designed for?\n", "true_answer": "x86_64-unknown-linux-musl", "source_doc": "huggingface/tokenizers/blob/main/bindings/node/npm/linux-x64-musl/README.md", "generated_answer": "The `tokenizers-linux-x64-musl` binary is designed for the **x86_64-unknown-linux-musl** architecture. (Answer according to context provided.)", "retrieved_docs": ["`tokenizers-linux-x64-musl`\n\nThis is the **x86_64-unknown-linux-musl** binary for `tokenizers`", "`tokenizers-linux-arm64-musl`\n\nThis is the **aarch64-unknown-linux-musl** binary for `tokenizers`", "`tokenizers-linux-x64-gnu`\n\nThis is the **x86_64-unknown-linux-gnu** binary for `tokenizers`", "`tokenizers-linux-arm64-gnu`\n\nThis is the **aarch64-unknown-linux-gnu** binary for `tokenizers`", "`tokenizers-android-arm64`\n\nThis is the **aarch64-linux-android** binary for `tokenizers`", "`tokenizers-freebsd-x64`\n\nThis is the **x86_64-unknown-freebsd** binary for `tokenizers`", "`tokenizers-win32-x64-msvc`\n\nThis is the **x86_64-pc-windows-msvc** binary for `tokenizers`"], "eval_score_GPT4": "5", "eval_feedback_GPT4": "Feedback: The response provided is completely correct, accurate, and factual. It matches the reference answer exactly, indicating that the `tokenizers-linux-x64-musl` binary is designed for the x86_64-unknown-linux-musl architecture."}, {"question": "What is the purpose of the BLIP-Diffusion model?\n", "true_answer": "The BLIP-Diffusion model is designed for controllable text-to-image generation and editing.", "source_doc": "huggingface/diffusers/blob/main/docs/source/en/api/pipelines/blip_diffusion.md", "generated_answer": "The BLIP-Diffusion model is not mentioned in the provided context. Therefore, it is not possible to answer the question about its purpose. Please provide more context or clarify the question to enable a response.", "retrieved_docs": ["!--Copyright 2023 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\n\nhttp://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License.\n-->\n\n# Stable Diffusion pipelines\n\nStable Diffusion is a text-to-image latent diffusion model created by the researchers and engineers from [CompVis](https://github.com/CompVis), [Stability AI](https://stability.ai/) and [LAION](https://laion.ai/). Latent diffusion applies the diffusion process over a lower dimensional latent space to reduce memory and compute complexity. This specific type of diffusion model was proposed in [High-Resolution Image Synthesis with Latent Diffusion Models](https://huggingface.co/papers/2112.10752) by Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, Bj\u00f6rn Ommer.\n\nStable Diffusion is trained on 512x512 images from a subset of the LAION-5B dataset. This model uses a frozen CLIP ViT-L/14 text encoder to condition the model on text prompts. With its 860M UNet and 123M text encoder, the model is relatively lightweight and can run on consumer GPUs.\n\nFor more details about how Stable Diffusion works and how it differs from the base latent diffusion model, take a look at the Stability AI [announcement](https://stability.ai/blog/stable-diffusion-announcement) and our own [blog post](https://huggingface.co/blog/stable_diffusion#how-does-stable-diffusion-work) for more technical details.", "## What about the general-purpose Stable Diffusion model?\n\nAs we showed with the Pokemon image generation task, it is possible to achieve a high level of optimization of the Stable Diffusion pipeline when using a relatively small amount of training resources. At the same time, it is well-known that training a general-purpose Stable Diffusion model is an [expensive task](https://www.mosaicml.com/blog/training-stable-diffusion-from-scratch-part-2). However, with enough budget and HW resources, it is possible to optimize the general-purpose model using the described approach and tune it to produce high-quality images. The only caveat we have is related to the token merging method that reduces the model capacity substantially. The rule of thumb here is the more complicated the dataset you have for the training, the less merging ratio you should use during the optimization.\n\nIf you enjoyed reading this post, you might also be interested in checking out [this post](https://huggingface.co/blog/stable-diffusion-inference-intel) that discusses other complementary approaches to optimize the performance of Stable Diffusion on 4th generation Intel Xeon CPUs.", "**Note**: It is highly recommended to have a basic understanding of how diffusion models work. If diffusion \nmodels are completely new to you, we recommend reading one of the following blog posts:\n- [The Annotated Diffusion Model](https://huggingface.co/blog/annotated-diffusion)\n- [Getting started with \ud83e\udde8 Diffusers](https://colab.research.google.com/github/huggingface/notebooks/blob/main/diffusers/diffusers_intro.ipynb)\n\nNow, let's get started by generating some images \ud83c\udfa8.\n\n## Running Stable Diffusion\n\n### License\n\nBefore using the model, you need to accept the model [license](https://huggingface.co/spaces/CompVis/stable-diffusion-license) in order to download and use the weights. **Note: the license does not need to be explicitly accepted through the UI anymore**.\n\nThe license is designed to mitigate the potential harmful effects of such a powerful machine learning system. \nWe request users to **read the license entirely and carefully**. Here we offer a summary:\n1. You can't use the model to deliberately produce nor share illegal or harmful outputs or content,\n2. We claim no rights on the outputs you generate, you are free to use them and are accountable for their use which should not go against the provisions set in the license, and\n3. You may re-distribute the weights and use the model commercially and/or as a service. If you do, please be aware you have to include the same use restrictions as the ones in the license and share a copy of the CreativeML OpenRAIL-M to all your users.\n\n### Usage\n\nFirst, you should install `diffusers==0.10.2` to run the following code snippets:\n\n```bash\npip install diffusers==0.10.2 transformers scipy ftfy accelerate\n```", "<Tip>\n\nThe quicktour is a simplified version of the introductory \ud83e\udde8 Diffusers [notebook](https://colab.research.google.com/github/huggingface/notebooks/blob/main/diffusers/diffusers_intro.ipynb) to help you get started quickly. If you want to learn more about \ud83e\udde8 Diffusers' goal, design philosophy, and additional details about its core API, check out the notebook!\n\n</Tip>\n\nBefore you begin, make sure you have all the necessary libraries installed:\n\n```py\n# uncomment to install the necessary libraries in Colab\n#!pip install --upgrade diffusers accelerate transformers\n```\n\n- [\ud83e\udd17 Accelerate](https://huggingface.co/docs/accelerate/index) speeds up model loading for inference and training.\n- [\ud83e\udd17 Transformers](https://huggingface.co/docs/transformers/index) is required to run the most popular diffusion models, such as [Stable Diffusion](https://huggingface.co/docs/diffusers/api/pipelines/stable_diffusion/overview).\n\n## DiffusionPipeline\n\nThe [`DiffusionPipeline`] is the easiest way to use a pretrained diffusion system for inference. It is an end-to-end system containing the model and the scheduler. You can use the [`DiffusionPipeline`] out-of-the-box for many tasks. Take a look at the table below for some supported tasks, and for a complete list of supported tasks, check out the [\ud83e\udde8 Diffusers Summary](./api/pipelines/overview#diffusers-summary) table.", "--\ntitle: \"VQ-Diffusion\" \nthumbnail: /blog/assets/117_vq_diffusion/thumbnail.png\nauthors:\n- user: williamberman\n---\n\n# VQ-Diffusion\n\n\nVector Quantized Diffusion (VQ-Diffusion) is a conditional latent diffusion model developed by the University of Science and Technology of China and Microsoft. Unlike most commonly studied diffusion models, VQ-Diffusion's noising and denoising processes operate on a quantized latent space, i.e., the latent space is composed of a discrete set of vectors. Discrete diffusion models are less explored than their continuous counterparts and offer an interesting point of comparison with autoregressive (AR) models.\n\n- [Hugging Face model card](https://huggingface.co/microsoft/vq-diffusion-ithq)\n- [Hugging Face Spaces](https://huggingface.co/spaces/patrickvonplaten/vq-vs-stable-diffusion)\n- [Original Implementation](https://github.com/microsoft/VQ-Diffusion)\n- [Paper](https://arxiv.org/abs/2111.14822)\n\n### Demo\n\n\ud83e\udde8 Diffusers lets you run VQ-Diffusion with just a few lines of code.\n\nInstall dependencies\n```bash\npip install 'diffusers[torch]' transformers ftfy\n```\n\nLoad the pipeline\n```python\nfrom diffusers import VQDiffusionPipeline\n\npipe = VQDiffusionPipeline.from_pretrained(\"microsoft/vq-diffusion-ithq\")\n```\n\nIf you want to use FP16 weights\n```python\nfrom diffusers import VQDiffusionPipeline\nimport torch\n\npipe = VQDiffusionPipeline.from_pretrained(\"microsoft/vq-diffusion-ithq\", torch_dtype=torch.float16, revision=\"fp16\")\n```\n\nMove to GPU\n```python\npipe.to(\"cuda\")\n```\n\nRun the pipeline!", "--\ntitle: The Annotated Diffusion Model\nthumbnail: /blog/assets/78_annotated-diffusion/thumbnail.png\nauthors:\n- user: nielsr\n- user: kashif\n---\n\n# The Annotated Diffusion Model\n\n\n<script async defer src=\"https://unpkg.com/medium-zoom-element@0/dist/medium-zoom-element.min.js\"></script>\n\n<a target=\"_blank\" href=\"https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/annotated_diffusion.ipynb\">\n    <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n</a>\n\nIn this blog post, we'll take a deeper look into **Denoising Diffusion Probabilistic Models** (also known as DDPMs, diffusion models, score-based generative models or simply [autoencoders](https://benanne.github.io/2022/01/31/diffusion.html)) as researchers have been able to achieve remarkable results with them for (un)conditional image/audio/video generation. Popular examples (at the time of writing) include [GLIDE](https://arxiv.org/abs/2112.10741) and [DALL-E 2](https://openai.com/dall-e-2/) by OpenAI, [Latent Diffusion](https://github.com/CompVis/latent-diffusion) by the University of Heidelberg and [ImageGen](https://imagen.research.google/) by Google Brain.", "!--Copyright 2023 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\n\nhttp://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License.\n-->\n\n# Text-to-image\n\n[[open-in-colab]]\n\nWhen you think of diffusion models, text-to-image is usually one of the first things that come to mind. Text-to-image generates an image from a text description (for example, \"Astronaut in a jungle, cold color palette, muted colors, detailed, 8k\") which is also known as a *prompt*.\n\nFrom a very high level, a diffusion model takes a prompt and some random initial noise, and iteratively removes the noise to construct an image. The *denoising* process is guided by the prompt, and once the denoising process ends after a predetermined number of time steps, the image representation is decoded into an image.\n\n<Tip>\n\nRead the [How does Stable Diffusion work?](https://huggingface.co/blog/stable_diffusion#how-does-stable-diffusion-work) blog post to learn more about how a latent diffusion model works.\n\n</Tip>\n\nYou can generate images from a prompt in \ud83e\udd17 Diffusers in two steps:\n\n1. Load a checkpoint into the [`AutoPipelineForText2Image`] class, which automatically detects the appropriate pipeline class to use based on the checkpoint:\n\n```py\nfrom diffusers import AutoPipelineForText2Image\nimport torch"], "eval_score_GPT4": "1", "eval_feedback_GPT4": "Feedback: The response avoids providing any factual information about the BLIP-Diffusion model by stating that the model is not mentioned and more context is needed. This indicates a lack of attempt to answer the question based on the knowledge of the model's purpose. According to the score rubric, the response should be factual and accurate in describing the purpose of the BLIP-Diffusion model. Since the response does not provide any information about the model's purpose, it cannot be considered correct or factual in relation to the reference answer, which clearly states the model's purpose. Therefore, the response must be rated as completely incorrect and not factual."}, {"question": "How can a user claim authorship of a paper on the Hugging Face Hub?\n", "true_answer": "By clicking their name on the corresponding Paper page and clicking \"claim authorship\", then confirming the request in paper settings for admin team validation.", "source_doc": "huggingface/hub-docs/blob/main/docs/hub/paper-pages.md", "generated_answer": "To claim authorship of a paper on the Hugging Face Hub, a user should follow these steps:\n\n1. Go to the corresponding Paper page on the Hub.\n2. Click on the user's name in the Paper page.\n3. Click on \"Claim authorship\" in the pop-up window.\n4. This will redirect the user to the Paper settings page.\n5. Confirm the request by clicking on \"Confirm\" in the Paper settings page.", "retrieved_docs": ["## Claiming authorship to a Paper\n\nThe Hub will attempt to automatically match paper to users based on their email. \n\n<div class=\"flex justify-center\">\n<img class=\"block dark:hidden\" width=\"300\" src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/hub/papers-authors.png\"/>\n<img class=\"hidden dark:block\" width=\"300\" src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/hub/papers-authors-dark.png\"/>\n</div>\n\nIf your paper is not linked to your account, you can click in your name in the corresponding Paper page and click \"claim authorship\". This will automatically re-direct to your paper settings where you can confirm the request. The admin team will validate your request soon. Once confirmed, the Paper page will show as verified.\n\n<div class=\"flex justify-center\">\n<img class=\"block dark:hidden\" width=\"300\" src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/hub/papers-settings.png\"/>\n<img class=\"hidden dark:block\" width=\"300\" src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/hub/papers-settings-dark.png\"/>\n</div>\n\n\n## Frequently Asked Questions\n\n### Can I control which Paper pages show in my profile?\n\nYes! You can visit your Papers in [settings](https://huggingface.co/settings/papers), where you will see a list of verified papers. There, you can click the \"Show on profile\" checkbox to hide/show it in your profile. \n\n### Do you support ACL anthology?", "By clicking on **Agree**, you agree to share your username and email address with the model authors. In some cases, additional fields might be requested. To help the model authors decide whether to grant you access, try to fill out the form as completely as possible.\n\nOnce the access request is sent, there are two possibilities. If the approval mechanism is automatic, you immediately get access to the model files. Otherwise, the requests have to be approved manually by the authors, which can take more time. \n\n<Tip warning={true}>\n\nThe model authors have complete control over model access. In particular, they can decide at any time to block your access to the model without prior notice, regardless of approval mechanism or if your request has already been approved.\n\n</Tip>\n\n### Download files\n\nTo download files from a gated model you'll need to be authenticated. In the browser, this is automatic as long as you are logged in with your account. If you are using a script, you will need to provide a [user token](./security-tokens). In the Hugging Face Python ecosystem (`transformers`, `diffusers`, `datasets`, etc.), you can login your machine using the [`huggingface_hub`](https://huggingface.co/docs/huggingface_hub/index) library and running in your terminal:\n\n```bash\nhuggingface-cli login\n```\n\nAlternatively, you can programmatically login using `login()` in a notebook or a script:\n\n```python\n>>> from huggingface_hub import login\n>>> login()\n```\n\nYou can also provide the `token` parameter to most loading methods in the libraries (`from_pretrained`, `hf_hub_download`, `load_dataset`, etc.), directly from your scripts.\n\nFor more details about how to login, check out the [login guide](https://huggingface.co/docs/huggingface_hub/quick-start#login).", "Paper Pages\n\nPaper pages allow people to find artifacts related to a paper such as models, datasets and apps/demos (Spaces). Paper pages also enable the community to discuss about the paper.\n\n<div class=\"flex justify-center\">\n<img class=\"block dark:hidden\" width=\"300\" src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/hub/papers-discussions.png\"/>\n<img class=\"hidden dark:block\" width=\"300\" src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/hub/papers-discussions-dark.png\"/>\n</div>\n\n## Linking a Paper to a model, dataset or Space\n\nIf the repository card (`README.md`) includes a link to a paper on arXiv, the Hugging Face Hub will extract the arXiv ID and include it in the repository's tags. Clicking on the arxiv tag will let you:\n\n* Visit the Paper page.\n* Filter for other models or datasets on the Hub that cite the same paper.\n\n<div class=\"flex justify-center\">\n<img class=\"block dark:hidden\" width=\"300\" src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/hub/datasets-arxiv.png\"/>\n<img class=\"hidden dark:block\" width=\"300\" src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/hub/datasets-arxiv-dark.png\"/>\n</div>\n\n## Claiming authorship to a Paper\n\nThe Hub will attempt to automatically match paper to users based on their email.", "### Do you support ACL anthology?\n\nWe're starting with Arxiv as it accounts for 95% of the paper URLs Hugging Face users have linked in their repos organically. We'll check how this evolve and potentially extend to other paper hosts in the future.\n\n### Can I have a Paper page even if I have no model/dataset/Space?\n\nYes. You can go to [the main Papers page](https://huggingface.co/papers), click search and write the name of the paper or the full Arxiv id. If the paper does not exist, you will get an option to index it. You can also just visit the page `hf.co/papers/xxxx.yyyyy` replacing with the arxiv id of the paper you wish to index.", "## How to use User Access Tokens?\n\nThere are plenty of ways to use a User Access Token to access the Hugging Face Hub, granting you the flexibility you need to build awesome apps on top of it.\n\nUser Access Tokens can be:\n- used **in place of a password** to access the Hugging Face Hub with git or with basic authentication.\n- passed as a **bearer token** when calling the [Inference API](https://huggingface.co/inference-api).\n- used in the Hugging Face Python libraries, such as `transformers` or `datasets`:\n\n```python\nfrom transformers import AutoModel\n\naccess_token = \"hf_...\"\n\nmodel = AutoModel.from_pretrained(\"private/model\", token=access_token)\n```\n\n<Tip warning={true}>\nTry not to leak your token! Though you can always rotate it, anyone will be able to read or write your private repos in the meantime which is \ud83d\udca9\n</Tip>\n\n### Best practices\n\nWe recommend you create one access token per app or usage. For instance, you could have a separate token for:\n * A local machine.\n * A Colab notebook.\n * An awesome custom inference server. \n \n This way, you can invalidate one token without impacting your other usages.\n\nWe also recommend only giving the appropriate role to each token you create. If you only need read access (e.g., loading a dataset with the `datasets` library or retrieving the weights of a model), only give your access token the `read` role.", "Requesting access can only be done from your browser. Go to the dataset on the Hub and you will be prompted to share your information:\n\n<div class=\"flex justify-center\">\n    <img class=\"block dark:hidden\" src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/hub/datasets-gated-user-side.png\"/>\n    <img class=\"hidden dark:block\" src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/hub/datasets-gated-user-side-dark.png\"/>\n</div>\n\nBy clicking on **Agree**, you agree to share your username and email address with the dataset authors. In some cases, additional fields might be requested. To help the dataset authors decide whether to grant you access, try to fill out the form as completely as possible.\n\nOnce the access request is sent, there are two possibilities. If the approval mechanism is automatic, you immediately get access to the dataset files. Otherwise, the requests have to be approved manually by the authors, which can take more time.\n\n<Tip warning={true}>\n\nThe dataset authors have complete control over dataset access. In particular, they can decide at any time to block your access to the dataset without prior notice, regardless of approval mechanism or if your request has already been approved.\n\n</Tip>\n\n### Download files\n\nTo download files from a gated dataset you'll need to be authenticated. In the browser, this is automatic as long as you are logged in with your account. If you are using a script, you will need to provide a [user token](./security-tokens). In the Hugging Face Python ecosystem (`transformers`, `diffusers`, `datasets`, etc.), you can login your machine using the [`huggingface_hub`](https://huggingface.co/docs/huggingface_hub/index) library and running in your terminal:", "--\ntitle: \"The Hugging Face Hub for Galleries, Libraries, Archives and Museums\"\nthumbnail: /blog/assets/144_hf_hub_glam_guide/thumbnail.png\nauthors:\n- user: davanstrien\n---\n\n## The Hugging Face Hub for Galleries, Libraries, Archives and Museums \n\n\n\n### What is the Hugging Face Hub?\n\nHugging Face aims to make high-quality machine learning accessible to everyone. This goal is pursued in various ways, including developing open-source code libraries such as the widely-used Transformers library, offering [free courses](https://huggingface.co/learn), and providing the Hugging Face Hub. \n\nThe [Hugging Face Hub](https://huggingface.co/) is a central repository where people can share and access machine learning models, datasets and demos. The Hub hosts over 190,000 machine learning models, 33,000 datasets and over 100,000 machine learning applications and demos. These models cover a wide range of tasks from pre-trained language models, text, image and audio classification models, object detection models, and a wide range of generative models. \n\nThe models, datasets and demos hosted on the Hub span a wide range of domains and languages, with regular community efforts to expand the scope of what is available via the Hub. This blog post intends to offer people working in or with the galleries, libraries, archives and museums (GLAM) sector to understand how they can use &mdash; and contribute to &mdash; the Hugging Face Hub.\n\nYou can read the whole post or jump to the most relevant sections!"], "eval_score_GPT4": "4", "eval_feedback_GPT4": "The response provided is mostly correct and factual. It outlines a series of steps that a user should follow to claim authorship of a paper on the Hugging Face Hub. The steps are in line with the reference answer, which suggests clicking on one's name on the Paper page, claiming authorship, and confirming the request in the paper settings. However, the response includes additional steps that are not mentioned in the reference answer, such as clicking on the user's name in the Paper page and being redirected to the Paper settings page. These additional steps are not incorrect but provide more detail than the reference answer. Therefore, the response is not completely correct as per the reference answer but is still accurate and factual."}, {"question": "What is the purpose of the /healthcheck endpoint in the Datasets server API?\n", "true_answer": "Ensure the app is running", "source_doc": "huggingface/datasets-server/blob/main/services/api/README.md", "generated_answer": "The purpose of the /healthcheck endpoint in the Datasets server API is to ensure that the application is running properly. When this endpoint is accessed, the server responds with a success message, indicating that it is operational. This endpoint is commonly used to perform basic health checks on web applications and services to ensure they are functioning as expected. In the context of the Datasets server, it can be used to verify that the server is responding to requests and that the API endpoints are available.", "retrieved_docs": ["Datasets server API - rows endpoint\n\n> /rows endpoint\n\n## Configuration\n\nThe service can be configured using environment variables. They are grouped by scope.\n\n### API service\n\nSee [../../libs/libapi/README.md](../../libs/libapi/README.md) for more information about the API configuration.\n\n### Common\n\nSee [../../libs/libcommon/README.md](../../libs/libcommon/README.md) for more information about the common configuration.\n\n## Endpoints\n\nSee https://huggingface.co/docs/datasets-server\n\n- /healthcheck: ensure the app is running\n- /metrics: return a list of metrics in the Prometheus format\n- /rows: get a slice of rows of a dataset split", "Datasets server SSE API\n\n> Server-sent events API for the Datasets server. It's used to update the Hub's backend cache.\n\n## Configuration\n\nThe service can be configured using environment variables. They are grouped by scope.\n\n### Common\n\nSee [../../libs/libcommon/README.md](../../libs/libcommon/README.md) for more information about the common configuration.\n\n## Endpoints\n\nSee https://huggingface.co/docs/datasets-server\n\n- /healthcheck: Ensure the app is running\n- /metrics: Return a list of metrics in the Prometheus format\n- /hub-cache: Return a dataset information as a Server-Sent Event (SSE) when a dataset is updated. If `?all=true` is passed in the parameters, and if the cache already has some entries, one SSE per cached dataset is sent to the client. Then, a SSE is sent when a dataset is inserted, modified or deleted. The event data is a JSON with the following structure. The `hub_cache` field is null for deleted entries, or when the response is an error. The `num_rows` value is `0` if it could not be determined.\n\n```typescript\n{\n    dataset: string;\n    hub_cache: null | {\n        preview: boolean;\n        viewer: boolean;\n        partial: boolean;\n        num_rows: int;\n    };\n}\n```", "Datasets server API - search service\n\n> /search endpoint\n> /filter endpoint\n\n## Configuration\n\nThe service can be configured using environment variables. They are grouped by scope.\n\n### Duckdb index full text search\n- `DUCKDB_INDEX_CACHE_DIRECTORY`: directory where the temporal duckdb index files are downloaded. Defaults to empty.\n- `DUCKDB_INDEX_TARGET_REVISION`: the git revision of the dataset where the index file is stored in the dataset repository.\n\n### API service\n\nSee [../../libs/libapi/README.md](../../libs/libapi/README.md) for more information about the API configuration.\n\n### Common\n\nSee [../../libs/libcommon/README.md](../../libs/libcommon/README.md) for more information about the common configuration.\n\n## Endpoints\n\nSee https://huggingface.co/docs/datasets-server\n\n- /healthcheck: ensure the app is running\n- /metrics: return a list of metrics in the Prometheus format\n- /search: get a slice of a search result over a dataset split\n- /filter: filter rows of a dataset split", "If you have access to the internal HF notion, see https://www.notion.so/huggingface2/Datasets-server-464848da2a984e999c540a4aa7f0ece5.\n\nThe application is distributed in several components.\n\n[api](./services/api) is a web server that exposes the [API endpoints](https://huggingface.co/docs/datasets-server). Apart from some endpoints (`valid`, `is-valid`), all the responses are served from pre-computed responses. That's the main point of this project: generating these responses takes time, and the API server provides this service to the users.\n\nThe precomputed responses are stored in a Mongo database called \"cache\". They are computed by [workers](./services/worker) which take their jobs from a job queue stored in a Mongo database called \"queue\", and store the results (error or valid response) into the \"cache\" (see [libcommon](./libs/libcommon)).\n\nThe API service exposes the `/webhook` endpoint which is called by the Hub on every creation, update or deletion of a dataset on the Hub. On deletion, the cached responses are deleted. On creation or update, a new job is appended in the \"queue\" database.\n\nNote that every worker has its own job queue:", "Check dataset validity\n\nBefore you download a dataset from the Hub, it is helpful to know if a specific dataset you're interested in is available. Datasets Server provides the `/is-valid` endpoint to check if a specific dataset works without any errors.\n\nThe API endpoint will return an error for datasets that cannot be loaded with the [\ud83e\udd17 Datasets](https://github.com/huggingface/datasets) library, for example, because the data hasn't been uploaded or the format is not supported.\n\n<Tip warning={true}>\n  The largest datasets are partially supported by Datasets Server. If they are{\" \"}\n  <a href=\"https://huggingface.co/docs/datasets/stream\">streamable</a>, Datasets\n  Server can extract the first 100 rows without downloading the whole dataset.\n  This is especially useful for previewing large datasets where downloading the\n  whole dataset may take hours! See the <code>preview</code> field in the\n  response of <code>/is-valid</code> to check if a dataset is partially\n  supported.\n</Tip>\n\nThis guide shows you how to check dataset validity programmatically, but free to try it out with [Postman](https://www.postman.com/huggingface/workspace/hugging-face-apis/request/23242779-17b761d0-b2b8-4638-a4f7-73be9049c324), [RapidAPI](https://rapidapi.com/hugging-face-hugging-face-default/api/hugging-face-datasets-api), or [ReDoc](https://redocly.github.io/redoc/?url=https://datasets-server.huggingface.co/openapi.json#operation/isValidDataset).\n\n## Check if a dataset is valid", "Server infrastructure\n\nThe [Datasets Server](https://github.com/huggingface/datasets-server) has two main components that work together to return queries about a dataset instantly:\n\n- a user-facing web API for exploring and returning information about a dataset\n- a server runs the queries ahead of time and caches them in a database\n\nWhile most of the documentation is focused on the web API, the server is crucial because it performs all the time-consuming preprocessing and stores the results so the web API can retrieve and serve them to the user. This saves a user time because instead of generating the response every time it gets requested, Datasets Server can return the preprocessed results instantly from the cache.\n\nThere are three elements that keep the server running: the job queue, workers, and the cache.\n\n## Job queue\n\nThe job queue is a list of jobs stored in a Mongo database that should be completed by the workers. The jobs are practically identical to the endpoints the user uses; only the server runs the jobs ahead of time, and the user gets the results when they use the endpoint.\n\nThere are three jobs:\n\n- `/splits` corresponds to the `/splits` endpoint. It refreshes a dataset and then returns that dataset's splits and configurations. For every split in the dataset, it'll create a new job.\n- `/first-rows` corresponds to the `/first-rows` endpoint. It gets the first 100 rows and columns of a dataset split.\n- `/parquet` corresponds to the `/parquet` endpoint. It downloads the whole dataset, converts it to [parquet](https://parquet.apache.org/) and publishes the parquet files to the Hub.\n\nYou might've noticed the `/rows` and `/search` endpoints don't have a job in the queue. The responses from these endpoints are generated on demand.\n\n## Workers", "Datasets server API\n\n> API on \ud83e\udd17 datasets\n\n## Configuration\n\nThe service can be configured using environment variables. They are grouped by scope.\n\n### API service\n\nSee [../../libs/libapi/README.md](../../libs/libapi/README.md) for more information about the API configuration.\n\n### Common\n\nSee [../../libs/libcommon/README.md](../../libs/libcommon/README.md) for more information about the common configuration.\n\n## Endpoints\n\nSee https://huggingface.co/docs/datasets-server\n\n- /healthcheck: Ensure the app is running\n- /metrics: Return a list of metrics in the Prometheus format\n- /webhook: Add, update or remove a dataset\n- /is-valid: Tell if a dataset is [valid](https://huggingface.co/docs/datasets-server/valid)\n- /splits: List the [splits](https://huggingface.co/docs/datasets-server/splits) names for a dataset\n- /first-rows: Extract the [first rows](https://huggingface.co/docs/datasets-server/first_rows) for a dataset split\n- /parquet: List the [parquet files](https://huggingface.co/docs/datasets-server/parquet) auto-converted for a dataset"], "eval_score_GPT4": "5", "eval_feedback_GPT4": "Feedback: The response provided is completely correct, accurate, and factual. It aligns well with the reference answer by stating the primary purpose of the /healthcheck endpoint, which is to ensure that the application is running properly. The response expands on the reference answer by explaining what happens when the endpoint is accessed and the typical use of such an endpoint in web applications and services, which supports the correctness and factual nature of the answer."}, {"question": "What is the default context window size for Local Attention in the LongT5 model?\n", "true_answer": "127 tokens", "source_doc": "huggingface/transformers/blob/main/docs/source/en/model_doc/longt5.md", "generated_answer": "The default context window size for Local Attention in the LongT5 model is `r=127` tokens, as mentioned in the context under the heading \"Local attention\".", "retrieved_docs": ["## Usage tips\n\n- [`LongT5ForConditionalGeneration`] is an extension of [`T5ForConditionalGeneration`] exchanging the traditional\nencoder *self-attention* layer with efficient either *local* attention or *transient-global* (*tglobal*) attention.\n- Unlike the T5 model, LongT5 does not use a task prefix. Furthermore, it uses a different pre-training objective\ninspired by the pre-training of [`PegasusForConditionalGeneration`].\n- LongT5 model is designed to work efficiently and very well on long-range *sequence-to-sequence* tasks where the\ninput sequence exceeds commonly used 512 tokens. It is capable of handling input sequences of a length up to 16,384 tokens.\n- For *Local Attention*, the sparse sliding-window local attention operation allows a given token to attend only `r`\ntokens to the left and right of it (with `r=127` by default). *Local Attention* does not introduce any new parameters\nto the model. The complexity of the mechanism is linear in input sequence length `l`: `O(l*r)`.\n- *Transient Global Attention* is an extension of the *Local Attention*. It, furthermore, allows each input token to\ninteract with all other tokens in the layer. This is achieved via splitting an input sequence into blocks of a fixed\nlength `k` (with a default `k=16`). Then, a global token for such a block is obtained via summing and normalizing the embeddings of every token\nin the block. Thanks to this, the attention allows each token to attend to both nearby tokens like in Local attention, and\nalso every global token like in the case of standard global attention (*transient* represents the fact the global tokens\nare constructed dynamically within each attention operation).  As a consequence, *TGlobal* attention introduces\na few new parameters -- global relative position biases and a layer normalization for global token's embedding.\nThe complexity of this mechanism is `O(l(r + l/k))`.\n- An example showing how to evaluate a fine-tuned LongT5 model on the [pubmed dataset](https://huggingface.co/datasets/scientific_papers) is below.", "## Local attention\n\n[Longformer](#longformer) uses local attention: often, the local context (e.g., what are the two tokens to the\nleft and right?) is enough to take action for a given token. Also, by stacking attention layers that have a small\nwindow, the last layer will have a receptive field of more than just the tokens in the window, allowing them to build a\nrepresentation of the whole sentence.\n\nSome preselected input tokens are also given global attention: for those few tokens, the attention matrix can access\nall tokens and this process is symmetric: all other tokens have access to those specific tokens (on top of the ones in\ntheir local window). This is shown in Figure 2d of the paper, see below for a sample attention mask:\n\n<div class=\"flex justify-center\">\n    <img scale=\"50 %\" align=\"center\" src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/local_attention_mask.png\"/>\n</div>\n\nUsing those attention matrices with less parameters then allows the model to have inputs having a bigger sequence\nlength.\n\n## Other tricks\n\n### Axial positional encodings", "## Longformer Self Attention\n\nLongformer self attention employs self attention on both a \"local\" context and a \"global\" context. Most tokens only\nattend \"locally\" to each other meaning that each token attends to its \\\\(\\frac{1}{2} w\\\\) previous tokens and\n\\\\(\\frac{1}{2} w\\\\) succeeding tokens with \\\\(w\\\\) being the window length as defined in\n`config.attention_window`. Note that `config.attention_window` can be of type `List` to define a\ndifferent \\\\(w\\\\) for each layer. A selected few tokens attend \"globally\" to all other tokens, as it is\nconventionally done for all tokens in `BertSelfAttention`.\n\nNote that \"locally\" and \"globally\" attending tokens are projected by different query, key and value matrices. Also note\nthat every \"locally\" attending token not only attends to tokens within its window \\\\(w\\\\), but also to all \"globally\"\nattending tokens so that global attention is *symmetric*.\n\nThe user can define which tokens attend \"locally\" and which tokens attend \"globally\" by setting the tensor\n`global_attention_mask` at run-time appropriately. All Longformer models employ the following logic for\n`global_attention_mask`:\n\n- 0: the token attends \"locally\",\n- 1: the token attends \"globally\".\n\nFor more information please also refer to [`~LongformerModel.forward`] method.\n\nUsing Longformer self attention, the memory and time complexity of the query-key matmul operation, which usually\nrepresents the memory and time bottleneck, can be reduced from \\\\(\\mathcal{O}(n_s \\times n_s)\\\\) to\n\\\\(\\mathcal{O}(n_s \\times w)\\\\), with \\\\(n_s\\\\) being the sequence length and \\\\(w\\\\) being the average window\nsize. It is assumed that the number of \"globally\" attending tokens is insignificant as compared to the number of\n\"locally\" attending tokens.", "Iz Beltagy, Matthew E. Peters, Arman Cohan\n\nLongformer addresses the memory bottleneck of transformers by replacing conventional self-attention with a combination of windowed/local/sparse (cf. [Sparse Transformers (2019)](https://arxiv.org/abs/1904.10509)) attention and global attention that scales linearly with the sequence length. As opposed to previous long-range transformer models (e.g. [Transformer-XL (2019)](https://arxiv.org/abs/1901.02860), [Reformer (2020)](https://arxiv.org/abs/2001.04451), [Adaptive Attention Span (2019)](https://arxiv.org/abs/1905.07799)), Longformer\u2019s self-attention layer is designed as a drop-in replacement for the standard self-attention, thus making it possible to leverage pre-trained checkpoints for further pre-training and/or fine-tuning on long sequence tasks.\n\nThe standard self-attention matrix (Figure a) scales quadratically with the input length:\n\n<figure>\n  <img src=\"/blog/assets/14_long_range_transformers/Longformer.png\" alt=\"Longformer attention\"/>\n  <figcaption>Figure taken from Longformer</figcaption>\n</figure>", "#### Main findings\n\n* The authors proposed the dilated windowed self-attention (Figure c) and showed that it yields better results on language modeling compared to just windowed/sparse self-attention (Figure b). The window sizes are increased through the layers. This pattern further outperforms previous architectures (such as Transformer-XL, or adaptive span attention) on downstream benchmarks.\n* Global attention allows the information to flow through the whole sequence and applying the global attention to task-motivated tokens (such as the tokens of the question in QA, CLS token for sentence classification) leads to stronger performance on downstream tasks. Using this global pattern, Longformer can be successfully applied to document-level NLP tasks in the transfer learning setting.\n* Standard pre-trained models can be adapted to long-range inputs by simply replacing the standard self-attention with the long-range self-attention proposed in this paper and then fine-tuning on the downstream task. This avoids costly pre-training specific to long-range inputs.\n\n#### Follow-up questions\n\n* The increasing size (throughout the layers) of the dilated windowed self-attention echoes findings in computer vision on increasing the receptive field of stacked CNN. How do these two findings relate? What are the transposable learnings?\n* Longformer\u2019s Encoder-Decoder architecture works well for tasks that do not require a long target length (e.g. summarization). However, how would it work for long-range seq2seq tasks which require a long target length (e.g. document translation, speech recognition, etc.) especially considering the cross-attention layer of encoder-decoder\u2019s models?\n* In practice, the sliding window self-attention relies on many indexing operations to ensure a symmetric query-key weights matrix. Those operations are very slow on TPUs which highlights the question of the applicability of such patterns on other hardware.\n\n### [Compressive Transformers for Long-Range Sequence Modelling](https://arxiv.org/abs/1911.05507)\n\nJack W. Rae, Anna Potapenko, Siddhant M. Jayakumar, Timothy P. Lillicrap", "This enhanced local self-attention is better than the vanilla local self-attention architecture but still has a major drawback in that every input vector can only attend to a local context of predefined size. For NLP tasks that do not require the transformer model to learn long-range dependencies between the input vectors, which include arguably *e.g.* speech recognition, named entity recognition and causal language modeling of short sentences, this might not be a big issue. Many NLP tasks do require the model to learn long-range dependencies, so that local self-attention could lead to significant performance degradation, *e.g.* \n* *Question-answering*: the model has to learn the relationship between the question tokens and relevant answer tokens which will most likely not be in the same local range\n* *Multiple-Choice*: the model has to compare multiple answer token segments to each other which are usually separated by a significant length\n* *Summarization*: the model has to learn the relationship between a long sequence of context tokens and a shorter sequence of summary tokens, whereas the relevant relationships between context and summary can most likely not be captured by local self-attention\n* etc...\n\nLocal self-attention on its own is most likely not sufficient for the transformer model to learn the relevant relationships of input vectors (tokens) to each other.\n\nTherefore, Reformer additionally employs an efficient self-attention layer that approximates global self-attention, called *LSH self-attention*.\n\n### LSH Self-Attention\n\nAlright, now that we have understood how local self-attention works, we can take a stab at the probably most innovative piece of Reformer: **Locality sensitive hashing (LSH) Self-Attention**. \n\nThe premise of LSH self-attention is to be more or less as efficient as local self-attention while approximating global self-attention.\n\nLSH self-attention relies on the LSH algorithm as presented in [Andoni et al (2015)](https://arxiv.org/abs/1509.02897), hence its name.", "The abstract from the paper is the following:\n\n*Recent work has shown that either (1) increasing the input length or (2) increasing model size can improve the\nperformance of Transformer-based neural models. In this paper, we present a new model, called LongT5, with which we\nexplore the effects of scaling both the input length and model size at the same time. Specifically, we integrated\nattention ideas from long-input transformers (ETC), and adopted pre-training strategies from summarization pre-training\n(PEGASUS) into the scalable T5 architecture. The result is a new attention mechanism we call {\\em Transient Global}\n(TGlobal), which mimics ETC's local/global attention mechanism, but without requiring additional side-inputs. We are\nable to achieve state-of-the-art results on several summarization tasks and outperform the original T5 models on\nquestion answering tasks.*\n\nThis model was contributed by [stancld](https://huggingface.co/stancld).\nThe original code can be found [here](https://github.com/google-research/longt5).\n\n## Usage tips"], "eval_score_GPT4": "5", "eval_feedback_GPT4": "The response provided is completely correct, accurate, and factual. It matches the reference answer by stating the default context window size for Local Attention in the LongT5 model is 127 tokens. The additional detail about the location of the information under the heading \"Local attention\" does not detract from the correctness of the answer."}, {"question": "What method is used to load a checkpoint for a task using `AutoPipeline`?\n", "true_answer": "from_pretrained()", "source_doc": "huggingface/diffusers/blob/main/docs/source/en/api/pipelines/auto_pipeline.md", "generated_answer": "The method used to load a checkpoint for a task using `AutoPipeline` is `from_pretrained()`. This method automatically detects the relevant pipeline class given the name or path to the pretrained weights and returns a pipeline instance ready for inference. This is done without the need to know the specific pipeline class to use, making it easy to load a checkpoint for a specific task. Additionally, for some workflows or if you're loading many pipelines, it is", "retrieved_docs": ["</Tip>\n\nThis tutorial shows you how to use an `AutoPipeline` to automatically infer the pipeline class to load for a specific task, given the pretrained weights.\n\n## Choose an AutoPipeline for your task\n\nStart by picking a checkpoint. For example, if you're interested in text-to-image with the [runwayml/stable-diffusion-v1-5](https://huggingface.co/runwayml/stable-diffusion-v1-5) checkpoint, use [`AutoPipelineForText2Image`]:\n\n```py\nfrom diffusers import AutoPipelineForText2Image\nimport torch\n\npipeline = AutoPipelineForText2Image.from_pretrained(\n    \"runwayml/stable-diffusion-v1-5\", torch_dtype=torch.float16, use_safetensors=True\n).to(\"cuda\")\nprompt = \"peasant and dragon combat, wood cutting style, viking era, bevel with rune\"\n\nimage = pipeline(prompt, num_inference_steps=25).images[0]\nimage\n```\n\n<div class=\"flex justify-center\">\n    <img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/diffusers/autopipeline-text2img.png\" alt=\"generated image of peasant fighting dragon in wood cutting style\"/>\n</div>\n\nUnder the hood, [`AutoPipelineForText2Image`]:\n\n1. automatically detects a `\"stable-diffusion\"` class from the [`model_index.json`](https://huggingface.co/runwayml/stable-diffusion-v1-5/blob/main/model_index.json) file\n2. loads the corresponding text-to-image [`StableDiffusionPipeline`] based on the `\"stable-diffusion\"` class name", "If you try to load an unsupported checkpoint, it'll throw an error:\n\n```py\nfrom diffusers import AutoPipelineForImage2Image\nimport torch\n\npipeline = AutoPipelineForImage2Image.from_pretrained(\n    \"openai/shap-e-img2img\", torch_dtype=torch.float16, use_safetensors=True\n)\n\"ValueError: AutoPipeline can't find a pipeline linked to ShapEImg2ImgPipeline for None\"\n```\n\n## Use multiple pipelines\n\nFor some workflows or if you're loading many pipelines, it is more memory-efficient to reuse the same components from a checkpoint instead of reloading them which would unnecessarily consume additional memory. For example, if you're using a checkpoint for text-to-image and you want to use it again for image-to-image, use the [`~AutoPipelineForImage2Image.from_pipe`] method. This method creates a new pipeline from the components of a previously loaded pipeline at no additional memory cost.\n\nThe [`~AutoPipelineForImage2Image.from_pipe`] method detects the original pipeline class and maps it to the new pipeline class corresponding to the task you want to do. For example, if you load a `\"stable-diffusion\"` class pipeline for text-to-image:\n\n```py\nfrom diffusers import AutoPipelineForText2Image, AutoPipelineForImage2Image\nimport torch\n\npipeline_text2img = AutoPipelineForText2Image.from_pretrained(\n    \"runwayml/stable-diffusion-v1-5\", torch_dtype=torch.float16, use_safetensors=True\n)\nprint(type(pipeline_text2img))\n\"<class 'diffusers.pipelines.stable_diffusion.pipeline_stable_diffusion.StableDiffusionPipeline'>\"\n```", "!--Copyright 2023 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\n\nhttp://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License.\n-->\n\n# AutoPipeline\n\n`AutoPipeline` is designed to:\n\n1. make it easy for you to load a checkpoint for a task without knowing the specific pipeline class to use\n2. use multiple pipelines in your workflow\n\nBased on the task, the `AutoPipeline` class automatically retrieves the relevant pipeline given the name or path to the pretrained weights with the `from_pretrained()` method.\n\nTo seamlessly switch between tasks with the same checkpoint without reallocating additional memory, use the `from_pipe()` method to transfer the components from the original pipeline to the new one.\n\n```py\nfrom diffusers import AutoPipelineForText2Image\nimport torch\n\npipeline = AutoPipelineForText2Image.from_pretrained(\n    \"runwayml/stable-diffusion-v1-5\", torch_dtype=torch.float16, use_safetensors=True\n).to(\"cuda\")\nprompt = \"Astronaut in a jungle, cold color palette, muted colors, detailed, 8k\"\n\nimage = pipeline(prompt, num_inference_steps=25).images[0]\n```\n\n<Tip>\n\nCheck out the [AutoPipeline](../../tutorials/autopipeline) tutorial to learn how to use this API!\n\n</Tip>\n\n`AutoPipeline` supports text-to-image, image-to-image, and inpainting for the following diffusion models:", "The original codebase can be found at [ai-forever/Kandinsky-2](https://github.com/ai-forever/Kandinsky-2).\n\n<Tip>\n\nCheck out the [Kandinsky Community](https://huggingface.co/kandinsky-community) organization on the Hub for the official model checkpoints for tasks like text-to-image, image-to-image, and inpainting.\n\n</Tip>\n\n<Tip>\n\nMake sure to check out the schedulers [guide](../../using-diffusers/schedulers) to learn how to explore the tradeoff between scheduler speed and quality, and see the [reuse components across pipelines](../../using-diffusers/loading#reuse-components-across-pipelines) section to learn how to efficiently load the same components into multiple pipelines.\n\n</Tip>\n\n## KandinskyV22PriorPipeline\n\n[[autodoc]] KandinskyV22PriorPipeline\n\t- all\n\t- __call__\n\t- interpolate\n\n## KandinskyV22Pipeline\n\n[[autodoc]] KandinskyV22Pipeline\n\t- all\n\t- __call__\n\n## KandinskyV22CombinedPipeline\n\n[[autodoc]] KandinskyV22CombinedPipeline\n\t- all\n\t- __call__\n\n## KandinskyV22ControlnetPipeline\n\n[[autodoc]] KandinskyV22ControlnetPipeline\n\t- all\n\t- __call__\n\n## KandinskyV22PriorEmb2EmbPipeline\n\n[[autodoc]] KandinskyV22PriorEmb2EmbPipeline\n\t- all\n\t- __call__\n\t- interpolate\n\n## KandinskyV22Img2ImgPipeline\n\n[[autodoc]] KandinskyV22Img2ImgPipeline\n\t- all\n\t- __call__", "\ud83d\udca1 Skip to the [DiffusionPipeline explained](#diffusionpipeline-explained) section if you are interested in learning in more detail about how the [`DiffusionPipeline`] class works.\n\n</Tip>\n\nThe [`DiffusionPipeline`] class is the simplest and most generic way to load the latest trending diffusion model from the [Hub](https://huggingface.co/models?library=diffusers&sort=trending). The [`DiffusionPipeline.from_pretrained`] method automatically detects the correct pipeline class from the checkpoint, downloads, and caches all the required configuration and weight files, and returns a pipeline instance ready for inference.\n\n```python\nfrom diffusers import DiffusionPipeline\n\nrepo_id = \"runwayml/stable-diffusion-v1-5\"\npipe = DiffusionPipeline.from_pretrained(repo_id, use_safetensors=True)\n```\n\nYou can also load a checkpoint with its specific pipeline class. The example above loaded a Stable Diffusion model; to get the same result, use the [`StableDiffusionPipeline`] class:\n\n```python\nfrom diffusers import StableDiffusionPipeline\n\nrepo_id = \"runwayml/stable-diffusion-v1-5\"\npipe = StableDiffusionPipeline.from_pretrained(repo_id, use_safetensors=True)\n```\n\nA checkpoint (such as [`CompVis/stable-diffusion-v1-4`](https://huggingface.co/CompVis/stable-diffusion-v1-4) or [`runwayml/stable-diffusion-v1-5`](https://huggingface.co/runwayml/stable-diffusion-v1-5)) may also be used for more than one task, like text-to-image or image-to-image. To differentiate what task you want to use the checkpoint for, you have to load it directly with its corresponding task-specific pipeline class:\n\n```python\nfrom diffusers import StableDiffusionImg2ImgPipeline", "The original codebase can be found at [ai-forever/Kandinsky-2](https://github.com/ai-forever/Kandinsky-2).\n\n<Tip>\n\nCheck out the [Kandinsky Community](https://huggingface.co/kandinsky-community) organization on the Hub for the official model checkpoints for tasks like text-to-image, image-to-image, and inpainting.\n\n</Tip>\n\n<Tip>\n\nMake sure to check out the Schedulers [guide](../../using-diffusers/schedulers) to learn how to explore the tradeoff between scheduler speed and quality, and see the [reuse components across pipelines](../../using-diffusers/loading#reuse-components-across-pipelines) section to learn how to efficiently load the same components into multiple pipelines.\n\n</Tip>\n\n## KandinskyPriorPipeline\n\n[[autodoc]] KandinskyPriorPipeline\n\t- all\n\t- __call__\n\t- interpolate\n\n## KandinskyPipeline\n\n[[autodoc]] KandinskyPipeline\n\t- all\n\t- __call__\n\n## KandinskyCombinedPipeline\n\n[[autodoc]] KandinskyCombinedPipeline\n\t- all\n\t- __call__\n\n## KandinskyImg2ImgPipeline\n\n[[autodoc]] KandinskyImg2ImgPipeline\n\t- all\n\t- __call__\n\n## KandinskyImg2ImgCombinedPipeline\n\n[[autodoc]] KandinskyImg2ImgCombinedPipeline\n\t- all\n\t- __call__\n\n## KandinskyInpaintPipeline\n\n[[autodoc]] KandinskyInpaintPipeline\n\t- all\n\t- __call__\n\n## KandinskyInpaintCombinedPipeline", "These ready-to-use checkpoints can be downloaded and used via the HuggingFace Hub:\n\n```python\n>>> from transformers import AutoModelForCausalLM, AutoTokenizer\n>>> device = \"cuda\" # the device to load the model onto\n\n>>> model = AutoModelForCausalLM.from_pretrained(\"mistralai/Mixtral-8x7B-v0.1\")\n>>> tokenizer = AutoTokenizer.from_pretrained(\"mistralai/Mistral-8x7B\")\n\n>>> prompt = \"My favourite condiment is\"\n\n>>> model_inputs = tokenizer([prompt], return_tensors=\"pt\").to(device)\n>>> model.to(device)\n\n>>> generated_ids = model.generate(**model_inputs, max_new_tokens=100, do_sample=True)\n>>> tokenizer.batch_decode(generated_ids)[0]\n\"The expected output\"\n```\n\nTo use the raw checkpoints with HuggingFace you can use the `convert_mixtral_weights_to_hf.py` script to convert them to the HuggingFace format:\n\n```bash\npython src/transformers/models/mixtral/convert_mixtral_weights_to_hf.py \\\n    --input_dir /path/to/downloaded/mistral/weights --output_dir /output/path\n```\n\nYou can then load the converted model from the `output/path`:\n\n```python\nfrom transformers import MixtralForCausalLM, LlamaTokenizer\n\ntokenizer = LlamaTokenizer.from_pretrained(\"/output/path\")\nmodel = MixtralForCausalLM.from_pretrained(\"/output/path\")\n```\n\n## Combining Mixtral and Flash Attention 2\n\nFirst, make sure to install the latest version of Flash Attention 2 to include the sliding window attention feature."], "eval_score_GPT4": "5", "eval_feedback_GPT4": "Feedback: The response provided is completely correct, accurate, and factual. It correctly identifies `from_pretrained()` as the method used to load a checkpoint for a task using `AutoPipeline`. The explanation that follows is also accurate, explaining that the method automatically detects the relevant pipeline class and returns a pipeline instance ready for inference, which aligns well with the reference answer."}, {"question": "What is the purpose of Diffusers library?\n", "true_answer": "To serve as a modular toolbox for both inference and training of state-of-the-art pretrained diffusion models across multiple modalities.", "source_doc": "huggingface/diffusers/blob/main/docs/source/en/conceptual/philosophy.md", "generated_answer": "The purpose of the Diffusers library is to provide state-of-the-art pretrained diffusion models for generating images, audio, and even 3D structures of molecules, with a focus on usability over performance, simplicity over ease, and customizability over abstractions. It serves as a modular toolbox for both inference and training, with interchangeable noise schedulers and pretrained models that can be combined to create custom diffusion systems. The library is designed to", "retrieved_docs": ["Using \ud83e\udde8 `diffusers` at Hugging Face\n\nDiffusers is the go-to library for state-of-the-art pretrained diffusion models for generating images, audio, and even 3D structures of molecules. Whether you\u2019re looking for a simple inference solution or want to train your own diffusion model, Diffusers is a modular toolbox that supports both. The library is designed with a focus on usability over performance, simple over easy, and customizability over abstractions.\n\n## Exploring Diffusers in the Hub\n\nThere are over 10,000 `diffusers` compatible pipelines on the Hub which you can find by filtering at the left of [the models page](https://huggingface.co/models?library=diffusers&sort=downloads). Diffusion systems are typically composed of multiple components such as text encoder, UNet, VAE, and scheduler. Even though they are not standalone models, the pipeline abstraction makes it easy to use them for inference or training.\n\nYou can find diffusion pipelines for many different tasks:\n\n* Generating images from natural language text prompts ([text-to-image](https://huggingface.co/models?library=diffusers&pipeline_tag=text-to-image&sort=downloads)).\n* Transforming images using natural language text prompts ([image-to-image](https://huggingface.co/models?library=diffusers&pipeline_tag=image-to-image&sort=downloads)).\n* Generating videos from natural language descriptions ([text-to-video](https://huggingface.co/models?library=diffusers&pipeline_tag=text-to-video&sort=downloads)).\n\n\nYou can try out the models directly in the browser if you want to test them out without downloading them, thanks to the in-browser widgets! \n\n<div class=\"flex justify-center\">\n<img class=\"hidden dark:block\" src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/refs%2Fpr%2F35/hub/libraries-diffusers_widget.png\"/>\n</div>", "\ud83e\udd17 Diffusers is the go-to library for state-of-the-art pretrained diffusion models for generating images, audio, and even 3D structures of molecules. Whether you're looking for a simple inference solution or training your own diffusion models, \ud83e\udd17 Diffusers is a modular toolbox that supports both. Our library is designed with a focus on [usability over performance](https://huggingface.co/docs/diffusers/conceptual/philosophy#usability-over-performance), [simple over easy](https://huggingface.co/docs/diffusers/conceptual/philosophy#simple-over-easy), and [customizability over abstractions](https://huggingface.co/docs/diffusers/conceptual/philosophy#tweakable-contributorfriendly-over-abstraction).\n\n\ud83e\udd17 Diffusers offers three core components:\n\n- State-of-the-art [diffusion pipelines](https://huggingface.co/docs/diffusers/api/pipelines/overview) that can be run in inference with just a few lines of code.\n- Interchangeable noise [schedulers](https://huggingface.co/docs/diffusers/api/schedulers/overview) for different diffusion speeds and output quality.\n- Pretrained [models](https://huggingface.co/docs/diffusers/api/models/overview) that can be used as building blocks, and combined with schedulers, for creating your own end-to-end diffusion systems.\n\n## Installation\n\nWe recommend installing \ud83e\udd17 Diffusers in a virtual environment from PyPI or Conda. For more details about installing [PyTorch](https://pytorch.org/get-started/locally/) and [Flax](https://flax.readthedocs.io/en/latest/#installation), please refer to their official documentation.\n\n### PyTorch\n\nWith `pip` (official package):\n\n```bash\npip install --upgrade diffusers[torch]\n```\n\nWith `conda` (maintained by the community):", "\ud83e\udde8 Diffusers Experimental\n\nWe are adding experimental code to support novel applications and usages of the Diffusers library.\nCurrently, the following experiments are supported:\n* Reinforcement learning via an implementation of the [Diffuser](https://arxiv.org/abs/2205.09991) model.", "--\ntitle: \"Ethical Guidelines for developing the Diffusers library\" \nthumbnail: /blog/assets/ethics-diffusers/thumbnail.png\nauthors:\n- user: giadap\n---\n\n# Ethical guidelines for developing the Diffusers library\n\nWe are on a journey to make our libraries more responsible, one commit at a time! \n\nAs part of the [Diffusers library documentation](https://huggingface.co/docs/diffusers/main/en/index), we are proud to announce the publication of an [ethical framework](https://huggingface.co/docs/diffusers/main/en/conceptual/ethical_guidelines). \n\nGiven diffusion models' real case applications in the world and potential negative impacts on society, this initiative aims to guide the technical decisions of the Diffusers library maintainers about community contributions. We wish to be transparent in how we make decisions, and above all, we aim to clarify what values guide those decisions.\n\nWe see ethics as a process that leverages guiding values, concrete actions, and continuous adaptation. For this reason, we are committed to adjusting our guidelines over time, following the evolution of the Diffusers project and the valuable feedback from the community that keeps it alive.\n\n# Ethical guidelines\n\n* **Transparency**: we are committed to being transparent in managing PRs, explaining our choices to users, and making technical decisions.\n* **Consistency**: we are committed to guaranteeing our users the same level of attention in project management, keeping it technically stable and consistent.\n* **Simplicity**: with a desire to make it easy to use and exploit the Diffusers library, we are committed to keeping the project\u2019s goals lean and coherent.\n* **Accessibility**: the Diffusers project helps lower the entry bar for contributors who can help run it even without technical expertise. Doing so makes research artifacts more accessible to the community.\n* **Reproducibility**: we aim to be transparent about the reproducibility of upstream code, models, and datasets when made available through the Diffusers library.\n* **Responsibility**: as a community and through teamwork, we hold a collective responsibility to our users by anticipating and mitigating this technology\u2019s potential risks and dangers.", "!--Copyright 2023 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\n\nhttp://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License.\n-->\n\n# Philosophy\n\n\ud83e\udde8 Diffusers provides **state-of-the-art** pretrained diffusion models across multiple modalities.\nIts purpose is to serve as a **modular toolbox** for both inference and training.\n\nWe aim at building a library that stands the test of time and therefore take API design very seriously.\n\nIn a nutshell, Diffusers is built to be a natural extension of PyTorch. Therefore, most of our design choices are based on [PyTorch's Design Principles](https://pytorch.org/docs/stable/community/design.html#pytorch-design-philosophy). Let's go over the most important ones:\n\n## Usability over Performance", "!--Copyright 2023 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\n\nhttp://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License.\n-->\n\n# Philosophy\n\n\ud83e\udde8 Diffusers provides **state-of-the-art** pretrained diffusion models across multiple modalities.\nIts purpose is to serve as a **modular toolbox** for both inference and training.\n\nWe aim at building a library that stands the test of time and therefore take API design very seriously.\n\nIn a nutshell, Diffusers is built to be a natural extension of PyTorch. Therefore, most of our design choices are based on [PyTorch's Design Principles](https://pytorch.org/docs/stable/community/design.html#pytorch-design-philosophy). Let's go over the most important ones:\n\n## Usability over Performance", "## Tweakable, contributor-friendly over abstraction\n\nFor large parts of the library, Diffusers adopts an important design principle of the [Transformers library](https://github.com/huggingface/transformers), which is to prefer copy-pasted code over hasty abstractions. This design principle is very opinionated and stands in stark contrast to popular design principles such as [Don't repeat yourself (DRY)](https://en.wikipedia.org/wiki/Don%27t_repeat_yourself).\nIn short, just like Transformers does for modeling files, Diffusers prefers to keep an extremely low level of abstraction and very self-contained code for pipelines and schedulers.\nFunctions, long code blocks, and even classes can be copied across multiple files which at first can look like a bad, sloppy design choice that makes the library unmaintainable.\n**However**, this design has proven to be extremely successful for Transformers and makes a lot of sense for community-driven, open-source machine learning libraries because:\n- Machine Learning is an extremely fast-moving field in which paradigms, model architectures, and algorithms are changing rapidly, which therefore makes it very difficult to define long-lasting code abstractions.\n- Machine Learning practitioners like to be able to quickly tweak existing code for ideation and research and therefore prefer self-contained code over one that contains many abstractions.\n- Open-source libraries rely on community contributions and therefore must build a library that is easy to contribute to. The more abstract the code, the more dependencies, the harder to read, and the harder to contribute to. Contributors simply stop contributing to very abstract libraries out of fear of breaking vital functionality. If contributing to a library cannot break other fundamental code, not only is it more inviting for potential new contributors, but it is also easier to review and contribute to multiple parts in parallel.\n\nAt Hugging Face, we call this design the **single-file policy** which means that almost all of the code of a certain class should be written in a single, self-contained file. To read more about the philosophy, you can have a look\nat [this blog post](https://huggingface.co/blog/transformers-design-philosophy)."], "eval_score_GPT4": "4", "eval_feedback_GPT4": "Feedback: The response provided captures the essence of the Diffusers library's purpose, mentioning its role in providing state-of-the-art pretrained diffusion models and highlighting its focus on usability, simplicity, and customizability. It also correctly identifies the library's function as a modular toolbox for inference and training, as well as its support for various modalities such as images, audio, and 3D structures. However, the response includes unnecessary contrasts (e.g., \"usability over performance, simplicity over ease\") that are not present in the reference answer and may introduce confusion. Despite this, the core information aligns well with the reference answer, indicating that the response is mostly correct, accurate, and factual."}, {"question": "What method does the EulerAncestralDiscreteScheduler use for sampling?\n", "true_answer": "Ancestral sampling with Euler method steps.", "source_doc": "huggingface/diffusers/blob/main/docs/source/en/api/schedulers/euler_ancestral.md", "generated_answer": "The EulerAncestralDiscreteScheduler uses ancestral sampling with Euler method steps, as described in the original implementation by Katherine Crowson in the [k-diffusion](https://github.com/crowsonkb/k-diffusion) repository. This is a fast scheduler that can often generate good outputs in 20-30 steps.", "retrieved_docs": ["!--Copyright 2023 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\n\nhttp://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License.\n-->\n\n# EulerAncestralDiscreteScheduler\n\nA scheduler that uses ancestral sampling with Euler method steps. This is a fast scheduler which can often generate good outputs in 20-30 steps. The scheduler is based on the original [k-diffusion](https://github.com/crowsonkb/k-diffusion/blob/481677d114f6ea445aa009cf5bd7a9cdee909e47/k_diffusion/sampling.py#L72) implementation by [Katherine Crowson](https://github.com/crowsonkb/).\n\n## EulerAncestralDiscreteScheduler\n[[autodoc]] EulerAncestralDiscreteScheduler\n\n## EulerAncestralDiscreteSchedulerOutput\n[[autodoc]] schedulers.scheduling_euler_ancestral_discrete.EulerAncestralDiscreteSchedulerOutput", "## Tips\n\nIt is recommended to set `solver_order` to 2 for guide sampling, and `solver_order=3` for unconditional sampling.\n\nDynamic thresholding from [Imagen](https://huggingface.co/papers/2205.11487) is supported, and for pixel-space\ndiffusion models, you can set both `algorithm_type=\"dpmsolver++\"` and `thresholding=True` to use dynamic\nthresholding. This thresholding method is unsuitable for latent-space diffusion models such as\nStable Diffusion.\n\n## DPMSolverSinglestepScheduler\n[[autodoc]] DPMSolverSinglestepScheduler\n\n## SchedulerOutput\n[[autodoc]] schedulers.scheduling_utils.SchedulerOutput", "!--Copyright 2023 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\n\nhttp://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License.\n-->\n\n# EulerDiscreteScheduler\n\nThe Euler scheduler (Algorithm 2) is from the [Elucidating the Design Space of Diffusion-Based Generative Models](https://huggingface.co/papers/2206.00364) paper by Karras et al. This is a fast scheduler which can often generate good outputs in 20-30 steps. The scheduler is based on the original [k-diffusion](https://github.com/crowsonkb/k-diffusion/blob/481677d114f6ea445aa009cf5bd7a9cdee909e47/k_diffusion/sampling.py#L51) implementation by [Katherine Crowson](https://github.com/crowsonkb/).\n\n\n## EulerDiscreteScheduler\n[[autodoc]] EulerDiscreteScheduler\n\n## EulerDiscreteSchedulerOutput\n[[autodoc]] schedulers.scheduling_euler_discrete.EulerDiscreteSchedulerOutput", "### Stable Diffusion Reference\n\nThis pipeline uses the Reference Control. Refer to the [sd-webui-controlnet discussion: Reference-only Control](https://github.com/Mikubill/sd-webui-controlnet/discussions/1236)[sd-webui-controlnet discussion: Reference-adain Control](https://github.com/Mikubill/sd-webui-controlnet/discussions/1280).\n\nBased on [this issue](https://github.com/huggingface/diffusers/issues/3566),\n- `EulerAncestralDiscreteScheduler` got poor results.\n\n```py\nimport torch\nfrom diffusers import UniPCMultistepScheduler\nfrom diffusers.utils import load_image\n\ninput_image = load_image(\"https://hf.co/datasets/huggingface/documentation-images/resolve/main/diffusers/input_image_vermeer.png\")\n\npipe = StableDiffusionReferencePipeline.from_pretrained(\n       \"runwayml/stable-diffusion-v1-5\",\n       safety_checker=None,\n       torch_dtype=torch.float16\n       ).to('cuda:0')\n\npipe.scheduler = UniPCMultistepScheduler.from_config(pipe.scheduler.config)\n\nresult_img = pipe(ref_image=input_image,\n      prompt=\"1girl\",\n      num_inference_steps=20,\n      reference_attn=True,\n      reference_adain=True).images[0]\n```\n\nReference Image\n\n![reference_image](https://hf.co/datasets/huggingface/documentation-images/resolve/main/diffusers/input_image_vermeer.png)\n\nOutput Image of `reference_attn=True` and `reference_adain=False`", "| DPM++ SDE           | [`DPMSolverSinglestepScheduler`]    |                                                                                                               |\n| DPM++ SDE Karras    | [`DPMSolverSinglestepScheduler`]    | init with `use_karras_sigmas=True`                                                                            |\n| DPM2                | [`KDPM2DiscreteScheduler`]          |                                                                                                               |\n| DPM2 Karras         | [`KDPM2DiscreteScheduler`]          | init with `use_karras_sigmas=True`                                                                            |\n| DPM2 a              | [`KDPM2AncestralDiscreteScheduler`] |                                                                                                               |\n| DPM2 a Karras       | [`KDPM2AncestralDiscreteScheduler`] | init with `use_karras_sigmas=True`                                                                            |\n| DPM adaptive        | N/A                                 |                                                                                                               |\n| DPM fast            | N/A                                 |                                                                                                               |\n| Euler               | [`EulerDiscreteScheduler`]          |                                                                                                               |\n| Euler a             | [`EulerAncestralDiscreteScheduler`] |                                                                                                               |\n| Heun                | [`HeunDiscreteScheduler`]           |                                                                                                               |\n| LMS                 | [`LMSDiscreteScheduler`]            |                                                                                                               |\n| LMS Karras          | [`LMSDiscreteScheduler`]            | init with `use_karras_sigmas=True`                                                                            |\n| N/A                 | [`DEISMultistepScheduler`]          |                                                                                                               |\n| N/A                 | [`UniPCMultistepScheduler`]         |                                                                                                               |", "!--Copyright 2023 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\n\nhttp://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License.\n-->\n\n# KDPM2AncestralDiscreteScheduler\n\nThe `KDPM2DiscreteScheduler` with ancestral sampling is inspired by the [Elucidating the Design Space of Diffusion-Based Generative Models](https://huggingface.co/papers/2206.00364) paper, and the scheduler is ported from and created by [Katherine Crowson](https://github.com/crowsonkb/).\n\nThe original codebase can be found at [crowsonkb/k-diffusion](https://github.com/crowsonkb/k-diffusion).\n\n## KDPM2AncestralDiscreteScheduler\n[[autodoc]] KDPM2AncestralDiscreteScheduler\n\n## SchedulerOutput\n[[autodoc]] schedulers.scheduling_utils.SchedulerOutput", "generator = torch.Generator(device=\"cuda\").manual_seed(8)\nimage = pipeline(prompt, generator=generator, num_inference_steps=30).images[0]\nimage\n```\n\n<p align=\"center\">\n    <br>\n    <img src=\"https://huggingface.co/datasets/patrickvonplaten/images/resolve/main/diffusers_docs/astronaut_euler_discrete.png\" width=\"400\"/>\n    <br>\n</p>\n\n\nand:\n\n```python\nfrom diffusers import EulerAncestralDiscreteScheduler\n\npipeline.scheduler = EulerAncestralDiscreteScheduler.from_config(pipeline.scheduler.config)\n\ngenerator = torch.Generator(device=\"cuda\").manual_seed(8)\nimage = pipeline(prompt, generator=generator, num_inference_steps=30).images[0]\nimage\n```\n\n<p align=\"center\">\n    <br>\n    <img src=\"https://huggingface.co/datasets/patrickvonplaten/images/resolve/main/diffusers_docs/astronaut_euler_ancestral.png\" width=\"400\"/>\n    <br>\n</p>\n\n\n[`DPMSolverMultistepScheduler`] gives a reasonable speed/quality trade-off and can be run with as little as 20 steps.\n\n```python\nfrom diffusers import DPMSolverMultistepScheduler\n\npipeline.scheduler = DPMSolverMultistepScheduler.from_config(pipeline.scheduler.config)\n\ngenerator = torch.Generator(device=\"cuda\").manual_seed(8)\nimage = pipeline(prompt, generator=generator, num_inference_steps=20).images[0]\nimage\n```"], "eval_score_GPT4": "5", "eval_feedback_GPT4": "The response correctly identifies the method used by the EulerAncestralDiscreteScheduler as ancestral sampling with Euler method steps. It also provides additional context about the implementation and efficiency of the scheduler, which is not required but does not detract from the accuracy of the answer. The response is in line with the reference answer and adheres to the facts."}, {"question": "What is the name of the large multimodal model that can solve image-text tasks and is based on Flamingo?\n", "true_answer": "IDEFICS", "source_doc": "huggingface/transformers/blob/main/docs/source/en/tasks/idefics.md", "generated_answer": "The name of the large multimodal model that can solve image-text tasks and is based on Flamingo is IDEFICS. It is an open-access reproduction of Flamingo and comes in two variants with 80 billion and 9 billion parameters. IDEFICS accepts sequences of images and texts as input and generates coherent text as output, allowing for tasks such as image captioning, visual question answering, and image-guided text generation. The", "retrieved_docs": ["[IDEFICS](../model_doc/idefics) is an open-access vision and language model based on [Flamingo](https://huggingface.co/papers/2204.14198), \na state-of-the-art visual language model initially developed by DeepMind. The model accepts arbitrary sequences of image \nand text inputs and generates coherent text as output. It can answer questions about images, describe visual content, \ncreate stories grounded in multiple images, and so on. IDEFICS comes in two variants - [80 billion parameters](https://huggingface.co/HuggingFaceM4/idefics-80b) \nand [9 billion parameters](https://huggingface.co/HuggingFaceM4/idefics-9b), both of which are available on the \ud83e\udd17 Hub. For each variant, you can also find fine-tuned instructed \nversions of the model adapted for conversational use cases.\n\nThis model is exceptionally versatile and can be used for a wide range of image and multimodal tasks. However, \nbeing a large model means it requires significant computational resources and infrastructure. It is up to you to decide whether \nthis approach suits your use case better than fine-tuning specialized models for each individual task. \n\nIn this guide, you'll learn how to: \n- [Load IDEFICS](#loading-the-model) and [load the quantized version of the model](#loading-the-quantized-version-of-the-model)\n- Use IDEFICS for: \n  - [Image captioning](#image-captioning)\n  - [Prompted image captioning](#prompted-image-captioning)\n  - [Few-shot prompting](#few-shot-prompting)\n  - [Visual question answering](#visual-question-answering)\n  - [Image classificaiton](#image-classification)\n  - [Image-guided text generation](#image-guided-text-generation)\n- [Run inference in batch mode](#running-inference-in-batch-mode)\n- [Run IDEFICS instruct for conversational use](#idefics-instruct-for-conversational-use)\n\nBefore you begin, make sure you have all the necessary libraries installed.", "IDEFICS is an open-access reproduction of Flamingo and is comparable in performance with the original closed-source model across various image-text understanding benchmarks. It comes in two variants - 80 billion parameters and 9 billion parameters.\n\n<p align=\"center\">\n    <img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/idefics/Figure_Evals_IDEFICS.png\" width=\"600\" alt=\"Plot comparing the performance of Flamingo, OpenFlamingo and IDEFICS\"/>\n</p>\n\nWe also provide fine-tuned versions [idefics-80B-instruct](https://huggingface.co/HuggingFaceM4/idefics-80b-instruct) and [idefics-9B-instruct](https://huggingface.co/HuggingFaceM4/idefics-9b-instruct) adapted for conversational use cases.\n\n## Training Data\n\nIDEFICS was trained on a mixture of openly available datasets: Wikipedia, Public Multimodal Dataset, and LAION, as well as a new 115B token dataset called [OBELICS](https://huggingface.co/datasets/HuggingFaceM4/OBELICS) that we created. OBELICS consists of 141 million interleaved image-text documents scraped from the web and contains 353 million images.\n\nWe provide an [interactive visualization](https://atlas.nomic.ai/map/f2fba2aa-3647-4f49-a0f3-9347daeee499/ee4a84bd-f125-4bcc-a683-1b4e231cb10f) of OBELICS that allows exploring the content of the dataset with [Nomic AI](https://home.nomic.ai/).", "The abstract from the paper is the following:\n\n*Large multimodal models trained on natural documents, which interleave images and text, outperform models trained on image-text pairs on various multimodal benchmarks that require reasoning over one or multiple images to generate a text. However, the datasets used to train these models have not been released, and the collection process has not been fully specified. We introduce the OBELICS dataset, an open web-scale filtered dataset of interleaved image-text documents comprising 141 million web pages extracted from Common Crawl, 353 million associated images, and 115 billion text tokens. We describe the dataset creation process, present comprehensive filtering rules, and provide an analysis of the dataset's content. To show the viability of OBELISC, we train an 80 billion parameters vision and language model on the dataset and obtain competitive performance on various multimodal benchmarks. We release the code to reproduce the dataset along with the dataset itself.*\n\nThis model was contributed by [HuggingFaceM4](https://huggingface.co/HuggingFaceM4). The original code can be found [here](<INSERT LINK TO GITHUB REPO HERE>). (TODO: don't have a public link yet).\n\n\n<Tip warning={true}>\n\nIDEFICS modeling code in Transformers is for finetuning and inferencing the pre-trained IDEFICS models.\n\nTo train a new IDEFICS model from scratch use the m4 codebase (a link will be provided once it's made public)\n\n</Tip>\n\n\n## IdeficsConfig\n\n[[autodoc]] IdeficsConfig\n\n## IdeficsModel\n\n[[autodoc]] IdeficsModel\n    - forward\n\n## IdeficsForVisionText2Text\n\n[[autodoc]] IdeficsForVisionText2Text\n    - forward\n\n## IdeficsImageProcessor\n\n[[autodoc]] IdeficsImageProcessor\n    - preprocess\n\n## IdeficsProcessor", "The development of state-of-the-art AI models should be more transparent. Our goal with IDEFICS is to reproduce and provide the AI community with systems that match the capabilities of large proprietary models like Flamingo. As such, we took important steps contributing to bringing transparency to these AI systems: we used only publicly available data, we provided tooling to explore training datasets, we shared [technical lessons and mistakes](https://github.com/huggingface/m4-logs/blob/master/memos/README.md) of building such artifacts and assessed the model\u2019s harmfulness by adversarially prompting it before releasing it. We are hopeful that IDEFICS will serve as a solid foundation for more open research in multimodal AI systems, alongside models like [OpenFlamingo](https://huggingface.co/openflamingo)-another open reproduction of Flamingo at the 9 billion parameter scale.\n\nTry out the [demo](https://huggingface.co/spaces/HuggingFaceM4/idefics_playground) and the [models](https://huggingface.co/HuggingFaceM4/idefics-80b-instruct) on the Hub!\n\n<p align=\"center\">\n    <img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/idefics/woodstock_ai.png\" width=\"600\" alt=\"Screenshot of IDEFICS generation for HF Woodstock of AI\"/>\n</p>\n\n## What is IDEFICS?\n\nIDEFICS is an 80 billion parameters multimodal model that accepts sequences of images and texts as input and generates coherent text as output. It can answer questions about images, describe visual content, create stories grounded in multiple images, etc.\n\nIDEFICS is an open-access reproduction of Flamingo and is comparable in performance with the original closed-source model across various image-text understanding benchmarks. It comes in two variants - 80 billion parameters and 9 billion parameters.", "!--Copyright 2023 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\n\nhttp://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License.\n\n\u26a0\ufe0f Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\nrendered properly in your Markdown viewer.\n\n-->\n\n# Image tasks with IDEFICS\n\n[[open-in-colab]]\n\nWhile individual tasks can be tackled by fine-tuning specialized models, an alternative approach \nthat has recently emerged and gained popularity is to use large models for a diverse set of tasks without fine-tuning. \nFor instance, large language models can handle such NLP tasks as summarization, translation, classification, and more. \nThis approach is no longer limited to a single modality, such as text, and in this guide, we will illustrate how you can \nsolve image-text tasks with a large multimodal model called IDEFICS.", "```\ninputs = processor(image, text=prompt, return_tensors=\"pt\").to(device, torch.float16)\n\ngenerated_ids = model.generate(**inputs, max_new_tokens=10)\ngenerated_text = processor.batch_decode(generated_ids, skip_special_tokens=True)[0].strip()\nprint(generated_text)\n```\n\n```\nTo light a fire.\n```\n\n## Conclusion\n\nBLIP-2 is a zero-shot visual-language model that can be used for multiple image-to-text tasks with image and image and \ntext prompts. It is an effective and efficient approach that can be applied to image understanding in numerous scenarios, \nespecially when examples are scarce. \n\n\nThe model bridges the gap between vision and natural language modalities by adding a transformer between pre-trained models. \nThe new pre-training paradigm allows this model to keep up with the advances in both individual modalities.\n\n\nIf you'd like to learn how to fine-tune BLIP-2 models for various vision-language tasks, check out [LAVIS library by Salesforce](https://github.com/salesforce/LAVIS) \nthat offers comprehensive support for model training.\n\n\nTo see BLIP-2 in action, try its demo on [Hugging Face Spaces](https://huggingface.co/spaces/Salesforce/BLIP2).\n\n\n## Acknowledgments\n\nMany thanks to the Salesforce Research team for working on BLIP-2, Niels Rogge for adding BLIP-2 to \ud83e\udd17 Transformers, and \nto Omar Sanseviero for reviewing this blog post.", "For the ITM objective, given an image and caption pair, the task is to predict whether the caption matches the image or not. The negative samples are usually randomly sampled from the dataset itself. The MLM and ITM objectives are often combined during the pre-training of multi-modal models. For instance, VisualBERT proposes a BERT-like architecture that uses a pre-trained object detection model, [Faster-RCNN](https://arxiv.org/abs/1506.01497), to detect objects. This model uses a combination of the MLM and ITM objectives during pre-training to implicitly align elements of an input text and regions in an associated input image with self-attention. \n\nAnother work, FLAVA, consists of an image encoder, a text encoder, and a multi-modal encoder to fuse and align the image and text representations for multi-modal reasoning, all of which are based on transformers. In order to achieve this, FLAVA uses a variety of pre-training objectives: MLM, ITM, as well as Masked-Image Modeling (MIM), and contrastive learning.\n### 5) No Training \n\nFinally, various optimization strategies aim to bridge image and text representations using the pre-trained image and text models or adapt pre-trained multi-modal models to new downstream tasks without additional training. \n\nFor example, [MaGiC](https://arxiv.org/abs/2205.02655) proposes iterative optimization through a pre-trained autoregressive language model to generate a caption for the input image. To do this, MaGiC computes a CLIP-based \u201cMagic score\u201d using CLIP embeddings of the generated tokens and the input image."], "eval_score_GPT4": "4", "eval_feedback_GPT4": "Feedback: The response correctly identifies \"IDEFICS\" as the name of the large multimodal model based on Flamingo, which aligns with the reference answer. However, the additional information provided about IDEFICS being an open-access reproduction of Flamingo and its variants is not verified and could potentially be inaccurate as the reference answer does not confirm this. The response should have been concise and limited to the name only, as per the instruction."}, {"question": "What is the purpose of the `gradio.Blocks` API?\n", "true_answer": "The `gradio.Blocks` API allows you to have full control over the data flows and layout of your application, enabling the building of complex, multi-step applications.", "source_doc": "huggingface/course/blob/main/chapters/en/chapter9/7.mdx", "generated_answer": "The purpose of the `gradio.Blocks` API is to provide a low-level API for building complex, multi-step applications with full control over the data flows and layout of the application. It allows users to group related demos as multiple tabs in one web app, change the layout of the demo, have multi-step interfaces, change component properties or visibility based on user input, and build more flexible data flows in general. It allows users to build very custom and complex applications", "retrieved_docs": ["Introduction to Gradio Blocks[[introduction-to-gradio-blocks]]\n\n<CourseFloatingBanner chapter={9}\n  classNames=\"absolute z-10 right-0 top-0\"\n  notebooks={[\n    {label: \"Google Colab\", value: \"https://colab.research.google.com/github/huggingface/notebooks/blob/master/course/en/chapter9/section7.ipynb\"},\n    {label: \"Aws Studio\", value: \"https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/master/course/en/chapter9/section7.ipynb\"},\n]} />\n\nIn the previous sections we have explored and created demos using the `Interface` class. In this section we will introduce our **newly developed** low-level API called `gradio.Blocks`.\n\nNow, what's the difference between `Interface` and `Blocks`?\n\n- \u26a1 `Interface`: a high-level API that allows you to create a full machine learning demo simply by providing a list of inputs and outputs.\n\n- \ud83e\uddf1 `Blocks`: a low-level API that allows you to have full control over the data flows and layout of your application. You can build very complex, multi-step applications using `Blocks` (as in \"building blocks\").\n\n\n### Why Blocks \ud83e\uddf1?[[why-blocks-]]\n\nAs we saw in the previous sections, the `Interface` class allows you to easily create full-fledged machine learning demos with just a few lines of code. The `Interface` API is extremely easy to use but lacks the flexibility that the `Blocks` API provides. For example, you might want to:", "### Contributors Shoutout:\n\n- [@nhankiet](https://github.com/nhankiet) made their first contribution in [PR 1819](https://github.com/gradio-app/gradio/pull/1819)\n\n## 3.0\n\n###### \ud83d\udd25 Gradio 3.0 is the biggest update to the library, ever.\n\n### New Features:\n\n###### 1. Blocks \ud83e\uddf1\n\nBlocks is a new, low-level API that allows you to have full control over the data flows and layout of your application. It allows you to build very complex, multi-step applications. For example, you might want to:\n\n- Group together related demos as multiple tabs in one web app\n- Change the layout of your demo instead of just having all of the inputs on the left and outputs on the right\n- Have multi-step interfaces, in which the output of one model becomes the input to the next model, or have more flexible data flows in general\n- Change a component's properties (for example, the choices in a Dropdown) or its visibility based on user input\n\nHere's a simple example that creates the demo below it:\n\n```python\nimport gradio as gr\n\ndef update(name):\n    return f\"Welcome to Gradio, {name}!\"\n\ndemo = gr.Blocks()\n\nwith demo:\n    gr.Markdown(\n    \"\"\"\n    # Hello World!\n    Start typing below to see the output.\n    \"\"\")\n    inp = gr.Textbox(placeholder=\"What is your name?\")\n    out = gr.Textbox()\n\n    inp.change(fn=update,\n               inputs=inp,\n               outputs=out)\n\ndemo.launch()\n```\n\n![hello-blocks](https://user-images.githubusercontent.com/9021060/168684108-78cbd24b-e6bd-4a04-a8d9-20d535203434.gif)", "### Contributors Shoutout:\n\n- [@nhankiet](https://github.com/nhankiet) made their first contribution in [PR 1819](https://github.com/gradio-app/gradio/pull/1819)\n\n## 3.0\n\n###### \ud83d\udd25 Gradio 3.0 is the biggest update to the library, ever.\n\n### New Features:\n\n###### 1. Blocks \ud83e\uddf1\n\nBlocks is a new, low-level API that allows you to have full control over the data flows and layout of your application. It allows you to build very complex, multi-step applications. For example, you might want to:\n\n- Group together related demos as multiple tabs in one web app\n- Change the layout of your demo instead of just having all of the inputs on the left and outputs on the right\n- Have multi-step interfaces, in which the output of one model becomes the input to the next model, or have more flexible data flows in general\n- Change a component's properties (for example, the choices in a Dropdown) or its visibility based on user input\n\nHere's a simple example that creates the demo below it:\n\n```python\nimport gradio as gr\n\ndef update(name):\n    return f\"Welcome to Gradio, {name}!\"\n\ndemo = gr.Blocks()\n\nwith demo:\n    gr.Markdown(\n    \"\"\"\n    # Hello World!\n    Start typing below to see the output.\n    \"\"\")\n    inp = gr.Textbox(placeholder=\"What is your name?\")\n    out = gr.Textbox()\n\n    inp.change(fn=update,\n               inputs=inp,\n               outputs=out)\n\ndemo.launch()\n```\n\n![hello-blocks](https://user-images.githubusercontent.com/9021060/168684108-78cbd24b-e6bd-4a04-a8d9-20d535203434.gif)", "This simple example above introduces 4 concepts that underlie Blocks:\n\n1. Blocks allow you to build web applications that combine markdown, HTML, buttons, and interactive components simply by instantiating objects in Python inside of a `with gradio.Blocks` context.\n<Tip>\n\ud83d\ude4bIf you're not familiar with the `with` statement in Python, we recommend checking out the excellent [tutorial](https://realpython.com/python-with-statement/) from Real Python. Come back here after reading that \ud83e\udd17\n</Tip>\nThe order in which you instantiate components matters as each element gets rendered into the web app in the order it was created. (More complex layouts are discussed below)\n\n2. You can define regular Python functions anywhere in your code and run them with user input using `Blocks`. In our example, we have a simple function that \"flips\" the input text, but you can write any Python function, from a simple calculation to processing the predictions from a machine learning model.\n\n3. You can assign events to any `Blocks` component. This will run your function when the component is clicked, changed, etc. When you assign an event, you pass in three parameters: `fn`: the function that should be called, `inputs`: the (list) of input component(s), and `outputs`: the (list) of output components that should be called.\n\n   In the example above, we run the `flip_text()` function when the value in the `Textbox` named input `input` changes. The event reads the value in `input`, passes it as the name parameter to `flip_text()`, which then returns a value that gets assigned to our second `Textbox` named `output`.\n\n   To see a list of events that each component supports, see the Gradio [documentation](https://www.gradio.app/docs/).", "You can build very custom and complex applications using `gr.Blocks()`. For example, the popular image generation [Automatic1111 Web UI](https://github.com/AUTOMATIC1111/stable-diffusion-webui) is built using Gradio Blocks. We dive deeper into the `gr.Blocks` on our series on [building with Blocks](https://www.gradio.app/guides/blocks-and-event-listeners).\n\n\n#### The Gradio Python & JavaScript Ecosystem\n\nThat's the gist of the core `gradio` Python library, but Gradio is actually so much more! Its an entire ecosystem of Python and JavaScript libraries that let you build machine learning applications, or query them programmatically, in Python or JavaScript. Here are other related parts of the Gradio ecosystem:\n\n* [Gradio Python Client](https://www.gradio.app/guides/getting-started-with-the-python-client) (`gradio_client`): query any Gradio app programmatically in Python.\n* [Gradio JavaScript Client](https://www.gradio.app/guides/getting-started-with-the-js-client) (`@gradio/client`): query any Gradio app programmatically in JavaScript.\n* [Gradio-Lite](https://www.gradio.app/guides/gradio-lite) (`@gradio/lite`): write Gradio apps in Python that run entirely in the browser (no server needed!), thanks to Pyodide. \n* [Hugging Face Spaces](https://huggingface.co/spaces): the most popular place to host Gradio applications \u2014 for free!\n\n### What's Next?\n\nKeep learning about Gradio sequentially using the Gradio Guides, which include explanations as well as example code and embedded interactive demos. Next up: [key features about Gradio demos](https://www.gradio.app/guides/key-features).", "You can build very custom and complex applications using `gr.Blocks()`. For example, the popular image generation [Automatic1111 Web UI](https://github.com/AUTOMATIC1111/stable-diffusion-webui) is built using Gradio Blocks. We dive deeper into the `gr.Blocks` on our series on [building with Blocks](https://www.gradio.app/guides/blocks-and-event-listeners).\n\n\n### The Gradio Python & JavaScript Ecosystem\n\nThat's the gist of the core `gradio` Python library, but Gradio is actually so much more! Its an entire ecosystem of Python and JavaScript libraries that let you build machine learning applications, or query them programmatically, in Python or JavaScript. Here are other related parts of the Gradio ecosystem:\n\n* [Gradio Python Client](https://www.gradio.app/guides/getting-started-with-the-python-client) (`gradio_client`): query any Gradio app programmatically in Python.\n* [Gradio JavaScript Client](https://www.gradio.app/guides/getting-started-with-the-js-client) (`@gradio/client`): query any Gradio app programmatically in JavaScript.\n* [Gradio-Lite](https://www.gradio.app/guides/gradio-lite) (`@gradio/lite`): write Gradio apps in Python that run entirely in the browser (no server needed!), thanks to Pyodide. \n* [Hugging Face Spaces](https://huggingface.co/spaces): the most popular place to host Gradio applications \u2014 for free!\n\n## What's Next?\n\nKeep learning about Gradio sequentially using the Gradio Guides, which include explanations as well as example code and embedded interactive demos. Next up: [key features about Gradio demos](https://www.gradio.app/guides/key-features).", "Gradio Demo: calculator_blocks\n\n\n```\n!pip install -q gradio \n```\n\n\n```\nimport gradio as gr\n\n\ndef calculator(num1, operation, num2):\n    if operation == \"add\":\n        return num1 + num2\n    elif operation == \"subtract\":\n        return num1 - num2\n    elif operation == \"multiply\":\n        return num1 * num2\n    elif operation == \"divide\":\n        return num1 / num2\n\n\nwith gr.Blocks() as demo:\n    with gr.Row():\n        with gr.Column():\n            num_1 = gr.Number(value=4)\n            operation = gr.Radio([\"add\", \"subtract\", \"multiply\", \"divide\"])\n            num_2 = gr.Number(value=0)\n            submit_btn = gr.Button(value=\"Calculate\")\n        with gr.Column():\n            result = gr.Number()\n\n    submit_btn.click(calculator, inputs=[num_1, operation, num_2], outputs=[result], api_name=False)\n    examples = gr.Examples(examples=[[5, \"add\", 3],\n                                     [4, \"divide\", 2],\n                                     [-4, \"multiply\", 2.5],\n                                     [0, \"subtract\", 1.2]],\n                           inputs=[num_1, operation, num_2])\n\nif __name__ == \"__main__\":\n    demo.launch(show_api=False)\n```"], "eval_score_GPT4": "5", "eval_feedback_GPT4": "The response provided is completely correct, accurate, and factual. It aligns well with the reference answer, providing a detailed explanation of the `gradio.Blocks` API's purpose, which includes full control over data flows and layout for building complex, multi-step applications. The response expands on the reference answer by giving specific examples of what can be achieved with the API, such as grouping demos, changing layouts, and building flexible data flows, which demonstrates a clear understanding of the API's capabilities."}, {"question": "What is the purpose of the two-stage model proposed in the paper \"Hierarchical Text-Conditional Image Generation with CLIP Latents\"?\n", "true_answer": "The purpose of the two-stage model is to generate a CLIP image embedding given a text caption and then generate an image conditioned on the image embedding.", "source_doc": "huggingface/diffusers/blob/main/docs/source/en/api/pipelines/unclip.md", "generated_answer": "The purpose of the two-stage model proposed in the paper \"Hierarchical Text-Conditional Image Generation with CLIP Latents\" is to generate CLIP image embeddings from CLIP text embeddings using a prior, and then condition a decoder on the generated image embeddings to generate images. This approach improves image diversity with minimal loss in photorealism and caption similarity, allows for language-guided image manipulations in a zero-shot fashion, and", "retrieved_docs": ["!--Copyright 2023 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\n\nhttp://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License.\n-->\n\n# Prior Transformer\n\nThe Prior Transformer was originally introduced in [Hierarchical Text-Conditional Image Generation with CLIP Latents](https://huggingface.co/papers/2204.06125) by Ramesh et al. It is used to predict CLIP image embeddings from CLIP text embeddings; image embeddings are predicted through a denoising diffusion process.\n\nThe abstract from the paper is:\n\n*Contrastive models like CLIP have been shown to learn robust representations of images that capture both semantics and style. To leverage these representations for image generation, we propose a two-stage model: a prior that generates a CLIP image embedding given a text caption, and a decoder that generates an image conditioned on the image embedding. We show that explicitly generating image representations improves image diversity with minimal loss in photorealism and caption similarity. Our decoders conditioned on image representations can also produce variations of an image that preserve both its semantics and style, while varying the non-essential details absent from the image representation. Moreover, the joint embedding space of CLIP enables language-guided image manipulations in a zero-shot fashion. We use diffusion models for the decoder and experiment with both autoregressive and diffusion models for the prior, finding that the latter are computationally more efficient and produce higher-quality samples.*\n\n## PriorTransformer\n\n[[autodoc]] PriorTransformer\n\n## PriorTransformerOutput\n\n[[autodoc]] models.prior_transformer.PriorTransformerOutput", "!--Copyright 2023 The HuggingFace Team. All rights reserved.\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\nhttp://www.apache.org/licenses/LICENSE-2.0\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License.\n-->\n\n# unCLIP\n\n[Hierarchical Text-Conditional Image Generation with CLIP Latents](https://huggingface.co/papers/2204.06125) is by Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu, Mark Chen. The unCLIP model in \ud83e\udd17 Diffusers comes from kakaobrain's [karlo](https://github.com/kakaobrain/karlo).\n\nThe abstract from the paper is following:\n\n*Contrastive models like CLIP have been shown to learn robust representations of images that capture both semantics and style. To leverage these representations for image generation, we propose a two-stage model: a prior that generates a CLIP image embedding given a text caption, and a decoder that generates an image conditioned on the image embedding. We show that explicitly generating image representations improves image diversity with minimal loss in photorealism and caption similarity. Our decoders conditioned on image representations can also produce variations of an image that preserve both its semantics and style, while varying the non-essential details absent from the image representation. Moreover, the joint embedding space of CLIP enables language-guided image manipulations in a zero-shot fashion. We use diffusion models for the decoder and experiment with both autoregressive and diffusion models for the prior, finding that the latter are computationally more efficient and produce higher-quality samples.*\n\nYou can find lucidrains' DALL-E 2 recreation at [lucidrains/DALLE2-pytorch](https://github.com/lucidrains/DALLE2-pytorch).", "- Improved Denoising Diffusion Probabilistic Models ([Nichol et al., 2021](https://arxiv.org/abs/2102.09672)): finds that learning the variance of the conditional distribution (besides the mean) helps in improving performance\n- Cascaded Diffusion Models for High Fidelity Image Generation ([Ho et al., 2021](https://arxiv.org/abs/2106.15282)): introduces cascaded diffusion, which comprises a pipeline of multiple diffusion models that generate images of increasing resolution for high-fidelity image synthesis\n- Diffusion Models Beat GANs on Image Synthesis ([Dhariwal et al., 2021](https://arxiv.org/abs/2105.05233)): show that diffusion models can achieve image sample quality superior to the current state-of-the-art generative models by improving the U-Net architecture, as well as introducing classifier guidance\n- Classifier-Free Diffusion Guidance ([Ho et al., 2021](https://openreview.net/pdf?id=qw8AKxfYbI)): shows that you don't need a classifier for guiding a diffusion model by jointly training a conditional and an unconditional diffusion model with a single neural network\n- Hierarchical Text-Conditional Image Generation with CLIP Latents (DALL-E 2) ([Ramesh et al., 2022](https://cdn.openai.com/papers/dall-e-2.pdf)): uses a prior to turn a text caption into a CLIP image embedding, after which a diffusion model decodes it into an image\n- Photorealistic Text-to-Image Diffusion Models with Deep Language Understanding (ImageGen) ([Saharia et al., 2022](https://arxiv.org/abs/2205.11487)): shows that combining a large pre-trained language model (e.g. T5) with cascaded diffusion works well for text-to-image synthesis\n\nNote that this list only includes important works until the time of writing, which is June 7th, 2022.", "## W\u00fcrstchen Overview\nW\u00fcrstchen is a diffusion model, whose text-conditional model works in a highly compressed latent space of images. Why is this important? Compressing data can reduce computational costs for both training and inference by magnitudes. Training on 1024x1024 images is way more expensive than training on 32x32. Usually, other works make use of a relatively small compression, in the range of 4x - 8x spatial compression. W\u00fcrstchen takes this to an extreme. Through its novel design, we achieve a 42x spatial compression. This was unseen before because common methods fail to faithfully reconstruct detailed images after 16x spatial compression. W\u00fcrstchen employs a two-stage compression, what we call Stage A and Stage B. Stage A is a VQGAN, and Stage B is a Diffusion Autoencoder (more details can be found in the [paper](https://huggingface.co/papers/2306.00637)). A third model, Stage C, is learned in that highly compressed latent space. This training requires fractions of the compute used for current top-performing models, while also allowing cheaper and faster inference.\n\n## W\u00fcrstchen v2 comes to Diffusers\n\nAfter the initial paper release, we have improved numerous things in the architecture, training and sampling, making W\u00fcrstchen competitive to current state-of-the-art models in many ways. We are excited to release this new version together with Diffusers. Here is a list of the improvements.\n\n- Higher resolution (1024x1024 up to 2048x2048)\n- Faster inference\n- Multi Aspect Resolution Sampling\n- Better quality\n\n\nWe are releasing 3 checkpoints for the text-conditional image generation model (Stage C). Those are:\n\n- v2-base\n- v2-aesthetic\n- **(default)** v2-interpolated (50% interpolation between v2-base and v2-aesthetic)\n\nWe recommend using v2-interpolated, as it has a nice touch of both photorealism and aesthetics. Use v2-base for finetunings as it does not have a style bias and use v2-aesthetic for very artistic generations.\nA comparison can be seen here:", "!--Copyright 2023 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\n\nhttp://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License.\n-->\n\n# Stable unCLIP\n\nStable unCLIP checkpoints are finetuned from [Stable Diffusion 2.1](./stable_diffusion/stable_diffusion_2) checkpoints to condition on CLIP image embeddings.\nStable unCLIP still conditions on text embeddings. Given the two separate conditionings, stable unCLIP can be used\nfor text guided image variation. When combined with an unCLIP prior, it can also be used for full text to image generation.\n\nThe abstract from the paper is:\n\n*Contrastive models like CLIP have been shown to learn robust representations of images that capture both semantics and style. To leverage these representations for image generation, we propose a two-stage model: a prior that generates a CLIP image embedding given a text caption, and a decoder that generates an image conditioned on the image embedding. We show that explicitly generating image representations improves image diversity with minimal loss in photorealism and caption similarity. Our decoders conditioned on image representations can also produce variations of an image that preserve both its semantics and style, while varying the non-essential details absent from the image representation. Moreover, the joint embedding space of CLIP enables language-guided image manipulations in a zero-shot fashion. We use diffusion models for the decoder and experiment with both autoregressive and diffusion models for the prior, finding that the latter are computationally more efficient and produce higher-quality samples.*\n\n## Tips", "<img src=\"https://github.com/dome272/Wuerstchen/assets/61938694/2914830f-cbd3-461c-be64-d50734f4b49d\" width=500>\n\n## Text-to-Image Generation\n\nFor the sake of usability, W\u00fcrstchen can be used with a single pipeline. This pipeline can be used as follows:\n\n```python\nimport torch\nfrom diffusers import AutoPipelineForText2Image\nfrom diffusers.pipelines.wuerstchen import DEFAULT_STAGE_C_TIMESTEPS\n\npipe = AutoPipelineForText2Image.from_pretrained(\"warp-ai/wuerstchen\", torch_dtype=torch.float16).to(\"cuda\")\n\ncaption = \"Anthropomorphic cat dressed as a fire fighter\"\nimages = pipe(\n    caption,\n    width=1024,\n    height=1536,\n    prior_timesteps=DEFAULT_STAGE_C_TIMESTEPS,\n    prior_guidance_scale=4.0,\n    num_images_per_prompt=2,\n).images\n```\n\nFor explanation purposes, we can also initialize the two main pipelines of W\u00fcrstchen individually. W\u00fcrstchen consists of 3 stages: Stage C, Stage B, Stage A. They all have different jobs and work only together. When generating text-conditional images, Stage C will first generate the latents in a very compressed latent space. This is what happens in the `prior_pipeline`. Afterwards, the generated latents will be passed to Stage B, which decompresses the latents into a bigger latent space of a VQGAN. These latents can then be decoded by Stage A, which is a VQGAN, into the pixel-space. Stage B & Stage A are both encapsulated in the `decoder_pipeline`. For more details, take a look at the [paper](https://huggingface.co/papers/2306.00637).", "--\ntitle: \"Introducing W\u00fcrstchen: Fast Diffusion for Image Generation\" \nthumbnail: /blog/assets/wuerstchen/thumbnail.jpg\nauthors:\n- user: dome272\n  guest: true\n- user: babbleberns\n  guest: true\n- user: kashif\n- user: sayakpaul\n- user: pcuenq\n---\n\n# Introducing W\u00fcrstchen: Fast Diffusion for Image Generation\n\n\n![Collage of images created with W\u00fcrstchen](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/wuertschen/collage_compressed.jpg)\n\n## What is W\u00fcrstchen?\n\nW\u00fcrstchen is a diffusion model, whose text-conditional component works in a highly compressed latent space of images. Why is this important? Compressing data can reduce computational costs for both training and inference by orders of magnitude. Training on 1024\u00d71024 images is way more expensive than training on 32\u00d732. Usually, other works make use of a relatively small compression, in the range of 4x - 8x spatial compression. W\u00fcrstchen takes this to an extreme. Through its novel design, it achieves a 42x spatial compression! This had never been seen before, because common methods fail to faithfully reconstruct detailed images after 16x spatial compression. W\u00fcrstchen employs a two-stage compression, what we call Stage A and Stage B. Stage A is a VQGAN, and Stage B is a Diffusion Autoencoder (more details can be found in the\u00a0\u00a0**[paper](https://arxiv.org/abs/2306.00637)**). Together Stage A and B are called the *Decoder*, because they decode the compressed images back into pixel space. A third model, Stage C, is learned in that highly compressed latent space. This training requires fractions of the compute used for current top-performing models, while also allowing cheaper and faster inference. We refer to Stage C as the *Prior*.\n\n![W\u00fcrstchen images with Prompts](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/wuertschen/generated_images.jpg)"], "eval_score_GPT4": "4", "eval_feedback_GPT4": "Feedback: The response provided is mostly correct and aligns well with the reference answer. It accurately describes the two-stage process of generating CLIP image embeddings from text embeddings and then using those embeddings to generate images. The response also correctly identifies the benefits of this approach, such as improved image diversity and the ability to perform language-guided image manipulations. However, the response is incomplete as it ends abruptly and does not finish the explanation. Based on the score rubric, the response is not completely correct due to the incomplete sentence, but it is mostly correct, accurate, and factual."}, {"question": "What command is used to install the requirements for a research project using \ud83e\udd17 Transformers?\n", "true_answer": "pip install -r requirements.txt", "source_doc": "huggingface/transformers/blob/main/examples/research_projects/README.md", "generated_answer": "The command to install the requirements for a research project using \ud83e\udd17 Transformers is:\n\n```bash\npip install -r requirements.txt\n```\n\nThis command should be run inside the folder of the chosen research project. It will install all the necessary dependencies specified in the `requirements.txt` file. Note that the specific version of \ud83e\udd17 Transformers required for the project may be indicated in this file as well.", "retrieved_docs": ["!---\nCopyright 2020 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\");\nyou may not use this file except in compliance with the License.\nYou may obtain a copy of the License at\n\n    http://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software\ndistributed under the License is distributed on an \"AS IS\" BASIS,\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\nSee the License for the specific language governing permissions and\nlimitations under the License.\n-->\n\n# Research projects\n\nThis folder contains various research projects using \ud83e\udd17 Transformers. They are not maintained and require a specific\nversion of \ud83e\udd17 Transformers that is indicated in the requirements file of each folder. Updating them to the most recent version of the library will require some work.\n\nTo use any of them, just run the command\n```\npip install -r requirements.txt\n```\ninside the folder of your choice.\n\nIf you need help with any of those, contact the author(s), indicated at the top of the `README` of each folder.", "Research projects\n\nThis folder contains various research projects using \ud83e\udde8 Diffusers.\nThey are not really maintained by the core maintainers of this library and often require a specific version of Diffusers that is indicated in the requirements file of each folder.\nUpdating them to the most recent version of the library will require some work.\n\nTo use any of them, just run the command\n\n```\npip install -r requirements.txt\n```\ninside the folder of your choice.\n\nIf you need help with any of those, please open an issue where you directly ping the author(s), as indicated at the top of the README of each folder.", "2.  Clone your `transformers` fork to your local disk, and add the base\n    repository as a remote:\n\n    ```bash\n    git clone https://github.com/[your Github handle]/transformers.git\n    cd transformers\n    git remote add upstream https://github.com/huggingface/transformers.git\n    ```\n\n3.  Set up a development environment, for instance by running the\n    following command:\n\n    ```bash\n    python -m venv .env\n    source .env/bin/activate\n    pip install -e \".[dev]\"\n    ```\n\nand return to the parent directory\n\n```bash\ncd ..\n```\n\n4.  We recommend adding the PyTorch version of *BigBird* to\n    Transformers. To install PyTorch, please follow the instructions [here](https://pytorch.org/get-started/locally/).\n\n**Note:** You don't need to have CUDA installed. Making the new model\nwork on CPU is sufficient.\n\n5.  To port *BigBird*, you will also need access to its\n    original repository:\n\n```bash\ngit clone https://github.com/google-research/bigbird.git \ncd big_bird\npip install -e .\n```\n\nNow you have set up a development environment to port *BigBird*\nto \ud83e\udd17 Transformers.\n\n### Run a pretrained checkpoint using the original repository\n\n**3. Set up debugging environment**", "To get the OS and software versions automatically, run the following command:\n\n```bash\ntransformers-cli env\n```\n\nYou can also run the same command from the root of the repository:\n\n```bash\npython src/transformers/commands/transformers_cli.py env\n```\n\n### Do you want a new feature?\n\nIf there is a new feature you'd like to see in \ud83e\udd17 Transformers, please open an issue and describe:\n\n1. What is the *motivation* behind this feature? Is it related to a problem or frustration with the library? Is it a feature related to something you need for a project? Is it something you worked on and think it could benefit the community?\n\n   Whatever it is, we'd love to hear about it!\n\n2. Describe your requested feature in as much detail as possible. The more you can tell us about it, the better we'll be able to help you.\n3. Provide a *code snippet* that demonstrates the features usage.\n4. If the feature is related to a paper, please include a link.\n\nIf your issue is well written we're already 80% of the way there by the time you create it.\n\nWe have added [templates](https://github.com/huggingface/transformers/tree/main/templates) to help you get started with your issue.\n\n## Do you want to implement a new model?\n\nNew models are constantly released and if you want to implement a new model, please provide the following information\n\n* A short description of the model and a link to the paper.\n* Link to the implementation if it is open-sourced.\n* Link to the model weights if they are available.\n\nIf you are willing to contribute the model yourself, let us know so we can help you add it to \ud83e\udd17 Transformers!", "To get the OS and software versions automatically, run the following command:\n\n```bash\ntransformers-cli env\n```\n\nYou can also run the same command from the root of the repository:\n\n```bash\npython src/transformers/commands/transformers_cli.py env\n```\n\n### Do you want a new feature?\n\nIf there is a new feature you'd like to see in \ud83e\udd17 Transformers, please open an issue and describe:\n\n1. What is the *motivation* behind this feature? Is it related to a problem or frustration with the library? Is it a feature related to something you need for a project? Is it something you worked on and think it could benefit the community?\n\n   Whatever it is, we'd love to hear about it!\n\n2. Describe your requested feature in as much detail as possible. The more you can tell us about it, the better we'll be able to help you.\n3. Provide a *code snippet* that demonstrates the features usage.\n4. If the feature is related to a paper, please include a link.\n\nIf your issue is well written we're already 80% of the way there by the time you create it.\n\nWe have added [templates](https://github.com/huggingface/transformers/tree/main/templates) to help you get started with your issue.\n\n## Do you want to implement a new model?\n\nNew models are constantly released and if you want to implement a new model, please provide the following information\n\n* A short description of the model and a link to the paper.\n* Link to the implementation if it is open-sourced.\n* Link to the model weights if they are available.\n\nIf you are willing to contribute the model yourself, let us know so we can help you add it to \ud83e\udd17 Transformers!", "Start by creating a virtual environment in your project directory:\n\n```bash\npython -m venv .env\n```\n\nActivate the virtual environment. On Linux and MacOs:\n\n```bash\nsource .env/bin/activate\n```\nActivate Virtual environment on Windows\n\n```bash\n.env/Scripts/activate\n```\n\nNow you're ready to install \ud83e\udd17 Transformers with the following command:\n\n```bash\npip install transformers\n```\n\nFor CPU-support only, you can conveniently install \ud83e\udd17 Transformers and a deep learning library in one line. For example, install \ud83e\udd17 Transformers and PyTorch with:\n\n```bash\npip install 'transformers[torch]'\n```\n\n\ud83e\udd17 Transformers and TensorFlow 2.0:\n\n```bash\npip install 'transformers[tf-cpu]'\n```\n\n<Tip warning={true}>\n\nM1 / ARM Users\n    \nYou will need to install the following before installing TensorFLow 2.0\n```\nbrew install cmake\nbrew install pkg-config\n```\n\n</Tip>\n\n\ud83e\udd17 Transformers and Flax:\n\n```bash\npip install 'transformers[flax]'\n```\n\nFinally, check if \ud83e\udd17 Transformers has been properly installed by running the following command. It will download a pretrained model:\n\n```bash\npython -c \"from transformers import pipeline; print(pipeline('sentiment-analysis')('we love you'))\"\n```\n\nThen print out the label and score:\n\n```bash\n[{'label': 'POSITIVE', 'score': 0.9998704791069031}]\n```\n\n## Install from source\n\nInstall \ud83e\udd17 Transformers from source with the following command:\n\n```bash\npip install git+https://github.com/huggingface/transformers\n```", "## Install from source\n\nInstall \ud83e\udd17 Transformers from source with the following command:\n\n```bash\npip install git+https://github.com/huggingface/transformers\n```\n\nThis command installs the bleeding edge `main` version rather than the latest `stable` version. The `main` version is useful for staying up-to-date with the latest developments. For instance, if a bug has been fixed since the last official release but a new release hasn't been rolled out yet. However, this means the `main` version may not always be stable. We strive to keep the `main` version operational, and most issues are usually resolved within a few hours or a day. If you run into a problem, please open an [Issue](https://github.com/huggingface/transformers/issues) so we can fix it even sooner!\n\nCheck if \ud83e\udd17 Transformers has been properly installed by running the following command:\n\n```bash\npython -c \"from transformers import pipeline; print(pipeline('sentiment-analysis')('I love you'))\"\n```\n\n## Editable install\n\nYou will need an editable install if you'd like to:\n\n* Use the `main` version of the source code.\n* Contribute to \ud83e\udd17 Transformers and need to test changes in the code.\n\nClone the repository and install \ud83e\udd17 Transformers with the following commands:\n\n```bash\ngit clone https://github.com/huggingface/transformers.git\ncd transformers\npip install -e .\n```\n\nThese commands will link the folder you cloned the repository to and your Python library paths. Python will now look inside the folder you cloned to in addition to the normal library paths. For example, if your Python packages are typically installed in `~/anaconda3/envs/main/lib/python3.7/site-packages/`, Python will also search the folder you cloned to: `~/transformers/`.\n\n<Tip warning={true}>\n\nYou must keep the `transformers` folder if you want to keep using the library.\n\n</Tip>"], "eval_score_GPT4": "5", "eval_feedback_GPT4": "The response provided is completely correct, accurate, and factual. It matches the reference answer and includes additional helpful context about the necessity of running the command within the folder of the research project and the possibility of the `requirements.txt` file specifying a specific version of \ud83e\udd17 Transformers."}, {"question": "What task does the `roberta-large-mnli` checkpoint perform?\n", "true_answer": "Text classification", "source_doc": "huggingface/course/blob/main/chapters/en/chapter1/10.mdx", "generated_answer": "The `roberta-large-mnli` checkpoint performs the task of natural language inference, specifically classifying if two sentences are logically linked across three labels (contradiction, neutral, entailment). This information can be found by checking the explanation provided in the answer choices for the first quiz question in the given context.", "retrieved_docs": ["!-- DISABLE-FRONTMATTER-SECTIONS -->\n\n# End-of-chapter quiz[[end-of-chapter-quiz]]\n\n<CourseFloatingBanner\n    chapter={1}\n    classNames=\"absolute z-10 right-0 top-0\"\n/>\n\nThis chapter covered a lot of ground! Don't worry if you didn't grasp all the details; the next chapters will help you understand how things work under the hood.\n\nFirst, though, let's test what you learned in this chapter!\n\n\n### 1. Explore the Hub and look for the `roberta-large-mnli` checkpoint. What task does it perform?\n\n\n<Question\n\tchoices={[\n\t\t{\n\t\t\ttext: \"Summarization\",\n\t\t\texplain: \"Look again on the <a href=\\\"https://huggingface.co/roberta-large-mnli\\\">roberta-large-mnli page</a>.\"\n\t\t},\n\t\t{\n\t\t\ttext: \"Text classification\",\n\t\t\texplain: \"More precisely, it classifies if two sentences are logically linked across three labels (contradiction, neutral, entailment) \u2014 a task also called <em>natural language inference</em>.\",\n\t\t\tcorrect: true\n\t\t},\n\t\t{\n\t\t\ttext: \"Text generation\",\n\t\t\texplain: \"Look again on the <a href=\\\"https://huggingface.co/roberta-large-mnli\\\">roberta-large-mnli page</a>.\"\n\t\t}\n\t]}\n/>\n\n### 2. What will the following code return?\n\n```py\nfrom transformers import pipeline\n\nner = pipeline(\"ner\", grouped_entities=True)\nner(\"My name is Sylvain and I work at Hugging Face in Brooklyn.\")\n```", "![image](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/Lora-for-sequence-classification-with-Roberta-Llama-Mistral/lora.png)\n\n## Setup\n\nRoBERTa has a limitatiom of maximum sequence length of 512, so we set the `MAX_LEN=512` for all models to ensure a fair comparison.\n\n```python\nMAX_LEN = 512 \nroberta_checkpoint = \"roberta-large\"\nmistral_checkpoint = \"mistralai/Mistral-7B-v0.1\"\nllama_checkpoint = \"meta-llama/Llama-2-7b-hf\"\n```\n\n## Data preparation\n### Data loading\n\nWe will load the dataset from Hugging Face:\n```python\nfrom datasets import load_dataset\ndataset = load_dataset(\"mehdiiraqui/twitter_disaster\")\n```\n Now, let's split the dataset into training and validation datasets. Then add the test set:\n\n```python\nfrom datasets import Dataset\n# Split the dataset into training and validation datasets\ndata = dataset['train'].train_test_split(train_size=0.8, seed=42)\n# Rename the default \"test\" split to \"validation\"\ndata['val'] = data.pop(\"test\")\n# Convert the test dataframe to HuggingFace dataset and add it into the first dataset\ndata['test'] = dataset['test']\n```\n\nHere's an overview of the dataset:", "- [Comparing the Performance of LLMs: A Deep Dive into Roberta, Llama 2, and Mistral for Disaster Tweets Analysis with LoRA](#comparing-the-performance-of-llms-a-deep-dive-into-roberta-llama-2-and-mistral-for-disaster-tweets-analysis-with-lora)\n    - [Introduction](#introduction)\n    - [Hardware Used](#hardware-used)\n    - [Goals](#goals)\n    - [Dependencies](#dependencies)\n    - [Pre-trained Models](#pre-trained-models)\n        - [RoBERTa](#roberta)\n        - [Llama 2](#llama-2)\n        - [Mistral 7B](#mistral-7b)\n    - [LoRA](#lora)\n    - [Setup](#setup)\n    - [Data preparation](#data-preparation)\n        - [Data loading](#data-loading)\n        - [Data Processing](#data-processing)\n    - [Models](#models)\n        - [RoBERTa](#roberta)\n            - [Load RoBERTA Checkpoints for the Classification Task](#load-roberta-checkpoints-for-the-classification-task)\n            - [LoRA setup for RoBERTa classifier](#lora-setup-for-roberta-classifier)\n        - [Mistral](#mistral)\n            - [Load checkpoints for the classfication model](#load-checkpoints-for-the-classfication-model)\n            - [LoRA setup for Mistral 7B classifier](#lora-setup-for-mistral-7b-classifier)\n        - [Llama 2](#llama-2)\n            - [Load checkpoints for the classification mode](#load-checkpoints-for-the-classfication-mode)", "# Data collator for padding a batch of examples to the maximum length seen in the batch\nllama_data_collator = DataCollatorWithPadding(tokenizer=llama_tokenizer)\n```\n\nNow that we have prepared the tokenized datasets, the next section will showcase how to load the pre-trained LLMs checkpoints and how to set the LoRa weights. \n\n## Models\n\n### RoBERTa\n\n#### Load RoBERTa Checkpoints for the Classification Task\n\nWe load the pre-trained RoBERTa model with a sequence classification head using the Hugging Face `AutoModelForSequenceClassification` class:\n\n```python\nfrom transformers import AutoModelForSequenceClassification \nroberta_model = AutoModelForSequenceClassification.from_pretrained(roberta_checkpoint, num_labels=2)\n```\n\n\n####  LoRA setup for RoBERTa classifier\n\nWe import LoRa configuration and set some parameters for RoBERTa classifier:\n- TaskType: Sequence classification\n- r(rank): Rank for our decomposition matrices\n- lora_alpha: Alpha parameter to scale the learned weights. LoRA paper advises fixing alpha at 16\n- lora_dropout: Dropout probability of the LoRA layers\n- bias: Whether to add bias term to LoRa layers\n\nThe code below uses the values recommended by the [Lora paper](https://arxiv.org/abs/2106.09685). [Later in this post](#hyperparameter-tuning) we will perform hyperparameter tuning of these parameters using `wandb`.\n\n```python\nfrom peft import get_peft_model, LoraConfig, TaskType\nroberta_peft_config = LoraConfig(\n    task_type=TaskType.SEQ_CLS, r=2, lora_alpha=16, lora_dropout=0.1, bias=\"none\",\n)\nroberta_model = get_peft_model(roberta_model, roberta_peft_config)\nroberta_model.print_trainable_parameters()\n```", "The results of this run can be seen [here](https://wandb.ai/patrickvonplaten/pretraining-wav2vec2/reports/Wav2Vec2-Large--VmlldzoxMTAwODM4?accessToken=wm3qzcnldrwsa31tkvf2pdmilw3f63d4twtffs86ou016xjbyilh55uoi3mo1qzc) and the checkpoint pretrained for 120,000 steps can be accessed [here](https://huggingface.co/patrickvonplaten/wav2vec2-large-repro-960h-libri-120k-steps)", "The Whisper checkpoints come in five configurations of varying model sizes.\nThe smallest four are trained on either English-only or multilingual data.\nThe largest checkpoint is multilingual only. All nine of the pre-trained checkpoints \nare available on the [Hugging Face Hub](https://huggingface.co/models?search=openai/whisper). The \ncheckpoints are summarised in the following table with links to the models on the Hub:", "The Whisper checkpoints come in five configurations of varying model sizes.\nThe smallest four are trained on either English-only or multilingual data.\nThe largest checkpoint is multilingual only. All nine of the pre-trained checkpoints \nare available on the [Hugging Face Hub](https://huggingface.co/models?search=openai/whisper). The \ncheckpoints are summarised in the following table with links to the models on the Hub:"], "eval_score_GPT4": "5", "eval_feedback_GPT4": "Feedback: The response provided is detailed and accurate in describing the task that the `roberta-large-mnli` checkpoint performs. It correctly identifies the task as natural language inference and further explains the classification across three labels, which aligns with the nature of the MNLI (Multi-Genre Natural Language Inference) dataset tasks. Although the reference answer is very brief and only mentions \"Text classification,\" the evaluated response is not incorrect because natural language inference is a form of text classification. Therefore, the response is completely correct, accurate, and factual when considering the specific task of the `roberta-large-mnli` checkpoint."}, {"question": "What service is replacing the Paid tier of the Inference API at Hugging Face?\n", "true_answer": "Inference Endpoints", "source_doc": "huggingface/blog/blob/main/pricing-update.md", "generated_answer": "Answer: The service replacing the Paid tier of the Inference API at Hugging Face is called Inference Endpoints. It allows for fast, enterprise-grade inference as a service and is recommended for those looking for this type of solution. However, the Inference API will still be available for free for everyone to use.", "retrieved_docs": ["--\ntitle: Introducing our new pricing\nthumbnail: /blog/assets/114_pricing-update/thumbnail.png\nauthors:\n- user: sbrandeis\n- user: pierric\n---\n\n# Introducing our new pricing\n\n\nAs you might have noticed, our [pricing page](https://huggingface.co/pricing) has changed a lot recently.\n\nFirst of all, we are sunsetting the Paid tier of the Inference API service. The Inference API will still be available for everyone to use for free. But if you're looking for a fast, enterprise-grade inference as a service, we recommend checking out our brand new solution for this: [Inference Endpoints](https://huggingface.co/inference-endpoints).\n\nAlong with Inference Endpoints, we've recently introduced hardware upgrades for [Spaces](https://huggingface.co/spaces/launch), which allows running ML demos with the hardware of your choice. No subscription is required to use these services; you only need to add a credit card to your account from your [billing settings](https://huggingface.co/settings/billing). You can also attach a payment method to any of [your organizations](https://huggingface.co/settings/organizations).\n\nYour billing settings centralize everything about our paid services. From there, you can manage your personal PRO subscription, update your payment method, and visualize your usage for the past three months. Usage for all our paid services and subscriptions will be charged at the start of each month, and a consolidated invoice will be available for your records.\n\n**TL;DR**: **At HF we monetize by providing simple access to compute for AI**, with services like AutoTrain, Spaces and Inference Endpoints, directly accessible from the Hub. [Read more](https://huggingface.co/docs/hub/billing) about our pricing and billing system.\n\nIf you have any questions, feel free to reach out. We welcome your feedback \ud83d\udd25", "A: No - the Hugging Face DLCs are open source and licensed under Apache 2.0.\n\n_Q: How can I run inference on my trained models?_\n\nA: You have multiple options to run inference on your trained models. One option is to use Hugging Face [Accelerated Inference-API](https://api-inference.huggingface.co/docs/python/html/index.html) hosted service: start by [uploading the trained models to your Hugging Face account](https://huggingface.co/new) to deploy them publicly, or privately. Another great option is to use [SageMaker Inference](https://docs.aws.amazon.com/sagemaker/latest/dg/your-algorithms-inference-main.html) to run your own inference code in Amazon SageMaker. We are working on offering an integrated solution for Amazon SageMaker with Hugging Face Inference DLCs in the future - stay tuned!\n\n_Q: Do you offer premium support or support SLAs for this solution?_\n\nA: AWS Technical Support tiers are available from AWS and cover development and production issues for AWS products and services - please refer to AWS Support for specifics and scope.\n\nIf you have questions which the Hugging Face community can help answer and/or benefit from, please [post them in the Hugging Face forum](https://discuss.huggingface.co/c/sagemaker/17).\n\nIf you need premium support from the Hugging Face team to accelerate your NLP roadmap, our Expert Acceleration Program offers direct guidance from our open source, science and ML Engineering team - [contact us to learn more](mailto:api-enterprise@huggingface.co).\n\n_Q: What are you planning next through this partnership?_\n\nA: Our common goal is to democratize state of the art Machine Learning. We will continue to innovate to make it easier for researchers, data scientists and ML practitioners to manage, train and run state of the art models. If you have feature requests for integration in AWS with Hugging Face, please [let us know in the Hugging Face community forum](https://discuss.huggingface.co/c/sagemaker/17).", "!--\u26a0\ufe0f Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\nrendered properly in your Markdown viewer.\n-->\n\n# Run Inference on servers\n\nInference is the process of using a trained model to make predictions on new data. As this process can be compute-intensive,\nrunning on a dedicated server can be an interesting option. The `huggingface_hub` library provides an easy way to call a\nservice that runs inference for hosted models. There are several services you can connect to:\n- [Inference API](https://huggingface.co/docs/api-inference/index): a service that allows you to run accelerated inference\non Hugging Face's infrastructure for free. This service is a fast way to get started, test different models, and\nprototype AI products.\n- [Inference Endpoints](https://huggingface.co/docs/inference-endpoints/index): a product to easily deploy models to production.\nInference is run by Hugging Face in a dedicated, fully managed infrastructure on a cloud provider of your choice.\n\nThese services can be called with the [`InferenceClient`] object. It acts as a replacement for the legacy\n[`InferenceApi`] client, adding specific support for tasks and handling inference on both\n[Inference API](https://huggingface.co/docs/api-inference/index) and [Inference Endpoints](https://huggingface.co/docs/inference-endpoints/index).\nLearn how to migrate to the new client in the [Legacy InferenceAPI client](#legacy-inferenceapi-client) section.\n\n<Tip>\n\n[`InferenceClient`] is a Python client making HTTP calls to our APIs. If you want to make the HTTP calls directly using\nyour preferred tool (curl, postman,...), please refer to the [Inference API](https://huggingface.co/docs/api-inference/index)\nor to the [Inference Endpoints](https://huggingface.co/docs/inference-endpoints/index) documentation pages.", "--\ntitle: \"Hugging Face Platform on the AWS Marketplace: Pay with your AWS Account\"\nthumbnail: /blog/assets/158_aws_marketplace/thumbnail.jpg\nauthors:\n- user: philschmid\n- user: sbrandeis\n- user: jeffboudier\n---\n\n# Hugging Face Platform on the AWS Marketplace: Pay with your AWS Account\n\n\nThe [Hugging Face Platform](https://aws.amazon.com/marketplace/pp/prodview-n6vsyhdjkfng2) has landed on the AWS Marketplace. Starting today, you can subscribe to the Hugging Face Platform through AWS Marketplace to pay for your Hugging Face usage directly with your AWS account. This new integrated billing method makes it easy to manage payment for usage of all our managed services by all members of your organization, including Inference Endpoints, Spaces Hardware Upgrades, and AutoTrain to easily train, test and deploy the most popular machine learning models like Llama 2, StarCoder, or BERT.\n\nBy making [Hugging Face available on AWS Marketplace](https://aws.amazon.com/marketplace/pp/prodview-n6vsyhdjkfng2), we are removing barriers to adopting AI and making it easier for companies to leverage large language models. Now with just a few clicks, AWS customers can subscribe and connect their Hugging Face Account with their AWS account. \n\nBy subscribing through AWS Marketplace, Hugging Face organization usage charges for services like Inference Endpoints will automatically appear on your AWS bill, instead of being charged by Hugging Face to the credit card on file for your organization.\n\nWe are excited about this launch as it will bring our technology to more developers who rely on AWS, and make it easier for businesses to consume Hugging Face services.\n\n## Getting Started\n\nBefore you can connect your AWS Account with your Hugging Face account, you need to fulfill the following prerequisites:", "Using Hugging Face Integrations\n\nRelated spaces: https://huggingface.co/spaces/gradio/en2es\nTags: HUB, SPACES, EMBED\n\nContributed by <a href=\"https://huggingface.co/osanseviero\">Omar Sanseviero</a> \ud83e\udd99\n\n## Introduction\n\nThe Hugging Face Hub is a central platform that has hundreds of thousands of [models](https://huggingface.co/models), [datasets](https://huggingface.co/datasets) and [demos](https://huggingface.co/spaces) (also known as Spaces). \n\nGradio has multiple features that make it extremely easy to leverage existing models and Spaces on the Hub. This guide walks through these features.\n\n\n## Demos with the Hugging Face Inference API\n\nHugging Face has a free service called the [Inference API](https://huggingface.co/inference-api), which allows you to send HTTP requests to models in the Hub. For transformers or diffusers-based models, the API can be 2 to 10 times faster than running the inference yourself. The API is free (rate limited), and you can switch to dedicated [Inference Endpoints](https://huggingface.co/pricing) when you want to use it in production. Gradio integrates directly with the Hugging Face Inference API so that you can create a demo simply by specifying a model's name (e.g. `Helsinki-NLP/opus-mt-en-es`), like this:\n\n```python\nimport gradio as gr\n\ndemo = gr.load(\"Helsinki-NLP/opus-mt-en-es\", src=\"models\")\n\ndemo.launch()\n```\n\nFor any Hugging Face model supported in the Inference API, Gradio automatically infers the expected input and output and make the underlying server calls, so you don't have to worry about defining the prediction function.", "Billing\n\nAt Hugging Face, we build a collaboration platform for the ML community (i.e., the Hub), and we **monetize by providing simple access to compute for AI**, with services like AutoTrain, Spaces and Inference Endpoints, directly accessible from the Hub, and billed by Hugging Face to the credit card on file.\n\nWe also partner with cloud providers, like [AWS](https://huggingface.co/blog/aws-partnership) and [Azure](https://huggingface.co/blog/hugging-face-endpoints-on-azure), to make it easy for customers to use Hugging Face directly in their cloud of choice. These solutions and usage are billed directly by the cloud provider. Ultimately we want people to be able to have great options to use Hugging Face wherever they build Machine Learning.\n\nAll user and organization accounts have a billing system. You can submit your payment info to access \"pay-as-you-go\" services.\n\nFrom the [Settings > Billing](https://huggingface.co/settings/billing) page, you can see a real time view of your paid usage across all HF services, for instance:\n\n- [Spaces](./spaces),\n- [AutoTrain](https://huggingface.co/docs/autotrain/index),\n- [Inference Endpoints](https://huggingface.co/docs/inference-endpoints/index)\n- [PRO subscription](https://huggingface.co/pricing) (for users)\n- [Enterprise Hub subscription](https://huggingface.co/enterprise) (for organizations)\n\n<div class=\"flex justify-center\">\n<img class=\"block dark:hidden\" src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/hub/billing.png\"/>\n<img class=\"hidden dark:block\" src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/hub/billing-dark.png\"/>\n</div>", "--\ntitle: Inference for PROs\nthumbnail: /blog/assets/inference_pro/thumbnail.png\nauthors:\n  - user: osanseviero\n  - user: pcuenq\n  - user: victor\n---\n\n# Inference for PROs\n\n\n![Inference for PROs image](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/inference-for-pros/Inference-for-pros.png)\n\nToday, we're introducing Inference for PRO users - a community offering that gives you access to APIs of curated endpoints for some of the most exciting models available, as well as improved rate limits for the usage of free Inference API. Use the following page to [subscribe to PRO](https://huggingface.co/subscribe/pro). \n\nHugging Face PRO users now have access to exclusive API endpoints for a curated list of powerful models that benefit from ultra-fast inference powered by [text-generation-inference](https://github.com/huggingface/text-generation-inference). This is a benefit on top of the free inference API, which is available to all Hugging Face users to facilitate testing and prototyping on 200,000+ models. PRO users enjoy higher rate limits on these models, as well as exclusive access to some of the best models available today.\n\n## Contents"], "eval_score_GPT4": "5", "eval_feedback_GPT4": "Feedback: The response correctly identifies \"Inference Endpoints\" as the service replacing the Paid tier of the Inference API at Hugging Face, which aligns with the reference answer. The additional information provided about the service being fast, enterprise-grade, and still having a free tier available does not detract from the accuracy of the response with respect to the question asked."}, {"question": "What architectural feature does SqueezeBERT use instead of fully-connected layers for the Q, K, V, and FFN layers?\n", "true_answer": "Grouped convolutions", "source_doc": "huggingface/transformers/blob/main/docs/source/en/model_doc/squeezebert.md", "generated_answer": "SqueezeBERT uses grouped convolutions instead of fully-connected layers for the Q, K, V, and FFN layers, as described in the paper \"SqueezeBERT: What can computer vision teach NLP about efficient neural networks?\" by Forrest N. Iandola, Albert E. Shaw, Ravi Krishna, and Kurt W. Keutzer. This architectural choice is similar to the one used in the SE ResNet model, which also", "retrieved_docs": ["!--Copyright 2020 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\n\nhttp://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License.\n\n\u26a0\ufe0f Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\nrendered properly in your Markdown viewer.\n\n-->\n\n# SqueezeBERT\n\n## Overview\n\nThe SqueezeBERT model was proposed in [SqueezeBERT: What can computer vision teach NLP about efficient neural networks?](https://arxiv.org/abs/2006.11316) by Forrest N. Iandola, Albert E. Shaw, Ravi Krishna, Kurt W. Keutzer. It's a\nbidirectional transformer similar to the BERT model. The key difference between the BERT architecture and the\nSqueezeBERT architecture is that SqueezeBERT uses [grouped convolutions](https://blog.yani.io/filter-group-tutorial)\ninstead of fully-connected layers for the Q, K, V and FFN layers.\n\nThe abstract from the paper is the following:", "(0): TransformerEncoderLayer(\n        (self_attn): MultiheadAttention(\n          (dropout_module): FairseqDropout()\n          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n        )\n      [...]\n# the full output is in the notebook\n```\nwhich looked very similar to BART's architecture, with some slight differences in a few layers - some were added, others removed. So this was great news as I didn't have to re-invent the wheel, but to only tweak a well-working design.", "<!--\nType: model-index\nCollections:\n- Name: SE ResNet\n  Paper:\n    Title: Squeeze-and-Excitation Networks\n    URL: https://paperswithcode.com/paper/squeeze-and-excitation-networks\nModels:\n- Name: seresnet152d\n  In Collection: SE ResNet\n  Metadata:\n    FLOPs: 20161904304\n    Parameters: 66840000\n    File Size: 268144497\n    Architecture:\n    - 1x1 Convolution\n    - Batch Normalization\n    - Bottleneck Residual Block\n    - Convolution\n    - Global Average Pooling\n    - Max Pooling\n    - ReLU\n    - Residual Block\n    - Residual Connection\n    - Softmax\n    - Squeeze-and-Excitation Block\n    Tasks:\n    - Image Classification\n    Training Techniques:\n    - Label Smoothing\n    - SGD with Momentum\n    - Weight Decay\n    Training Data:\n    - ImageNet\n    Training Resources: 8x NVIDIA Titan X GPUs\n    ID: seresnet152d\n    LR: 0.6\n    Epochs: 100\n    Layers: 152\n    Dropout: 0.2\n    Crop Pct: '0.94'\n    Momentum: 0.9\n    Batch Size: 1024\n    Image Size: '256'\n    Interpolation: bicubic\n  Code: https://github.com/rwightman/pytorch-image-models/blob/a7f95818e44b281137503bcf4b3e3e94d8ffa52f/timm/models/resnet.py#L1206", "<!--\nType: model-index\nCollections:\n- Name: SE ResNet\n  Paper:\n    Title: Squeeze-and-Excitation Networks\n    URL: https://paperswithcode.com/paper/squeeze-and-excitation-networks\nModels:\n- Name: seresnet152d\n  In Collection: SE ResNet\n  Metadata:\n    FLOPs: 20161904304\n    Parameters: 66840000\n    File Size: 268144497\n    Architecture:\n    - 1x1 Convolution\n    - Batch Normalization\n    - Bottleneck Residual Block\n    - Convolution\n    - Global Average Pooling\n    - Max Pooling\n    - ReLU\n    - Residual Block\n    - Residual Connection\n    - Softmax\n    - Squeeze-and-Excitation Block\n    Tasks:\n    - Image Classification\n    Training Techniques:\n    - Label Smoothing\n    - SGD with Momentum\n    - Weight Decay\n    Training Data:\n    - ImageNet\n    Training Resources: 8x NVIDIA Titan X GPUs\n    ID: seresnet152d\n    LR: 0.6\n    Epochs: 100\n    Layers: 152\n    Dropout: 0.2\n    Crop Pct: '0.94'\n    Momentum: 0.9\n    Batch Size: 1024\n    Image Size: '256'\n    Interpolation: bicubic\n  Code: https://github.com/rwightman/pytorch-image-models/blob/a7f95818e44b281137503bcf4b3e3e94d8ffa52f/timm/models/resnet.py#L1206", "<!--\nType: model-index\nCollections:\n- Name: Legacy SE ResNeXt\n  Paper:\n    Title: Squeeze-and-Excitation Networks\n    URL: https://paperswithcode.com/paper/squeeze-and-excitation-networks\nModels:\n- Name: legacy_seresnext101_32x4d\n  In Collection: Legacy SE ResNeXt\n  Metadata:\n    FLOPs: 10287698672\n    Parameters: 48960000\n    File Size: 196466866\n    Architecture:\n    - 1x1 Convolution\n    - Batch Normalization\n    - Convolution\n    - Global Average Pooling\n    - Grouped Convolution\n    - Max Pooling\n    - ReLU\n    - ResNeXt Block\n    - Residual Connection\n    - Softmax\n    - Squeeze-and-Excitation Block\n    Tasks:\n    - Image Classification\n    Training Techniques:\n    - Label Smoothing\n    - SGD with Momentum\n    - Weight Decay\n    Training Data:\n    - ImageNet\n    Training Resources: 8x NVIDIA Titan X GPUs\n    ID: legacy_seresnext101_32x4d\n    LR: 0.6\n    Epochs: 100\n    Layers: 101\n    Dropout: 0.2\n    Crop Pct: '0.875'\n    Momentum: 0.9\n    Batch Size: 1024\n    Image Size: '224'\n    Interpolation: bilinear", "<!--\nType: model-index\nCollections:\n- Name: Legacy SE ResNeXt\n  Paper:\n    Title: Squeeze-and-Excitation Networks\n    URL: https://paperswithcode.com/paper/squeeze-and-excitation-networks\nModels:\n- Name: legacy_seresnext101_32x4d\n  In Collection: Legacy SE ResNeXt\n  Metadata:\n    FLOPs: 10287698672\n    Parameters: 48960000\n    File Size: 196466866\n    Architecture:\n    - 1x1 Convolution\n    - Batch Normalization\n    - Convolution\n    - Global Average Pooling\n    - Grouped Convolution\n    - Max Pooling\n    - ReLU\n    - ResNeXt Block\n    - Residual Connection\n    - Softmax\n    - Squeeze-and-Excitation Block\n    Tasks:\n    - Image Classification\n    Training Techniques:\n    - Label Smoothing\n    - SGD with Momentum\n    - Weight Decay\n    Training Data:\n    - ImageNet\n    Training Resources: 8x NVIDIA Titan X GPUs\n    ID: legacy_seresnext101_32x4d\n    LR: 0.6\n    Epochs: 100\n    Layers: 101\n    Dropout: 0.2\n    Crop Pct: '0.875'\n    Momentum: 0.9\n    Batch Size: 1024\n    Image Size: '224'\n    Interpolation: bilinear", "<!--\nType: model-index\nCollections:\n- Name: Legacy SE ResNet\n  Paper:\n    Title: Squeeze-and-Excitation Networks\n    URL: https://paperswithcode.com/paper/squeeze-and-excitation-networks\nModels:\n- Name: legacy_seresnet101\n  In Collection: Legacy SE ResNet\n  Metadata:\n    FLOPs: 9762614000\n    Parameters: 49330000\n    File Size: 197822624\n    Architecture:\n    - 1x1 Convolution\n    - Batch Normalization\n    - Bottleneck Residual Block\n    - Convolution\n    - Global Average Pooling\n    - Max Pooling\n    - ReLU\n    - Residual Block\n    - Residual Connection\n    - Softmax\n    - Squeeze-and-Excitation Block\n    Tasks:\n    - Image Classification\n    Training Techniques:\n    - Label Smoothing\n    - SGD with Momentum\n    - Weight Decay\n    Training Data:\n    - ImageNet\n    Training Resources: 8x NVIDIA Titan X GPUs\n    ID: legacy_seresnet101\n    LR: 0.6\n    Epochs: 100\n    Layers: 101\n    Dropout: 0.2\n    Crop Pct: '0.875'\n    Momentum: 0.9\n    Batch Size: 1024\n    Image Size: '224'\n    Interpolation: bilinear\n  Code: https://github.com/rwightman/pytorch-image-models/blob/d8e69206be253892b2956341fea09fdebfaae4e3/timm/models/senet.py#L426"], "eval_score_GPT4": "5", "eval_feedback_GPT4": "The response correctly identifies that SqueezeBERT uses grouped convolutions instead of fully-connected layers for the Q, K, V, and FFN layers. The information provided is accurate and factual, aligning with the reference answer. The response goes beyond the reference answer by citing the paper and comparing the architectural choice to SE ResNet model, which is not required but does not detract from the correctness of the answer."}, {"question": "What type of license is the HuggingFace Team's software distributed under?\n", "true_answer": "Apache License, Version 2.0", "source_doc": "huggingface/simulate/blob/main/docs/source/tutorials/interaction.mdx", "generated_answer": "The HuggingFace Team's software is distributed under the Apache License, Version 2.0, as stated in extracts 0, 1, 2, and 6 provided in the context. The license requires that the software may not be used except in compliance with the license, and it is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. The specific language governing permissions and limitations under the license can be found by obtaining", "retrieved_docs": ["!---\nCopyright 2020 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\");\nyou may not use this file except in compliance with the License.\nYou may obtain a copy of the License at\n\n    http://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software\ndistributed under the License is distributed on an \"AS IS\" BASIS,\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\nSee the License for the specific language governing permissions and\nlimitations under the License.\n-->\n\n<p align=\"center\">\n  <picture>\n    <source media=\"(prefers-color-scheme: dark)\" srcset=\"https://huggingface.co/datasets/huggingface/documentation-images/raw/main/transformers-logo-dark.svg\">\n    <source media=\"(prefers-color-scheme: light)\" srcset=\"https://huggingface.co/datasets/huggingface/documentation-images/raw/main/transformers-logo-light.svg\">\n    <img alt=\"Hugging Face Transformers Library\" src=\"https://huggingface.co/datasets/huggingface/documentation-images/raw/main/transformers-logo-light.svg\" width=\"352\" height=\"59\" style=\"max-width: 100%;\">\n  </picture>\n  <br/>\n  <br/>\n</p>", "!---\nCopyright 2020 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\");\nyou may not use this file except in compliance with the License.\nYou may obtain a copy of the License at\n\n    http://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software\ndistributed under the License is distributed on an \"AS IS\" BASIS,\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\nSee the License for the specific language governing permissions and\nlimitations under the License.\n-->\n\n<p align=\"center\">\n  <picture>\n    <source media=\"(prefers-color-scheme: dark)\" srcset=\"https://huggingface.co/datasets/huggingface/documentation-images/raw/main/transformers-logo-dark.svg\">\n    <source media=\"(prefers-color-scheme: light)\" srcset=\"https://huggingface.co/datasets/huggingface/documentation-images/raw/main/transformers-logo-light.svg\">\n    <img alt=\"Hugging Face Transformers Library\" src=\"https://huggingface.co/datasets/huggingface/documentation-images/raw/main/transformers-logo-light.svg\" width=\"352\" height=\"59\" style=\"max-width: 100%;\">\n  </picture>\n  <br/>\n  <br/>\n</p>", "!---\nCopyright 2023 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\");\nyou may not use this file except in compliance with the License.\nYou may obtain a copy of the License at\n\n    http://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software\ndistributed under the License is distributed on an \"AS IS\" BASIS,\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\nSee the License for the specific language governing permissions and\nlimitations under the License.\n-->\n\n<p align=\"center\">\n  <picture>\n    <source media=\"(prefers-color-scheme: dark)\" srcset=\"https://huggingface.co/datasets/huggingface/documentation-images/raw/main/transformers-logo-dark.svg\">\n    <source media=\"(prefers-color-scheme: light)\" srcset=\"https://huggingface.co/datasets/huggingface/documentation-images/raw/main/transformers-logo-light.svg\">\n    <img alt=\"Hugging Face Transformers Library\" src=\"https://huggingface.co/datasets/huggingface/documentation-images/raw/main/transformers-logo-light.svg\" width=\"352\" height=\"59\" style=\"max-width: 100%;\">\n  </picture>\n  <br/>\n  <br/>\n</p>", "!---\nCopyright 2023 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\");\nyou may not use this file except in compliance with the License.\nYou may obtain a copy of the License at\n\n    http://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software\ndistributed under the License is distributed on an \"AS IS\" BASIS,\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\nSee the License for the specific language governing permissions and\nlimitations under the License.\n-->\n\n<p align=\"center\">\n  <picture>\n    <source media=\"(prefers-color-scheme: dark)\" srcset=\"https://huggingface.co/datasets/huggingface/documentation-images/raw/main/transformers-logo-dark.svg\">\n    <source media=\"(prefers-color-scheme: light)\" srcset=\"https://huggingface.co/datasets/huggingface/documentation-images/raw/main/transformers-logo-light.svg\">\n    <img alt=\"Hugging Face Transformers Library\" src=\"https://huggingface.co/datasets/huggingface/documentation-images/raw/main/transformers-logo-light.svg\" width=\"352\" height=\"59\" style=\"max-width: 100%;\">\n  </picture>\n  <br/>\n  <br/>\n</p>", "!---\nCopyright 2020 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\");\nyou may not use this file except in compliance with the License.\nYou may obtain a copy of the License at\n\n    http://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software\ndistributed under the License is distributed on an \"AS IS\" BASIS,\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\nSee the License for the specific language governing permissions and\nlimitations under the License.\n-->\n\n# \ud83e\udd17 Benchmark results\n\nHere, you can find a list of the different benchmark results created by the community.\n\nIf you would like to list benchmark results on your favorite models of the [model hub](https://huggingface.co/models) here, please open a Pull Request and add it below.", "!---\nCopyright 2020 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\");\nyou may not use this file except in compliance with the License.\nYou may obtain a copy of the License at\n\n    http://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software\ndistributed under the License is distributed on an \"AS IS\" BASIS,\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\nSee the License for the specific language governing permissions and\nlimitations under the License.\n-->\n\n# \ud83e\udd17 Benchmark results\n\nHere, you can find a list of the different benchmark results created by the community.\n\nIf you would like to list benchmark results on your favorite models of the [model hub](https://huggingface.co/models) here, please open a Pull Request and add it below.", "!---\nCopyright 2023 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\");\nyou may not use this file except in compliance with the License.\nYou may obtain a copy of the License at\n\n    http://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software\ndistributed under the License is distributed on an \"AS IS\" BASIS,\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\nSee the License for the specific language governing permissions and\nlimitations under the License.\n-->\n\n# \ud83e\udd17 Transformers Notebooks\n\nYou can find here a list of the official notebooks provided by Hugging Face.\n\nAlso, we would like to list here interesting content created by the community.\nIf you wrote some notebook(s) leveraging \ud83e\udd17 Transformers and would like to be listed here, please open a\nPull Request so it can be included under the Community notebooks.\n\n\n## Hugging Face's notebooks \ud83e\udd17\n\n### Documentation notebooks\n\nYou can open any page of the documentation as a notebook in Colab (there is a button directly on said pages) but they are also listed here if you need them:"], "eval_score_GPT4": "5", "eval_feedback_GPT4": "Feedback: The response correctly identifies the license as the Apache License, Version 2.0, which is in agreement with the reference answer. The additional details provided about the license terms and conditions are accurate and factual, although they go beyond the simplicity of the reference answer. However, the score rubric does not penalize for additional correct information. Therefore, the response meets the criteria for a score of 5."}, {"question": "What are the two parameter-reduction techniques proposed in the ALBERT model to lower memory consumption and increase training speed?\n", "true_answer": "Splitting the embedding matrix into two smaller matrices and using repeating layers split among groups.", "source_doc": "huggingface/transformers/blob/main/docs/source/en/model_doc/albert.md", "generated_answer": "The two parameter-reduction techniques proposed in the ALBERT model to lower memory consumption and increase training speed are embedding size E being different from hidden size H justified because the embeddings are context independent (one embedding vector represents one token), whereas hidden states are context dependent (one hidden state represents a sequence of tokens) so it's more logical to have H >> E, and layers being split in groups that share parameters (to save memory).", "retrieved_docs": ["The main drawback of this approach is that it requires staggering amounts of GPU memory - almost an order of magnitude \nlarger than its dense equivalent. Various distillation and approaches are proposed to how to overcome the much higher memory requirements.\n\nThere is direct trade-off though, you can use just a few experts with a 2-3x smaller base model instead of dozens or \nhundreds experts leading to a 5x smaller model and thus increase the training speed moderately while increasing the \nmemory requirements moderately as well.\n\nMost related papers and implementations are built around Tensorflow/TPUs:\n\n- [GShard: Scaling Giant Models with Conditional Computation and Automatic Sharding](https://arxiv.org/abs/2006.16668)\n- [Switch Transformers: Scaling to Trillion Parameter Models with Simple and Efficient Sparsity](https://arxiv.org/abs/2101.03961)\n- [GLaM: Generalist Language Model (GLaM)](https://ai.googleblog.com/2021/12/more-efficient-in-context-learning-with.html)", "## Efficient training strategies\n\nEven training the smallest LLaMA model requires an enormous amount of memory. Some quick math: in bf16, every parameter uses 2 bytes (in fp32 4 bytes) in addition to 8 bytes used, e.g., in the Adam optimizer (see the [performance docs](https://huggingface.co/docs/transformers/perf_train_gpu_one#optimizer) in Transformers for more info). So a 7B parameter model would use `(2+8)*7B=70GB` just to fit in memory and would likely need more when you compute intermediate values such as attention scores. So you couldn\u2019t train the model even on a single 80GB A100 like that. You can use some tricks, like more efficient optimizers of half-precision training, to squeeze a bit more into memory, but you\u2019ll run out sooner or later.\n\nAnother option is to use Parameter-Efficient Fine-Tuning (PEFT) techniques, such as the [`peft`](https://github.com/huggingface/peft) library, which can perform Low-Rank Adaptation (LoRA) on a model loaded in 8-bit. \n\n![](https://huggingface.co/datasets/trl-internal-testing/example-images/resolve/main/blog/stackllama/lora-animated.gif)   \n*Low-Rank Adaptation of linear layers: extra parameters (in orange) are added next to the frozen layer (in blue), and the resulting encoded hidden states are added together with the hidden states of the frozen layer.*\n\nLoading the model in 8bit reduces the memory footprint drastically since you only need one byte per parameter for the weights (e.g. 7B LlaMa is 7GB in memory). Instead of training the original weights directly, LoRA adds small adapter layers on top of some specific layers (usually the attention layers); thus, the number of trainable parameters is drastically reduced.", "You might want to use what is called **parameter efficient fine-tuning** (PEFT).\nThis technique first freezes up the parameters of your pretrained model of interest, then adds a number of new parameters on top of it, called the adapters. What you then fine-tune on your task are only the (lightweight) adapter weights, considerably smaller than the original model. You then just need to share your small adapter weights (and the base model)! You'll find a list of interesting approaches for PEFT [here](https://github.com/huggingface/peft).\n\n### Quantization: Models running everywhere\nWe've seen that well-performing models now come in all shapes and sizes\u2026 but even then, it doesn't mean that they are accessible to all! A 30B parameters model can require more than 66G of RAM just to load in memory (not even use), and not everyone in the community has the hardware necessary to do so.\n\nThat's where quantization comes in! Quantization is a special technique which reduces a model's size by changing the precision of its parameters. \n\nWhat does it mean? \n\nIn a computer, numbers are stored with a given precision (such as `float32`, `float16`, `int8`, and so forth). A precision indicates both the number type (is it a floating point number or an integer) as well as on how much memory the number is stored: `float32` stores floating point numbers on 32 bits. For a more in-depth explanation, see [this link](https://huggingface.co/docs/optimum/concept_guides/quantization#going-further-how-do-machines-represent-numbers). So, the higher the precision, the more physical memory a number takes, as it will be stored on more bits.", "## Usage tips\n\n- ALBERT is a model with absolute position embeddings so it's usually advised to pad the inputs on the right rather\n  than the left.\n- ALBERT uses repeating layers which results in a small memory footprint, however the computational cost remains\n  similar to a BERT-like architecture with the same number of hidden layers as it has to iterate through the same\n  number of (repeating) layers.\n- Embedding size E is different from hidden size H justified because the embeddings are context independent (one embedding vector represents one token), whereas hidden states are context dependent (one hidden state represents a sequence of tokens) so it's more logical to have H >> E. Also, the embedding matrix is large since it's V x E (V being the vocab size). If E < H, it has less parameters.\n- Layers are split in groups that share parameters (to save memory).\nNext sentence prediction is replaced by a sentence ordering prediction: in the inputs, we have two sentences A and B (that are consecutive) and we either feed A followed by B or B followed by A. The model must predict if they have been swapped or not.\n\n\n\nThis model was contributed by [lysandre](https://huggingface.co/lysandre). This model jax version was contributed by\n[kamalkraj](https://huggingface.co/kamalkraj). The original code can be found [here](https://github.com/google-research/ALBERT).\n\n\n## Resources\n\n\nThe resources provided in the following sections consist of a list of official Hugging Face and community (indicated by \ud83c\udf0e) resources to help you get started with AlBERT. If you're interested in submitting a resource to be included here, please feel free to open a Pull Request and we'll review it! The resource should ideally demonstrate something new instead of duplicating an existing resource.\n\n\n<PipelineTag pipeline=\"text-classification\"/>\n\n\n- [`AlbertForSequenceClassification`] is supported by this [example script](https://github.com/huggingface/transformers/tree/main/examples/pytorch/text-classification).", "!--Copyright 2022 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\n\nhttp://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\n\n\u26a0\ufe0f Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\nrendered properly in your Markdown viewer.\n\n-->\n\n# Methods and tools for efficient training on a single GPU\n\nThis guide demonstrates practical techniques that you can use to increase the efficiency of your model's training by \noptimizing memory utilization, speeding up the training, or both. If you'd like to understand how GPU is utilized during \ntraining, please refer to the [Model training anatomy](model_memory_anatomy) conceptual guide first. This guide \nfocuses on practical techniques.  \n\n<Tip>\n\nIf you have access to a machine with multiple GPUs, these approaches are still valid, plus you can leverage additional methods outlined in the [multi-GPU section](perf_train_gpu_many).\n\n</Tip>\n\nWhen training large models, there are two aspects that should be considered at the same time: \n\n* Data throughput/training time\n* Model performance\n\nMaximizing the throughput (samples/second) leads to lower training cost. This is generally achieved by utilizing the GPU \nas much as possible and thus filling GPU memory to its limit. If the desired batch size exceeds the limits of the GPU memory, \nthe memory optimization techniques, such as gradient accumulation, can help.", "OFT preserves the hyperspherical energy by learning an orthogonal transformation for neurons to keep the cosine similarity between them unchanged. In practice, this means taking the matrix product of an orthogonal matrix with the pretrained weight matrix. However, to be parameter-efficient, the orthogonal matrix is represented as a block-diagonal matrix with rank `r` blocks. Whereas LoRA reduces the number of trainable parameters with low-rank structures, OFT reduces the number of trainable parameters with a sparse block-diagonal matrix structure.\n\n## Adaptive Low-Rank Adaptation (AdaLoRA)\n\n[AdaLoRA](https://hf.co/papers/2303.10512) manages the parameter budget introduced from LoRA by allocating more parameters - in other words, a higher rank `r` - for important weight matrices that are better adapted for a task and pruning less important ones. The rank is controlled by a method similar to singular value decomposition (SVD). The \u2206W is parameterized with two orthogonal matrices and a diagonal matrix which contains singular values. This parametrization method avoids iteratively applying SVD which is computationally expensive. Based on this method, the rank of \u2206W is adjusted according to an importance score. \u2206W is divided into triplets and each triplet is scored according to its contribution to model performance. Triplets with low importance scores are pruned and triplets with high importance scores are kept for finetuning.\n\n## Llama-Adapter\n\n[Llama-Adapter](https://hf.co/papers/2303.16199) is a method for adapting Llama into a instruction-following model. To help adapt the model for instruction-following, the adapter is trained with a 52K instruction-output dataset.\n\nA set of of learnable adaption prompts are prefixed to the input instruction tokens. These are inserted into the upper layers of the model because it is better to learn with the higher-level semantics of the pretrained model. The instruction-output tokens prefixed to the input guide the adaption prompt to generate a contextual response.", "!--Copyright 2023 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\n\nhttp://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License.\n-->\n\n# Reduce memory usage\n\nA barrier to using diffusion models is the large amount of memory required. To overcome this challenge, there are several memory-reducing techniques you can use to run even some of the largest models on free-tier or consumer GPUs. Some of these techniques can even be combined to further reduce memory usage.\n\n<Tip>\n\nIn many cases, optimizing for memory or speed leads to improved performance in the other, so you should try to optimize for both whenever you can. This guide focuses on minimizing memory usage, but you can also learn more about how to [Speed up inference](fp16).\n\n</Tip>\n\nThe results below are obtained from generating a single 512x512 image from the prompt a photo of an astronaut riding a horse on mars with 50 DDIM steps on a Nvidia Titan RTX, demonstrating the speed-up you can expect as a result of reduced memory consumption.\n\n|                  | latency | speed-up |\n| ---------------- | ------- | ------- |\n| original         | 9.50s   | x1      |\n| fp16             | 3.61s   | x2.63   |\n| channels last    | 3.30s   | x2.88   |\n| traced UNet      | 3.21s   | x2.96   |\n| memory-efficient attention  | 2.63s  | x3.61   |\n\n## Sliced VAE"], "eval_score_GPT4": "5", "eval_feedback_GPT4": "Feedback: The response correctly identifies two parameter-reduction techniques in the ALBERT model, which are the separation of embedding size E from hidden size H and the sharing of parameters across layers. The explanation provided aligns well with the reference answer, although it does not use the exact terminology of \"splitting the embedding matrix into two smaller matrices.\" However, the essence of the first technique is captured by explaining the rationale behind having a smaller embedding size compared to the hidden size. The second technique is accurately described as \"layers being split in groups that share parameters,\" which is a restatement of \"using repeating layers split among groups\" from the reference answer. The response is factual and demonstrates a clear understanding of the parameter-reduction techniques used in ALBERT."}, {"question": "What are the three main steps for fine-tuning a model with the \ud83e\udd17 Datasets library?\n", "true_answer": "1. Load a dataset from the Hugging Face Hub. 2. Preprocess the data with `Dataset.map()`. 3. Load and compute metrics.", "source_doc": "huggingface/course/blob/main/chapters/en/chapter5/1.mdx", "generated_answer": "The three main steps for fine-tuning a model with the \ud83e\udd17 Datasets library are:\n\n1. Load a dataset from the Hugging Face Hub or create your own dataset using the library's features for slicing, dicing, and memory mapping.\n2. Preprocess the data using `Dataset.map()` to transform it into a format that the model can understand.\n3. Load and compute metrics using the `Dataset.map()` function", "retrieved_docs": ["Introduction[[introduction]]\n\n<CourseFloatingBanner\n    chapter={5}\n    classNames=\"absolute z-10 right-0 top-0\"\n/>\n\nIn [Chapter 3](/course/chapter3) you got your first taste of the \ud83e\udd17 Datasets library and saw that there were three main steps when it came to fine-tuning a model:\n\n1. Load a dataset from the Hugging Face Hub.\n2. Preprocess the data with `Dataset.map()`.\n3. Load and compute metrics.\n\nBut this is just scratching the surface of what \ud83e\udd17 Datasets can do! In this chapter, we will take a deep dive into the library. Along the way, we'll find answers to the following questions:\n\n* What do you do when your dataset is not on the Hub?\n* How can you slice and dice a dataset? (And what if you _really_ need to use Pandas?)\n* What do you do when your dataset is huge and will melt your laptop's RAM?\n* What the heck are \"memory mapping\" and Apache Arrow?\n* How can you create your own dataset and push it to the Hub?\n\nThe techniques you learn here will prepare you for the advanced tokenization and fine-tuning tasks in [Chapter 6](/course/chapter6) and [Chapter 7](/course/chapter7) -- so grab a coffee and let's get started!", "</Tip>\n\nTo fine-tune a model in TensorFlow, follow these steps:\n1. Define the training hyperparameters, and set up an optimizer and a learning rate schedule.\n2. Instantiate a pretrained model.\n3. Convert a \ud83e\udd17 Dataset to a `tf.data.Dataset`.\n4. Compile your model.\n5. Add callbacks to calculate metrics and upload your model to \ud83e\udd17 Hub\n6. Use the `fit()` method to run the training.\n\nStart by defining the hyperparameters, optimizer and learning rate schedule:\n\n```py\n>>> from transformers import create_optimizer\n\n>>> batch_size = 2\n>>> num_epochs = 50\n>>> num_train_steps = len(train_ds) * num_epochs\n>>> learning_rate = 6e-5\n>>> weight_decay_rate = 0.01\n\n>>> optimizer, lr_schedule = create_optimizer(\n...     init_lr=learning_rate,\n...     num_train_steps=num_train_steps,\n...     weight_decay_rate=weight_decay_rate,\n...     num_warmup_steps=0,\n... )\n```\n\nThen, load SegFormer with [`TFAutoModelForSemanticSegmentation`] along with the label mappings, and compile it with the\noptimizer. Note that Transformers models all have a default task-relevant loss function, so you don't need to specify one unless you want to:\n\n```py\n>>> from transformers import TFAutoModelForSemanticSegmentation\n\n>>> model = TFAutoModelForSemanticSegmentation.from_pretrained(\n...     checkpoint,\n...     id2label=id2label,\n...     label2id=label2id,\n... )\n>>> model.compile(optimizer=optimizer)  # No loss argument!\n```", "</Tip>\n\nTo fine-tune a model in TensorFlow, follow these steps:\n1. Define the training hyperparameters, and set up an optimizer and a learning rate schedule.\n2. Instantiate a pre-trained model.\n3. Convert a \ud83e\udd17 Dataset to a `tf.data.Dataset`.\n4. Compile your model.\n5. Add callbacks and use the `fit()` method to run the training.\n6. Upload your model to \ud83e\udd17 Hub to share with the community.\n\nStart by defining the hyperparameters, optimizer and learning rate schedule:\n\n```py\n>>> from transformers import create_optimizer\n\n>>> batch_size = 16\n>>> num_epochs = 5\n>>> num_train_steps = len(food[\"train\"]) * num_epochs\n>>> learning_rate = 3e-5\n>>> weight_decay_rate = 0.01\n\n>>> optimizer, lr_schedule = create_optimizer(\n...     init_lr=learning_rate,\n...     num_train_steps=num_train_steps,\n...     weight_decay_rate=weight_decay_rate,\n...     num_warmup_steps=0,\n... )\n```\n\nThen, load ViT with [`TFAutoModelForImageClassification`] along with the label mappings:\n\n```py\n>>> from transformers import TFAutoModelForImageClassification\n\n>>> model = TFAutoModelForImageClassification.from_pretrained(\n...     checkpoint,\n...     id2label=id2label,\n...     label2id=label2id,\n... )\n```\n\nConvert your datasets to the `tf.data.Dataset` format using the [`~datasets.Dataset.to_tf_dataset`] and your `data_collator`:", "FrameworkSwitchCourse {fw} />\n\n# Fine-tuning, Check![[fine-tuning-check]]\n\n<CourseFloatingBanner\n    chapter={3}\n    classNames=\"absolute z-10 right-0 top-0\"\n/>\n\nThat was fun! In the first two chapters you learned about models and tokenizers, and now you know how to fine-tune them for your own data. To recap, in this chapter you:\n\n{#if fw === 'pt'}\n* Learned about datasets in the [Hub](https://huggingface.co/datasets)\n* Learned how to load and preprocess datasets, including using dynamic padding and collators\n* Implemented your own fine-tuning and evaluation of a model\n* Implemented a lower-level training loop\n* Used \ud83e\udd17 Accelerate to easily adapt your training loop so it works for multiple GPUs or TPUs\n\n{:else}\n* Learned about datasets in the [Hub](https://huggingface.co/datasets)\n* Learned how to load and preprocess datasets\n* Learned how to fine-tune and evaluate a model with Keras\n* Implemented a custom metric\n\n{/if}", "This concludes the introduction to fine-tuning using the Keras API. An example of doing this for most common NLP tasks will be given in [Chapter 7](/course/chapter7). If you would like to hone your skills on the Keras API, try to fine-tune a model on the GLUE SST-2 dataset, using the data processing you did in section 2.", "In all of the above examples, you will need to make sure that you either already have or can easily obtain a large enough \ndomain-specific dataset at a reasonable cost to fine-tune a model. You will also need to have enough time and resources \nto fine-tune a model.\n\nIf the above examples are not the case for you, optimizing prompts can prove to be more beneficial.", "If you're looking to fine-tune a model on an existing instruction dataset, you need to know how a dataset was compiled. Some of the existing instruction datasets are either crowd-sourced or use outputs of existing models (e.g., the models behind ChatGPT). [ALPACA](https://crfm.stanford.edu/2023/03/13/alpaca.html) dataset created by Stanford is created through the outputs of models behind ChatGPT. Moreover, there are various crowd-sourced instruction datasets with open-source licenses, like [oasst1](https://huggingface.co/datasets/OpenAssistant/oasst1) (created by thousands of people voluntarily!) or [databricks/databricks-dolly-15k](https://huggingface.co/datasets/databricks/databricks-dolly-15k). If you'd like to create a dataset yourself, you can check out [the dataset card of Dolly](https://huggingface.co/datasets/databricks/databricks-dolly-15k#sources) on how to create an instruction dataset. Models fine-tuned on these datasets can be distributed. \n\nYou can find a comprehensive table of some open-source/open-access models below."], "eval_score_GPT4": "4", "eval_feedback_GPT4": "Feedback: The response provided is mostly correct and factual. It correctly identifies the three main steps for fine-tuning a model with the \ud83e\udd17 Datasets library: loading a dataset, preprocessing the data, and computing metrics. However, the third step is not accurately described. The response mentions using the `Dataset.map()` function to load and compute metrics, which is not entirely accurate. The `Dataset.map()` function is primarily used for preprocessing. Computing metrics is typically done after the model has made predictions, not during the data mapping process. Therefore, the response is not completely accurate in describing the third step."}, {"question": "What is the maximum improvement in throughput achieved by Hugging Face Infinity compared to vanilla transformers?\n", "true_answer": "+800%", "source_doc": "huggingface/blog/blob/main/infinity-cpu-performance.md", "generated_answer": "According to the benchmark presented in the post, Hugging Face Infinity achieves up to 800% higher throughput compared to vanilla transformers for sequence lengths up to 64 tokens. The exact improvement varies for different sequence lengths, as shown in the table provided:\n\n| Sequence Length | Infinity    | Transformers | improvement |\n|-----------------|-------------|--------------|-------------|\n| 8               | 248 req/", "retrieved_docs": ["<br>\n<figure class=\"image table text-center m-0 w-full\">\n  <medium-zoom background=\"rgba(0,0,0,.7)\" alt=\"Latency\" src=\"assets/46_infinity_cpu_performance/latency.png\"></medium-zoom>\n  <figcaption>Figure 4. Latency (Batch=1, Physical Cores=2)</figcaption>\n</figure>\n<br>\n\n---\n\n## Conclusion\n\nIn this post, we showed how Hugging Face Infinity performs on the new Intel Ice Lake Xeon CPU. We created a detailed benchmark with over 190 different configurations sharing the results you can expect when using Hugging Face Infinity on CPU, what would be the best configuration to optimize your Infinity Container for latency, and what would be the best configuration to maximize throughput.\n\nHugging Face Infinity can deliver up to 800% higher throughput compared to vanilla transformers, and down to 1-4ms latency for sequence lengths up to 64 tokens.\n\nThe flexibility to optimize transformer models for throughput, latency, or both enables businesses to either reduce the amount of infrastructure cost for the same workload or to enable real-time use cases that were not possible before. \n\nIf you are interested in trying out Hugging Face Infinity sign up for your trial at [hf.co/infinity-trial](https://hf.co/infinity-trial) \n\n\n## Resources", "To target these needs, Hugging Face built two open-sourced libraries: __Accelerate__ and __Optimum__. While [\ud83e\udd17 Accelerate](https://huggingface.co/docs/accelerate/index) focuses on out-of-the-box distributed training, [\ud83e\udd17 Optimum](https://huggingface.co/docs/optimum/index), as an extension of transformers, accelerates model training and inference by leveraging the maximum efficiency of users\u2019 targeted hardware. Optimum integrated machine learning accelerators like ONNX Runtime and specialized hardware like [Intel's Habana Gaudi](https://huggingface.co/blog/habana-gaudi-2-benchmark), so users can benefit from considerable speedup in both training and inference. Besides, Optimum seamlessly integrates other Hugging Face\u2019s tools while inheriting the same ease of use as Transformers. Developers can easily adapt their work to achieve lower latency with less computing power.\n\n## ONNX Runtime Training\n\n[ONNX Runtime](https://onnxruntime.ai/) accelerates [large model training](https://onnxruntime.ai/docs/get-started/training-pytorch.html) to speed up throughput by up to 40% standalone, and 130% when composed with [DeepSpeed](https://www.deepspeed.ai/tutorials/zero/) for popular HuggingFace transformer based models. ONNX Runtime is already integrated as part of Optimum and enables faster training through Hugging Face\u2019s Optimum training framework.", "### Throughput\n\nBelow you can find the throughput comparison for running infinity on 2 physical cores with batch size 1, compared with vanilla transformers.\n\n<br>\n<figure class=\"image table text-center m-0 w-full\">\n  <medium-zoom background=\"rgba(0,0,0,.7)\" alt=\"Throughput\" src=\"assets/46_infinity_cpu_performance/throughput.png\"></medium-zoom>\n  <figcaption>Figure 3. Throughput: Infinity vs Transformers</figcaption>\n</figure>\n<br>\n\n\n| Sequence Length | Infinity    | Transformers | improvement |\n|-----------------|-------------|--------------|-------------|\n| 8               | 248 req/sec | 49 req/sec   | +506%       |\n| 16              | 212 req/sec | 50 req/sec   | +424%       |\n| 32              | 150 req/sec | 40 req/sec   | +375%       |\n| 64              | 97 req/sec  | 28 req/sec   | +346%       |\n| 128             | 55 req/sec  | 18 req/sec   | +305%       |\n| 256             | 27 req/sec  | 9 req/sec    | +300%       |\n| 384             | 17 req/sec  | 5 req/sec    | +340%       |\n| 512             | 12 req/sec  | 4 req/sec    | +300%       |\n\n\n### Latency \n\nBelow, you can find the latency results for an experiment running Hugging Face Infinity on 2 Physical Cores with Batch Size 1. It is remarkable to see how robust and constant Infinity is, with minimal deviation for p95, p99, or p100 (max latency). This result is confirmed for other experiments as well in the benchmark.", "Test it out to achieve __lower latency, higher throughput, and larger maximum batch size__ while training models in \ud83e\udd17 Transformers!\n\n## Performance\n\nThe chart below shows impressive acceleration __from 39% to 130%__ for Hugging Face models with Optimum when __using ONNX Runtime and DeepSpeed ZeRO Stage 1__ for training.\nThe performance measurements were done on selected Hugging Face models with PyTorch as the baseline run, only ONNX Runtime for training as the second run, and ONNX\nRuntime + DeepSpeed ZeRO Stage 1 as the final run, showing maximum gains. The Optimizer used for the baseline PyTorch runs is the AdamW optimizer and the ORT Training\nruns use the Fused Adam Optimizer(available in `ORTTrainingArguments`). The runs were performed on a single Nvidia A100 node with 8 GPUs.\n\n<figure class=\"image table text-center m-0 w-full\">\n  <img src=\"https://huggingface.co/datasets/optimum/documentation-images/resolve/main/onnxruntime/onnxruntime-training-benchmark.png\" alt=\"ONNX Runtime Training Benchmark\"/>\n</figure>\n\nThe version information used for these runs is as follows:\n```\nPyTorch: 1.14.0.dev20221103+cu116; ORT: 1.14.0.dev20221103001+cu116; DeepSpeed: 0.6.6; HuggingFace: 4.24.0.dev0; Optimum: 1.4.1.dev0; Cuda: 11.6.2\n```\n\n## Start by setting up the environment\n\nTo use ONNX Runtime for training, you need a machine with at least one NVIDIA or AMD GPU.\n\nTo use `ORTTrainer` or `ORTSeq2SeqTrainer`, you need to install ONNX Runtime Training module and Optimum.\n\n### Install ONNX Runtime", "The main bottleneck is the latency of predictions which can make large deployments expensive to run and real-time use cases impractical. Solving this is a difficult engineering challenge for any Machine Learning Engineering team and requires the use of advanced techniques to optimize models all the way down to the hardware.\n\nWith [Hugging Face Infinity](https://huggingface.co/infinity), we offer a containerized solution that makes it easy to deploy low-latency, high-throughput, hardware-accelerated inference pipelines for the most popular Transformer models. Companies can get both the accuracy of Transformers and the efficiency necessary for large volume deployments, all in a simple to use package. In this blog post, we want to share detailed performance results for Infinity running on the latest generation of Intel Xeon CPU, to achieve optimal cost, efficiency, and latency for your Transformer deployments.\n\n\n## What is Hugging Face Infinity\n\nHugging Face Infinity is a containerized solution for customers to deploy end-to-end optimized inference pipelines for State-of-the-Art Transformer models, on any infrastructure.\n\nHugging Face Infinity consists of 2 main services:\n* The Infinity Container is a hardware-optimized inference solution delivered as a Docker container.\n* Infinity Multiverse is a Model Optimization Service through which a Hugging Face Transformer model is optimized for the Target Hardware. Infinity Multiverse is compatible with Infinity Container.\n\nThe Infinity Container is built specifically to run on a Target Hardware architecture and exposes an HTTP /predict endpoint to run inference.\n\n<br>\n<figure class=\"image table text-center m-0 w-full\">\n  <medium-zoom background=\"rgba(0,0,0,.7)\" alt=\"Product overview\" src=\"assets/46_infinity_cpu_performance/overview.png\"></medium-zoom>\n  <figcaption>Figure 1. Infinity Overview</figcaption>\n</figure>\n<br>", "\u27a1\ufe0f[Learn more about ONNX.](https://onnx.ai/about.html)\n</div>\n    </div>\n        </div>\n<html itemscope itemtype=\"https://schema.org/FAQPage\">\n  <div itemscope itemprop=\"mainEntity\" itemtype=\"https://schema.org/Question\">\n    <a id=\"2-what-is-hugging-face-optimum\"><h2 itemprop=\"name\"> 2. What is Hugging Face Optimum?</h2></a>\n    <div itemscope itemprop=\"acceptedAnswer\" itemtype=\"https://schema.org/Answer\">\n      <div itemprop=\"text\">\n            \n[Hugging Face Optimum](https://github.com/huggingface/optimum)\u00a0is an open-source library and an extension of\u00a0[Hugging Face Transformers](https://github.com/huggingface/transformers), that provides a unified API of performance optimization tools to achieve maximum efficiency to train and run models on accelerated hardware, including toolkits for optimized performance on\u00a0[Graphcore IPU](https://github.com/huggingface/optimum-graphcore)\u00a0and\u00a0[Habana Gaudi](https://github.com/huggingface/optimum-habana). \n \nOptimum can be used for converting, quantization, graph optimization, accelerated training & inference with support for\u00a0[transformers pipelines](https://huggingface.co/docs/transformers/main/en/main_classes/pipelines#pipelines).\n\nBelow you can see a typical developer journey of how you can leverage Optimum with ONNX.\n\n<figure class=\"image table text-center m-0 w-full\">\n    <img src=\"assets/81_convert_transformers_to_onnx/user-journey.png\" alt=\"developer journey optimum\"/>\n</figure>", "## Conclusion - Accelerating Transformers for Production\n\nIn this post, we showed how the new Intel Ice Lake Xeon CPUs are suitable for running AI workloads at scale along with the software elements you can swap and tune in order to exploit the full potential of the hardware.\nAll these items are to be considered after setting-up the various lower-level knobs detailed in [the previous blog](https://huggingface.co/blog/bert-cpu-scaling-part-1) to maximize the usage of all the cores and resources.\n\nAt Hugging Face, we are on a mission to democratize state-of-the-art Machine Learning, and a critical part of our work is to make these state-of-the-art models as efficient as possible, to use less energy and memory at scale, and to be more affordable to run by companies of all sizes. \n\nOur collaboration with Intel through the \ud83e\udd17 [Hardware Partner Program](https://huggingface.co/hardware) enables us to make advanced efficiency and optimization techniques easily available to the community, through our new \ud83e\udd17 [Optimum open source library](https://github.com/huggingface/optimum) dedicated to production performance.\n\nFor companies looking to accelerate their Transformer models inference, our new \ud83e\udd17 [Infinity product offers a plug-and-play containerized solution](https://huggingface.co/infinity), achieving down to 1ms latency on GPU and 2ms on Intel Xeon Ice Lake CPUs.\n\nIf you found this post interesting or useful to your work, please consider giving Optimum a star. And if this post was music to your ears, consider [joining our Machine Learning Optimization team](https://apply.workable.com/huggingface/)!"], "eval_score_GPT4": "4", "eval_feedback_GPT4": "The response correctly identifies the maximum improvement in throughput as 800%, which aligns with the reference answer. However, the response is incomplete as it does not finish the sentence or the table provided. This incomplete information could lead to some confusion or misinterpretation of the data. Despite this, the key fact of the 800% improvement is present and correct."}, {"question": "What is the command to upload a spaCy pipeline to the Hugging Face Hub?\n", "true_answer": "python -m spacy huggingface-hub push en_ner_fashion-0.0.0-py3-none-any.whl", "source_doc": "huggingface/blog/blob/main/spacy.md", "generated_answer": "The command to upload a spaCy pipeline to the Hugging Face Hub is:\n\n```bash\npython -m spacy huggingface-hub push [whl_path] [--org] [--msg] [--local-repo] [--verbose]\n```\n\nReplace `[whl_path]` with the path to the wheel file of your packaged spaCy pipeline, and use the optional arguments to customize the", "retrieved_docs": ["--\ntitle: \"Welcome spaCy to the Hugging Face Hub\"\nthumbnail: /blog/assets/23_spacy/thumbnail.png\n\nauthors:\n- user: osanseviero\n- user: ines\n---\n\n# Welcome spaCy to the Hugging Face Hub\n\n\n[spaCy](https://github.com/explosion/spaCy) is a popular library for advanced Natural Language Processing used widely across industry. spaCy makes it easy to use and train pipelines for tasks like named entity recognition, text classification, part of speech tagging and more, and lets you build powerful applications to process and analyze large volumes of text.\n\nHugging Face makes it really easy to share your spaCy pipelines with the community! With a single command, you can upload any pipeline package, with a pretty model card and all required metadata auto-generated for you. The inference API currently supports NER out-of-the-box, and you can try out your pipeline interactively in your browser. You'll also get a live URL for your package that you can `pip install` from anywhere for a smooth path from prototype all the way to production!\n\n### Finding models\n\nOver 60 canonical models can be found in the [spaCy](https://hf.co/spacy) org. These models are from the [latest 3.1 release](https://explosion.ai/blog/spacy-v3-1), so you can try the latest realesed models right now! On top of this, you can find all spaCy models from the community here https://huggingface.co/models?filter=spacy.\n\n\n### Widgets\n\nThis integration includes support for NER widgets, so all models with a NER component will have this out of the box! Coming soon there will be support for text classification and POS.", "You can then upload any pipeline packaged with [`spacy package`](https://spacy.io/api/cli#package). Make sure to set `--build wheel` to output a binary .whl file. The uploader will read all metadata from the pipeline package, including the auto-generated pretty `README.md` and the model details available in the `meta.json`.\n\n```bash\nhuggingface-cli login\npython -m spacy package ./en_ner_fashion ./output --build wheel\ncd ./output/en_ner_fashion-0.0.0/dist\npython -m spacy huggingface-hub push en_ner_fashion-0.0.0-py3-none-any.whl\n```\n\nIn just a minute, you can get your packaged model in the Hub, try it out directly in the browser, and share it with the rest of the community. All the required metadata will be uploaded for you and you even get a cool model card.\n\nThe command will output two things:\n\n* Where to find your repo in the Hub! For example, https://huggingface.co/spacy/en_core_web_sm\n* And how to install the pipeline directly from the Hub!\n\n\n### From a Python script\n\nYou can use the `push` function from Python. It returns a dictionary containing the `\"url\"` and \"`whl_url`\" of the published model and the wheel file, which you can later install with `pip install`.\n\n```py\nfrom spacy_huggingface_hub import push\n\nresult = push(\"./en_ner_fashion-0.0.0-py3-none-any.whl\")\nprint(result[\"url\"])\n```\n\n## Additional resources", "# Adding HuggingFace argument\n    parser.add_argument(\"--repo-id\", type=str, default=\"ThomasSimonini/ppo-CartPole-v1\", help=\"id of the model repository from the Hugging Face Hub {username/repo_name}\")\n\n    args = parser.parse_args()\n    args.batch_size = int(args.num_envs * args.num_steps)\n    args.minibatch_size = int(args.batch_size // args.num_minibatches)\n    # fmt: on\n    return args\n\n\ndef package_to_hub(\n    repo_id,\n    model,\n    hyperparameters,\n    eval_env,\n    video_fps=30,\n    commit_message=\"Push agent to the Hub\",\n    token=None,\n    logs=None,\n):\n    \"\"\"\n    Evaluate, Generate a video and Upload a model to Hugging Face Hub.\n    This method does the complete pipeline:\n    - It evaluates the model\n    - It generates the model card\n    - It generates a replay video of the agent\n    - It pushes everything to the hub\n    :param repo_id: id of the model repository from the Hugging Face Hub\n    :param model: trained model\n    :param eval_env: environment used to evaluate the agent\n    :param fps: number of fps for rendering the video\n    :param commit_message: commit message\n    :param logs: directory on local machine of tensorboard logs you'd like to upload\n    \"\"\"\n    msg.info(\n        \"This function will save, evaluate, generate a video of your agent, \"\n        \"create a model card and push everything to the hub. \"\n        \"It might take up to 1min. \\n \"\n        \"This is a work in progress: if you encounter a bug, please open an issue.\"\n    )\n    # Step 1: Clone or create the repo\n    repo_url = HfApi().create_repo(\n        repo_id=repo_id,\n        token=token,\n        private=False,\n        exist_ok=True,\n    )\n\n    with tempfile.TemporaryDirectory() as tmpdirname:\n        tmpdirname = Path(tmpdirname)", "You can install spacy-huggingface-hub from pip:\n\n```bash\npip install spacy-huggingface-hub\n```\n\nYou can then check if the command has been registered successfully\n\n```bash\npython -m spacy huggingface-hub --help\n```\n\nTo push with the CLI, you can use the `huggingface-hub push` command as seen below.\n\n```bash\npython -m spacy huggingface-hub push [whl_path] [--org] [--msg] [--local-repo] [--verbose]\n```", "Using spaCy at Hugging Face\n\n`spaCy` is a popular library for advanced Natural Language Processing used widely across industry. `spaCy` makes it easy to use and train pipelines for tasks like named entity recognition, text classification, part of speech tagging and more, and lets you build powerful applications to process and analyze large volumes of text.\n\n## Exploring spaCy models in the Hub\n\nThe official models from `spaCy` 3.3 are in the `spaCy` [Organization Page](https://huggingface.co/spacy). Anyone in the community can also share their `spaCy` models, which you can find by filtering at the left of the [models page](https://huggingface.co/models?library=spacy).\n\nAll models on the Hub come up with useful features\n1. An automatically generated model card with label scheme, metrics, components, and more.\n2. An evaluation sections at top right where you can look at the metrics.\n3. Metadata tags that help for discoverability and contain information such as license and language.\n4. An interactive widget you can use to play out with the model directly in the browser\n5. An Inference API that allows to make inference requests.\n\n<div class=\"flex justify-center\">\n<img class=\"block dark:hidden\" src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/hub/libraries-spacy_widget.png\"/>\n<img class=\"hidden dark:block\" src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/hub/libraries-spacy_widget-dark.png\"/>\n</div>\n\n\n## Using existing models\n\nAll `spaCy` models from the Hub can be directly installed using pip install.\n\n```bash\npip install https://huggingface.co/spacy/en_core_web_sm/resolve/main/en_core_web_sm-any-py3-none-any.whl\n```", "Start by creating an instance of a [`DiffusionPipeline`] and specify which pipeline checkpoint you would like to download.\nYou can use the [`DiffusionPipeline`] for any [checkpoint](https://huggingface.co/models?library=diffusers&sort=downloads) stored on the Hugging Face Hub.\nIn this quicktour, you'll load the [`stable-diffusion-v1-5`](https://huggingface.co/runwayml/stable-diffusion-v1-5) checkpoint for text-to-image generation.\n\n<Tip warning={true}>\n\nFor [Stable Diffusion](https://huggingface.co/CompVis/stable-diffusion) models, please carefully read the [license](https://huggingface.co/spaces/CompVis/stable-diffusion-license) first before running the model. \ud83e\udde8 Diffusers implements a [`safety_checker`](https://github.com/huggingface/diffusers/blob/main/src/diffusers/pipelines/stable_diffusion/safety_checker.py) to prevent offensive or harmful content, but the model's improved image generation capabilities can still produce potentially harmful content.\n\n</Tip>\n\nLoad the model with the [`~DiffusionPipeline.from_pretrained`] method:\n\n```python\n>>> from diffusers import DiffusionPipeline\n\n>>> pipeline = DiffusionPipeline.from_pretrained(\"runwayml/stable-diffusion-v1-5\", use_safetensors=True)\n```\n\nThe [`DiffusionPipeline`] downloads and caches all modeling, tokenization, and scheduling components. You'll see that the Stable Diffusion pipeline is composed of the [`UNet2DConditionModel`] and [`PNDMScheduler`] among other things:", "</Tip>\n\n## huggingface-cli login\n\nIn many cases, you must be logged in to a Hugging Face account to interact with the Hub (download private repos, upload files, create PRs, etc.). To do so, you need a [User Access Token](https://huggingface.co/docs/hub/security-tokens) from your [Settings page](https://huggingface.co/settings/tokens). The User Access Token is used to authenticate your identity to the Hub. Make sure to set a token with write access if you want to upload or modify content.\n\nOnce you have your token, run the following command in your terminal:\n\n```bash\n>>> huggingface-cli login\n```\n\nThis command will prompt you for a token. Copy-paste yours and press *Enter*. Then you'll be asked if the token should also be saved as a git credential. Press *Enter* again (default to yes) if you plan to use `git` locally. Finally, it will call the Hub to check that your token is valid and save it locally."], "eval_score_GPT4": "5", "eval_feedback_GPT4": "The response provides the correct command structure for uploading a spaCy pipeline to the Hugging Face Hub, including the use of placeholders for user-specific details and optional arguments. It matches the reference answer in terms of the command to be used but includes additional detail by showing the optional arguments that can be used with the command. This additional information does not detract from the accuracy of the response but rather enhances it by providing a more comprehensive answer. Therefore, the response is completely correct, accurate, and factual."}, {"question": "What is the time and memory complexity of the Nystr\u00f6mformer's approximation of self-attention?\n", "true_answer": "O(n)", "source_doc": "huggingface/blog/blob/main/nystromformer.md", "generated_answer": "The Nystr\u00f6mformer's approximation of self-attention has a time and memory complexity of O(n) instead of the standard self-attention's O(n^2) complexity, where n is the length of the input sequence. This improvement is achieved by using the Nystr\u00f6m method to approximate the softmax matrix in self-attention, which involves sampling landmarks (or Nystr\u00f6m points) from queries and keys instead of sampling directly from the", "retrieved_docs": ["--\ntitle: \"Nystr\u00f6mformer: Approximating self-attention in linear time and memory via the Nystr\u00f6m method\"\nthumbnail: /blog/assets/86_nystromformer/thumbnail.png\nauthors:\n- user: asi\n  guest: true\n---\n\n# Nystr\u00f6mformer: Approximating self-attention in linear time and memory via the Nystr\u00f6m method\n\n\n<script async defer src=\"https://unpkg.com/medium-zoom-element@0/dist/medium-zoom-element.min.js\"></script>\n\n## Introduction\n\nTransformers have exhibited remarkable performance on various Natural Language Processing and Computer Vision tasks. Their success can be attributed to the self-attention mechanism, which captures the pairwise interactions between all the tokens in an input. However, the standard self-attention mechanism has a time and memory complexity of \\\\(O(n^2)\\\\) (where \\\\(n\\\\) is the length of the input sequence), making it expensive to train on long input sequences. \n\nThe [Nystr\u00f6mformer](https://arxiv.org/abs/2102.03902) is one of many efficient Transformer models that approximates standard self-attention with \\\\(O(n)\\\\) complexity. Nystr\u00f6mformer exhibits competitive performance on various downstream NLP and CV tasks while improving upon the efficiency of standard self-attention. The aim of this blog post is to give readers an overview of the Nystr\u00f6m method and how it can be adapted to approximate self-attention.\n\n\n## Nystr\u00f6m method for matrix approximation\n\nAt the heart of Nystr\u00f6mformer is the Nystr\u00f6m method for matrix approximation. It allows us to approximate a matrix by sampling some of its rows and columns. Let's consider a matrix \\\\(P^{n \\times n}\\\\), which is expensive to compute in its entirety. So, instead, we approximate it using the Nystr\u00f6m method. We start by sampling \\\\(m\\\\) rows and columns from \\\\(P\\\\). We can then arrange the sampled rows and columns as follows:", "This is the Nystr\u00f6m approximation of the softmax matrix in the self-attention mechanism. We multiply this matrix with the values ( \\\\(V\\\\)) to obtain a linear approximation of self-attention. Note that we never calculated the product \\\\(QK^T\\\\), avoiding the \\\\(O(n^2)\\\\) complexity. \n\n\n## How do we select landmarks?\n\nInstead of sampling \\\\(m\\\\) rows from \\\\(Q\\\\) and \\\\(K\\\\), the authors propose to construct \\\\(\\tilde{Q}\\\\) and \\\\(\\tilde{K}\\\\)\nusing segment means. In this procedure, \\\\(n\\\\) tokens are grouped into \\\\(m\\\\) segments, and the mean of each segment is computed. Ideally, \\\\(m\\\\) is much smaller than \\\\(n\\\\). According to experiments from the paper, selecting just \\\\(32\\\\) or \\\\(64\\\\) landmarks produces competetive performance compared to standard self-attention and other efficient attention mechanisms, even for long sequences lengths ( \\\\(n=4096\\\\) or \\\\(8192\\\\)). \n\nThe overall algorithm is summarised by the following figure from the paper:\n\n<figure class=\"image table text-center m-0 w-full\">\n  <medium-zoom background=\"rgba(0,0,0,.7)\" alt=\"Efficient self-attention with the Nystr\u00f6m method\" src=\"assets/86_nystromformer/paper_figure.png\"></medium-zoom>\n  <figcaption>Efficient self-attention with the Nystr\u00f6m method</figcaption>\n</figure>\n\nThe three orange matrices above correspond to the three matrices we constructed using the key and query landmarks. Also, notice that there is a DConv box. This corresponds to a skip connection added to the values using a 1D depthwise convolution.\n\n\n## How is Nystr\u00f6mformer implemented?", "As shown in the second line, \\\\(\\hat{P}\\\\) can be expressed as a product of three matrices. The reason for doing so will become clear later.\n\n\n## Can we approximate self-attention with the Nystr\u00f6m method?\n\nOur goal is to ultimately approximate the softmax matrix in standard self attention: S = softmax \\\\( \\frac{QK^T}{\\sqrt{d}} \\\\)\n\nHere, \\\\(Q\\\\) and \\\\(K\\\\) denote the queries and keys respectively. Following the procedure discussed above, we would sample \\\\(m\\\\) rows and columns from \\\\(S\\\\), form four submatrices, and obtain \\\\(\\hat{S}\\\\):\n\n<figure class=\"image table text-center m-0 w-full\">\n  <medium-zoom background=\"rgba(0,0,0,.7)\" alt=\"Nystr\u00f6m approximation of S\" src=\"assets/86_nystromformer/s_hat.png\"></medium-zoom>\n  <figcaption>Nystr\u00f6m approximation of S</figcaption>\n</figure>\n\nBut, what does it mean to sample a column from \\\\(S\\\\)? It means we select one element from each row. Recall how S is calculated: the final operation is a row-wise softmax. To find a single entry in a row, we must access all other entries (for the denominator in softmax). So, sampling one column requires us to know all other columns in the matrix. Therefore, we cannot directly apply the Nystr\u00f6m method to approximate the softmax matrix.\n\n\n## How can we adapt the Nystr\u00f6m method to approximate self-attention?", "## How can we adapt the Nystr\u00f6m method to approximate self-attention?\n\nInstead of sampling from \\\\(S\\\\), the authors propose to sample landmarks (or Nystr\u00f6m points) from queries and keys. We denote the query landmarks and key landmarks as \\\\(\\tilde{Q}\\\\) and \\\\(\\tilde{K}\\\\) respectively. \\\\(\\tilde{Q}\\\\) and \\\\(\\tilde{K}\\\\) can be used to construct three matrices corresponding to those in the Nystr\u00f6m approximation of \\\\(S\\\\). We define the following matrices:\n\n$$\\tilde{F} = softmax(\\frac{Q\\tilde{K}^T}{\\sqrt{d}}) \\hspace{40pt} \\tilde{A} = softmax(\\frac{\\tilde{Q}\\tilde{K}^T}{\\sqrt{d}})^+ \\hspace{40pt} \\tilde{B} = softmax(\\frac{\\tilde{Q}K^T}{\\sqrt{d}})$$\n\nThe sizes of \\\\(\\tilde{F}\\\\), \\\\(\\tilde{A}\\\\), and \\\\(\\tilde{B}) are \\\\(n \\times m, m \\times m,\\\\) and \\\\(m \\times n\\\\) respectively. \nWe replace the three matrices in the Nystr\u00f6m approximation of \\\\(S\\\\) with the new matrices we have defined to obtain an alternative Nystr\u00f6m approximation:\n\n$$\\begin{aligned}\\hat{S} &= \\tilde{F} \\tilde{A} \\tilde{B} \\\\ &= softmax(\\frac{Q\\tilde{K}^T}{\\sqrt{d}}) softmax(\\frac{\\tilde{Q}\\tilde{K}^T}{\\sqrt{d}})^+  softmax(\\frac{\\tilde{Q}K^T}{\\sqrt{d}}) \\end{aligned}$$", "The abstract from the paper is the following:\n\n*Transformers have emerged as a powerful tool for a broad range of natural language processing tasks. A key component\nthat drives the impressive performance of Transformers is the self-attention mechanism that encodes the influence or\ndependence of other tokens on each specific token. While beneficial, the quadratic complexity of self-attention on the\ninput sequence length has limited its application to longer sequences -- a topic being actively studied in the\ncommunity. To address this limitation, we propose Nystr\u00f6mformer -- a model that exhibits favorable scalability as a\nfunction of sequence length. Our idea is based on adapting the Nystr\u00f6m method to approximate standard self-attention\nwith O(n) complexity. The scalability of Nystr\u00f6mformer enables application to longer sequences with thousands of\ntokens. We perform evaluations on multiple downstream tasks on the GLUE benchmark and IMDB reviews with standard\nsequence length, and find that our Nystr\u00f6mformer performs comparably, or in a few cases, even slightly better, than\nstandard self-attention. On longer sequence tasks in the Long Range Arena (LRA) benchmark, Nystr\u00f6mformer performs\nfavorably relative to other efficient self-attention methods. Our code is available at this https URL.*\n\nThis model was contributed by [novice03](https://huggingface.co/novice03). The original code can be found [here](https://github.com/mlpen/Nystromformer).\n\n## Resources\n\n- [Text classification task guide](../tasks/sequence_classification)\n- [Token classification task guide](../tasks/token_classification)\n- [Question answering task guide](../tasks/question_answering)\n- [Masked language modeling task guide](../tasks/masked_language_modeling)\n- [Multiple choice task guide](../tasks/multiple_choice)\n\n## NystromformerConfig\n\n[[autodoc]] NystromformerConfig\n\n## NystromformerModel\n\n[[autodoc]] NystromformerModel\n    - forward\n\n## NystromformerForMaskedLM", "1. **[Nystr\u00f6mformer](https://huggingface.co/docs/transformers/model_doc/nystromformer)** (from the University of Wisconsin - Madison) released with the paper [Nystr\u00f6mformer: A Nystr\u00f6m-Based Algorithm for Approximating Self-Attention](https://arxiv.org/abs/2102.03902) by Yunyang Xiong, Zhanpeng Zeng, Rudrasis Chakraborty, Mingxing Tan, Glenn Fung, Yin Li, Vikas Singh.\n1. **[OneFormer](https://huggingface.co/docs/transformers/model_doc/oneformer)** (from SHI Labs) released with the paper [OneFormer: One Transformer to Rule Universal Image Segmentation](https://arxiv.org/abs/2211.06220) by Jitesh Jain, Jiachen Li, MangTik Chiu, Ali Hassani, Nikita Orlov, Humphrey Shi.\n1. **[OpenLlama](https://huggingface.co/docs/transformers/model_doc/open-llama)** (from [s-JoL](https://huggingface.co/s-JoL)) released on GitHub (now removed).\n1. **[OPT](https://huggingface.co/docs/transformers/master/model_doc/opt)** (from Meta AI) released with the paper [OPT: Open Pre-trained Transformer Language Models](https://arxiv.org/abs/2205.01068) by Susan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya Chen, Shuohui Chen et al.", "1. **[Nystr\u00f6mformer](https://huggingface.co/docs/transformers/model_doc/nystromformer)** (from the University of Wisconsin - Madison) released with the paper [Nystr\u00f6mformer: A Nystr\u00f6m-Based Algorithm for Approximating Self-Attention](https://arxiv.org/abs/2102.03902) by Yunyang Xiong, Zhanpeng Zeng, Rudrasis Chakraborty, Mingxing Tan, Glenn Fung, Yin Li, Vikas Singh.\n1. **[OneFormer](https://huggingface.co/docs/transformers/model_doc/oneformer)** (from SHI Labs) released with the paper [OneFormer: One Transformer to Rule Universal Image Segmentation](https://arxiv.org/abs/2211.06220) by Jitesh Jain, Jiachen Li, MangTik Chiu, Ali Hassani, Nikita Orlov, Humphrey Shi.\n1. **[OpenLlama](https://huggingface.co/docs/transformers/model_doc/open-llama)** (from [s-JoL](https://huggingface.co/s-JoL)) released on GitHub (now removed).\n1. **[OPT](https://huggingface.co/docs/transformers/master/model_doc/opt)** (from Meta AI) released with the paper [OPT: Open Pre-trained Transformer Language Models](https://arxiv.org/abs/2205.01068) by Susan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya Chen, Shuohui Chen et al."], "eval_score_GPT4": "4", "eval_feedback_GPT4": "Feedback: The response correctly identifies that the Nystr\u00f6mformer's approximation of self-attention has a reduced complexity compared to standard self-attention. However, it specifies the complexity as O(n), which is not fully accurate without specifying that this is the complexity per head and that the overall complexity also depends on other factors such as the number of landmarks used. The reference answer is simply O(n), which could be interpreted as the complexity per head or as an oversimplification. The response could be more accurate by clarifying these points, but it does not fundamentally misrepresent the complexity of the Nystr\u00f6mformer. Therefore, the response is mostly correct, accurate, and factual."}, {"question": "What is the goal of the Named Entity Recognition task in token classification?\n", "true_answer": "The goal of the Named Entity Recognition task is to find the entities in a piece of text, such as person, location, or organization.", "source_doc": "huggingface/course/blob/main/subtitles/en/raw/tasks.md", "generated_answer": "The goal of the Named Entity Recognition task in token classification is to find the entities (such as persons, locations, or organizations) in a sentence and assign a label to each token for those entities. This can be formulated as attributing a label to each token by having one class per entity and one class for \"no entity.\" The task is commonly used in applications such as extracting important information from invoices using named entity recognition models, such as date, organization name or", "retrieved_docs": ["ote: the following transcripts are associated with Merve Noyan's videos in the Hugging Face Tasks playlist: https://www.youtube.com/playlist?list=PLo2EIpI_JMQtyEr-sLJSy5_SnLCb4vtQf\n\nToken Classification video\n\nWelcome to the Hugging Face tasks series! In this video we\u2019ll take a look at the token classification task.\nToken classification is the task of assigning a label to each token in a sentence. There are various token classification tasks and the most common are Named Entity Recognition and Part-of-Speech Tagging.\nLet\u2019s take a quick look at the Named Entity Recognition task. The goal of this task is to find the entities in a piece of text, such as person, location, or organization. This task is formulated as labelling each token with one class for each entity, and another class for tokens that have no entity.\nAnother token classification task is part-of-speech tagging. The goal of this task is to label the words for a particular part of a speech, such as noun, pronoun, adjective, verb and so on. This task is formulated as labelling each token with parts of speech.\nToken classification models are evaluated on Accuracy, Recall, Precision and F1-Score. The metrics are calculated for each of the classes. We calculate true positive, true negative and false positives to calculate precision and recall, and take their harmonic mean to get F1-Score. Then we calculate it for every class and take the overall average to evaluate our model.\nAn example dataset used for this task is ConLL2003. Here, each token belongs to a certain named entity class, denoted as the indices of the list containing the labels.\nYou can extract important information from invoices using named entity recognition models, such as date, organization name or address.\nFor more information about the Token classification task, check out the Hugging Face course.\n\n\nQuestion Answering video", "{/if}\n\nThe first application we'll explore is token classification. This generic task encompasses any problem that can be formulated as \"attributing a label to each token in a sentence,\" such as:\n\n- **Named entity recognition (NER)**: Find the entities (such as persons, locations, or organizations) in a sentence. This can be formulated as attributing a label to each token by having one class per entity and one class for \"no entity.\"\n- **Part-of-speech tagging (POS)**: Mark each word in a sentence as corresponding to a particular part of speech (such as noun, verb, adjective, etc.).\n- **Chunking**: Find the tokens that belong to the same entity. This task (which can be combined with POS or NER) can be formulated as attributing one label (usually `B-`) to any tokens that are at the beginning of a chunk, another label (usually `I-`) to tokens that are inside a chunk, and a third label (usually `O`) to tokens that don't belong to any chunk.\n\n<Youtube id=\"wVHdVlPScxA\"/>\n\nOf course, there are many other types of token classification problem; those are just a few representative examples. In this section, we will fine-tune a model (BERT) on a NER task, which will then be able to compute predictions like this one:", "!--Copyright 2022 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\n\nhttp://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License.\n\n\u26a0\ufe0f Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\nrendered properly in your Markdown viewer.\n\n-->\n\n# Token classification\n\n[[open-in-colab]]\n\n<Youtube id=\"wVHdVlPScxA\"/>\n\nToken classification assigns a label to individual tokens in a sentence. One of the most common token classification tasks is Named Entity Recognition (NER). NER attempts to find a label for each entity in a sentence, such as a person, location, or organization.\n\nThis guide will show you how to:\n\n1. Finetune [DistilBERT](https://huggingface.co/distilbert-base-uncased) on the [WNUT 17](https://huggingface.co/datasets/wnut_17) dataset to detect new entities.\n2. Use your finetuned model for inference.\n\n<Tip>\nThe task illustrated in this tutorial is supported by the following model architectures:\n\n<!--This tip is automatically generated by `make fix-copies`, do not fill manually!-->", "</Tip>\n\n#### Named Entity Recognition\n\nNamed Entity Recognition (NER) is a task of finding named entities in a piece of text, such as a person, location, or organization.\nLet's modify the instructions in the prompt to make the LLM perform this task. Here, let's also set `return_full_text = False` \nso that output doesn't contain the prompt:\n\n```python\n>>> torch.manual_seed(1) # doctest: +IGNORE_RESULT\n>>> prompt = \"\"\"Return a list of named entities in the text.\n... Text: The Golden State Warriors are an American professional basketball team based in San Francisco.\n... Named entities:\n... \"\"\"\n\n>>> sequences = pipe(\n...     prompt,\n...     max_new_tokens=15,\n...     return_full_text = False,    \n... )\n\n>>> for seq in sequences:\n...     print(f\"{seq['generated_text']}\")\n- Golden State Warriors\n- San Francisco\n```\n\nAs you can see, the model correctly identified two named entities from the given text.\n\n#### Translation\n\nAnother task LLMs can perform is translation. You can choose to use encoder-decoder models for this task, however, here,\nfor the simplicity of the examples, we'll keep using Falcon-7b-instruct, which does a decent job. Once again, here's how \nyou can write a basic prompt to instruct a model to translate a piece of text from English to Italian: \n\n```python\n>>> torch.manual_seed(2) # doctest: +IGNORE_RESULT\n>>> prompt = \"\"\"Translate the English text to Italian.\n... Text: Sometimes, I've believed as many as six impossible things before breakfast.\n... Translation:\n... \"\"\"\n\n>>> sequences = pipe(\n...     prompt,\n...     max_new_tokens=20,\n...     do_sample=True,\n...     top_k=10,\n...     return_full_text = False,\n... )", "Named-Entity Recognition\n\nRelated spaces: https://huggingface.co/spaces/rajistics/biobert_ner_demo, https://huggingface.co/spaces/abidlabs/ner, https://huggingface.co/spaces/rajistics/Financial_Analyst_AI\nTags: NER, TEXT, HIGHLIGHT\n\n## Introduction\n\nNamed-entity recognition (NER), also known as token classification or text tagging, is the task of taking a sentence and classifying every word (or \"token\") into different categories, such as names of people or names of locations, or different parts of speech.\n\nFor example, given the sentence:\n\n> Does Chicago have any Pakistani restaurants?\n\nA named-entity recognition algorithm may identify:\n\n- \"Chicago\" as a **location**\n- \"Pakistani\" as an **ethnicity**\n\nand so on.\n\nUsing `gradio` (specifically the `HighlightedText` component), you can easily build a web demo of your NER model and share that with the rest of your team.\n\nHere is an example of a demo that you'll be able to build:\n\n$demo_ner_pipeline\n\nThis tutorial will show how to take a pretrained NER model and deploy it with a Gradio interface. We will show two different ways to use the `HighlightedText` component -- depending on your NER model, either of these two ways may be easier to learn!\n\n### Prerequisites\n\nMake sure you have the `gradio` Python package already [installed](/getting_started). You will also need a pretrained named-entity recognition model. You can use your own, while in this tutorial, we will use one from the `transformers` library.\n\n### Approach 1: List of Entity Dictionaries\n\nMany named-entity recognition models output a list of dictionaries. Each dictionary consists of an _entity_, a \"start\" index, and an \"end\" index. This is, for example, how NER models in the `transformers` library operate:", "et's study how to preprocess a dataset for token classification! Token classification regroups any task that can be framed as labelling each word (or token) in a sentence, like identifying the persons, organizations and locations for instance. For our example, we will use the Conll dataset, in which we remove columns we won't use and rename the other ones to get to a dataset with just two columns: words and labels. If you have your own dataset for token classification, just make sure you clean your data to get to the same point, with one column containing words (as list of strings) and another containing labels (as integers spanning from to to your number of labels -1).() Make sure you have your label names stored somewhere - here we get them from the dataset features - so you are able to map the integers to some real labels when inspecting your data! Here we are doing named entity recognitions, so ours labels are either O for words that do not belong to any entity, LOC, for location, PER, for person, ORG for organization and MISC for miscellaneous. Each label has two versions: the B- labels indicate a word that begins an entity while the I- labels indicate a word that is inside an entity. The first step to preprocess our data is to tokenize the words. This is very easily done with a tokenizer, we just have to tell it we have pre-tokenized the data with the flag is_split_into_words. Then comes the hard part. Since we have added special tokens and each word may have been split into several tokens, our labels won't match the tokens anymore. This is where the word IDs our fast tokenizer provide come to the rescue. They match each token to the word it belongs to which allows us to map each token to its label. We just have to make sure we change the B- labels to their I- counterparts for tokens that are inside (but not at the beginning) of a word. The special tokens get a label of -100, which is how we tell the Transformer loss functions to ignore them when computing the loss", "Two common types of token classification are:\n\n* named entity recognition (NER): label a token according to an entity category like organization, person, location or date. NER is especially popular in biomedical settings, where it can label genes, proteins, and drug names.\n* part-of-speech tagging (POS): label a token according to its part-of-speech like noun, verb, or adjective. POS is useful for helping translation systems understand how two identical words are grammatically different (bank as a noun versus bank as a verb).\n\n```py\n>>> from transformers import pipeline"], "eval_score_GPT4": "4", "eval_feedback_GPT4": "Feedback: The response provided is mostly correct, accurate, and factual. It correctly identifies the goal of Named Entity Recognition (NER) as finding and labeling entities such as persons, locations, or organizations within a sentence. It also accurately describes the process of assigning a label to each token, which aligns with the reference answer. However, the response goes slightly beyond the reference answer by mentioning the application of NER in extracting information from invoices, which is not required to answer the question but does not detract from the correctness of the response."}, {"question": "What is the resolution of images used by the CLIPSeg model?\n", "true_answer": "352 x 352 pixels", "source_doc": "huggingface/blog/blob/main/clipseg-zero-shot.md", "generated_answer": "The resolution of images used by the CLIPSeg model is 352 x 352 pixels. (Source: https://huggingface.co/blog/clipseg-zero-shot)", "retrieved_docs": ["## CLIPSeg: image segmentation with CLIP\n\n[CLIPSeg](https://arxiv.org/abs/2112.10003) is a model that uses CLIP representations to create image segmentation masks. It was published by Timo L\u00fcddecke and Alexander Ecker. They achieved zero-shot image segmentation by training a Transformer-based decoder on top of the CLIP model, which is kept frozen. The decoder takes in the CLIP representation of an image, and the CLIP representation of the thing you want to segment. Using these two inputs, the CLIPSeg decoder creates a binary segmentation mask. To be more precise, the decoder doesn\u2019t only use the final CLIP representation of the image we want to segment, but it also uses the outputs of some of the layers of CLIP. \n\n<figure class=\"image table text-center m-0 w-full\">\n  <medium-zoom background=\"rgba(0,0,0,.7)\" alt=\"Overview of the CLIPSeg model\" src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/123_clipseg-zero-shot/clipseg-overview.png\"></medium-zoom>\n  <figcaption><a href=\"https://arxiv.org/abs/2112.10003\">Source</a></figcaption>\n</figure>\n\nThe decoder is trained on the [PhraseCut dataset](https://arxiv.org/abs/2008.01187), which contains over 340,000 phrases with corresponding image segmentation masks. The authors also experimented with various augmentations to expand the size of the dataset. The goal here is not only to be able to segment the categories that are present in the dataset, but also to segment unseen categories. Experiments indeed show that the decoder can generalize to unseen categories.", "Image segmentation is a well-known task within the field of computer vision. It allows a computer to not only know what is in an image (classification), where objects are in the image (detection), but also what the outlines of those objects are. Knowing the outlines of objects is essential in fields such as robotics and autonomous driving. For example, a robot has to know the shape of an object to grab it correctly. Segmentation can also be combined with [image inpainting](https://t.co/5q8YHSOfx7) to allow users to describe which part of the image they want to replace.\n\nOne limitation of most image segmentation models is that they only work with a fixed list of categories. For example, you cannot simply use a segmentation model trained on oranges to segment apples. To teach the segmentation model an additional category, you have to label data of the new category and train a new model, which can be costly and time-consuming. But what if there was a model that can already segment almost any kind of object, without any further training? That\u2019s exactly what [CLIPSeg](https://arxiv.org/abs/2112.10003), a zero-shot segmentation model, achieves.\n\nCurrently, CLIPSeg still has its limitations. For example, the model uses images of 352 x 352 pixels, so the output is quite low-resolution. This means we cannot expect pixel-perfect results when we work with images from modern cameras. If we want more precise segmentations, we can fine-tune a state-of-the-art segmentation model, as shown in [our previous blog post](https://huggingface.co/blog/fine-tune-segformer). In that case, we can still use CLIPSeg to generate some rough labels, and then refine them in a labeling tool such as [Segments.ai](https://segments.ai/?utm_source=hf&utm_medium=blog&utm_campaign=clipseg). Before we describe how to do that, let\u2019s first take a look at how CLIPSeg works.\n\n## CLIP: the magic model behind CLIPSeg", "<figure class=\"image table text-center m-0 w-full\">\n  <medium-zoom background=\"rgba(0,0,0,.7)\" alt=\"Overview of the CLIPSeg model\" src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/123_clipseg-zero-shot/clip-tv-example.png\"></medium-zoom>\n  <figcaption>Example of image classification using CLIP (<a href=\"https://openai.com/blog/clip/\">source</a>).</figcaption>\n</figure>\n\nWhat\u2019s more, CLIP is not only useful for classification, but it can also be used for [image search](https://huggingface.co/spaces/DrishtiSharma/Text-to-Image-search-using-CLIP) (can you see how this is similar to classification?), [text-to-image models](https://huggingface.co/spaces/kamiyamai/stable-diffusion-webui) ([DALL-E 2](https://openai.com/dall-e-2/) is powered by CLIP), [object detection](https://segments.ai/zeroshot?utm_source=hf&utm_medium=blog&utm_campaign=clipseg) ([OWL-ViT](https://arxiv.org/abs/2205.06230)), and most importantly for us: image segmentation. Now you see why CLIP was truly a breakthrough in machine learning.\n\nThe reason why CLIP works so well is that the model was trained on a huge dataset of images with text captions. The dataset contained a whopping 400 million image-text pairs taken from the internet. These images contain a wide variety of objects and concepts, and CLIP is great at creating a representation for each of them.\n\n## CLIPSeg: image segmentation with CLIP", "## CLIP: the magic model behind CLIPSeg\n\n[CLIP](https://huggingface.co/docs/transformers/main/en/model_doc/clip), which stands for **C**ontrastive **L**anguage\u2013**I**mage **P**re-training, is a model developed by OpenAI in 2021. You can give CLIP an image or a piece of text, and CLIP will output an abstract *representation* of your input. This abstract representation, also called an *embedding*, is really just a vector (a list of numbers). You can think of this vector as a point in high-dimensional space. CLIP is trained so that the representations of similar pictures and texts are similar as well. This means that if we input an image and a text description that fits that image, the representations of the image and the text will be similar (i.e., the high-dimensional points will be close together).\n\nAt first, this might not seem very useful, but it is actually very powerful. As an example, let\u2019s take a quick look at how CLIP can be used to classify images without ever having been trained on that task. To classify an image, we input the image and the different categories we want to choose from to CLIP (e.g. we input an image and the words \u201capple\u201d, \u201corange\u201d, \u2026).  CLIP then gives us back an embedding of the image and of each category. Now, we simply have to check which category embedding is closest to the embedding of the image, et voil\u00e0! Feels like magic, doesn\u2019t it?", "--\ntitle: Zero-shot image segmentation with CLIPSeg\nthumbnail: /blog/assets/123_clipseg-zero-shot/thumb.png\nauthors:\n- user: segments-tobias\n  guest: true\n- user: nielsr\n---\n\n# Zero-shot image segmentation with CLIPSeg\n\n\n<script async defer src=\"https://unpkg.com/medium-zoom-element@0/dist/medium-zoom-element.min.js\"></script>\n\n<a target=\"_blank\" href=\"https://colab.research.google.com/github/huggingface/blog/blob/main/notebooks/123_clipseg-zero-shot.ipynb\">\n    <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n</a>\n\n**This guide shows how you can use [CLIPSeg](https://huggingface.co/docs/transformers/main/en/model_doc/clipseg), a zero-shot image segmentation model, using [`\ud83e\udd17 transformers`](https://huggingface.co/transformers). CLIPSeg creates rough segmentation masks that can be used for robot perception, image inpainting, and many other tasks. If you need more precise segmentation masks, we\u2019ll show how you can refine the results of CLIPSeg on [Segments.ai](https://segments.ai/?utm_source=hf&utm_medium=blog&utm_campaign=clipseg).**", "## Usage tips\n\n- [`CLIPSegForImageSegmentation`] adds a decoder on top of [`CLIPSegModel`]. The latter is identical to [`CLIPModel`].\n- [`CLIPSegForImageSegmentation`] can generate image segmentations based on arbitrary prompts at test time. A prompt can be either a text\n(provided to the model as `input_ids`) or an image (provided to the model as `conditional_pixel_values`). One can also provide custom\nconditional embeddings (provided to the model as `conditional_embeddings`).\n\n## Resources\n\nA list of official Hugging Face and community (indicated by \ud83c\udf0e) resources to help you get started with CLIPSeg. If you're interested in submitting a resource to be included here, please feel free to open a Pull Request and we'll review it! The resource should ideally demonstrate something new instead of duplicating an existing resource.\n\n<PipelineTag pipeline=\"image-segmentation\"/>\n\n- A notebook that illustrates [zero-shot image segmentation with CLIPSeg](https://github.com/NielsRogge/Transformers-Tutorials/blob/master/CLIPSeg/Zero_shot_image_segmentation_with_CLIPSeg.ipynb).\n\n## CLIPSegConfig\n\n[[autodoc]] CLIPSegConfig\n    - from_text_vision_configs\n\n## CLIPSegTextConfig\n\n[[autodoc]] CLIPSegTextConfig\n\n## CLIPSegVisionConfig\n\n[[autodoc]] CLIPSegVisionConfig\n\n## CLIPSegProcessor\n\n[[autodoc]] CLIPSegProcessor\n\n## CLIPSegModel\n\n[[autodoc]] CLIPSegModel\n    - forward\n    - get_text_features\n    - get_image_features\n\n## CLIPSegTextModel\n\n[[autodoc]] CLIPSegTextModel\n    - forward\n\n## CLIPSegVisionModel", "```py\nfrom PIL import Image\nimport requests\n\nurl = \"http://images.cocodataset.org/val2017/000000039769.jpg\"\nimage = Image.open(requests.get(url, stream=True).raw)\ntexts = [\"a cat\", \"a remote\", \"a blanket\"]\n\ninputs = processor(text=texts, images=[image] * len(texts), padding=True, return_tensors=\"pt\")\n```\n\nSimilar to ViLT, it\u2019s important to refer to the [original work](https://arxiv.org/abs/2112.10003) to see what kind of text prompts are used to train the model in order to get the best performance during inference. While CLIPSeg is trained on simple object descriptions (e.g., \u201ca car\u201d), its CLIP backbone is pre-trained on engineered text templates (e.g., \u201can image of a car\u201d, \u201ca photo of a car\u201d) and kept frozen during training. Once the inputs are preprocessed, we can perform inference to get a binary segmentation map of shape (height, width) for each text query.\n\n```py\nimport torch\n\nwith torch.no_grad():\n    outputs = model(**inputs)\n\nlogits = outputs.logits\nprint(logits.shape)\n>>> torch.Size([3, 352, 352])\n```\n\nLet\u2019s visualize the results to see how well CLIPSeg performed (code is adapted from [this post](https://huggingface.co/blog/clipseg-zero-shot)).\n\n```py\nimport matplotlib.pyplot as plt\n\nlogits = logits.unsqueeze(1)"], "eval_score_GPT4": "5", "eval_feedback_GPT4": "The response provided is completely correct, accurate, and factual. It matches the reference answer exactly, stating the resolution as 352 x 352 pixels and even includes a source for the information, which is not required but adds to the credibility of the response."}, {"question": "What can you use Gradio for?\n", "true_answer": "Create a demo for your machine learning model, share your machine learning model with others, and debug your model.", "source_doc": "huggingface/course/blob/main/chapters/en/chapter9/9.mdx", "generated_answer": "Gradio is an open-source Python package that allows you to quickly build and share demos, web applications, and APIs for your machine learning models, data processing pipelines, and any arbitary Python functions. It requires Python 3.8 or higher and can be installed using pip. Gradio apps can be deployed on Hugging Face Spaces for free or on your own web server with Nginx. Gradio provides lightweight clients for Python, JavaScript, and TypeScript", "retrieved_docs": ["`gradio_client`: Use a Gradio app as an API -- in 3 lines of Python\n\nThis directory contains the source code for `gradio_client`, a lightweight Python library that makes it very easy to use any Gradio app as an API.\n\nAs an example, consider this [Hugging Face Space that transcribes audio files](https://huggingface.co/spaces/abidlabs/whisper) that are recorded from the microphone.\n\n![](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/gradio-guides/whisper-screenshot.jpg)\n\nUsing the `gradio_client` library, we can easily use the Gradio as an API to transcribe audio files programmatically.\n\nHere's the entire code to do it:\n\n```python\nfrom gradio_client import Client\n\nclient = Client(\"abidlabs/whisper\")\nclient.predict(\"audio_sample.wav\")\n\n>> \"This is a test of the whisper speech recognition model.\"\n```\n\nThe Gradio client works with any Gradio Space, whether it be an image generator, a stateful chatbot, or a tax calculator.\n\n## Installation\n\nIf you already have a recent version of `gradio`, then the `gradio_client` is included as a dependency.\n\nOtherwise, the lightweight `gradio_client` package can be installed from pip (or pip3) and works with Python versions 3.8 or higher:\n\n```bash\n$ pip install gradio_client\n```\n\n## Basic Usage\n\n### Connecting to a Space or a Gradio app\n\nStart by connecting instantiating a `Client` object and connecting it to a Gradio app that is running on Spaces (or anywhere else)!\n\n**Connecting to a Space**\n\n```python\nfrom gradio_client import Client", "Gradio-Lite: Serverless Gradio Running Entirely in Your Browser\n\nTags: SERVERLESS, BROWSER, PYODIDE\n\nGradio is a popular Python library for creating interactive machine learning apps. Traditionally, Gradio applications have relied on server-side infrastructure to run, which can be a hurdle for developers who need to host their applications. \n\nEnter Gradio-lite (`@gradio/lite`): a library that leverages [Pyodide](https://pyodide.org/en/stable/) to bring Gradio directly to your browser. In this blog post, we'll explore what `@gradio/lite` is, go over example code, and discuss the benefits it offers for running Gradio applications.\n\n## What is `@gradio/lite`?\n\n`@gradio/lite` is a JavaScript library that enables you to run Gradio applications directly within your web browser. It achieves this by utilizing Pyodide, a Python runtime for WebAssembly, which allows Python code to be executed in the browser environment. With `@gradio/lite`, you can **write regular Python code for your Gradio applications**, and they will **run seamlessly in the browser** without the need for server-side infrastructure.\n\n## Getting Started\n\nLet's build a \"Hello World\" Gradio app in `@gradio/lite`\n\n\n### 1. Import JS and CSS \n\nStart by creating a new HTML file, if you don't have one already. Importing the JavaScript and CSS corresponding to the `@gradio/lite` package by using the following code:", "Getting Started with the Gradio Python client\n\nTags: CLIENT, API, SPACES\n\nThe Gradio Python client makes it very easy to use any Gradio app as an API. As an example, consider this [Hugging Face Space that transcribes audio files](https://huggingface.co/spaces/abidlabs/whisper) that are recorded from the microphone.\n\n![](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/gradio-guides/whisper-screenshot.jpg)\n\nUsing the `gradio_client` library, we can easily use the Gradio as an API to transcribe audio files programmatically.\n\nHere's the entire code to do it:\n\n```python\nfrom gradio_client import Client\n\nclient = Client(\"abidlabs/whisper\")\nclient.predict(\"audio_sample.wav\")\n\n>> \"This is a test of the whisper speech recognition model.\"\n```\n\nThe Gradio client works with any hosted Gradio app, whether it be an image generator, a text summarizer, a stateful chatbot, a tax calculator, or anything else! The Gradio Client is mostly used with apps hosted on [Hugging Face Spaces](https://hf.space), but your app can be hosted anywhere, such as your own server.\n\n**Prerequisites**: To use the Gradio client, you do _not_ need to know the `gradio` library in great detail. However, it is helpful to have general familiarity with Gradio's concepts of input and output components.\n\n## Installation\n\nIf you already have a recent version of `gradio`, then the `gradio_client` is included as a dependency.\n\nOtherwise, the lightweight `gradio_client` package can be installed from pip (or pip3) and is tested to work with Python versions 3.9 or higher:\n\n```bash\n$ pip install gradio_client\n```", "## Leveraging Existing Gradio Components\n\nMost of Gradio's frontend components are published on [npm](https://www.npmjs.com/), the javascript package repository.\nThis means that you can use them to save yourself time while incorporating common patterns in your component, like uploading files.\nFor example, the `@gradio/upload` package has `Upload` and `ModifyUpload` components for properly uploading files to the Gradio server. \nHere is how you can use them to create a user interface to upload and display PDF files.\n\n```typescript\n<script>\n\timport { type FileData, normalise_file, Upload, ModifyUpload } from \"@gradio/upload\";\n\timport { Empty, UploadText, BlockLabel } from \"@gradio/atoms\";\n</script>\n\n<BlockLabel Icon={File} label={label || \"PDF\"} />\n{#if value === null && interactive}\n    <Upload\n        filetype=\"application/pdf\"\n        on:load={handle_load}\n        {root}\n        >\n        <UploadText type=\"file\" i18n={gradio.i18n} />\n    </Upload>\n{:else if value !== null}\n    {#if interactive}\n        <ModifyUpload i18n={gradio.i18n} on:clear={handle_clear}/>\n    {/if}\n    <iframe title={value.orig_name || \"PDF\"} src={value.data} height=\"{height}px\" width=\"100%\"></iframe>\n{:else}\n    <Empty size=\"large\"> <File/> </Empty>\t\n{/if}\n```\n\nYou can also combine existing Gradio components to create entirely unique experiences.\nLike rendering a gallery of chatbot conversations. \nThe possibilities are endless, please read the documentation on our javascript packages [here](https://gradio.app/main/docs/js).\nWe'll be adding more packages and documentation over the coming weeks!\n\n## Matching Gradio Core's Design System", "Getting Started with the Gradio JavaScript client\n\nTags: CLIENT, API, SPACES\n\nThe Gradio JavaScript client makes it very easy to use any Gradio app as an API. As an example, consider this [Hugging Face Space that transcribes audio files](https://huggingface.co/spaces/abidlabs/whisper) that are recorded from the microphone.\n\n![](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/gradio-guides/whisper-screenshot.jpg)\n\nUsing the `@gradio/client` library, we can easily use the Gradio as an API to transcribe audio files programmatically.\n\nHere's the entire code to do it:\n\n```js\nimport { client } from \"@gradio/client\";\n\nconst response = await fetch(\n\t\"https://github.com/audio-samples/audio-samples.github.io/raw/master/samples/wav/ted_speakers/SalmanKhan/sample-1.wav\"\n);\nconst audio_file = await response.blob();\n\nconst app = await client(\"abidlabs/whisper\");\nconst transcription = await app.predict(\"/predict\", [audio_file]);\n\nconsole.log(transcription.data);\n// [ \"I said the same phrase 30 times.\" ]\n```\n\nThe Gradio client works with any hosted Gradio app, whether it be an image generator, a text summarizer, a stateful chatbot, a tax calculator, or anything else! The Gradio Client is mostly used with apps hosted on [Hugging Face Spaces](https://hf.space), but your app can be hosted anywhere, such as your own server.\n\n**Prequisites**: To use the Gradio client, you do _not_ need to know the `gradio` library in great detail. However, it is helpful to have general familiarity with Gradio's concepts of input and output components.", "Quickstart\n\nGradio is an open-source Python package that allows you to quickly **build** a demo or web application for your machine learning model, API, or any arbitary Python function. You can then **share** a link to your demo or web application in just a few seconds using Gradio's built-in sharing features. *No JavaScript, CSS, or web hosting experience needed!*\n\n<img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/gradio-guides/lcm-screenshot-3.gif\" style=\"padding-bottom: 10px\">\n\nIt just takes a few lines of Python to create a beautiful demo like the one above, so let's get started \ud83d\udcab\n\n## Installation\n\n**Prerequisite**: Gradio requires [Python 3.8 or higher](https://www.python.org/downloads/)\n\n\nWe recommend installing Gradio using `pip`, which is included by default in Python. Run this in your terminal or command prompt:\n\n```bash\npip install gradio\n```\n\n\nTip: it is best to install Gradio in a virtual environment. Detailed installation instructions for all common operating systems <a href=\"https://www.gradio.app/main/guides/installing-gradio-in-a-virtual-environment\">are provided here</a>. \n\n## Building Your First Demo\n\nYou can run Gradio in your favorite code editor, Jupyter notebook, Google Colab, or anywhere else you write Python. Let's write your first Gradio app:\n\n\n$code_hello_world_4\n\n\nTip: We shorten the imported name from <code>gradio</code> to <code>gr</code> for better readability of code. This is a widely adopted convention that you should follow so that anyone working with your code can easily understand it.", "Running a Gradio App on your Web Server with Nginx\n\nTags: DEPLOYMENT, WEB SERVER, NGINX\n\n## Introduction\n\nGradio is a Python library that allows you to quickly create customizable web apps for your machine learning models and data processing pipelines. Gradio apps can be deployed on [Hugging Face Spaces](https://hf.space) for free.\n\nIn some cases though, you might want to deploy a Gradio app on your own web server. You might already be using [Nginx](https://www.nginx.com/), a highly performant web server, to serve your website (say `https://www.example.com`), and you want to attach Gradio to a specific subpath on your website (e.g. `https://www.example.com/gradio-demo`).\n\nIn this Guide, we will guide you through the process of running a Gradio app behind Nginx on your own web server to achieve this.\n\n**Prerequisites**\n\n1. A Linux web server with [Nginx installed](https://www.nginx.com/blog/setting-up-nginx/) and [Gradio installed](/quickstart)\n2. A working Gradio app saved as a python file on your web server\n\n## Editing your Nginx configuration file\n\n1. Start by editing the Nginx configuration file on your web server. By default, this is located at: `/etc/nginx/nginx.conf`\n\nIn the `http` block, add the following line to include server block configurations from a separate file:\n\n```bash\ninclude /etc/nginx/sites-enabled/*;\n```\n\n2. Create a new file in the `/etc/nginx/sites-available` directory (create the directory if it does not already exist), using a filename that represents your app, for example: `sudo nano /etc/nginx/sites-available/my_gradio_app`"], "eval_score_GPT4": "5", "eval_feedback_GPT4": "Feedback: The response provided is completely correct, accurate, and factual. It aligns well with the reference answer by detailing the uses of Gradio for building and sharing demos, web applications, and APIs for machine learning models, which is a more detailed version of what is mentioned in the reference answer. The response also includes additional relevant information about the technical requirements and deployment options, which supports the main use cases of Gradio without deviating from the facts."}, {"question": "What TensorFlow API function is used to load a saved tensor file?\n", "true_answer": "safetensors.tensorflow.load_file", "source_doc": "huggingface/safetensors/blob/main/docs/source/api/tensorflow.mdx", "generated_answer": "The TensorFlow API function used to load a saved tensor file is `tf.keras.models.load_model()` or `tf.saved_model.load()`. Both functions can be used to load a saved TensorFlow model, but `load_model()` is a convenience function that automatically loads the model weights, architecture, and optimizer from a HDF5 or Keras model file, while `load()` is a more low-level function that allows you to", "retrieved_docs": ["Tensorflow API\n\n[[autodoc]] safetensors.tensorflow.load_file\n[[autodoc]] safetensors.tensorflow.load\n[[autodoc]] safetensors.tensorflow.save_file\n[[autodoc]] safetensors.tensorflow.save", "## Why are shared tensors not saved in `safetensors` ?\n\nMultiple reasons for that:\n\n- *Not all frameworks support them* for instance `tensorflow` does not.\n  So if someone saves shared tensors in torch, there is no way to \n  load them in a similar fashion so we could not keep the same `Dict[str, Tensor]`\n  API.\n- *It makes lazy loading very quickly.*\n  Lazy loading is the ability to load only some tensors, or part of tensors for\n  a given file. This is trivial to do without sharing tensors but with tensor sharing\n\n  ```python\n  with safe_open(\"model.safetensors\", framework=\"pt\") as f:\n      a = f.get_tensor(\"a\")\n      b = f.get_tensor(\"b\")\n  ```\n\n  Now it's impossible with this given code to \"reshare\" buffers after the fact.\n  Once we give the `a` tensor we have no way to give back the same memory when\n  you ask for `b`. (In this particular example we could keep track of given buffers\n  but this is not the case in general, since you could do arbitrary work with `a`\n  like sending it to another device before asking for `b`)\n- *It can lead to much larger file than necessary*.\n  If you are saving a shared tensor which is only a fraction of a larger tensor,\n  then saving it with pytorch leads to saving the entire buffer instead of saving\n  just what is needed.\n\n  ```python\n  a = torch.zeros((100, 100))\n  b = a[:1, :]\n  torch.save({\"b\": b}, \"model.bin\")\n  # File is 41k instead of the expected 400 bytes\n  # In practice it could happen that you save several 10GB instead of 1GB.\n  ```\n\nNow with all those reasons being mentioned, nothing is set in stone in there.\nShared tensors do not cause unsafety, or denial of service potential, so this\ndecision could be revisited if current workarounds are not satisfactory.\n\n## How does it work ?", "### Checkpoint Writing and Loading\n\nWhen a tensor is checkpointed from a XLA device and then loaded back from the checkpoint, it will be loaded back to the original device. Before checkpointing tensors in your model, you want to ensure that all of your tensors are on CPU devices instead of XLA devices. This way, when you load back the tensors, you\u2019ll load them through CPU devices and then have the opportunity to place them on whatever XLA devices you desire. We provide the `xm.save()` API for this, which already takes care of only writing to storage location from only one process on each host (or one globally if using a shared file system across hosts).\n\n```python\nclass PreTrainedModel(nn.Module, ModuleUtilsMixin, GenerationMixin):\n\u2026\n    def save_pretrained(self, save_directory):\n        ...\n        if getattr(self.config, \"xla_device\", False):\n            import torch_xla.core.xla_model as xm\n\n            if xm.is_master_ordinal():\n                # Save configuration file\n                model_to_save.config.save_pretrained(save_directory)\n            # xm.save takes care of saving only from master\n            xm.save(state_dict, output_model_file)\n```\n\n```python\nclass Trainer:\n\u2026\n   def train(self, *args, **kwargs):\n       ...\n       if is_torch_tpu_available():\n           xm.rendezvous(\"saving_optimizer_states\")\n           xm.save(self.optimizer.state_dict(),\n                   os.path.join(output_dir, \"optimizer.pt\"))\n           xm.save(self.lr_scheduler.state_dict(),\n                   os.path.join(output_dir, \"scheduler.pt\"))\n```\n\n\n## PyTorch / XLA Library", "Torch shared tensors\n\n\n## TL;DR\n\nUsing specific functions, which should work in most cases for you.\nThis is not without side effects.\n\n```python\nfrom safetensors.torch import load_model, save_model\n\nsave_model(model, \"model.safetensors\")\n# Instead of save_file(model.state_dict(), \"model.safetensors\")\n\nload_model(model, \"model.safetensors\")\n# Instead of model.load_state_dict(load_file(\"model.safetensors\"))\n```\n\n## What are shared tensors ?\n\nPytorch uses shared tensors for some computation.\nThis is extremely interesting to reduce memory usage in general.\n\nOne very classic use case is in transformers the `embeddings` are shared with\n`lm_head`. By using the same matrix, the model uses less parameters, and gradients \nflow much better to the `embeddings` (which is the start of the model, so they don't\nflow easily there, whereas `lm_head` is at the tail of the model, so gradients are\nextremely good over there, since they are the same tensors, they both benefit)\n\n\n```python\nfrom torch import nn\n\nclass Model(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.a = nn.Linear(100, 100)\n        self.b = self.a\n\n    def forward(self, x):\n        return self.b(self.a(x))\n\n\nmodel = Model()\nprint(model.state_dict())\n# odict_keys(['a.weight', 'a.bias', 'b.weight', 'b.bias'])\ntorch.save(model.state_dict(), \"model.bin\")\n# This file is now 41k instead of ~80k, because A and B are the same weight hence only 1 is saved on disk with both `a` and `b` pointing to the same buffer\n```", "```py\n>>> import tensorflow as tf\n\n>>> tf_predictions = tf.nn.softmax(tf_outputs.logits, axis=-1)\n>>> tf_predictions  # doctest: +IGNORE_RESULT\n```\n</tf>\n</frameworkcontent>\n\n<Tip>\n\nAll \ud83e\udd17 Transformers models (PyTorch or TensorFlow) output the tensors *before* the final activation\nfunction (like softmax) because the final activation function is often fused with the loss. Model outputs are special dataclasses so their attributes are autocompleted in an IDE. The model outputs behave like a tuple or a dictionary (you can index with an integer, a slice or a string) in which case, attributes that are None are ignored.\n\n</Tip>\n\n### Save a model\n\n<frameworkcontent>\n<pt>\nOnce your model is fine-tuned, you can save it with its tokenizer using [`PreTrainedModel.save_pretrained`]:\n\n```py\n>>> pt_save_directory = \"./pt_save_pretrained\"\n>>> tokenizer.save_pretrained(pt_save_directory)  # doctest: +IGNORE_RESULT\n>>> pt_model.save_pretrained(pt_save_directory)\n```\n\nWhen you are ready to use the model again, reload it with [`PreTrainedModel.from_pretrained`]:\n\n```py\n>>> pt_model = AutoModelForSequenceClassification.from_pretrained(\"./pt_save_pretrained\")\n```\n</pt>\n<tf>\nOnce your model is fine-tuned, you can save it with its tokenizer using [`TFPreTrainedModel.save_pretrained`]:", "If you find that loading with `to_tf_dataset` is slow, you can also use the `num_workers` argument. This spins\nup multiple subprocesses to load data in parallel. This feature is recent and still somewhat experimental - please file\nan issue if you encounter any bugs while using it!\n\n### When to use to_tf_dataset\n\nThe astute reader may have noticed at this point that we have offered two approaches to achieve the same goal - if you\nwant to pass your dataset to a TensorFlow model, you can either convert the dataset to a `Tensor` or `dict` of `Tensors`\nusing `.with_format('tf')`, or you can convert the dataset to a `tf.data.Dataset` with `to_tf_dataset()`. Either of these\ncan be passed to `model.fit()`, so which should you choose?\n\nThe key thing to recognize is that when you convert the whole dataset to `Tensor`s, it is static and fully loaded into\nRAM. This is simple and convenient, but if any of the following apply, you should probably use `to_tf_dataset()`\ninstead:", "<Tip>\n\nRefer to the Performance [guide](performance) for more details about memory-saving techniques.\n\n</Tip>\n\n## Unable to load a saved TensorFlow model\n\nTensorFlow's [model.save](https://www.tensorflow.org/tutorials/keras/save_and_load#save_the_entire_model) method will save the entire model - architecture, weights, training configuration - in a single file. However, when you load the model file again, you may run into an error because \ud83e\udd17 Transformers may not load all the TensorFlow-related objects in the model file. To avoid issues with saving and loading TensorFlow models, we recommend you:\n\n- Save the model weights as a `h5` file extension with [`model.save_weights`](https://www.tensorflow.org/tutorials/keras/save_and_load#save_the_entire_model) and then reload the model with [`~TFPreTrainedModel.from_pretrained`]:\n\n```py\n>>> from transformers import TFPreTrainedModel\n>>> from tensorflow import keras\n\n>>> model.save_weights(\"some_folder/tf_model.h5\")\n>>> model = TFPreTrainedModel.from_pretrained(\"some_folder\")\n```\n\n- Save the model with [`~TFPretrainedModel.save_pretrained`] and load it again with [`~TFPreTrainedModel.from_pretrained`]:\n\n```py\n>>> from transformers import TFPreTrainedModel\n\n>>> model.save_pretrained(\"path_to/model\")\n>>> model = TFPreTrainedModel.from_pretrained(\"path_to/model\")\n```\n\n## ImportError\n\nAnother common error you may encounter, especially if it is a newly released model, is `ImportError`:"], "eval_score_GPT4": "3", "eval_feedback_GPT4": "Feedback: The response provided mentions `tf.keras.models.load_model()` and `tf.saved_model.load()`, which are indeed TensorFlow API functions used to load saved models, not specifically saved tensor files. The reference answer, however, mentions `safetensors.tensorflow.load_file`, which is not a recognized TensorFlow API function. Since the reference answer is incorrect, it is not possible to evaluate the response against it. However, if we consider the actual TensorFlow API, the response is partially correct as it correctly identifies functions to load models, which may include tensors as part of the model's data. Therefore, the response is somewhat correct, accurate, and factual when considering the actual TensorFlow API, but cannot be evaluated against the provided reference answer."}, {"question": "Where can you access the logs of your Endpoints in Hugging Face Endpoints?\n", "true_answer": "In the \"Logs\" tab of your Endpoint through the UI.", "source_doc": "huggingface/hf-endpoints-documentation/blob/main/docs/source/guides/logs.mdx", "generated_answer": "The logs of your Endpoints in Hugging Face Endpoints can be accessed through the \"Logs\" tab in the UI of your Endpoint. Both the build logs of your Image artifacts and the Container Logs during inference are available. The Container Logs are only accessible when the Endpoint is in the \"Running\" state. If the Endpoint creation is in the \"Failed\" state, you can check the Build Logs to determine the reason.", "retrieved_docs": ["Access and read Logs\n\nHugging Face Endpoints provides access to the logs of your Endpoints through the UI in the \u201cLogs\u201d tab of your Endpoint. \n\nYou will have access to the build logs of your Image artifacts as well as access to the Container Logs during inference.\n\n<img src=\"https://raw.githubusercontent.com/huggingface/hf-endpoints-documentation/main/assets/9_selection.png\" alt=\"select logs\" />\n\nThe Container Logs are only available when your Endpoint is in the \u201cRunning\u201d state. \n\n_Note: If your Endpoint creation is in the \u201cFailed\u201d state, you can check the Build Logs to see what the reason was, e.g. wrong version of a dependency, etc._\n\n**Build Logs:**\n\n<img src=\"https://raw.githubusercontent.com/huggingface/hf-endpoints-documentation/main/assets/9_build_logs.png\" alt=\"build logs\" />\n\n**Container Logs:**\n\n<img src=\"https://raw.githubusercontent.com/huggingface/hf-endpoints-documentation/main/assets/9_logs.png\" alt=\"container logs\" />", "Security & Compliance\n\n\ud83e\udd17 Inference Endpoints is built with security and secure inference at its core. Below you can find an overview of the security measures we have in place.\n\n## Data Security/Privacy\n\nHugging Face does not store any customer data in terms of payloads or tokens that are passed to the Inference Endpoint. We are storing logs for 30 days. Every Inference Endpoints uses TLS/SSL to encrypt the data in transit.\n\nWe also recommend using AWS or Azure Private Link for organizations. This allows you to access your Inference Endpoint through a private connection, without exposing it to the internet.\n\nHugging Face also offers Business Associate Addendum or GDPR data processing agreement through the Inference Endpoint enterprise plan.\n\n## Model Security/Privacy:\n\nYou can set a model repository as private if you do not want to publicly expose it. Hugging Face does not own any model or data you upload to the Hugging Face hub. Hugging Face does provide malware and pickle scans over the contents of the model repository as with all items in the Hub.\n\n## Inference Endpoints and Hub Security\n\nThe Hugging Face Hub, which Inference Endpoints is part, is also SOC2 Type 2 certified. The Hugging Face Hub offers Role Based Access Control. For more on hub security: https://huggingface.co/docs/hub/security\n\n<img width=\"150\" src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/hub/security-soc-1.jpg\">\n\n## Inference Endpoint Security level\n\nWe currently offer three types of endpoints, in order or increasing security level:\n\n- **Public**: A Public Endpoint is available from the internet, secured with TLS/SSL, and requires no authentication.\n- **Protected**: A Protected Endpoint is available from the internet, secured with TLS/SSL, and requires a valid Hugging Face token for authentication.\n- **Private** A Private Endpoint is only available through an intra-region secured AWS or Azure PrivateLink connection. Private Endpoints are not accessible from the internet.", "Access and view Metrics\n\nHugging Face Endpoints provides access to the metrics and analytics of your Endpoints through the UI on the detailed overview in the \u201cAnalytics\u201d tab of your Endpoints. \n\n<img src=\"https://raw.githubusercontent.com/huggingface/hf-endpoints-documentation/main/assets/10_metric.png\" alt=\"metric dashboard\" />\n\n## Access Metrics via API\n\nThe Hugging Face Inference Endpoints API exposes a [route to access the metrics](/docs/inference-endpoints/api_reference#getendpointmetrics) of your Endpoints. You can use this route to send customized Prometheus queries to your Endpoints.", "Create an Endpoint\n\nAfter your first login, you will be directed to the [Endpoint creation page](https://ui.endpoints.huggingface.co/new). As an example, this guide will go through the steps to deploy [distilbert-base-uncased-finetuned-sst-2-english](https://huggingface.co/distilbert-base-uncased-finetuned-sst-2-english) for text classification. \n\n## 1. Enter the Hugging Face Repository ID and your desired endpoint name:\n\n<img src=\"https://raw.githubusercontent.com/huggingface/hf-endpoints-documentation/main/assets/1_repository.png\" alt=\"select repository\" />\n\n## 2. Select your Cloud Provider and region. Initially, only AWS will be available as a Cloud Provider with the `us-east-1` and `eu-west-1` regions. We will add Azure soon, and if you need to test Endpoints with other Cloud Providers or regions, please let us know.\n\n<img src=\"https://raw.githubusercontent.com/huggingface/hf-endpoints-documentation/main/assets/1_region.png\" alt=\"select region\" />\n\n## 3. Define the [Security Level](security) for the Endpoint:\n\n<img src=\"https://raw.githubusercontent.com/huggingface/hf-endpoints-documentation/main/assets/1_security.png\" alt=\"define security\" />\n\n## 4. Create your Endpoint by clicking **Create Endpoint**. By default, your Endpoint is created with a medium CPU (2 x 4GB vCPUs with Intel Xeon Ice Lake) The cost estimate assumes the Endpoint will be up for an entire month, and does not take autoscaling into account.", "Public and Protected Endpoints do not require any additional configuration. For Private Endpoints, you need to provide the AWS account ID of the account which also should have access to \ud83e\udd17 Inference Endpoints.\n\n<img\n  src=\"https://raw.githubusercontent.com/huggingface/hf-endpoints-documentation/main/assets/endpoint_types.png\"\n  alt=\"endpoint types\"\n/>\n\nHugging Face Privacy Policy - https://huggingface.co/privacy", "Send Requests to Endpoints\n\nYou can send requests to Inference Endpoints using the UI leveraging the Inference Widget or programmatically, e.g. with cURL, `@huggingface/inference`, `huggingface_hub` or any REST client. The Endpoint overview not only provides a interactive widget for you to test the Endpoint, but also generates code for `python`, `javascript` and `curl`. You can use this code to quickly get started with your Endpoint in your favorite programming language.\n\nBelow are also examples on how to use the `@huggingface/inference` library to call an inference endpoint.\n\n## Use the UI to send requests\n\nThe Endpoint overview provides access to the Inference Widget which can be used to send requests (see step 6 of [Create an Endpoint](/docs/inference-endpoints/guides/create_endpoint)). This allows you to quickly test your Endpoint with different inputs and share it with team members.\n\n## Use cURL to send requests\n\nThe cURL command for the request above should look like this. You'll need to provide your user token which can be found in your Hugging Face [account settings](https://huggingface.co/settings/tokens):\n\nExample Request:\n\n```bash\ncurl https://uu149rez6gw9ehej.eu-west-1.aws.endpoints.huggingface.cloud/distilbert-sentiment \\\n\t-X POST \\\n\t-d '{\"inputs\": \"Deploying my first endpoint was an amazing experience.\"}' \\\n\t-H \"Authorization: Bearer <Token>\"\n```\n\nThe Endpoints API offers the same API definitions as the [Inference API](https://huggingface.co/docs/api-inference/detailed_parameters) and the [SageMaker Inference Toolkit](https://huggingface.co/docs/sagemaker/reference#inference-toolkit-api). All the request payloads are documented in the [Supported Tasks](/docs/inference-endpoints/supported_tasks) section.", "Within minutes, you can test your endpoint and add its inference API to your application. It\u2019s never been easier! \n\n![Predicting with a Hugging Face Endpoint on Azure](assets/75_hugging_face_endpoints_on_azure/04.jpg \"Predicting with a Hugging Face Endpoint on Azure\")\n\nIf you'd like to see the service in action, you can click on the image below to launch a video walkthrough.\n\n[![Video walkthrough of Hugging Face Endpoints](assets/75_hugging_face_endpoints_on_azure/05.jpg)](https://youtu.be/cjXYjN2mNVM \"Video walkthrough of Hugging Face Endpoints\")\n\nHugging Face Model Catalog on Azure Machine Learning is available today in public preview in all Azure Regions where Azure Machine Learning is available. Give the service a try and [let us know your feedback and questions in the forum](https://discuss.huggingface.co/c/azureml/68)!"], "eval_score_GPT4": "5", "eval_feedback_GPT4": "Feedback: The response provided is completely correct, accurate, and factual. It aligns with the reference answer by stating that the logs can be accessed through the \"Logs\" tab in the UI of the Endpoint. Additionally, the response goes into further detail by explaining the types of logs available (build logs and container logs) and the conditions under which they can be accessed, which is consistent with the expected functionality of Hugging Face Endpoints. This additional information supports the correctness and factual nature of the response without deviating from the accuracy required by the reference answer."}, {"question": "What is the latest task added to Hugging Face AutoTrain for Computer Vision?\n", "true_answer": "Image Classification", "source_doc": "huggingface/blog/blob/main/autotrain-image-classification.md", "generated_answer": "The latest task added to Hugging Face AutoTrain for Computer Vision is image classification. This allows users to train models for this task without any configuration needed, making \"no-code\" machine learning better than ever. Other supported vision tasks on the Hugging Face Hub include image segmentation, object detection (with zero-shot capabilities), video classification, depth estimation, image-to-image synthesis, unconditional image generation, and zero-shot image classification.", "retrieved_docs": ["## \ud83e\udd17 AutoTrain\n\n[AutoTrain](https://huggingface.co/autotrain) provides a \u201cno-code\u201d solution to train state-of-the-art Machine Learning models for tasks like text classification, text summarization, named entity recognition, and more. For Computer Vision, we currently support [image classification](https://huggingface.co/blog/autotrain-image-classification), but one can expect more task coverage.\n\nAutoTrain also enables [automatic model evaluation](https://huggingface.co/spaces/autoevaluate/model-evaluator). This application allows you to evaluate \ud83e\udd17 Transformers [models](https://huggingface.co/models?library=transformers&sort=downloads) across a wide variety of [datasets](https://huggingface.co/datasets) on the Hub. The results of your evaluation will be displayed on the [public leaderboards](https://huggingface.co/spaces/autoevaluate/leaderboards). You can check [this blog post](https://huggingface.co/blog/eval-on-the-hub) for more details.\n\n## The technical philosophy\n\nIn this section, we wanted to share our philosophy behind adding support for Computer Vision in \ud83e\udd17\u00a0Transformers so that the community is aware of the design choices specific to this area.\n\nEven though Transformers started with NLP, we support multiple modalities today, for example \u2013 vision, audio, vision-language, and Reinforcement Learning. For all of these modalities, all the corresponding models from Transformers enjoy some common benefits:\n\n- Easy model download with a single line of code with `from_pretrained()`\n- Easy model upload with `push_to_hub()`\n- Support for loading huge checkpoints with efficient checkpoint sharding techniques\n- Optimization support (with tools like [Optimum](https://huggingface.co/docs/optimum))\n- Initialization from model configurations\n- Support for both PyTorch and TensorFlow (non-exhaustive)\n- and many more", "--\ntitle: Image Classification with AutoTrain \nthumbnail: /blog/assets/105_autotrain-image-classification/thumbnail.png\nauthors:\n- user: nimaboscarino\n---\n\n# Image Classification with AutoTrain\n\n\n<script async defer src=\"https://unpkg.com/medium-zoom-element@0/dist/medium-zoom-element.min.js\"></script>\n\nSo you\u2019ve heard all about the cool things that are happening in the machine learning world, and you want to join in. There\u2019s just one problem \u2013 you don\u2019t know how to code! \ud83d\ude31 Or maybe you\u2019re a seasoned software engineer who wants to add some ML to your side-project, but you don\u2019t have the time to pick up a whole new tech stack! For many people, the technical barriers to picking up machine learning feel insurmountable. That\u2019s why Hugging Face created [AutoTrain](https://huggingface.co/autotrain), and with the latest feature we\u2019ve just added, we\u2019re making \u201cno-code\u201d machine learning better than ever. Best of all, you can create your first project for \u2728 free! \u2728\n\n[Hugging Face AutoTrain](https://huggingface.co/autotrain) lets you train models with **zero** configuration needed. Just choose your task (translation? how about question answering?), upload your data, and let Hugging Face do the rest of the work! By letting AutoTrain experiment with number of different models, there's even a good chance that you'll end up with a model that performs better than a model that's been hand-trained by an engineer \ud83e\udd2f We\u2019ve been expanding the number of tasks that we support, and we\u2019re proud to announce that **you can now use AutoTrain for Computer Vision**! Image Classification is the latest task we\u2019ve added, with more on the way. But what does this mean for you?", "--\ntitle: The State of Computer Vision at Hugging Face \ud83e\udd17\nthumbnail: /blog/assets/cv_state/thumbnail.png\nauthors:\n- user: sayakpaul\n---\n\n# The State of Computer Vision at Hugging Face \ud83e\udd17\n\n\nAt Hugging Face, we pride ourselves on democratizing the field of artificial intelligence together with the community. As a part of that mission, we began focusing our efforts on computer vision over the last year. What started as a [PR for having Vision Transformers (ViT) in \ud83e\udd17 Transformers](https://github.com/huggingface/transformers/pull/10950) has now grown into something much bigger \u2013 8 core vision tasks, over 3000 models, and over 100 datasets on the Hugging Face Hub.\n\nA lot of exciting things have happened since ViTs joined the Hub. In this blog post, we\u2019ll summarize what went down and what\u2019s coming to support the continuous progress of Computer Vision from the \ud83e\udd17 ecosystem.\n\nHere is a list of things we\u2019ll cover:\n\n- [Supported vision tasks and Pipelines](#support-for-pipelines)\n- [Training your own vision models](#training-your-own-models)\n- [Integration with `timm`](#\ud83e\udd17-\ud83e\udd1d-timm)\n- [Diffusers](#\ud83e\udde8-diffusers)\n- [Support for third-party libraries](#support-for-third-party-libraries)\n- [Deployment](#deployment)\n- and much more!\n\n## Enabling the community: One task at a time \ud83d\udc41\n\nThe Hugging Face Hub is home to over 100,000 public models for different tasks such as next-word prediction, mask filling, token classification, sequence classification, and so on. As of today, we support [8 core vision tasks](https://huggingface.co/tasks) providing many model checkpoints:\n\n- Image classification\n- Image segmentation\n- (Zero-shot) object detection\n- Video classification\n- Depth estimation\n- Image-to-image synthesis\n- Unconditional image generation\n- Zero-shot image classification", "Once AutoTrain creates your project, you just need to connect your data. If you have the data locally, you can drag and drop the folder into the window. Since we can also use [any of the image classification datasets on the Hugging Face Hub](https://huggingface.co/datasets?task_categories=task_categories:image-classification), in this example I\u2019ve decided to use the [NimaBoscarino/butterflies](https://huggingface.co/datasets/NimaBoscarino/butterflies) dataset. You can select separate training and validation datasets if available, or you can ask AutoTrain to split the data for you.\n\n<div class=\"grid grid-cols-2 gap-4\">\n  <figure class=\"image table text-center m-0 w-full\">\n  </figure>\n\n  <figure class=\"image table text-center m-0 w-full\">\n    <medium-zoom background=\"rgba(0,0,0,.7)\" alt=\"A form showing configurations to select for the imported dataset, including split types and data columns.\" src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/autotrain-image-classification/add-dataset.png\"></medium-zoom>\n  </figure>\n</div>\n\nOnce the data has been added, simply choose the number of model candidates that you\u2019d like AutoModel to try out, review the expected training cost (training with 5 candidate models and less than 500 images is free \ud83e\udd29), and start training!", "## AutoTrain with HuggingFace\n\nAutomated Machine Learning is a term for automating a Machine Learning pipeline. It also includes data cleaning, model selection, and hyper-parameter optimization too. We can use \ud83e\udd17 transformers for automated hyper-parameter searching. Hyper-parameter optimization is a difficult and time-consuming process.\n    \nWhile we can build our pipeline ourselves by using transformers and other powerful APIs, it is also possible to fully automate this with [AutoTrain](https://huggingface.co/autotrain). AutoTrain is built on many powerful APIs like transformers, [datasets](https://github.com/huggingface/datasets) and [inference-api](https://huggingface.co/docs/transformers/main_classes/trainer).\n\nCleaning the data, model selection, and hyper-parameter optimization steps are all fully automated in AutoTrain. One can fully utilize this framework to build production-ready SOTA transformer models for a specific task. Currently, AutoTrain supports binary and multi-label text classification, token classification, extractive question answering, text summarization, and text scoring. It also supports many languages like English, German, French, Spanish, Finnish, Swedish, Hindi, Dutch, and [more](https://huggingface.co/autotrain). If your language is not supported by AutoTrain, it is also possible to use custom models with custom tokenizers.\n\n## Kili\n\n[Kili](https://kili-technology.com/) is an end-to-end AI training platform for data-centric businesses. Kili provides optimized labeling features and quality management tools to manage your data. You can quickly annotate the image, video, text, pdf, and voice data while controlling the quality of the dataset. It also has powerful APIs for GraphQL and Python which eases data management a lot. \n\nIt is available either online or on-premise and it enables modern Machine Learning technics either on computer vision or on NLP and OCR. It supports text classification, named entity recognition (NER), relation extraction, and more NLP/OCR tasks. It also supports computer vision tasks like object detection, image transcription, video classification, semantic segmentation, and many more!", "## Training your own models\n\nWhile being able to use a model for off-the-shelf inference is a great way to get started, fine-tuning is where the community gets the most benefits. This is especially true when your datasets are custom, and you\u2019re not getting good performance out of the pre-trained models.\n\nTransformers provides a [Trainer API](https://huggingface.co/docs/transformers/main_classes/trainer) for everything related to training. Currently, `Trainer` seamlessly supports the following tasks: image classification, image segmentation, video classification, object detection, and depth estimation. Fine-tuning models for other vision tasks are also supported, just not by `Trainer`.\n\nAs long as the loss computation is included in a model from Transformers computes loss for a given task, it should be eligible for fine-tuning for the task. If you find issues, please [report](https://github.com/huggingface/transformers/issues) them on GitHub.\n\n**Where do I find the code?**\n\n- [Model documentation](https://huggingface.co/docs/transformers/index#supported-models)\n- [Hugging Face notebooks](https://github.com/huggingface/notebooks)\n- [Hugging Face example scripts](https://github.com/huggingface/transformers/tree/main/examples)\n- [Task pages](https://huggingface.co/tasks)", "</div>\n\n## What's the Hugging Face Hub?\n\nWe are helping the community work together towards the goal of advancing Machine Learning \ud83d\udd25.\n\nThe Hugging Face Hub is a platform with over 120k models, 20k datasets, and 50k demos in which people can easily collaborate in their ML workflows. The Hub works as a central place where anyone can share, explore, discover, and experiment with open-source Machine Learning.\n\nNo single company, including the Tech Titans, will be able to \u201csolve AI\u201d by themselves \u2013 the only way we'll achieve this is by sharing knowledge and resources in a community-centric approach. We are building the largest open-source collection of models, datasets, demos and metrics on the Hugging Face Hub to democratize and advance ML for everyone \ud83d\ude80.\n\nWe encourage you to read the [Code of Conduct](https://huggingface.co/code-of-conduct) and the [Content Guidelines](https://huggingface.co/content-guidelines) to familiarize yourself with the values that we expect our community members to uphold \ud83e\udd17.\n\n## What can you find on the Hub?\n\nThe Hugging Face Hub hosts Git-based repositories, which are version-controlled buckets that can contain all your files. \ud83d\udcbe\n\nOn it, you'll be able to upload and discover...\n\n- Models, _hosting the latest state-of-the-art models for NLP, vision, and audio tasks_\n- Datasets, _featuring a wide variety of data for different domains and modalities_..\n- Spaces, _interactive apps for demonstrating ML models directly in your browser_.\n\nThe Hub offers **versioning, commit history, diffs, branches, and over a dozen library integrations**! You can learn more about the features that all repositories share in the [**Repositories documentation**](./repositories).\n\n## Models"], "eval_score_GPT4": "5", "eval_feedback_GPT4": "The response correctly identifies \"image classification\" as the latest task added to Hugging Face AutoTrain for Computer Vision, which aligns with the reference answer. The additional information provided about the capabilities of AutoTrain and other supported tasks does not detract from the accuracy of the response regarding the latest task added."}, {"question": "What is the default repository type created by the `create_repo` function on Hugging Face Hub?\n", "true_answer": "model", "source_doc": "huggingface/huggingface_hub/blob/main/docs/source/en/guides/repository.md", "generated_answer": "The default repository type created by the `create_repo` function on Hugging Face Hub is a model repository. However, you can specify a different repository type, such as dataset or space, using the `repo_type` parameter.", "retrieved_docs": ["### Create a repository\n\nCreate an empty repository with [`create_repo`] and give it a name with the `repo_id` parameter. The `repo_id` is your namespace followed by the repository name: `username_or_org/repo_name`.\n\n```py\n>>> from huggingface_hub import create_repo\n>>> create_repo(\"lysandre/test-model\")\n'https://huggingface.co/lysandre/test-model'\n```\n\nBy default, [`create_repo`] creates a model repository. But you can use the `repo_type` parameter to specify another repository type. For example, if you want to create a dataset repository:\n\n```py\n>>> from huggingface_hub import create_repo\n>>> create_repo(\"lysandre/test-dataset\", repo_type=\"dataset\")\n'https://huggingface.co/datasets/lysandre/test-dataset'\n```\n\nWhen you create a repository, you can set your repository visibility with the `private` parameter.\n\n```py\n>>> from huggingface_hub import create_repo\n>>> create_repo(\"lysandre/test-private\", private=True)\n```\n\nIf you want to change the repository visibility at a later time, you can use the [`update_repo_visibility`] function.\n\n### Delete a repository\n\nDelete a repository with [`delete_repo`]. Make sure you want to delete a repository because this is an irreversible process!\n\nSpecify the `repo_id` of the repository you want to delete:\n\n```py\n>>> delete_repo(repo_id=\"lysandre/my-corrupted-dataset\", repo_type=\"dataset\")\n```\n\n### Duplicate a repository (only for Spaces)", "This is equivalent to `huggingface_hub.space_info(repo_id, revision)`.\n\n\n## Repo API\n\nThe following endpoints manage repository settings like creating and deleting a repository.\n### POST /api/repos/create\n\nCreate a repository. It's a model repo by default.\n\nParameters:\n- `type`: Type of repo (dataset or space; model by default).\n- `name`: Name of repo.\n- `organization`: Name of organization (optional).\n- `private`: Whether the repo is private.\n- `sdk`: When the type is `space` (streamlit, gradio, docker or static)\n\nPayload:\n\n```js\npayload = {\n    \"type\":\"model\",\n    \"name\":\"name\",\n    \"organization\": \"organization\",\n    \"private\":\"private\",\n    \"sdk\": \"sdk\"\n}\n```\n\nThis is equivalent to `huggingface_hub.create_repo()`.\n\n### DELETE /api/repos/delete\n\nDelete a repository. It's a model repo by default.\n\nParameters:\n- `type`: Type of repo (dataset or space; model by default).\n- `name`: Name of repo.\n- `organization`: Name of organization (optional).\n\nPayload:\n\n```js\npayload = {\n    \"type\": \"model\",\n    \"name\": \"name\",\n    \"organization\": \"organization\",\n}\n```\n\nThis is equivalent to `huggingface_hub.delete_repo()`.\n\n### PUT /api/repos/{repo_type}/{repo_id}/settings\n\nUpdate repo visibility.\n\nPayload:\n\n```js\npayload = {\n    \"private\": \"private\",\n}\n```\n\nThis is equivalent to `huggingface_hub.update_repo_visibility()`.", "# Adding HuggingFace argument\n    parser.add_argument(\"--repo-id\", type=str, default=\"ThomasSimonini/ppo-CartPole-v1\", help=\"id of the model repository from the Hugging Face Hub {username/repo_name}\")\n\n    args = parser.parse_args()\n    args.batch_size = int(args.num_envs * args.num_steps)\n    args.minibatch_size = int(args.batch_size // args.num_minibatches)\n    # fmt: on\n    return args\n\n\ndef package_to_hub(\n    repo_id,\n    model,\n    hyperparameters,\n    eval_env,\n    video_fps=30,\n    commit_message=\"Push agent to the Hub\",\n    token=None,\n    logs=None,\n):\n    \"\"\"\n    Evaluate, Generate a video and Upload a model to Hugging Face Hub.\n    This method does the complete pipeline:\n    - It evaluates the model\n    - It generates the model card\n    - It generates a replay video of the agent\n    - It pushes everything to the hub\n    :param repo_id: id of the model repository from the Hugging Face Hub\n    :param model: trained model\n    :param eval_env: environment used to evaluate the agent\n    :param fps: number of fps for rendering the video\n    :param commit_message: commit message\n    :param logs: directory on local machine of tensorboard logs you'd like to upload\n    \"\"\"\n    msg.info(\n        \"This function will save, evaluate, generate a video of your agent, \"\n        \"create a model card and push everything to the hub. \"\n        \"It might take up to 1min. \\n \"\n        \"This is a work in progress: if you encounter a bug, please open an issue.\"\n    )\n    # Step 1: Clone or create the repo\n    repo_url = HfApi().create_repo(\n        repo_id=repo_id,\n        token=token,\n        private=False,\n        exist_ok=True,\n    )\n\n    with tempfile.TemporaryDirectory() as tmpdirname:\n        tmpdirname = Path(tmpdirname)", "If you don't want to use a Google Colab or a Jupyter Notebook, you need to use this command instead: `huggingface-cli login`\n\n3\ufe0f\u20e3 We're now ready to push our trained agent to the \ud83e\udd17 Hub \ud83d\udd25 using `package_to_hub()` function\n\nLet's fill the `package_to_hub` function:\n- `model`: our trained model.\n- `model_name`: the name of the trained model that we defined in `model_save`\n- `model_architecture`: the model architecture we used, in our case PPO\n- `env_id`: the name of the environment, in our case `LunarLander-v2`\n- `eval_env`: the evaluation environment defined in eval_env\n- `repo_id`: the name of the Hugging Face Hub Repository that will be created/updated `(repo_id = {username}/{repo_name})`\n\n\ud83d\udca1 **A good name is `{username}/{model_architecture}-{env_id}` **\n\n- `commit_message`: message of the commit\n\n```python\nimport gymnasium as gym\nfrom stable_baselines3.common.vec_env import DummyVecEnv\nfrom stable_baselines3.common.env_util import make_vec_env\n\nfrom huggingface_sb3 import package_to_hub\n\n## TODO: Define a repo_id\n## repo_id is the id of the model repository from the Hugging Face Hub (repo_id = {organization}/{repo_name} for instance ThomasSimonini/ppo-LunarLander-v2\nrepo_id = \n\n# TODO: Define the name of the environment\nenv_id = \n\n# Create the evaluation env and set the render_mode=\"rgb_array\"\neval_env = DummyVecEnv([lambda: gym.make(env_id, render_mode=\"rgb_array\")])\n\n\n# TODO: Define the model architecture we used\nmodel_architecture = \"\"", "!--\u26a0\ufe0f Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\nrendered properly in your Markdown viewer.\n-->\n\n# Upload files to the Hub\n\nSharing your files and work is an important aspect of the Hub. The `huggingface_hub` offers several options for uploading your files to the Hub. You can use these functions independently or integrate them into your library, making it more convenient for your users to interact with the Hub. This guide will show you how to push files:\n\n- without using Git.\n- that are very large with [Git LFS](https://git-lfs.github.com/).\n- with the `commit` context manager.\n- with the [`~Repository.push_to_hub`] function.\n\nWhenever you want to upload files to the Hub, you need to log in to your Hugging Face account. For more details about authentication, check out [this section](../quick-start#authentication).\n\n## Upload a file\n\nOnce you've created a repository with [`create_repo`], you can upload a file to your repository using [`upload_file`].\n\nSpecify the path of the file to upload, where you want to upload the file to in the repository, and the name of the repository you want to add the file to. Depending on your repository type, you can optionally set the repository type as a `dataset`, `model`, or `space`.\n\n```py\n>>> from huggingface_hub import HfApi\n>>> api = HfApi()\n>>> api.upload_file(\n...     path_or_fileobj=\"/path/to/local/folder/README.md\",\n...     path_in_repo=\"README.md\",\n...     repo_id=\"username/test-dataset\",\n...     repo_type=\"dataset\",\n... )\n```\n\n## Upload a folder", "We can now push the repository to the Hugging Face Hub. For this, we will use `push` from `hub_utils`. Hugging Face Hub requires tokens for authentication, therefore you need to pass your token in either  `notebook_login` if you're logging in from a notebook, or `huggingface-cli login` if you're logging in from the CLI.\n\n```python\n# if the repository doesn't exist remotely on the Hugging Face Hub, it will be created when we set create_remote to True\nrepo_id = \"skops-user/my-awesome-model\"\nhub_utils.push(\n    repo_id=repo_id,\n    source=local_repo,\n    token=token,\n    commit_message=\"pushing files to the repo from the example!\",\n    create_remote=True,\n)\n```\n\nOnce we push the model to the Hub, anyone can use it unless the repository is private. You can download the models using `download`. Apart from the model file, the repository contains the model configuration and the environment requirements.\n\n```python\ndownload_repo = \"downloaded-model\"\nhub_utils.download(repo_id=repo_id, dst=download_repo)\n```\n\nThe inference widget is enabled to make predictions in the repository.\n\n![Hosted Inference Widget](assets/94_skops/skops_widget.png)\n\nIf the requirements of your project have changed, you can use `update_env` to update the environment.\n\n```python\nhub_utils.update_env(path=local_repo, requirements=[\"scikit-learn\"])\n```\n\nYou can see the example repository pushed with above code [here](https://huggingface.co/scikit-learn/skops-blog-example).\nWe have prepared two examples to show how to save your models and use model card utilities. You can find them in the resources section below.", "!--\u26a0\ufe0f Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\nrendered properly in your Markdown viewer.\n-->\n\n# Create and manage a repository\n\nThe Hugging Face Hub is a collection of git repositories. [Git](https://git-scm.com/) is a widely used tool in software\ndevelopment to easily version projects when working collaboratively. This guide will show you how to interact with the\nrepositories on the Hub, especially:\n\n- Create and delete a repository.\n- Manage branches and tags. \n- Rename your repository.\n- Update your repository visibility.\n- Manage a local copy of your repository.\n\n<Tip warning={true}>\n\nIf you are used to working with platforms such as GitLab/GitHub/Bitbucket, your first instinct\nmight be to use `git` CLI to clone your repo (`git clone`), commit changes (`git add, git commit`) and push them\n(`git push`). This is valid when using the Hugging Face Hub. However, software engineering and machine learning do\nnot share the same requirements and workflows. Model repositories might maintain large model weight files for different\nframeworks and tools, so cloning the repository can lead to you maintaining large local folders with massive sizes. As\na result, it may be more efficient to use our custom HTTP methods. You can read our [Git vs HTTP paradigm](../concepts/git_vs_http)\nexplanation page for more details.\n\n</Tip>\n\nIf you want to create and manage a repository on the Hub, your machine must be logged in. If you are not, please refer to\n[this section](../quick-start#authentication). In the rest of this guide, we will assume that your machine is logged in.\n\n## Repo creation and deletion\n\nThe first step is to know how to create and delete repositories. You can only manage repositories that you own (under\nyour username namespace) or from organizations in which you have write permissions.\n\n### Create a repository"], "eval_score_GPT4": "5", "eval_feedback_GPT4": "Feedback: The response provided is completely correct, accurate, and factual. It aligns with the reference answer by stating that the default repository type is a model repository. Additionally, it provides extra information that is accurate regarding the ability to specify a different repository type, which does not detract from the correctness of the answer to the given instruction."}, {"question": "How many splits does the \"duorc\" dataset have?\n", "true_answer": "Six", "source_doc": "huggingface/datasets-server/blob/main/docs/source/splits.mdx", "generated_answer": "The \"duorc\" dataset has six splits, as shown in the JSON response from the `/splits` endpoint:\n\n```json\n{\n  \"splits\": [\n    { \"dataset\": \"duorc\", \"config\": \"ParaphraseRC\", \"split\": \"train\" },\n    { \"dataset\": \"duorc\", \"config\": \"ParaphraseRC\", \"split\": \"validation\" },\n    {", "retrieved_docs": ["The endpoint response is a JSON containing the size of the dataset, as well as each of its configurations and splits. It provides the number of rows, the number of colums (where applicable) and the size in bytes for the different forms of the data: original files, size in memory (RAM) and auto-converted parquet files. For example, the [duorc](https://huggingface.co/datasets/duorc) dataset has 187.213 rows along all its configurations and splits, for a total of 97MB.", "The `/splits` endpoint accepts the dataset name as its query parameter:\n\n<inferencesnippet>\n<python>\n```python\nimport requests\nheaders = {\"Authorization\": f\"Bearer {API_TOKEN}\"}\nAPI_URL = \"https://datasets-server.huggingface.co/splits?dataset=duorc\"\ndef query():\n    response = requests.get(API_URL, headers=headers)\n    return response.json()\ndata = query()\n```\n</python>\n<js>\n```js\nimport fetch from \"node-fetch\";\nasync function query(data) {\n    const response = await fetch(\n        \"https://datasets-server.huggingface.co/splits?dataset=duorc\",\n        {\n            headers: { Authorization: `Bearer ${API_TOKEN}` },\n            method: \"GET\"\n        }\n    );\n    const result = await response.json();\n    return result;\n}\nquery().then((response) => {\n    console.log(JSON.stringify(response));\n});\n```\n</js>\n<curl>\n```curl\ncurl https://datasets-server.huggingface.co/splits?dataset=duorc \\\n        -X GET \\\n        -H \"Authorization: Bearer ${API_TOKEN}\"\n```\n</curl>\n</inferencesnippet>\n\nThe endpoint response is a JSON containing a list of the dataset's splits and configurations. For example, the [duorc](https://huggingface.co/datasets/duorc) dataset has six splits and two configurations:", "```json\n{\n  \"splits\": [\n    { \"dataset\": \"duorc\", \"config\": \"ParaphraseRC\", \"split\": \"train\" },\n    { \"dataset\": \"duorc\", \"config\": \"ParaphraseRC\", \"split\": \"validation\" },\n    { \"dataset\": \"duorc\", \"config\": \"ParaphraseRC\", \"split\": \"test\" },\n    { \"dataset\": \"duorc\", \"config\": \"SelfRC\", \"split\": \"train\" },\n    { \"dataset\": \"duorc\", \"config\": \"SelfRC\", \"split\": \"validation\" },\n    { \"dataset\": \"duorc\", \"config\": \"SelfRC\", \"split\": \"test\" }\n  ],\n  \"pending\": [],\n  \"failed\": []\n}\n```", "\"config\": \"SelfRC\",\n        \"split\": \"validation\",\n        \"num_bytes_parquet_files\": 3114390,\n        \"num_bytes_memory\": 51662519,\n        \"num_rows\": 12961,\n        \"num_columns\": 7\n      },\n      {\n        \"dataset\": \"duorc\",\n        \"config\": \"SelfRC\",\n        \"split\": \"test\",\n        \"num_bytes_parquet_files\": 3035736,\n        \"num_bytes_memory\": 49142710,\n        \"num_rows\": 12559,\n        \"num_columns\": 7\n      }\n    ]\n  },\n  \"pending\": [],\n  \"failed\": [],\n  \"partial\": false\n}\n```", "```json\n{\n    \"dataset_info\": {\n        \"description\": \"DuoRC contains 186,089 unique question-answer pairs created from a collection of 7680 pairs of movie plots where each pair in the collection reflects two versions of the same movie.\\n\",\n        \"citation\": \"@inproceedings{DuoRC,\\nauthor = { Amrita Saha and Rahul Aralikatte and Mitesh M. Khapra and Karthik Sankaranarayanan},title = {{DuoRC: Towards Complex Language Understanding with Paraphrased Reading Comprehension}},\\nbooktitle = {Meeting of the Association for Computational Linguistics (ACL)},\\nyear = {2018}\\n}\\n\",\n        \"homepage\": \"https://duorc.github.io/\",\n        \"license\": \"https://raw.githubusercontent.com/duorc/duorc/master/LICENSE\",\n        \"features\": {\n            \"plot_id\": {\n                \"dtype\": \"string\",\n                \"_type\": \"Value\"\n            },\n            \"plot\": {\n                \"dtype\": \"string\",\n                \"_type\": \"Value\"\n            },\n            \"title\": {\n                \"dtype\": \"string\",\n                \"_type\": \"Value\"\n            },\n            \"question_id\": {\n                \"dtype\": \"string\",\n                \"_type\": \"Value\"\n            },\n            \"question\": {\n                \"dtype\": \"string\",\n                \"_type\": \"Value\"\n            },\n            \"answers\": {\n                \"feature\": {\n                    \"dtype\": \"string\",\n                    \"_type\": \"Value\"\n                },", "\"answers\": {\n                \"feature\": {\n                    \"dtype\": \"string\",\n                    \"_type\": \"Value\"\n                },\n                \"_type\": \"Sequence\"\n            },\n            \"no_answer\": {\n                \"dtype\": \"bool\",\n                \"_type\": \"Value\"\n            }\n        },\n        \"builder_name\": \"duorc\",\n        \"config_name\": \"SelfRC\",\n        \"version\": {\n            \"version_str\": \"1.0.0\",\n            \"major\": 1,\n            \"minor\": 0,\n            \"patch\": 0\n        },\n        \"splits\": {\n            \"train\": {\n                \"name\": \"train\",\n                \"num_bytes\": 239852729,\n                \"num_examples\": 60721,\n                \"dataset_name\": \"duorc\"\n            },\n            \"validation\": {\n                \"name\": \"validation\",\n                \"num_bytes\": 51662519,\n                \"num_examples\": 12961,\n                \"dataset_name\": \"duorc\"\n            },\n            \"test\": {\n                \"name\": \"test\",\n                \"num_bytes\": 49142710,\n                \"num_examples\": 12559,\n                \"dataset_name\": \"duorc\"\n            }\n        },\n        \"download_checksums\": {\n            \"https://raw.githubusercontent.com/duorc/duorc/master/dataset/SelfRC_train.json\": {\n                \"num_bytes\": 24388192,", "\"num_bytes_parquet_files\": 26005668,\n        \"num_bytes_memory\": 496682909,\n        \"num_rows\": 69524,\n        \"num_columns\": 7\n      },\n      {\n        \"dataset\": \"duorc\",\n        \"config\": \"ParaphraseRC\",\n        \"split\": \"validation\",\n        \"num_bytes_parquet_files\": 5566868,\n        \"num_bytes_memory\": 106510489,\n        \"num_rows\": 15591,\n        \"num_columns\": 7\n      },\n      {\n        \"dataset\": \"duorc\",\n        \"config\": \"ParaphraseRC\",\n        \"split\": \"test\",\n        \"num_bytes_parquet_files\": 6136591,\n        \"num_bytes_memory\": 115215760,\n        \"num_rows\": 15857,\n        \"num_columns\": 7\n      },\n      {\n        \"dataset\": \"duorc\",\n        \"config\": \"SelfRC\",\n        \"split\": \"train\",\n        \"num_bytes_parquet_files\": 14851720,\n        \"num_bytes_memory\": 239852729,\n        \"num_rows\": 60721,\n        \"num_columns\": 7\n      },\n      {\n        \"dataset\": \"duorc\",\n        \"config\": \"SelfRC\",\n        \"split\": \"validation\",\n        \"num_bytes_parquet_files\": 3114390,"], "eval_score_GPT4": "4", "eval_feedback_GPT4": "The response provided is correct and factual, as it aligns with the reference answer that states there are six splits in the \"duorc\" dataset. The response goes further to provide evidence from the dataset's JSON response, which supports the accuracy of the claim. However, the JSON response is incomplete, but the relevant information provided is correct."}, {"question": "What is the purpose of Fully Sharded Data Parallel (FSDP) in distributed training?\n", "true_answer": "FSDP is developed for distributed training of large pretrained models up to 1T parameters by sharding the model parameters, gradients, and optimizer states across data parallel processes.", "source_doc": "huggingface/peft/blob/main/docs/source/accelerate/fsdp.md", "generated_answer": "Fully Sharded Data Parallel (FSDP) is a data parallel method used in distributed training to reduce memory usage and improve GPU memory efficiency, especially for training large models with a large number of parameters. FSDP shards the model parameters, gradients, and optimizer states across the available GPUs (also called workers or ranks), which allows for a more efficient use of resources and reduces the high cost of large-scale training. By sharding these components, FSDP", "retrieved_docs": ["!--\u26a0\ufe0f Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\nrendered properly in your Markdown viewer.\n-->\n\n# Fully Sharded Data Parallel\n\n[Fully sharded data parallel](https://pytorch.org/docs/stable/fsdp.html) (FSDP) is developed for distributed training of large pretrained models up to 1T parameters. FSDP achieves this by sharding the model parameters, gradients, and optimizer states across data parallel processes and it can also offload sharded model parameters to a CPU. The memory efficiency afforded by FSDP allows you to scale training to larger batch or model sizes.\n\n<Tip warning={true}>\n\nCurrently, FSDP does not confer any reduction in GPU memory usage and FSDP with CPU offload actually consumes 1.65x more GPU memory during training. You can track this PyTorch [issue](https://github.com/pytorch/pytorch/issues/91165) for any updates.\n\n</Tip>\n\nFSDP is supported in \ud83e\udd17 Accelerate, and you can use it with \ud83e\udd17 PEFT. This guide will help you learn how to use our FSDP [training script](https://github.com/huggingface/peft/blob/main/examples/conditional_generation/peft_lora_seq2seq_accelerate_fsdp.py). You'll configure the script to train a large model for conditional generation.\n\n## Configuration\n\nBegin by running the following command to [create a FSDP configuration file](https://huggingface.co/docs/accelerate/main/en/usage_guides/fsdp) with \ud83e\udd17 Accelerate. Use the `--config_file` flag to save the configuration file to a specific location, otherwise it is saved as a `default_config.yaml` file in the \ud83e\udd17 Accelerate cache.\n\nThe configuration file is used to set the default options when you launch the training script.", "!--Copyright 2023 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\n\nhttp://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License.\n\n\u26a0\ufe0f Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\nrendered properly in your Markdown viewer.\n\n-->\n\n# Fully Sharded Data Parallel\n\n[Fully Sharded Data Parallel (FSDP)](https://pytorch.org/blog/introducing-pytorch-fully-sharded-data-parallel-api/) is a data parallel method that shards a model's parameters, gradients and optimizer states across the number of available GPUs (also called workers or *rank*). Unlike [DistributedDataParallel (DDP)](https://pytorch.org/docs/stable/generated/torch.nn.parallel.DistributedDataParallel.html), FSDP reduces memory-usage because a model is replicated on each GPU. This improves GPU memory-efficiency and allows you to train much larger models on fewer GPUs. FSDP is integrated with the Accelerate, a library for easily managing training in distributed environments, which means it is available for use from the [`Trainer`] class.\n\nBefore you start, make sure Accelerate is installed and at least PyTorch 2.1.0 or newer.\n\n```bash\npip install accelerate\n```\n\n## FSDP configuration", "--\ntitle: \"Accelerate Large Model Training using PyTorch Fully Sharded Data Parallel\"\nthumbnail: /blog/assets/62_pytorch_fsdp/fsdp-thumbnail.png\nauthors:\n- user: smangrul\n- user: sgugger\n---\n\n# Accelerate Large Model Training using PyTorch Fully Sharded Data Parallel\n\n\nIn this post we will look at how we can leverage **[Accelerate](https://github.com/huggingface/accelerate)** Library for training large models which enables users to leverage the latest features of **[PyTorch FullyShardedDataParallel (FSDP)](https://pytorch.org/blog/introducing-pytorch-fully-sharded-data-parallel-api/)**.\n\n# Motivation \ud83e\udd17\n\n**With the ever increasing scale, size and parameters of the Machine Learning (ML) models, ML practitioners are finding it difficult to train or even load such large models on their hardware.** On one hand, it has been found that large models learn quickly (data and compute efficient) and are significantly more performant when compared to smaller models [1]; on the other hand, it  becomes prohibitive to train such models on most of the available hardware.", "## Next steps\n\nFSDP can be a powerful tool for training really large models and you have access to more than one GPU or TPU. By sharding the model parameters, optimizer and gradient states, and even offloading them to the CPU when they're inactive, FSDP can reduce the high cost of large-scale training. If you're interested in learning more, the following may be helpful:\n\n* Follow along with the more in-depth Accelerate guide for [FSDP](https://huggingface.co/docs/accelerate/usage_guides/fsdp).\n* Read the [Introducing PyTorch Fully Sharded Data Parallel (FSDP) API](https://pytorch.org/blog/introducing-pytorch-fully-sharded-data-parallel-api/) blog post.\n* Read the [Scaling PyTorch models on Cloud TPUs with FSDP](https://pytorch.org/blog/scaling-pytorch-models-on-cloud-tpus-with-fsdp/) blog post.", "In this post we will look at Data Parallelism using ZeRO and more specifically the latest PyTorch feature **[FullyShardedDataParallel (FSDP)](https://pytorch.org/blog/introducing-pytorch-fully-sharded-data-parallel-api/)**.  **[DeepSpeed](https://github.com/microsoft/deepspeed)** and **[FairScale](https://github.com/facebookresearch/fairscale/)** have implemented the core ideas of the ZERO paper. These have already been integrated in `transformers` Trainer and accompanied by great blog [Fit More and Train Faster With ZeRO via DeepSpeed and FairScale](https://huggingface.co/blog/zero-deepspeed-fairscale) [10]. PyTorch recently upstreamed the Fairscale FSDP into PyTorch Distributed with additional optimizations.\n\n# Accelerate \ud83d\ude80:  Leverage PyTorch FSDP without any code\u00a0changes\n\nWe will look at the task of Causal Language Modelling using GPT-2 Large (762M) and XL (1.5B) model variants.\n\nBelow is the code for pre-training GPT-2 model. It is similar to the official causal language modeling example [here](https://github.com/huggingface/transformers/blob/main/examples/pytorch/language-modeling/run_clm_no_trainer.py) with the addition of 2 arguments `n_train` (2000) and `n_val` (500) to prevent preprocessing/training on entire data in order to perform quick proof of concept benchmarks.\n\n<a href=\"./assets/62_pytorch_fsdp/run_clm_no_trainer.py\" target=\"_parent\">run_clm_no_trainer.py</a>", "[2] [ZeRO: Memory Optimizations Toward Training Trillion Parameter Models](https://arxiv.org/pdf/1910.02054v3.pdf)\n\n[3] [DeepSpeed: Extreme-scale model training for everyone - Microsoft Research](https://www.microsoft.com/en-us/research/blog/deepspeed-extreme-scale-model-training-for-everyone/)\n\n[4] [Megatron-LM: Training Multi-Billion Parameter Language Models Using\nModel Parallelism](https://arxiv.org/pdf/1909.08053.pdf)\n\n[5] [Introducing GPipe, an Open Source Library for Efficiently Training Large-scale Neural Network Models](https://ai.googleblog.com/2019/03/introducing-gpipe-open-source-library.html)\n\n[6] [Which hardware do you need to train a 176B parameters model?](https://bigscience.huggingface.co/blog/which-hardware-to-train-a-176b-parameters-model)\n\n[7] [Introducing PyTorch Fully Sharded Data Parallel (FSDP) API | PyTorch](https://pytorch.org/blog/introducing-pytorch-fully-sharded-data-parallel-api/)\n\n[8] [Getting Started with Fully Sharded Data Parallel(FSDP) \u2014 PyTorch Tutorials 1.11.0+cu102 documentation](https://pytorch.org/tutorials/intermediate/FSDP_tutorial.html)\n\n[9] [Training a 1 Trillion Parameter Model With PyTorch Fully Sharded Data Parallel on AWS | by PyTorch | PyTorch | Mar, 2022 | Medium](https://medium.com/pytorch/training-a-1-trillion-parameter-model-with-pytorch-fully-sharded-data-parallel-on-aws-3ac13aa96cff)", "## Full Finetuning\n\nWe will look at how to do full fine-tuning of `bigcode/starcoder` (15B params) on 8 A100 80GB GPUs using PyTorch Fully Sharded Data Parallel (FSDP) technique. For more information on FSDP, please refer to [Fine-tuning Llama 2 70B using PyTorch FSDP](https://huggingface.co/blog/ram-efficient-pytorch-fsdp) and [Accelerate Large Model Training using PyTorch Fully Sharded Data Parallel](https://huggingface.co/blog/pytorch-fsdp).\n\n**Resources**\n1. Codebase: [link](https://github.com/pacman100/DHS-LLM-Workshop/tree/main/personal_copilot/training). It uses the recently added Flash Attention V2 support in Transformers. \n2. FSDP Config: [fsdp_config.yaml](https://github.com/pacman100/DHS-LLM-Workshop/blob/main/personal_copilot/training/configs/fsdp_config.yaml)\n3. Model: [bigcode/stacoder](https://huggingface.co/bigcode/starcoder)\n4. Dataset: [smangrul/hf-stack-v1](https://huggingface.co/datasets/smangrul/hf-stack-v1)\n5. Fine-tuned Model: [smangrul/peft-lora-starcoder15B-v2-personal-copilot-A100-40GB-colab](https://huggingface.co/smangrul/peft-lora-starcoder15B-v2-personal-copilot-A100-40GB-colab)"], "eval_score_GPT4": "4", "eval_feedback_GPT4": "The response provided is mostly correct and factual. It accurately describes FSDP as a data parallel method used in distributed training to reduce memory usage and improve GPU memory efficiency. It also correctly states that FSDP shards the model parameters, gradients, and optimizer states across GPUs. However, the response is incomplete as it ends abruptly and does not mention the capability of FSDP to handle large pretrained models up to 1T parameters, which is a significant detail included in the reference answer. Therefore, the response is not completely accurate as it omits this important aspect of FSDP's purpose. Based on the score rubric, the response falls into the category of being mostly correct, accurate, and factual, but not complete."}, {"question": "What file format is used to save and store PyTorch model weights more securely than `.bin` files?\n", "true_answer": "`.safetensors`", "source_doc": "huggingface/safetensors/blob/main/docs/source/convert-weights.md", "generated_answer": "The more secure file format for saving and storing PyTorch model weights is `.safetensors`. This format is recommended by Hugging Face for converting weights stored in `.bin` files created using Python's `pickle` utility, which is not secure as it may contain malicious code that can be executed. The Convert Space provided by Hugging Face can be used to convert weights stored on the Hub to `.safetensors` format. Alternatively,", "retrieved_docs": ["Convert weights to safetensors\n\nPyTorch model weights are commonly saved and stored as `.bin` files with Python's [`pickle`](https://docs.python.org/3/library/pickle.html) utility. To save and store your model weights in the more secure `safetensor` format, we recommend converting your weights to `.safetensors`.\n\nThe easiest way to convert your model weights is to use the [Convert Space](https://huggingface.co/spaces/diffusers/convert), given your model weights are already stored on the Hub. The Convert Space downloads the pickled weights, converts them, and opens a Pull Request to upload the newly converted `.safetensors` file to your repository.\n\n<Tip warning={true}>\n\nFor larger models, the Space may be a bit slower because its resources are tied up in converting other models. You can also try running the [convert.py](https://github.com/huggingface/safetensors/blob/main/bindings/python/convert.py) script (this is what the Space is running) locally to convert your weights.\n\nFeel free to ping [@Narsil](https://huggingface.co/Narsil) for any issues with the Space.\n\n</Tip>", "!--Copyright 2023 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\n\nhttp://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License.\n-->\n\n# Load safetensors\n\n[[open-in-colab]]\n\n[safetensors](https://github.com/huggingface/safetensors) is a safe and fast file format for storing and loading tensors. Typically, PyTorch model weights are saved or *pickled* into a `.bin` file with Python's [`pickle`](https://docs.python.org/3/library/pickle.html) utility. However, `pickle` is not secure and pickled files may contain malicious code that can be executed. safetensors is a secure alternative to `pickle`, making it ideal for sharing model weights.\n\nThis guide will show you how you load `.safetensor` files, and how to convert Stable Diffusion model weights stored in other formats to `.safetensor`. Before you start, make sure you have safetensors installed:\n\n```py\n# uncomment to install the necessary libraries in Colab\n#!pip install safetensors\n```\n\nIf you look at the [`runwayml/stable-diffusion-v1-5`](https://huggingface.co/runwayml/stable-diffusion-v1-5/tree/main) repository, you'll see weights inside the `text_encoder`, `unet` and `vae` subfolders are stored in the `.safetensors` format. By default, \ud83e\udd17 Diffusers automatically loads these `.safetensors` files from their subfolders if they're available in the model repository.", "This is why large models on the Hugging Face Hub are not saved and shared with one big file containing all the weights, but **several** of them. If you go to the [BLOOM model page](https://huggingface.co/bigscience/bloom/tree/main) for instance, you will see there is 72 files named `pytorch_model_xxxxx-of-00072.bin`, which each contain part of the model weights. Using this format, we can load one part of the state dict in memory, put the weights inside the model, move them on the right device, then discard this state dict part before going to the next. Instead of requiring to have enough RAM to accommodate the whole model, we only need enough RAM to get the biggest checkpoint part, which we call a **shard**, so 7.19GB in the case of BLOOM.\n\nWe call the checkpoints saved in several files like BLOOM *sharded checkpoints*, and we have standardized their format as such: \n- One file (called `pytorch_model.bin.index.json`) contains some metadata and a map parameter name to file name, indicating where to find each weight\n- All the other files are standard PyTorch state dicts, they just contain a part of the model instead of the whole one. You can have a look at the content of the index file [here](https://huggingface.co/bigscience/bloom/blob/main/pytorch_model.bin.index.json).\n\nTo load such a sharded checkpoint into a model, we just need to loop over the various shards. Accelerate provides a function called `load_checkpoint_in_model` that will do this for you if you have cloned one of the repos of the Hub, or you can directly use the `from_pretrained` method of Transformers, which will handle the downloading and caching for you:\n\n```python\nimport torch\nfrom transformers import AutoModelForCausalLM", "When a model is saved under ZeRO-2, you end up having the normal `pytorch_model.bin` file with the model weights, but\nthey are only the fp16 version of the weights.\n\nUnder ZeRO-3, things are much more complicated, since the model weights are partitioned out over multiple GPUs,\ntherefore `\"stage3_gather_16bit_weights_on_model_save\": true` is required to get the `Trainer` to save the fp16\nversion of the weights. If this setting is `False` `pytorch_model.bin` won't be created. This is because by default DeepSpeed's `state_dict` contains a placeholder and not the real weights. If we were to save this `state_dict` it won't be possible to load it back.\n\n\n```json\n{\n    \"zero_optimization\": {\n        \"stage3_gather_16bit_weights_on_model_save\": true\n    }\n}\n```\n\n**FP32 Weights:**\n\nWhile the fp16 weights are fine for resuming training, if you finished finetuning your model and want to upload it to\nthe [models hub](https://huggingface.co/models) or pass it to someone else you most likely will want to get the fp32\nweights. This ideally shouldn't be done during training since this is a process that requires a lot of memory, and\ntherefore best to be performed offline after the training is complete. But if desired and you have plenty of free CPU\nmemory it can be done in the same training script. The following sections will discuss both approaches.\n\n\n**Live FP32 Weights Recovery:**\n\nThis approach may not work if you model is large and you have little free CPU memory left, at the end of the training.\n\nIf you have saved at least one checkpoint, and you want to use the latest one, you can do the following:\n\n```python\nfrom transformers.trainer_utils import get_last_checkpoint\nfrom deepspeed.utils.zero_to_fp32 import load_state_dict_from_zero_checkpoint", "```py\n>>> import json\n\n>>> with tempfile.TemporaryDirectory() as tmp_dir:\n...     model.save_pretrained(tmp_dir, max_shard_size=\"200MB\")\n...     with open(os.path.join(tmp_dir, \"pytorch_model.bin.index.json\"), \"r\") as f:\n...         index = json.load(f)\n\n>>> print(index.keys())\ndict_keys(['metadata', 'weight_map'])\n```\n\nThe metadata just consists of the total size of the model for now. We plan to add other information in the future:\n\n```py\n>>> index[\"metadata\"]\n{'total_size': 433245184}\n```\n\nThe weights map is the main part of this index, which maps each parameter name (as usually found in a PyTorch model `state_dict`) to the file it's stored in:\n\n```py\n>>> index[\"weight_map\"]\n{'embeddings.LayerNorm.bias': 'pytorch_model-00001-of-00003.bin',\n 'embeddings.LayerNorm.weight': 'pytorch_model-00001-of-00003.bin',\n ...\n```\n\nIf you want to directly load such a sharded checkpoint inside a model without using [`~PreTrainedModel.from_pretrained`] (like you would do `model.load_state_dict()` for a full checkpoint) you should use [`~modeling_utils.load_sharded_checkpoint`]:\n\n```py\n>>> from transformers.modeling_utils import load_sharded_checkpoint", "### Load model weights from TF or Flax\n\nTensorFlow and Flax checkpoints are not affected, and can be loaded within PyTorch architectures using the `from_tf` and `from_flax` kwargs for the `from_pretrained` method to circumvent this issue.\n\nE.g.:\n\n```python\nfrom transformers import AutoModel\n\nmodel = AutoModel.from_pretrained(\"bert-base-cased\", from_flax=True)\n```\n\n### Use your own serialization format\n\n- [MsgPack](https://msgpack.org/index.html)\n- [Protobuf](https://developers.google.com/protocol-buffers)\n- [Cap'n'proto](https://capnproto.org/)\n- [Avro](https://avro.apache.org/)\n- [safetensors](https://github.com/huggingface/safetensors)\n\nThis last format, `safetensors`, is a simple serialization format that we are working on and experimenting with currently! Please help or contribute if you can \ud83d\udd25.\n\n### Improve `torch.load/save`\n\nThere's an open discussion in progress at PyTorch on having a [Safe way of loading only weights from *.pt file by default](https://github.com/pytorch/pytorch/issues/52181) \u2013 please chime in there!\n\n### Hub\u2019s Security Scanner\n\n#### What we have now\n\nWe have created a security scanner that scans every file pushed to the Hub and runs security checks. At the time of writing, it runs two types of scans:\n\n- ClamAV scans\n- Pickle Import scans\n\nFor ClamAV scans, files are run through the open-source antivirus [ClamAV](https://www.clamav.net). While this covers a good amount of dangerous files, it doesn\u2019t cover pickle exploits.", "Now let's use a maximum shard size of 200MB:\n\n```py\n>>> with tempfile.TemporaryDirectory() as tmp_dir:\n...     model.save_pretrained(tmp_dir, max_shard_size=\"200MB\")\n...     print(sorted(os.listdir(tmp_dir)))\n['config.json', 'pytorch_model-00001-of-00003.bin', 'pytorch_model-00002-of-00003.bin', 'pytorch_model-00003-of-00003.bin', 'pytorch_model.bin.index.json']\n```\n\nOn top of the configuration of the model, we see three different weights files, and an `index.json` file which is our index. A checkpoint like this can be fully reloaded using the [`~PreTrainedModel.from_pretrained`] method:\n\n```py\n>>> with tempfile.TemporaryDirectory() as tmp_dir:\n...     model.save_pretrained(tmp_dir, max_shard_size=\"200MB\")\n...     new_model = AutoModel.from_pretrained(tmp_dir)\n```\n\nThe main advantage of doing this for big models is that during step 2 of the workflow shown above, each shard of the checkpoint is loaded after the previous one, capping the memory usage in RAM to the model size plus the size of the biggest shard.\n\nBehind the scenes, the index file is used to determine which keys are in the checkpoint, and where the corresponding weights are stored. We can load that index like any json and get a dictionary:\n\n```py\n>>> import json"], "eval_score_GPT4": "5", "eval_feedback_GPT4": "The response correctly identifies `.safetensors` as the more secure file format for saving and storing PyTorch model weights, which aligns with the reference answer. The additional information provided about the recommendation by Hugging Face and the use of the Convert Space adds to the accuracy and factual nature of the response."}, {"question": "What type of security certification does Hugging Face have?\n", "true_answer": "SOC2 Type 2 certified", "source_doc": "huggingface/hub-docs/blob/main/docs/hub/security.md", "generated_answer": "Hugging Face is SOC2 Type 2 certified, which means they provide security certification to their customers and actively monitor and patch any security weaknesses. This information can be found in the \"Security & Compliance\" section of the Hugging Face documentation. (Source: 0: Security)", "retrieved_docs": ["Security\n\nThe Hugging Face Hub offers several security features to ensure that your code and data are secure. Beyond offering [private repositories](./repositories-settings#private-repositories) for models, datasets, and Spaces, the Hub supports access tokens, commit signatures, and malware scanning.\n\nHugging Face is GDPR compliant. If a contract or specific data storage is something you'll need, we recommend taking a look at our [Expert Acceleration Program](https://huggingface.co/support). Hugging Face can also offer Business Associate Addendums or GDPR data processing agreements through an [Enterprise Plan](https://huggingface.co/pricing). \n\nHugging Face is also [SOC2 Type 2 certified](https://us.aicpa.org/interestareas/frc/assuranceadvisoryservices/aicpasoc2report.html), meaning we provide security certification to our customers and actively monitor and patch any security weaknesses.\n\n<img width=\"150\" src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/hub/security-soc-1.jpg\">\n\nFor any other security questions, please feel free to send us an email at security@huggingface.co.\n\n## Contents\n\n- [User Access Tokens](./security-tokens)\n- [Git over SSH](./security-git-ssh)\n- [Signing commits with GPG](./security-gpg)\n- [Single Sign-On (SSO)](./security-sso)\n- [Malware Scanning](./security-malware)\n- [Pickle Scanning](./security-pickle)\n- [Secrets Scanning](./security-secrets)", "Security & Compliance\n\n\ud83e\udd17 Inference Endpoints is built with security and secure inference at its core. Below you can find an overview of the security measures we have in place.\n\n## Data Security/Privacy\n\nHugging Face does not store any customer data in terms of payloads or tokens that are passed to the Inference Endpoint. We are storing logs for 30 days. Every Inference Endpoints uses TLS/SSL to encrypt the data in transit.\n\nWe also recommend using AWS or Azure Private Link for organizations. This allows you to access your Inference Endpoint through a private connection, without exposing it to the internet.\n\nHugging Face also offers Business Associate Addendum or GDPR data processing agreement through the Inference Endpoint enterprise plan.\n\n## Model Security/Privacy:\n\nYou can set a model repository as private if you do not want to publicly expose it. Hugging Face does not own any model or data you upload to the Hugging Face hub. Hugging Face does provide malware and pickle scans over the contents of the model repository as with all items in the Hub.\n\n## Inference Endpoints and Hub Security\n\nThe Hugging Face Hub, which Inference Endpoints is part, is also SOC2 Type 2 certified. The Hugging Face Hub offers Role Based Access Control. For more on hub security: https://huggingface.co/docs/hub/security\n\n<img width=\"150\" src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/hub/security-soc-1.jpg\">\n\n## Inference Endpoint Security level\n\nWe currently offer three types of endpoints, in order or increasing security level:\n\n- **Public**: A Public Endpoint is available from the internet, secured with TLS/SSL, and requires no authentication.\n- **Protected**: A Protected Endpoint is available from the internet, secured with TLS/SSL, and requires a valid Hugging Face token for authentication.\n- **Private** A Private Endpoint is only available through an intra-region secured AWS or Azure PrivateLink connection. Private Endpoints are not accessible from the internet.", "- **Does taking this course lead to a certification?**\nCurrently we do not have any certification for this course. However, we are working on a certification program for the Hugging Face ecosystem -- stay tuned!\n\n- **How much time should I spend on this course?**\nEach chapter in this course is designed to be completed in 1 week, with approximately 6-8 hours of work per week. However, you can take as much time as you need to complete the course.\n\n- **Where can I ask a question if I have one?**\nIf you have a question about any section of the course, just click on the \"*Ask a question*\" banner at the top of the page to be automatically redirected to the right section of the [Hugging Face forums](https://discuss.huggingface.co/):\n\n<img src=\"https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter1/forum-button.png\" alt=\"Link to the Hugging Face forums\" width=\"75%\">\n\nNote that a list of [project ideas](https://discuss.huggingface.co/c/course/course-event/25) is also available on the forums if you wish to practice more once you have completed the course.\n\n- **Where can I get the code for the course?**\nFor each section, click on the banner at the top of the page to run the code in either Google Colab or Amazon SageMaker Studio Lab:\n\n<img src=\"https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter1/notebook-buttons.png\" alt=\"Link to the Hugging Face course notebooks\" width=\"75%\">", "### Supported Identity Providers\n\nYou can easily integrate Hugging Face Hub with a variety of Identity Providers, such as Okta, OneLogin or Azure Active Directory (Azure AD). Hugging Face Hub can work with any OIDC-compliant or SAML Identity Provider.\n\n## How to configure OIDC/SAML provider in the Hub\n\nWe have some guides available to help with configuring based on your chosen SSO provider, or to take inspiration from:\n\n- [How to configure OIDC with Okta in the Hub](./security-sso-okta-oidc)\n- [How to configure SAML with Okta in the Hub](./security-sso-okta-saml)\n- [How to configure SAML with Azure in the Hub](./security-sso-azure-saml)\n\n### Users Management\n\n<div class=\"flex justify-center\">\n\t<img class=\"block dark:hidden\" src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/hub/sso/sso-settings-users.png\"/>\n\t<img class=\"hidden dark:block\" src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/hub/sso/sso-settings-users-dark.png\"/>\n</div>\n\n#### Session Timeout\n\nThis value sets the duration of the session for members of your organization.\n\nAfter this time, members will be prompted to re-authenticate with your Identity Provider to access the organization's resources.\n\nThe default value is 7 days.\n\n#### Role Mapping\n\nWhen enabled, Role Mapping allows you to dynamically assign [roles](./organizations-security#access-control-in-organizations) to organization members based on data provided by your Identity Provider.\n\nThis section allows you to define a mapping from your IdP's user profile data from your IdP to the assigned role in Hugging Face.", "\ud83c\udf8e Network of peers with whom ambassadors can collaborate. \n\n\ud83e\uddd1\ud83c\udffb\u200d\ud83d\udcbb Workshops and support from the Hugging Face team! \n\n\ud83e\udd17 Insight into the latest projects, features, and more!\n\n\ud83c\udf81 Merchandise and assets. \n\n\u2728 Being officially recognized as a Hugging Face\u2019s Ambassador\n\n<br />\n\n**Eligibility Requirements for Students**\n\n- Validate your student status \n- Have taken at least one machine learning/data science course (online courses are considered as well)\n- Be enrolled in an accredited college or university\n- Be a user of the Hugging Face Hub and/or the Hugging Face\u2019s libraries\n- Acknowledge the [Code of Conduct](https://huggingface2.notion.site/huggingface2/Hugging-Face-Code-of-Conduct-45eeeafa9ef44c5e888a2952619fdfa8). Community is at the center of the Hugging Face ecosystem. Because of that, we strictly adhere to our [Code of conduct](https://huggingface2.notion.site/huggingface2/Hugging-Face-Code-of-Conduct-45eeeafa9ef44c5e888a2952619fdfa8). If any ambassador infringes it or behaves inadequately, they will be excluded from the Program.\n\n**[Apply here](https://docs.google.com/forms/d/e/1FAIpQLScY9kTi-TjZipRFRviluRCwSjFf3CCsMbKedzO1tq2S0wtbNQ/viewform?usp=sf_link) to become an ambassador!**\n\n**Timeline:**\n\n- Deadline for the end of the [application](https://docs.google.com/forms/d/e/1FAIpQLScY9kTi-TjZipRFRviluRCwSjFf3CCsMbKedzO1tq2S0wtbNQ/viewform?usp=sf_link) is June 13.\n- The Program will start on June 30, 2022.\n- The Program will end on December 31, 2022.", "!---\nCopyright 2020 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\");\nyou may not use this file except in compliance with the License.\nYou may obtain a copy of the License at\n\n    http://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software\ndistributed under the License is distributed on an \"AS IS\" BASIS,\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\nSee the License for the specific language governing permissions and\nlimitations under the License.\n-->\n\n<!---\nA useful guide for English-Traditional Japanese translation of Hugging Face documentation\n- Use square quotes, e.g.,\u300c\u5f15\u7528\u300d\n\nDictionary\n\nAPI: API(\u7ffb\u8a33\u3057\u306a\u3044)\nadd: \u8ffd\u52a0\ncheckpoint: \u30c1\u30a7\u30c3\u30af\u30dd\u30a4\u30f3\u30c8\ncode: \u30b3\u30fc\u30c9\ncommunity: \u30b3\u30df\u30e5\u30cb\u30c6\u30a3\nconfidence: \u4fe1\u983c\u5ea6\ndataset: \u30c7\u30fc\u30bf\u30bb\u30c3\u30c8\ndocumentation: \u30c9\u30ad\u30e5\u30e1\u30f3\u30c8\nexample: \u4f8b\nfinetune: \u5fae\u8abf\u6574\nHugging Face: Hugging Face(\u7ffb\u8a33\u3057\u306a\u3044)\nimplementation: \u5b9f\u88c5\ninference: \u63a8\u8ad6\nlibrary: \u30e9\u30a4\u30d6\u30e9\u30ea\nmodule: \u30e2\u30b8\u30e5\u30fc\u30eb\nNLP/Natural Language Processing: NLP\u3068\u8868\u793a\u3055\u308c\u308b\u5834\u5408\u306f\u7ffb\u8a33\u3055\u308c\u305a\u3001Natural Language Processing\u3068\u8868\u793a\u3055\u308c\u308b\u5834\u5408\u306f\u7ffb\u8a33\u3055\u308c\u308b\nonline demos: \u30aa\u30f3\u30e9\u30a4\u30f3\u30c7\u30e2\npipeline: pipeline(\u7ffb\u8a33\u3057\u306a\u3044)\npretrained/pretrain: \u5b66\u7fd2\u6e08\u307f\nPython data structures (e.g., list, set, dict): \u30ea\u30b9\u30c8\u3001\u30bb\u30c3\u30c8\u3001\u30c7\u30a3\u30af\u30b7\u30e7\u30ca\u30ea\u3068\u8a33\u3055\u308c\u3001\u62ec\u5f27\u5185\u306f\u539f\u6587\u82f1\u8a9e\nrepository: repository(\u7ffb\u8a33\u3057\u306a\u3044)\nsummary: \u6982\u8981\ntoken-: token-(\u7ffb\u8a33\u3057\u306a\u3044)\nTrainer: Trainer(\u7ffb\u8a33\u3057\u306a\u3044)\ntransformer: transformer(\u7ffb\u8a33\u3057\u306a\u3044)\ntutorial: \u30c1\u30e5\u30fc\u30c8\u30ea\u30a2\u30eb\nuser: \u30e6\u30fc\u30b6\n-->", "2\ufe0f\u20e3 Sign in and get your authentication token from the Hugging Face website.\n- Create a new token (https://huggingface.co/settings/tokens) **with write role**\n\n<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/notebooks/create-token.jpg\" alt=\"Create HF Token\">\n\n- Copy the token\n- Run the cell below and paste the token\n\nIf you don't want to use Google Colab or a Jupyter Notebook, you need to use this command instead: `huggingface-cli login`\n\n```python\nfrom huggingface_hub import notebook_login\nnotebook_login()\n!git config --global credential.helper store\n```\n\n```python\nfrom sample_factory.enjoy import enjoy\n\nhf_username = \"ThomasSimonini\"  # insert your HuggingFace username here\n\ncfg = parse_vizdoom_cfg(\n    argv=[\n        f\"--env={env}\",\n        \"--num_workers=1\",\n        \"--save_video\",\n        \"--no_render\",\n        \"--max_num_episodes=10\",\n        \"--max_num_frames=100000\",\n        \"--push_to_hub\",\n        f\"--hf_repository={hf_username}/rl_course_vizdoom_health_gathering_supreme\",\n    ],\n    evaluation=True,\n)\nstatus = enjoy(cfg)\n```\n\n## Let's load another model\n\n\n\n\nThis agent's performance was good, but we can do better! Let's download and visualize an agent trained for 10B timesteps from the hub.\n\n```bash\n#download the agent from the hub\npython -m sample_factory.huggingface.load_from_hub -r edbeeching/doom_health_gathering_supreme_2222 -d ./train_dir\n```"], "eval_score_GPT4": "5", "eval_feedback_GPT4": "The response correctly identifies Hugging Face as having SOC2 Type 2 certification, which aligns with the reference answer. The additional information provided about the certification's implications for security and where to find this information in the Hugging Face documentation is accurate and enhances the factual content of the response."}, {"question": "What do RAG models combine to generate outputs?\n", "true_answer": "Pretrained dense retrieval (DPR) and sequence-to-sequence models.", "source_doc": "huggingface/transformers/blob/main/docs/source/en/model_doc/rag.md", "generated_answer": "RAG models combine a question encoder and a generator to generate outputs. During a forward pass, the input is encoded with the question encoder and passed to the retriever to extract relevant context documents. These documents are then prepended to the input and passed to the generator to generate the output. This process allows RAG models to make use of multiple sources of knowledge, including the information contained in the contextual passages, to outperform other state-of-the-art models", "retrieved_docs": ["## RagConfig\n\n[[autodoc]] RagConfig\n\n## RagTokenizer\n\n[[autodoc]] RagTokenizer\n\n## Rag specific outputs\n\n[[autodoc]] models.rag.modeling_rag.RetrievAugLMMarginOutput\n\n[[autodoc]] models.rag.modeling_rag.RetrievAugLMOutput\n\n## RagRetriever\n\n[[autodoc]] RagRetriever\n\n<frameworkcontent>\n<pt>\n\n## RagModel\n\n[[autodoc]] RagModel\n    - forward\n\n## RagSequenceForGeneration\n\n[[autodoc]] RagSequenceForGeneration\n    - forward\n    - generate\n\n## RagTokenForGeneration\n\n[[autodoc]] RagTokenForGeneration\n    - forward\n    - generate\n\n</pt>\n<tf>\n\n## TFRagModel\n\n[[autodoc]] TFRagModel\n    - call\n\n## TFRagSequenceForGeneration\n\n[[autodoc]] TFRagSequenceForGeneration\n    - call\n    - generate\n\n## TFRagTokenForGeneration\n\n[[autodoc]] TFRagTokenForGeneration\n    - call\n    - generate\n\n</tf>\n</frameworkcontent>", "Intro\n\nAuthors: @patrickvonplaten and @lhoestq\n\nAimed at tackling the knowledge-intensive NLP tasks (think tasks a human wouldn't be expected to solve without access to external knowledge sources), RAG models are seq2seq models with access to a retrieval mechanism providing relevant context documents at training and evaluation time.\n\nA RAG model encapsulates two core components: a question encoder and a generator.\nDuring a forward pass, we encode the input with the question encoder and pass it\nto the retriever to extract relevant context documents. The documents are then prepended to the input.\nSuch contextualized inputs are passed to the generator.\n\nRead more about RAG  at https://arxiv.org/abs/2005.11401.\n\n# Note\n\n\u26a0\ufe0f This project should be run with pytorch-lightning==1.3.1 which has a potential security vulnerability\n\n# Finetuning\n\nOur finetuning logic is based on scripts from [`examples/legacy/seq2seq`](https://github.com/huggingface/transformers/tree/main/examples/legacy/seq2seq). We accept training data in the same format as specified there - we expect a directory consisting of 6 text files:\n```bash\ntrain.source\ntrain.target\nval.source\nval.target\ntest.source\ntest.target\n```\n\nA sample finetuning command (run ` ./examples/research_projects/rag/finetune_rag.py --help` to list all available options):", "Recently, [Huggingface](https://huggingface.co/) partnered with [Facebook AI](https://ai.facebook.com/) to introduce the [RAG](https://twitter.com/huggingface/status/1310597560906780680) model as part of its Transformers library. \n\n\n[RAG](https://ai.facebook.com/blog/retrieval-augmented-generation-streamlining-the-creation-of-intelligent-natural-language-processing-models/) acts just like any other [seq2seq model](https://blog.keras.io/a-ten-minute-introduction-to-sequence-to-sequence-learning-in-keras.html). However, [RAG](https://ai.facebook.com/blog/retrieval-augmented-generation-streamlining-the-creation-of-intelligent-natural-language-processing-models/) has an intermediate component that retrieves contextual documents from an external knowledge base (like a Wikipedia text corpus). These documents are then used in conjunction with the input sequence and passed into the underlying seq2seq [generator](https://huggingface.co/blog/how-to-generate).\n\n\nThis information retrieval step allows [RAG](https://ai.facebook.com/blog/retrieval-augmented-generation-streamlining-the-creation-of-intelligent-natural-language-processing-models/) to make use of multiple sources of knowledge -- those that are baked into the model parameters and the information that is contained in the contextual passages, allowing it to outperform other state-of-the-art models in tasks like question answering. You can try it for yourself using this [demo provided by Huggingface](https://huggingface.co/rag/)!", "A sample finetuning command (run ` ./examples/research_projects/rag/finetune_rag.py --help` to list all available options):\n\n```bash\npython examples/research_projects/rag/finetune_rag.py \\\n    --data_dir $DATA_DIR \\\n    --output_dir $OUTPUT_DIR \\\n    --model_name_or_path $MODEL_NAME_OR_PATH \\\n    --model_type rag_sequence \\\n    --fp16 \\\n    --gpus 8\n```\nWe publish two `base` models which can serve as a starting point for finetuning on downstream tasks (use them as `model_name_or_path`):\n- [`facebook/rag-sequence-base`](https://huggingface.co/facebook/rag-sequence-base) - a base for finetuning `RagSequenceForGeneration` models,\n- [`facebook/rag-token-base`](https://huggingface.co/facebook/rag-token-base) - a base for finetuning `RagTokenForGeneration` models.\n\nThe `base` models initialize the question encoder with [`facebook/dpr-question_encoder-single-nq-base`](https://huggingface.co/facebook/dpr-question_encoder-single-nq-base) and the generator with [`facebook/bart-large`](https://huggingface.co/facebook/bart-large).", "python examples/rag/finetune_rag.py \\\n    --data_dir $DATA_DIR \\\n    --output_dir $OUTPUT_DIR \\\n    --model_name_or_path $MODEL_NAME_OR_PATH \\\n    --model_type rag_sequence \\\n    --fp16 \\\n    --gpus 8 \\\n    --profile \\\n    --do_train \\\n    --do_predict \\\n    --n_val -1 \\\n    --train_batch_size 8 \\\n    --eval_batch_size 1 \\\n    --max_source_length 128 \\\n    --max_target_length 25 \\\n    --val_max_target_length 25 \\\n    --test_max_target_length 25 \\\n    --label_smoothing 0.1 \\\n    --dropout 0.1 \\\n    --attention_dropout 0.1 \\\n    --weight_decay 0.001 \\\n    --adam_epsilon 1e-08 \\\n    --max_grad_norm 0.1 \\\n    --lr_scheduler polynomial \\\n    --learning_rate 3e-05 \\\n    --num_train_epochs 100 \\\n    --warmup_steps 500 \\\n    --gradient_accumulation_steps 1 \\\n    --distributed_retriever ray \\\n    --num_retrieval_workers 4\n\n# Stop the Ray cluster.\nray stop\n```\n\n## What\u2019s next?\n\nUsing RAG with [Huggingface transformers](https://github.com/huggingface/transformers/tree/master/examples/research_projects/rag) and the [Ray retrieval implementation](https://github.com/huggingface/transformers/blob/master/examples/research_projects/rag/finetune_rag_ray.sh) for faster distributed fine-tuning, you can leverage RAG for retrieval-based generation on your own knowledge-intensive tasks.", "```\n## End-to-end evaluation\n\nWe support two formats of the gold data file (controlled by the `gold_data_mode` parameter):\n- `qa` - where a single line has the following format: `input [tab] output_list`, e.g.:\n```\nwho is the owner of reading football club\t['Xiu Li Dai', 'Dai Yongge', 'Dai Xiuli', 'Yongge Dai']\n```\n- `ans` - where a single line contains a single expected answer, e.g.:\n```\nXiu Li Dai\n```\n\nPredictions of the model for the samples from the `evaluation_set` will be saved under the path specified by the `predictions_path` parameter.\nIf this path already exists, the script will use saved predictions to calculate metrics.\nAdd `--recalculate` parameter to force the script to perform inference from scratch.\n\nAn example e2e evaluation run could look as follows:\n```bash\npython examples/research_projects/rag/eval_rag.py \\\n    --model_name_or_path facebook/rag-sequence-nq \\\n    --model_type rag_sequence \\\n    --evaluation_set path/to/test.source \\\n    --gold_data_path path/to/gold_data \\\n    --predictions_path path/to/e2e_preds.txt \\\n    --eval_mode e2e \\\n    --gold_data_mode qa \\\n    --n_docs 5 \\ # You can experiment with retrieving different number of documents at evaluation time\n    --print_predictions \\\n    --recalculate \\ # adding this parameter will force recalculating predictions even if predictions_path already exists\n```\n\n# Use your own knowledge source\n\nBy default, RAG uses the English Wikipedia as a knowledge source, known as the 'wiki_dpr' dataset.\nWith `use_custom_knowledge_dataset.py` you can build your own knowledge source, *e.g.* for RAG.", "Also, hyperparameter tuning is another aspect of transformer fine tuning and can have [huge impacts on accuracy](https://medium.com/distributed-computing-with-ray/hyperparameter-optimization-for-transformers-a-guide-c4e32c6c989b). For scalable and easy hyperparameter tuning, check out the [Ray Tune](https://docs.ray.io/en/latest/tune/) library. By using [Ray Tune\u2019s integration with PyTorch Lightning](https://medium.com/distributed-computing-with-ray/scaling-up-pytorch-lightning-hyperparameter-tuning-with-ray-tune-4bd9e1ff9929), or the [built-in integration with Huggingface transformers](https://huggingface.co/blog/ray-tune), you can run experiments to find the perfect hyperparameters for your RAG model.\n\nAnd lastly, stay tuned for a potential Tensorflow implementation of [RAG](https://ai.facebook.com/blog/retrieval-augmented-generation-streamlining-the-creation-of-intelligent-natural-language-processing-models) on [Huggingface](https://huggingface.co/)!\n\nIf you plan to try RAG+Ray integration out, please feel free to share your experiences on the [Ray Discourse](https://discuss.ray.io/) or join the [Ray community Slack](https://docs.google.com/forms/d/e/1FAIpQLSfAcoiLCHOguOm8e7Jnn-JJdZaCxPGjgVCvFijHB5PLaQLeig/viewform) for further discussion -- we\u2019d love to hear from you!\n\n\n> Also published at https://medium.com/distributed-computing-with-ray/retrieval-augmented-generation-with-huggingface-transformers-and-ray-b09b56161b1e"], "eval_score_GPT4": "4", "eval_feedback_GPT4": "The response correctly identifies that RAG models combine a question encoder and a generator to produce outputs. It also accurately describes the process by which the input is encoded and then used to retrieve relevant documents that are subsequently passed to the generator. However, the response does not specifically mention \"pretrained dense retrieval (DPR)\" as part of the combination, which is a key component of RAG models as indicated in the reference answer. The mention of \"retriever\" could be interpreted as referring to DPR, but it is not explicitly stated. Therefore, the response is mostly correct but lacks the specificity of the reference answer."}, {"question": "What library does MarkupLMFeatureExtractor use to extract data from HTML and XML files?\n", "true_answer": "Beautiful Soup", "source_doc": "huggingface/transformers/blob/main/docs/source/en/model_doc/markuplm.md", "generated_answer": "MarkupLMFeatureExtractor uses the Python library Beautiful Soup (https://www.crummy.com/software/BeautifulSoup/) to extract data from HTML and XML files. This information can be found in the context provided. The answer is not explicitly stated in the question, but it can be inferred from the description of the MarkupLMProcessor and the usage guide provided. The context also mentions that one can still use their own parsing solution of choice and", "retrieved_docs": ["[`MarkupLMFeatureExtractor`] uses [Beautiful Soup](https://www.crummy.com/software/BeautifulSoup/bs4/doc/), a Python library for\npulling data out of HTML and XML files, under the hood. Note that you can still use your own parsing solution of\nchoice, and provide the nodes and xpaths yourself to [`MarkupLMTokenizer`] or [`MarkupLMTokenizerFast`].\n\nIn total, there are 5 use cases that are supported by the processor. Below, we list them all. Note that each of these\nuse cases work for both batched and non-batched inputs (we illustrate them for non-batched inputs).\n\n**Use case 1: web page classification (training, inference) + token classification (inference), parse_html = True**\n\nThis is the simplest case, in which the processor will use the feature extractor to get all nodes and xpaths from the HTML.\n\n```python\n>>> from transformers import MarkupLMProcessor\n\n>>> processor = MarkupLMProcessor.from_pretrained(\"microsoft/markuplm-base\")\n\n>>> html_string = \"\"\"\n...  <!DOCTYPE html>\n...  <html>\n...  <head>\n...  <title>Hello world</title>\n...  </head>\n...  <body>\n...  <h1>Welcome</h1>\n...  <p>Here is my website.</p>\n...  </body>\n...  </html>\"\"\"\n\n>>> # note that you can also add provide all tokenizer parameters here such as padding, truncation\n>>> encoding = processor(html_string, return_tensors=\"pt\")\n>>> print(encoding.keys())\ndict_keys(['input_ids', 'token_type_ids', 'attention_mask', 'xpath_tags_seq', 'xpath_subs_seq'])\n```", "## Usage: MarkupLMProcessor\n\nThe easiest way to prepare data for the model is to use [`MarkupLMProcessor`], which internally combines a feature extractor\n([`MarkupLMFeatureExtractor`]) and a tokenizer ([`MarkupLMTokenizer`] or [`MarkupLMTokenizerFast`]). The feature extractor is\nused to extract all nodes and xpaths from the HTML strings, which are then provided to the tokenizer, which turns them into the\ntoken-level inputs of the model (`input_ids` etc.). Note that you can still use the feature extractor and tokenizer separately,\nif you only want to handle one of the two tasks.\n\n```python\nfrom transformers import MarkupLMFeatureExtractor, MarkupLMTokenizerFast, MarkupLMProcessor\n\nfeature_extractor = MarkupLMFeatureExtractor()\ntokenizer = MarkupLMTokenizerFast.from_pretrained(\"microsoft/markuplm-base\")\nprocessor = MarkupLMProcessor(feature_extractor, tokenizer)\n```\n\nIn short, one can provide HTML strings (and possibly additional data) to [`MarkupLMProcessor`],\nand it will create the inputs expected by the model. Internally, the processor first uses\n[`MarkupLMFeatureExtractor`] to get a list of nodes and corresponding xpaths. The nodes and\nxpaths are then provided to [`MarkupLMTokenizer`] or [`MarkupLMTokenizerFast`], which converts them\nto token-level `input_ids`, `attention_mask`, `token_type_ids`, `xpath_subs_seq`, `xpath_tags_seq`.\nOptionally, one can provide node labels to the processor, which are turned into token-level `labels`.", "**Use case 2: web page classification (training, inference) + token classification (inference), parse_html=False**\n\nIn case one already has obtained all nodes and xpaths, one doesn't need the feature extractor. In that case, one should\nprovide the nodes and corresponding xpaths themselves to the processor, and make sure to set `parse_html` to `False`.\n\n```python\n>>> from transformers import MarkupLMProcessor\n\n>>> processor = MarkupLMProcessor.from_pretrained(\"microsoft/markuplm-base\")\n>>> processor.parse_html = False\n\n>>> nodes = [\"hello\", \"world\", \"how\", \"are\"]\n>>> xpaths = [\"/html/body/div/li[1]/div/span\", \"/html/body/div/li[1]/div/span\", \"html/body\", \"html/body/div\"]\n>>> encoding = processor(nodes=nodes, xpaths=xpaths, return_tensors=\"pt\")\n>>> print(encoding.keys())\ndict_keys(['input_ids', 'token_type_ids', 'attention_mask', 'xpath_tags_seq', 'xpath_subs_seq'])\n```\n\n**Use case 3: token classification (training), parse_html=False**\n\nFor token classification tasks (such as [SWDE](https://paperswithcode.com/dataset/swde)), one can also provide the\ncorresponding node labels in order to train a model. The processor will then convert these into token-level `labels`.\nBy default, it will only label the first wordpiece of a word, and label the remaining wordpieces with -100, which is the\n`ignore_index` of PyTorch's CrossEntropyLoss. In case you want all wordpieces of a word to be labeled, you can\ninitialize the tokenizer with `only_label_first_subword` set to `False`.", ">>> question = \"What's his name?\"\n>>> encoding = processor(html_string, questions=question, return_tensors=\"pt\")\n>>> print(encoding.keys())\ndict_keys(['input_ids', 'token_type_ids', 'attention_mask', 'xpath_tags_seq', 'xpath_subs_seq'])\n```\n\n**Use case 5: web page question answering (inference), parse_html=False**\n\nFor question answering tasks (such as WebSRC), you can provide a question to the processor. If you have extracted\nall nodes and xpaths yourself, you can provide them directly to the processor. Make sure to set `parse_html` to `False`.\n\n```python\n>>> from transformers import MarkupLMProcessor\n\n>>> processor = MarkupLMProcessor.from_pretrained(\"microsoft/markuplm-base\")\n>>> processor.parse_html = False\n\n>>> nodes = [\"hello\", \"world\", \"how\", \"are\"]\n>>> xpaths = [\"/html/body/div/li[1]/div/span\", \"/html/body/div/li[1]/div/span\", \"html/body\", \"html/body/div\"]\n>>> question = \"What's his name?\"\n>>> encoding = processor(nodes=nodes, xpaths=xpaths, questions=question, return_tensors=\"pt\")\n>>> print(encoding.keys())\ndict_keys(['input_ids', 'token_type_ids', 'attention_mask', 'xpath_tags_seq', 'xpath_subs_seq'])\n```\n\n## Resources", "!--Copyright 2020 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\n\nhttp://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License.\n\n\u26a0\ufe0f Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\nrendered properly in your Markdown viewer.\n\n-->\n\n# Use tokenizers from \ud83e\udd17 Tokenizers\n\nThe [`PreTrainedTokenizerFast`] depends on the [\ud83e\udd17 Tokenizers](https://huggingface.co/docs/tokenizers) library. The tokenizers obtained from the \ud83e\udd17 Tokenizers library can be\nloaded very simply into \ud83e\udd17 Transformers.\n\nBefore getting in the specifics, let's first start by creating a dummy tokenizer in a few lines:\n\n```python\n>>> from tokenizers import Tokenizer\n>>> from tokenizers.models import BPE\n>>> from tokenizers.trainers import BpeTrainer\n>>> from tokenizers.pre_tokenizers import Whitespace\n\n>>> tokenizer = Tokenizer(BPE(unk_token=\"[UNK]\"))\n>>> trainer = BpeTrainer(special_tokens=[\"[UNK]\", \"[CLS]\", \"[SEP]\", \"[PAD]\", \"[MASK]\"])\n\n>>> tokenizer.pre_tokenizer = Whitespace()\n>>> files = [...]\n>>> tokenizer.train(files, trainer)\n```\n\nWe now have a tokenizer trained on the files we defined. We can either continue using it in that runtime, or save it to\na JSON file for future re-use.", "The abstract from the paper is the following:\n\n*Multimodal pre-training with text, layout, and image has made significant progress for Visually-rich Document\nUnderstanding (VrDU), especially the fixed-layout documents such as scanned document images. While, there are still a\nlarge number of digital documents where the layout information is not fixed and needs to be interactively and\ndynamically rendered for visualization, making existing layout-based pre-training approaches not easy to apply. In this\npaper, we propose MarkupLM for document understanding tasks with markup languages as the backbone such as\nHTML/XML-based documents, where text and markup information is jointly pre-trained. Experiment results show that the\npre-trained MarkupLM significantly outperforms the existing strong baseline models on several document understanding\ntasks. The pre-trained model and code will be publicly available.*\n\nThis model was contributed by [nielsr](https://huggingface.co/nielsr). The original code can be found [here](https://github.com/microsoft/unilm/tree/master/markuplm).\n\n## Usage tips\n\n- In addition to `input_ids`, [`~MarkupLMModel.forward`] expects 2 additional inputs, namely `xpath_tags_seq` and `xpath_subs_seq`.\nThese are the XPATH tags and subscripts respectively for each token in the input sequence.\n- One can use [`MarkupLMProcessor`] to prepare all data for the model. Refer to the [usage guide](#usage-markuplmprocessor) for more info.\n\n<img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/model_doc/markuplm_architecture.jpg\"\nalt=\"drawing\" width=\"600\"/> \n\n<small> MarkupLM architecture. Taken from the <a href=\"https://arxiv.org/abs/2110.08518\">original paper.</a> </small>\n\n## Usage: MarkupLMProcessor", "## Resources\n\n- [Demo notebooks](https://github.com/NielsRogge/Transformers-Tutorials/tree/master/MarkupLM)\n- [Text classification task guide](../tasks/sequence_classification)\n- [Token classification task guide](../tasks/token_classification)\n- [Question answering task guide](../tasks/question_answering)\n\n## MarkupLMConfig\n\n[[autodoc]] MarkupLMConfig\n    - all\n\n## MarkupLMFeatureExtractor\n\n[[autodoc]] MarkupLMFeatureExtractor\n    - __call__\n\n## MarkupLMTokenizer\n\n[[autodoc]] MarkupLMTokenizer\n    - build_inputs_with_special_tokens\n    - get_special_tokens_mask\n    - create_token_type_ids_from_sequences\n    - save_vocabulary\n\n## MarkupLMTokenizerFast\n\n[[autodoc]] MarkupLMTokenizerFast\n    - all\n\n## MarkupLMProcessor\n\n[[autodoc]] MarkupLMProcessor\n    - __call__\n\n## MarkupLMModel\n\n[[autodoc]] MarkupLMModel\n    - forward\n\n## MarkupLMForSequenceClassification\n\n[[autodoc]] MarkupLMForSequenceClassification\n    - forward\n\n## MarkupLMForTokenClassification\n\n[[autodoc]] MarkupLMForTokenClassification\n    - forward\n\n## MarkupLMForQuestionAnswering\n\n[[autodoc]] MarkupLMForQuestionAnswering\n    - forward"], "eval_score_GPT4": "5", "eval_feedback_GPT4": "The response correctly identifies Beautiful Soup as the library used by MarkupLMFeatureExtractor to extract data from HTML and XML files, which aligns with the reference answer. The additional information provided in the response does not detract from the accuracy of the answer to the question asked. Therefore, the response meets the criteria for a score of 5 as it is completely correct, accurate, and factual."}, {"question": "What is the file size limit for syncing to HF Spaces without using Git-LFS?\n", "true_answer": "10MB", "source_doc": "huggingface/hub-docs/blob/main/docs/hub/spaces-github-actions.md", "generated_answer": "The file size limit for syncing to HF Spaces without using Git-LFS is not explicitly stated in the provided context. However, it is mentioned that for files larger than 10MB, Spaces requires Git-LFS. If you don't want to use Git-LFS, you may need to review your files and check your history using a tool like BFG Repo-Cleaner to remove any large files from your history. Therefore, it can be", "retrieved_docs": ["Managing Spaces with Github Actions\n\nYou can keep your app in sync with your GitHub repository with **Github Actions**. Remember that for files larger than 10MB, Spaces requires Git-LFS. If you don't want to use Git-LFS, you may need to review your files and check your history. Use a tool like [BFG Repo-Cleaner](https://rtyley.github.io/bfg-repo-cleaner/) to remove any large files from your history. BFG Repo-Cleaner will keep a local copy of your repository as a backup.\n\nFirst, you should set up your GitHub repository and Spaces app together. Add your Spaces app as an additional remote to your existing Git repository.\n\n```bash\ngit remote add space https://huggingface.co/spaces/HF_USERNAME/SPACE_NAME\n```\n\nThen force push to sync everything for the first time:\n\n```bash\ngit push --force space main\n```\n\nNext, set up a GitHub Action to push your main branch to Spaces. In the example below:\n\n* Replace `HF_USERNAME` with your username and `SPACE_NAME` with your Space name. \n* Create a [Github secret](https://docs.github.com/en/actions/security-guides/encrypted-secrets#creating-encrypted-secrets-for-an-environment) with your `HF_TOKEN`. You can find your Hugging Face API token under **API Tokens** on your Hugging Face profile.\n\n```yaml\nname: Sync to Hugging Face hub\non:\n  push:\n    branches: [main]\n\n  # to run this workflow manually from the Actions tab\n  workflow_dispatch:", "```yaml\nname: Sync to Hugging Face hub\non:\n  push:\n    branches: [main]\n\n  # to run this workflow manually from the Actions tab\n  workflow_dispatch:\n\njobs:\n  sync-to-hub:\n    runs-on: ubuntu-latest\n    steps:\n      - uses: actions/checkout@v3\n        with:\n          fetch-depth: 0\n          lfs: true\n      - name: Push to hub\n        env:\n          HF_TOKEN: ${{ secrets.HF_TOKEN }}\n        run: git push https://HF_USERNAME:$HF_TOKEN@huggingface.co/spaces/HF_USERNAME/SPACE_NAME main\n```\n\nFinally, create an Action that automatically checks the file size of any new pull request:\n\n\n```yaml\nname: Check file size\non:               # or directly `on: [push]` to run the action on every push on any branch\n  pull_request:\n    branches: [main]\n\n  # to run this workflow manually from the Actions tab\n  workflow_dispatch:\n\njobs:\n  sync-to-hub:\n    runs-on: ubuntu-latest\n    steps:\n      - name: Check large files\n        uses: ActionsDesk/lfs-warning@v2.0\n        with:\n          filesizelimit: 10485760 # this is 10MB so we can sync to HF Spaces\n```", "Managing Spaces with CircleCI Workflows\n\nYou can keep your app in sync with your GitHub repository with a **CircleCI workflow**. \n\n[CircleCI](https://circleci.com) is a continuous integration and continuous delivery (CI/CD) platform that helps automate the software development process. A [CircleCI workflow](https://circleci.com/docs/workflows/) is a set of automated tasks defined in a configuration file, orchestrated by CircleCI, to streamline the process of building, testing, and deploying software applications.\n\n*Note: For files larger than 10MB, Spaces requires Git-LFS. If you don't want to use Git-LFS, you may need to review your files and check your history. Use a tool like [BFG Repo-Cleaner](https://rtyley.github.io/bfg-repo-cleaner/) to remove any large files from your history. BFG Repo-Cleaner will keep a local copy of your repository as a backup.*\n\nFirst, set up your GitHub repository and Spaces app together. Add your Spaces app as an additional remote to your existing Git repository.\n\n```bash\ngit remote add space https://huggingface.co/spaces/HF_USERNAME/SPACE_NAME\n```\n\nThen force push to sync everything for the first time:\n\n```bash\ngit push --force space main\n```\n\nNext, set up a [CircleCI workflow](https://circleci.com/docs/workflows/) to push your `main` git branch to Spaces. \n\nIn the example below:", "Under the hood, Spaces stores your code inside a git repository, just like the model and dataset repositories. Thanks to this, the same tools we use for all the [other repositories on the Hub](./repositories) (`git` and `git-lfs`) also work for Spaces. Follow the same flow as in [Getting Started with Repositories](./repositories-getting-started) to add files to your Space. Each time a new commit is pushed, the Space will automatically rebuild and restart.\n\nFor step-by-step tutorials to creating your first Space, see the guides below:\n* [Creating a Gradio Space](./spaces-sdks-gradio)\n* [Creating a Streamlit Space](./spaces-sdks-streamlit)\n* [Creating a Docker Space](./spaces-sdks-docker-first-demo)\n\n## Hardware resources\n\nEach Spaces environment is limited to 16GB RAM, 2 CPU cores and 50GB of (not persistent) disk space by default, which you can use free of charge. You can upgrade to better hardware, including a variety of GPU accelerators and persistent storage, for a [competitive price](https://huggingface.co/pricing#spaces). To request an upgrade, please click the _Settings_ button in your Space and select your preferred hardware environment.", "## Step 10: Enable Git-LFS for Large File Storage\n\nNavigate to your repository. Use the following commands to track large build files.\n\n```\ngit lfs install\ngit lfs track Build/* \n```\n\n## Step 11: Push your Changes\n\nFinally, use the following Git commands to push your changes:\n\n```\ngit add .\ngit commit -m \"Add Unity WebGL build files\"\ngit push\n```\n\n## Done!\n\nCongratulations! Refresh your Space. You should now be able to play your game in a Hugging Face Space.\n\nWe hope you found this tutorial helpful. If you have any questions or would like to get more involved in using Hugging Face for Games, join the [Hugging Face Discord](https://hf.co/join/discord)!", "## Adding files to a repository (terminal)[[terminal]]\n\n### Cloning repositories\n\nDownloading repositories to your local machine is called *cloning*. You can use the following commands to load your repo and navigate to it:\n\n```bash\ngit clone https://huggingface.co/<your-username>/<your-model-name>\ncd <your-model-name>\n```\n\nYou can clone over SSH with the following command:\n```bash\ngit clone git@hf.co:<your-username>/<your-model-name>\ncd <your-model-name>\n```\n\nYou'll need to add your SSH public key to [your user settings](https://huggingface.co/settings/keys) to push changes or access private repositories.\n\n\n### Set up\n\nNow's the time, you can add any files you want to the repository! \ud83d\udd25\n\nDo you have files larger than 10MB? Those files should be tracked with `git-lfs`, which you can initialize with:\n\n```bash\ngit lfs install\n```\n\nNote that if your files are larger than **5GB** you'll also need to run:\n\n```bash\nhuggingface-cli lfs-enable-largefiles .\n```\n\nWhen you use Hugging Face to create a repository, Hugging Face automatically provides a list of common file extensions for common Machine Learning large files in the `.gitattributes` file, which `git-lfs` uses to efficiently track changes to your large files. However, you might need to add new extensions if your file types are not already handled. You can do so with `git lfs track \"*.your_extension\"`.\n\n### Pushing files", "### Pushing files\n\nYou can use Git to save new files and any changes to already existing files as a bundle of changes called a *commit*, which can be thought of as a \"revision\" to your project. To create a commit, you have to `add` the files to let Git know that we're planning on saving the changes and then `commit` those changes. In order to sync the new commit with the Hugging Face Hub, you then `push` the commit to the Hub.\n\n```bash\n# Create any files you like! Then...\ngit add .\ngit commit -m \"First model version\"  # You can choose any descriptive message\ngit push\n```\n\nAnd you're done! You can check your repository on Hugging Face with all the recently added files. For example, in the screenshot below the user added a number of files. Note that some files in this example have a size of `1.04 GB`, so the repo uses Git LFS to track it.\n\n<div class=\"flex justify-center\">\n<img class=\"block dark:hidden\" src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/hub/repo_with_files.png\"/>\n<img class=\"hidden dark:block\" src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/hub/repo_with_files-dark.png\"/>\n</div>\n\n\n## Viewing a repo's history\n\nEvery time you go through the `add`-`commit`-`push` cycle, the repo will keep track of every change you've made to your files. The UI allows you to explore the model files and commits and to see the difference (also known as *diff*) introduced by each commit. To see the history, you can click on the **History: X commits** link."], "eval_score_GPT4": "4", "eval_feedback_GPT4": "The response correctly identifies the file size limit for syncing to HF Spaces without using Git-LFS as being related to the 10MB threshold. The respondent has correctly inferred that files larger than 10MB require Git-LFS, which implies that the limit for syncing without Git-LFS is 10MB. Although the response could be more direct, it provides the necessary information that aligns with the reference answer."}, {"question": "What is the title of the paper introducing the ByT5 model?\n", "true_answer": "ByT5: Towards a token-free future with pre-trained byte-to-byte models", "source_doc": "huggingface/transformers/blob/main/docs/source/en/model_doc/byt5.md", "generated_answer": "The title of the paper introducing the ByT5 model is \"ByT5: Towards a token-free future with pre-trained byte-to-byte models\" (arXiv:2105.13626).", "retrieved_docs": ["1. **[ByT5](https://huggingface.co/docs/transformers/model_doc/byt5)** (from Google Research) released with the paper [ByT5: Towards a token-free future with pre-trained byte-to-byte models](https://arxiv.org/abs/2105.13626) by Linting Xue, Aditya Barua, Noah Constant, Rami Al-Rfou, Sharan Narang, Mihir Kale, Adam Roberts, Colin Raffel.\n1. **[CamemBERT](https://huggingface.co/docs/transformers/model_doc/camembert)** (from Inria/Facebook/Sorbonne) released with the paper [CamemBERT: a Tasty French Language Model](https://arxiv.org/abs/1911.03894) by Louis Martin*, Benjamin Muller*, Pedro Javier Ortiz Su\u00e1rez*, Yoann Dupont, Laurent Romary, \u00c9ric Villemonte de la Clergerie, Djam\u00e9 Seddah and Beno\u00eet Sagot.\n1. **[CANINE](https://huggingface.co/docs/transformers/model_doc/canine)** (from Google Research) released with the paper [CANINE: Pre-training an Efficient Tokenization-Free Encoder for Language Representation](https://arxiv.org/abs/2103.06874) by Jonathan H. Clark, Dan Garrette, Iulia Turc, John Wieting.\n1. **[Chinese-CLIP](https://huggingface.co/docs/transformers/model_doc/chinese_clip)** (from OFA-Sys) released with the paper [Chinese CLIP: Contrastive Vision-Language Pretraining in Chinese](https://arxiv.org/abs/2211.01335) by An Yang, Junshu Pan, Junyang Lin, Rui Men, Yichang Zhang, Jingren Zhou, Chang Zhou.", "1. **[ByT5](https://huggingface.co/docs/transformers/model_doc/byt5)** (from Google Research) released with the paper [ByT5: Towards a token-free future with pre-trained byte-to-byte models](https://arxiv.org/abs/2105.13626) by Linting Xue, Aditya Barua, Noah Constant, Rami Al-Rfou, Sharan Narang, Mihir Kale, Adam Roberts, Colin Raffel.\n1. **[CamemBERT](https://huggingface.co/docs/transformers/model_doc/camembert)** (from Inria/Facebook/Sorbonne) released with the paper [CamemBERT: a Tasty French Language Model](https://arxiv.org/abs/1911.03894) by Louis Martin*, Benjamin Muller*, Pedro Javier Ortiz Su\u00e1rez*, Yoann Dupont, Laurent Romary, \u00c9ric Villemonte de la Clergerie, Djam\u00e9 Seddah and Beno\u00eet Sagot.\n1. **[CANINE](https://huggingface.co/docs/transformers/model_doc/canine)** (from Google Research) released with the paper [CANINE: Pre-training an Efficient Tokenization-Free Encoder for Language Representation](https://arxiv.org/abs/2103.06874) by Jonathan H. Clark, Dan Garrette, Iulia Turc, John Wieting.\n1. **[Chinese-CLIP](https://huggingface.co/docs/transformers/model_doc/chinese_clip)** (from OFA-Sys) released with the paper [Chinese CLIP: Contrastive Vision-Language Pretraining in Chinese](https://arxiv.org/abs/2211.01335) by An Yang, Junshu Pan, Junyang Lin, Rui Men, Yichang Zhang, Jingren Zhou, Chang Zhou.", "1. **[ByT5](https://huggingface.co/docs/transformers/model_doc/byt5)** (from Google Research) released with the paper [ByT5: Towards a token-free future with pre-trained byte-to-byte models](https://arxiv.org/abs/2105.13626) by Linting Xue, Aditya Barua, Noah Constant, Rami Al-Rfou, Sharan Narang, Mihir Kale, Adam Roberts, Colin Raffel.\n1. **[CamemBERT](https://huggingface.co/docs/transformers/model_doc/camembert)** (from Inria/Facebook/Sorbonne) released with the paper [CamemBERT: a Tasty French Language Model](https://arxiv.org/abs/1911.03894) by Louis Martin*, Benjamin Muller*, Pedro Javier Ortiz Su\u00e1rez*, Yoann Dupont, Laurent Romary, \u00c9ric Villemonte de la Clergerie, Djam\u00e9 Seddah and Beno\u00eet Sagot.\n1. **[CANINE](https://huggingface.co/docs/transformers/model_doc/canine)** (from Google Research) released with the paper [CANINE: Pre-training an Efficient Tokenization-Free Encoder for Language Representation](https://arxiv.org/abs/2103.06874) by Jonathan H. Clark, Dan Garrette, Iulia Turc, John Wieting.\n1. **[Chinese-CLIP](https://huggingface.co/docs/transformers/model_doc/chinese_clip)** (from OFA-Sys) released with the paper [Chinese CLIP: Contrastive Vision-Language Pretraining in Chinese](https://arxiv.org/abs/2211.01335) by An Yang, Junshu Pan, Junyang Lin, Rui Men, Yichang Zhang, Jingren Zhou, Chang Zhou.", "1. **[ByT5](https://huggingface.co/docs/transformers/model_doc/byt5)** (from Google Research) released with the paper [ByT5: Towards a token-free future with pre-trained byte-to-byte models](https://arxiv.org/abs/2105.13626) by Linting Xue, Aditya Barua, Noah Constant, Rami Al-Rfou, Sharan Narang, Mihir Kale, Adam Roberts, Colin Raffel.\n1. **[CamemBERT](https://huggingface.co/docs/transformers/model_doc/camembert)** (from Inria/Facebook/Sorbonne) released with the paper [CamemBERT: a Tasty French Language Model](https://arxiv.org/abs/1911.03894) by Louis Martin*, Benjamin Muller*, Pedro Javier Ortiz Su\u00e1rez*, Yoann Dupont, Laurent Romary, \u00c9ric Villemonte de la Clergerie, Djam\u00e9 Seddah and Beno\u00eet Sagot.\n1. **[CANINE](https://huggingface.co/docs/transformers/model_doc/canine)** (from Google Research) released with the paper [CANINE: Pre-training an Efficient Tokenization-Free Encoder for Language Representation](https://arxiv.org/abs/2103.06874) by Jonathan H. Clark, Dan Garrette, Iulia Turc, John Wieting.\n1. **[Chinese-CLIP](https://huggingface.co/docs/transformers/model_doc/chinese_clip)** (from OFA-Sys) released with the paper [Chinese CLIP: Contrastive Vision-Language Pretraining in Chinese](https://arxiv.org/abs/2211.01335) by An Yang, Junshu Pan, Junyang Lin, Rui Men, Yichang Zhang, Jingren Zhou, Chang Zhou.", "1. **[ByT5](https://huggingface.co/docs/transformers/model_doc/byt5)** (from Google Research) released with the paper [ByT5: Towards a token-free future with pre-trained byte-to-byte models](https://arxiv.org/abs/2105.13626) by Linting Xue, Aditya Barua, Noah Constant, Rami Al-Rfou, Sharan Narang, Mihir Kale, Adam Roberts, Colin Raffel.\n1. **[CamemBERT](https://huggingface.co/docs/transformers/model_doc/camembert)** (from Inria/Facebook/Sorbonne) released with the paper [CamemBERT: a Tasty French Language Model](https://arxiv.org/abs/1911.03894) by Louis Martin*, Benjamin Muller*, Pedro Javier Ortiz Su\u00e1rez*, Yoann Dupont, Laurent Romary, \u00c9ric Villemonte de la Clergerie, Djam\u00e9 Seddah and Beno\u00eet Sagot.\n1. **[CANINE](https://huggingface.co/docs/transformers/model_doc/canine)** (from Google Research) released with the paper [CANINE: Pre-training an Efficient Tokenization-Free Encoder for Language Representation](https://arxiv.org/abs/2103.06874) by Jonathan H. Clark, Dan Garrette, Iulia Turc, John Wieting.\n1. **[Chinese-CLIP](https://huggingface.co/docs/transformers/model_doc/chinese_clip)** (from OFA-Sys) released with the paper [Chinese CLIP: Contrastive Vision-Language Pretraining in Chinese](https://arxiv.org/abs/2211.01335) by An Yang, Junshu Pan, Junyang Lin, Rui Men, Yichang Zhang, Jingren Zhou, Chang Zhou.", "1. **[ByT5](https://huggingface.co/docs/transformers/model_doc/byt5)** (from Google Research) released with the paper [ByT5: Towards a token-free future with pre-trained byte-to-byte models](https://arxiv.org/abs/2105.13626) by Linting Xue, Aditya Barua, Noah Constant, Rami Al-Rfou, Sharan Narang, Mihir Kale, Adam Roberts, Colin Raffel.\n1. **[CamemBERT](https://huggingface.co/docs/transformers/model_doc/camembert)** (from Inria/Facebook/Sorbonne) released with the paper [CamemBERT: a Tasty French Language Model](https://arxiv.org/abs/1911.03894) by Louis Martin*, Benjamin Muller*, Pedro Javier Ortiz Su\u00e1rez*, Yoann Dupont, Laurent Romary, \u00c9ric Villemonte de la Clergerie, Djam\u00e9 Seddah and Beno\u00eet Sagot.\n1. **[CANINE](https://huggingface.co/docs/transformers/model_doc/canine)** (from Google Research) released with the paper [CANINE: Pre-training an Efficient Tokenization-Free Encoder for Language Representation](https://arxiv.org/abs/2103.06874) by Jonathan H. Clark, Dan Garrette, Iulia Turc, John Wieting.\n1. **[Chinese-CLIP](https://huggingface.co/docs/transformers/model_doc/chinese_clip)** (from OFA-Sys) released with the paper [Chinese CLIP: Contrastive Vision-Language Pretraining in Chinese](https://arxiv.org/abs/2211.01335) by An Yang, Junshu Pan, Junyang Lin, Rui Men, Yichang Zhang, Jingren Zhou, Chang Zhou.", "- [t5-11b](https://huggingface.co/t5-11b).\n\nBased on the original T5 model, Google has released some follow-up works:\n\n- **T5v1.1**: T5v1.1 is an improved version of T5 with some architectural tweaks, and is pre-trained on C4 only without\n  mixing in the supervised tasks. Refer to the documentation of T5v1.1 which can be found [here](t5v1.1).\n\n- **mT5**: mT5 is a multilingual T5 model. It is pre-trained on the mC4 corpus, which includes 101 languages. Refer to\n  the documentation of mT5 which can be found [here](mt5).\n\n- **byT5**: byT5 is a T5 model pre-trained on byte sequences rather than SentencePiece subword token sequences. Refer\n  to the documentation of byT5 which can be found [here](byt5).\n\n- **UL2**: UL2 is a T5 like model pretrained on various denoising objectives\n\n- **Flan-T5**: Flan is a pretraining methods that is based on prompting. The Flan-T5 are T5 models trained on the Flan collection of \n    datasets which include: `taskmaster2`, `djaym7/wiki_dialog`, `deepmind/code_contests`, `lambada`, `gsm8k`, `aqua_rat`, `esnli`, `quasc` and `qed`.\n\n- **FLan-UL2** : the UL2 model finetuned using the \"Flan\" prompt tuning and dataset collection.\n\n- **UMT5**: UmT5 is a multilingual T5 model trained on an improved and refreshed mC4 multilingual corpus,  29 trillion characters across 107 language, using a new sampling method, UniMax. Refer to\n the documentation of mT5 which can be found [here](umt5).\n\n## Training"], "eval_score_GPT4": "5", "eval_feedback_GPT4": "Feedback: The response provided is completely correct, accurate, and factual. It matches the reference answer exactly and includes additional information such as the arXiv identifier, which is not necessary but does not detract from the correctness of the answer."}, {"question": "What is the dimension of the feature vector for the base BERT model?\n", "true_answer": "768", "source_doc": "huggingface/course/blob/main/subtitles/en/raw/chapter1/05_encoders.md", "generated_answer": "The dimension of the feature vector for the base BERT model is 768, as stated in the context provided. This information can be found in the output of the `AutoModel` class from the `transformers` library, which is shown in the code snippet provided in step 4. The relevant line is:\n\n```python\n(output): BertOutput(\n  (dense): Linear(in_features=3072, out_features", "retrieved_docs": ["Let's dive in this representation. It contains one vector per word that was passed through the encoder. Each of these vector is a numerical representation of the word in question. The dimension of that vector is defined by the architecture of the model, for the base BERT model, it is 768. These representations contain the value of a word; but contextualized. For example, the vector attributed to the word \"to\", isn't the representation of only the \"to\" word. It also takes into account the words around it, which we call the \u201ccontext\u201d.As in, it looks to the left context, the word on the left of the one we're studying (here the word \"Welcome\") and the context on the right (here the word \"NYC\") and outputs a value for the word, within its context. It is therefore a contextualized value. One could say that the vector of 768 values holds the \"meaning\" of that word in the text. How it does this is thanks to the self-attention mechanism. The self-attention mechanism relates to different positions (or different words) in a single sequence, in order to compute a representation of that sequence. As we've seen before, this means that the resulting representation of a word has been affected by other words in the sequence. We won't dive into the specifics here, but we'll offer some further readings if you want to get a better understanding at what happens under the hood. So when should one use an encoder? Encoders can be used as standalone models in a wide variety of tasks. For example BERT, arguably the most famous transformer model, is a standalone encoder model and at the time of release, beat the state of the art in many sequence classification tasks, question answering tasks, and masked language modeling, to only cite a few. The idea is that encoders are very powerful at extracting vectors that carry meaningful information about a sequence. This vector can then be handled down the road by additional layers of neurons to make sense of them", "- `distilbert-base-uncased`: DistilBERT English language model pretrained on the same data used to pretrain Bert (concatenation of the Toronto Book Corpus and full English Wikipedia) using distillation with the supervision of the `bert-base-uncased` version of Bert. The model has 6 layers, 768 dimension and 12 heads, totalizing 66M parameters.\n- `distilbert-base-uncased-distilled-squad`: A finetuned version of `distilbert-base-uncased` finetuned using (a second step of) knowledge distillation on SQuAD 1.0. This model reaches a F1 score of 86.9 on the dev set (for comparison, Bert `bert-base-uncased` version reaches a 88.5 F1 score).\n- `distilbert-base-cased`: DistilBERT English language model pretrained on the same data used to pretrain Bert (concatenation of the Toronto Book Corpus and full English Wikipedia) using distillation with the supervision of the `bert-base-cased` version of Bert. The model has 6 layers, 768 dimension and 12 heads, totalizing 65M parameters.\n- `distilbert-base-cased-distilled-squad`: A finetuned version of `distilbert-base-cased` finetuned using (a second step of) knowledge distillation on SQuAD 1.0. This model reaches a F1 score of 87.1 on the dev set (for comparison, Bert `bert-base-cased` version reaches a 88.7 F1 score).", "- `distilbert-base-german-cased`: DistilBERT German language model pretrained on 1/2 of the data used to pretrain Bert using distillation with the supervision of the `bert-base-german-dbmdz-cased` version of German DBMDZ Bert. For NER tasks the model reaches a F1 score of 83.49 on the CoNLL-2003 test set (for comparison, `bert-base-german-dbmdz-cased` reaches a 84.52 F1 score), and a F1 score of 85.23 on the GermEval 2014 test set (`bert-base-german-dbmdz-cased` reaches a 86.89 F1 score).\n- `distilgpt2`: DistilGPT2 English language model pretrained with the supervision of `gpt2` (the smallest version of GPT2) on [OpenWebTextCorpus](https://skylion007.github.io/OpenWebTextCorpus/), a reproduction of OpenAI's WebText dataset. The model has 6 layers, 768 dimension and 12 heads, totalizing 82M parameters (compared to 124M parameters for GPT2). On average, DistilGPT2 is two times faster than GPT2.\n- `distilroberta-base`: DistilRoBERTa English language model pretrained with the supervision of `roberta-base` solely on [OpenWebTextCorpus](https://skylion007.github.io/OpenWebTextCorpus/), a reproduction of OpenAI's WebText dataset (it is ~4 times less training data than the teacher RoBERTa). The model has 6 layers, 768 dimension and 12 heads, totalizing 82M parameters (compared to 125M parameters for RoBERTa-base). On average DistilRoBERTa is twice as fast as Roberta-base.", ">>> model = AutoModel.from_pretrained(\"bert-base-uncased\")\n>>> print(model)  # doctest: +IGNORE_RESULT\n...\n          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n      )\n      (11): BertLayer(\n        (attention): BertAttention(\n          (self): BertSelfAttention(\n            (query): Linear(in_features=768, out_features=768, bias=True)\n            (key): Linear(in_features=768, out_features=768, bias=True)\n            (value): Linear(in_features=768, out_features=768, bias=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (output): BertSelfOutput(\n            (dense): Linear(in_features=768, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (intermediate): BertIntermediate(\n          (dense): Linear(in_features=768, out_features=3072, bias=True)\n          (intermediate_act_fn): GELUActivation()\n        )\n        (output): BertOutput(\n          (dense): Linear(in_features=3072, out_features=768, bias=True)\n          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n      )\n    )\n  )\n  (pooler): BertPooler(\n    (dense): Linear(in_features=768, out_features=768, bias=True)\n    (activation): Tanh()\n  )\n)\n```", "The following table shows a complete list of all investigated model\nvariants including the number of randomly initialized weights, *i.e.*\n\\\"random\\\", and the number of weights initialized from the respective\npre-trained checkpoints, *i.e.* \\\"leveraged\\\". All models are based on a\n12-layer architecture with 768-dim hidden size embeddings, corresponding\nto the `bert-base-cased`, `bert-base-uncased`, `roberta-base`, and\n`gpt2` checkpoints in the \ud83e\udd17Transformers model hub.\n\n  |Model          |random   |leveraged   |total\n  |-------------- |:------- |----------  |-------\n  |Rnd2Rnd        |221M     |0           |221M\n  |Rnd2BERT       |112M     |109M        |221M\n  |BERT2Rnd       |112M     |109M        |221M\n  |Rnd2GPT2       |114M     |125M        |238M\n  |BERT2BERT      |26M      |195M        |221M\n  |BERTShare      |26M      |109M        |135M\n  |RoBERTaShare   |26M      |126M        |152M\n  |BERT2GPT2      |26M      |234M        |260M\n  |RoBERTa2GPT2   |26M      |250M        |276M", "bert-base-cased             model     a8d257ba9925ef39f3036bfc338acf5283c512d9         1.4G        9 3 days ago    main        /home/wauplin/.cache/huggingface/hub/models--bert-base-cased/snapshots/a8d257ba9925ef39f3036bfc338acf5283c512d9\nt5-base                     model     23aa4f41cb7c08d4b05c8f327b22bfa0eb8c7ad9        10.1K        3 1 week ago    main        /home/wauplin/.cache/huggingface/hub/models--t5-base/snapshots/23aa4f41cb7c08d4b05c8f327b22bfa0eb8c7ad9\nt5-small                    model     98ffebbb27340ec1b1abd7c45da12c253ee1882a       726.2M        6 1 week ago    refs/pr/1   /home/wauplin/.cache/huggingface/hub/models--t5-small/snapshots/98ffebbb27340ec1b1abd7c45da12c253ee1882a\nt5-small                    model     d0a119eedb3718e34c648e594394474cf95e0617       485.8M        6 4 weeks ago               /home/wauplin/.cache/huggingface/hub/models--t5-small/snapshots/d0a119eedb3718e34c648e594394474cf95e0617", "Important to remember is that for each output vector \\\\(\\mathbf{z}_{i}\\\\), the whole input sequence \\\\(\\mathbf{X}\\\\) is processed. The tensor of the inner dot-product \\\\(\\mathbf{Q}\\mathbf{K}^T\\\\) has an asymptotic memory complexity of \\\\(\\mathcal{O}(n^2)\\\\) which usually represents the memory bottleneck in a transformer model. \n\nThis is also the reason why `bert-base-cased` has a `config.max_position_embedding_size` of only 512.\n\n### Local Self-Attention\n\n **Local self-attention** is the obvious solution to reducing the \\\\(\\mathcal{O}(n^2)\\\\) memory bottleneck, allowing us to model longer sequences with a reduced computational cost. \nIn local self-attention the input \\\\( \\mathbf{X} = \\mathbf{X}_{1:n} = \\mathbf{x}_{1}, \\ldots, \\mathbf{x}_{n} \\\\) \nis cut into \\\\(n_{c}\\\\) chunks: \\\\( \\mathbf{X} = \\left[\\mathbf{X}_{1:l_{c}}, \\ldots, \\mathbf{X}_{(n_{c} - 1) * l_{c} : n_{c} * l_{c}}\\right] \\\\) each \nof length `config.local_chunk_length`, *i.e.* \\\\(l_{c}\\\\), and subsequently global self-attention is applied on each chunk separately.\n\nLet's take our input sequence for \\\\(n=16, d_h=3\\\\) again for visualization:\n\n![alt text](https://raw.githubusercontent.com/patrickvonplaten/scientific_images/master/reformer_benchmark/input.png)"], "eval_score_GPT4": "5", "eval_feedback_GPT4": "The response correctly identifies the dimension of the feature vector for the base BERT model as 768, which is in agreement with the reference answer. The additional context provided about the `AutoModel` class from the `transformers` library and the code snippet, although not necessary, does not detract from the factuality of the response."}, {"question": "What special identifier does the WordPiece Model use for continuing subwords?\n", "true_answer": "##", "source_doc": "huggingface/tokenizers/blob/main/docs/source-doc-builder/components.mdx", "generated_answer": "The WordPiece Model uses the special identifier `##` for continuing subwords. (Source: Extract 2)", "retrieved_docs": ["| Name | Description |\n| :--- | :--- |\n| ByteLevel | Reverts the ByteLevel PreTokenizer. This PreTokenizer encodes at the byte-level, using a set of visible Unicode characters to represent each byte, so we need a Decoder to revert this process and get something readable again. |\n| Metaspace | Reverts the Metaspace PreTokenizer. This PreTokenizer uses a special identifer `\u2581` to identify whitespaces, and so this Decoder helps with decoding these. |\n| WordPiece | Reverts the WordPiece Model. This model uses a special identifier `##` for continuing subwords, and so this Decoder helps with decoding these. |", "#### Byte-level BPE\n\nA base vocabulary that includes all possible base characters can be quite large if *e.g.* all unicode characters are\nconsidered as base characters. To have a better base vocabulary, [GPT-2](https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf) uses bytes\nas the base vocabulary, which is a clever trick to force the base vocabulary to be of size 256 while ensuring that\nevery base character is included in the vocabulary. With some additional rules to deal with punctuation, the GPT2's\ntokenizer can tokenize every text without the need for the <unk> symbol. [GPT-2](model_doc/gpt) has a vocabulary\nsize of 50,257, which corresponds to the 256 bytes base tokens, a special end-of-text token and the symbols learned\nwith 50,000 merges.\n\n<a id='wordpiece'></a>\n\n### WordPiece\n\nWordPiece is the subword tokenization algorithm used for [BERT](model_doc/bert), [DistilBERT](model_doc/distilbert), and [Electra](model_doc/electra). The algorithm was outlined in [Japanese and Korean\nVoice Search (Schuster et al., 2012)](https://static.googleusercontent.com/media/research.google.com/ja//pubs/archive/37842.pdf) and is very similar to\nBPE. WordPiece first initializes the vocabulary to include every character present in the training data and\nprogressively learns a given number of merge rules. In contrast to BPE, WordPiece does not choose the most frequent\nsymbol pair, but the one that maximizes the likelihood of the training data once added to the vocabulary.", "WordPiece tokenization[[wordpiece-tokenization]]\n\n<CourseFloatingBanner chapter={6}\n  classNames=\"absolute z-10 right-0 top-0\"\n  notebooks={[\n    {label: \"Google Colab\", value: \"https://colab.research.google.com/github/huggingface/notebooks/blob/master/course/en/chapter6/section6.ipynb\"},\n    {label: \"Aws Studio\", value: \"https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/master/course/en/chapter6/section6.ipynb\"},\n]} />\n\nWordPiece is the tokenization algorithm Google developed to pretrain BERT. It has since been reused in quite a few Transformer models based on BERT, such as DistilBERT, MobileBERT, Funnel Transformers, and MPNET. It's very similar to BPE in terms of the training, but the actual tokenization is done differently.\n\n<Youtube id=\"qpv6ms_t_1A\"/>\n\n<Tip>\n\n\ud83d\udca1 This section covers WordPiece in depth, going as far as showing a full implementation. You can skip to the end if you just want a general overview of the tokenization algorithm.\n\n</Tip>\n\n## Training algorithm[[training-algorithm]]\n\n<Tip warning={true}>\n\n\u26a0\ufe0f Google never open-sourced its implementation of the training algorithm of WordPiece, so what follows is our best guess based on the published literature. It may not be 100% accurate.\n\n</Tip>\n\nLike BPE, WordPiece starts from a small vocabulary including the special tokens used by the model and the initial alphabet. Since it identifies subwords by adding a prefix (like `##` for BERT), each word is initially split by adding that prefix to all the characters inside the word. So, for instance, `\"word\"` gets split like this:\n\n```\nw ##o ##r ##d\n```", "### 9. Select the sentences that apply to the WordPiece model of tokenization.\n\n<Question\n\tchoices={[\n\t\t{\n\t\t\ttext: \"WordPiece is a subword tokenization algorithm that starts with a small vocabulary and learns merge rules.\",\n\t\t\texplain: \"That's the case indeed!\",\n\t\t\tcorrect: true\n\t\t},\n\t\t{\n\t\t\ttext: \"WordPiece is a subword tokenization algorithm that starts with a big vocabulary and progressively removes tokens from it.\",\n\t\t\texplain: \"No, that's the approach taken by a different tokenization algorithm.\"\n\t\t},\n\t\t{\n\t\t\ttext: \"WordPiece tokenizers learn merge rules by merging the pair of tokens that is the most frequent.\",\n\t\t\texplain: \"No, that's the strategy applied by another tokenization algorithm.\"\n\t\t},\n\t\t{\n\t\t\ttext: \"A WordPiece tokenizer learns a merge rule by merging the pair of tokens that maximizes a score that privileges frequent pairs with less frequent individual parts.\",\n\t\t\texplain: \"That's correct!\",\n\t\t\tcorrect: true\n\t\t},\n\t\t{\n\t\t\ttext: \"WordPiece tokenizes words into subwords by finding the most likely segmentation into tokens, according to the model.\",\n\t\t\texplain: \"No, that's how another tokenization algorithm works.\"\n\t\t},\n\t\t{\n\t\t\ttext: \"WordPiece tokenizes words into subwords by finding the longest subword starting from the beginning that is in the vocabulary, then repeating the process for the rest of the text.\",\n\t\t\texplain: \"Yes, this is how WordPiece proceeds for the encoding.\",\n\t\t\tcorrect: true\n\t\t},\n\t]}\n/>\n\n### 10. Select the sentences that apply to the Unigram model of tokenization.", "Now that we've seen a little of how some different tokenizers process text, we can start to explore the underlying algorithms themselves. We'll begin with a quick look at the broadly widely applicable SentencePiece; then, over the next three sections, we'll examine how the three main algorithms used for subword tokenization work.\n\n## SentencePiece[[sentencepiece]]\n\n[SentencePiece](https://github.com/google/sentencepiece) is a tokenization algorithm for the preprocessing of text that you can use with any of the models we will see in the next three sections. It considers the text as a sequence of Unicode characters, and replaces spaces with a special character, `\u2581`. Used in conjunction with the Unigram algorithm (see [section 7](/course/chapter7/7)), it doesn't even require a pre-tokenization step, which is very useful for languages where the space character is not used (like Chinese or Japanese).\n\nThe other main feature of SentencePiece is *reversible tokenization*: since there is no special treatment of spaces, decoding the tokens is done simply by concatenating them and replacing the `_`s with spaces -- this results in the normalized text. As we saw earlier, the BERT tokenizer removes repeating spaces, so its tokenization is not reversible.\n\n## Algorithm overview[[algorithm-overview]]\n\nIn the following sections, we'll dive into the three main subword tokenization algorithms: BPE (used by GPT-2 and others), WordPiece (used for example by BERT), and Unigram (used by T5 and others). Before we get started, here's a quick overview of how they each work. Don't hesitate to come back to this table after reading each of the next sections if it doesn't make sense to you yet.", "<a id='sentencepiece'></a>\n\n### SentencePiece\n\nAll tokenization algorithms described so far have the same problem: It is assumed that the input text uses spaces to\nseparate words. However, not all languages use spaces to separate words. One possible solution is to use language\nspecific pre-tokenizers, *e.g.* [XLM](model_doc/xlm) uses a specific Chinese, Japanese, and Thai pre-tokenizer).\nTo solve this problem more generally, [SentencePiece: A simple and language independent subword tokenizer and\ndetokenizer for Neural Text Processing (Kudo et al., 2018)](https://arxiv.org/pdf/1808.06226.pdf) treats the input\nas a raw input stream, thus including the space in the set of characters to use. It then uses the BPE or unigram\nalgorithm to construct the appropriate vocabulary.\n\nThe [`XLNetTokenizer`] uses SentencePiece for example, which is also why in the example earlier the\n`\"\u2581\"` character was included in the vocabulary. Decoding with SentencePiece is very easy since all tokens can just be\nconcatenated and `\"\u2581\"` is replaced by a space.\n\nAll transformers models in the library that use SentencePiece use it in combination with unigram. Examples of models\nusing SentencePiece are [ALBERT](model_doc/albert), [XLNet](model_doc/xlnet), [Marian](model_doc/marian), and [T5](model_doc/t5).", "## Models\n\nModels are the core algorithms used to actually tokenize, and therefore,\nthey are the only mandatory component of a Tokenizer.\n\n| Name | Description |\n| :--- | :--- |\n| WordLevel | This is the \u201cclassic\u201d tokenization algorithm. It let\u2019s you simply map words to IDs without anything fancy. This has the advantage of being really simple to use and understand, but it requires extremely large vocabularies for a good coverage. Using this `Model` requires the use of a `PreTokenizer`. No choice will be made by this model directly, it simply maps input tokens to IDs.  |\n| BPE | One of the most popular subword tokenization algorithm. The Byte-Pair-Encoding works by starting with characters, while merging those that are the most frequently seen together, thus creating new tokens. It then works iteratively to build new tokens out of the most frequent pairs it sees in a corpus. BPE is able to build words it has never seen by using multiple subword tokens, and thus requires smaller vocabularies, with less chances of having \u201cunk\u201d (unknown) tokens.  |\n| WordPiece | This is a subword tokenization algorithm quite similar to BPE, used mainly by Google in models like BERT. It uses a greedy algorithm, that tries to build long words first, splitting in multiple tokens when entire words don\u2019t exist in the vocabulary. This is different from BPE that starts from characters, building bigger tokens as possible. It uses the famous `##` prefix to identify tokens that are part of a word (ie not starting a word).  |\n| Unigram | Unigram is also a subword tokenization algorithm, and works by trying to identify the best set of subword tokens to maximize the probability for a given sentence. This is different from BPE in the way that this is not deterministic based on a set of rules applied sequentially. Instead Unigram will be able to compute multiple ways of tokenizing, while choosing the most probable one. |\n\n## Post-Processors\n\nAfter the whole pipeline, we sometimes want to insert some special\ntokens before feed a tokenized string into a model like \"[CLS] My\nhorse is amazing [SEP]\". The `PostProcessor` is the component doing\njust that."], "eval_score_GPT4": "5", "eval_feedback_GPT4": "Feedback: The response provided is completely correct, accurate, and factual. It correctly identifies `##` as the special identifier used by the WordPiece Model for continuing subwords, which matches the reference answer."}, {"question": "What is the purpose of the \ud83e\udde8 Diffusers tutorials?\n", "true_answer": "To provide a gentle introduction to diffusion models and help understand the library fundamentals.", "source_doc": "huggingface/diffusers/blob/main/docs/source/en/tutorials/tutorial_overview.md", "generated_answer": "The purpose of the \ud83e\udde8 Diffusers tutorials is to provide a gentle introduction to diffusion models and help users understand the library fundamentals, including the core components and how to use the library as a modular toolbox for building their own diffusion systems. These beginner-friendly tutorials are designed to provide a rapid way to generate things using a pipeline for inference and then deconstruct that pipeline to understand how to use the library. After completing the tutorials, users will have", "retrieved_docs": ["\ud83e\udd17 Diffusers is the go-to library for state-of-the-art pretrained diffusion models for generating images, audio, and even 3D structures of molecules. Whether you're looking for a simple inference solution or training your own diffusion models, \ud83e\udd17 Diffusers is a modular toolbox that supports both. Our library is designed with a focus on [usability over performance](https://huggingface.co/docs/diffusers/conceptual/philosophy#usability-over-performance), [simple over easy](https://huggingface.co/docs/diffusers/conceptual/philosophy#simple-over-easy), and [customizability over abstractions](https://huggingface.co/docs/diffusers/conceptual/philosophy#tweakable-contributorfriendly-over-abstraction).\n\n\ud83e\udd17 Diffusers offers three core components:\n\n- State-of-the-art [diffusion pipelines](https://huggingface.co/docs/diffusers/api/pipelines/overview) that can be run in inference with just a few lines of code.\n- Interchangeable noise [schedulers](https://huggingface.co/docs/diffusers/api/schedulers/overview) for different diffusion speeds and output quality.\n- Pretrained [models](https://huggingface.co/docs/diffusers/api/models/overview) that can be used as building blocks, and combined with schedulers, for creating your own end-to-end diffusion systems.\n\n## Installation\n\nWe recommend installing \ud83e\udd17 Diffusers in a virtual environment from PyPI or Conda. For more details about installing [PyTorch](https://pytorch.org/get-started/locally/) and [Flax](https://flax.readthedocs.io/en/latest/#installation), please refer to their official documentation.\n\n### PyTorch\n\nWith `pip` (official package):\n\n```bash\npip install --upgrade diffusers[torch]\n```\n\nWith `conda` (maintained by the community):", "Using \ud83e\udde8 `diffusers` at Hugging Face\n\nDiffusers is the go-to library for state-of-the-art pretrained diffusion models for generating images, audio, and even 3D structures of molecules. Whether you\u2019re looking for a simple inference solution or want to train your own diffusion model, Diffusers is a modular toolbox that supports both. The library is designed with a focus on usability over performance, simple over easy, and customizability over abstractions.\n\n## Exploring Diffusers in the Hub\n\nThere are over 10,000 `diffusers` compatible pipelines on the Hub which you can find by filtering at the left of [the models page](https://huggingface.co/models?library=diffusers&sort=downloads). Diffusion systems are typically composed of multiple components such as text encoder, UNet, VAE, and scheduler. Even though they are not standalone models, the pipeline abstraction makes it easy to use them for inference or training.\n\nYou can find diffusion pipelines for many different tasks:\n\n* Generating images from natural language text prompts ([text-to-image](https://huggingface.co/models?library=diffusers&pipeline_tag=text-to-image&sort=downloads)).\n* Transforming images using natural language text prompts ([image-to-image](https://huggingface.co/models?library=diffusers&pipeline_tag=image-to-image&sort=downloads)).\n* Generating videos from natural language descriptions ([text-to-video](https://huggingface.co/models?library=diffusers&pipeline_tag=text-to-video&sort=downloads)).\n\n\nYou can try out the models directly in the browser if you want to test them out without downloading them, thanks to the in-browser widgets! \n\n<div class=\"flex justify-center\">\n<img class=\"hidden dark:block\" src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/refs%2Fpr%2F35/hub/libraries-diffusers_widget.png\"/>\n</div>", "\ud83e\udde8 Diffusers Experimental\n\nWe are adding experimental code to support novel applications and usages of the Diffusers library.\nCurrently, the following experiments are supported:\n* Reinforcement learning via an implementation of the [Diffuser](https://arxiv.org/abs/2205.09991) model.", "## Next steps\n\nHopefully, you generated some cool images with \ud83e\udde8 Diffusers in this quicktour! For your next steps, you can:\n\n* Train or finetune a model to generate your own images in the [training](./tutorials/basic_training) tutorial.\n* See example official and community [training or finetuning scripts](https://github.com/huggingface/diffusers/tree/main/examples#-diffusers-examples) for a variety of use cases.\n* Learn more about loading, accessing, changing, and comparing schedulers in the [Using different Schedulers](./using-diffusers/schedulers) guide.\n* Explore prompt engineering, speed and memory optimizations, and tips and tricks for generating higher-quality images with the [Stable Diffusion](./stable_diffusion) guide.\n* Dive deeper into speeding up \ud83e\udde8 Diffusers with guides on [optimized PyTorch on a GPU](./optimization/fp16), and inference guides for running [Stable Diffusion on Apple Silicon (M1/M2)](./optimization/mps) and [ONNX Runtime](./optimization/onnx).", "## Community highlights\n\nOne of the most gratifying experiences of the past year has been seeing how the community is incorporating \ud83e\udd17 Diffusers into their projects. From adapting Low-rank adaptation (LoRA) for faster training of text-to-image models to building a state-of-the-art inpainting tool, here are a few of our favorite projects:", "!--Copyright 2023 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\n\nhttp://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License.\n-->\n\n# Overview\n\nWelcome to \ud83e\udde8 Diffusers! If you're new to diffusion models and generative AI, and want to learn more, then you've come to the right place. These beginner-friendly tutorials are designed to provide a gentle introduction to diffusion models and help you understand the library fundamentals - the core components and how \ud83e\udde8 Diffusers is meant to be used.\n\nYou'll learn how to use a pipeline for inference to rapidly generate things, and then deconstruct that pipeline to really understand how to use the library as a modular toolbox for building your own diffusion systems. In the next lesson, you'll learn how to train your own diffusion model to generate what you want.\n\nAfter completing the tutorials, you'll have gained the necessary skills to start exploring the library on your own and see how to use it for your own projects and applications.\n\nFeel free to join our community on [Discord](https://discord.com/invite/JfAtkvEtRb) or the [forums](https://discuss.huggingface.co/c/discussion-related-to-httpsgithubcomhuggingfacediffusers/63) to connect and collaborate with other users and developers!\n\nLet's start diffusing! \ud83e\udde8", "!--Copyright 2023 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\n\nhttp://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License.\n-->\n\n# Philosophy\n\n\ud83e\udde8 Diffusers provides **state-of-the-art** pretrained diffusion models across multiple modalities.\nIts purpose is to serve as a **modular toolbox** for both inference and training.\n\nWe aim at building a library that stands the test of time and therefore take API design very seriously.\n\nIn a nutshell, Diffusers is built to be a natural extension of PyTorch. Therefore, most of our design choices are based on [PyTorch's Design Principles](https://pytorch.org/docs/stable/community/design.html#pytorch-design-philosophy). Let's go over the most important ones:\n\n## Usability over Performance"], "eval_score_GPT4": "5", "eval_feedback_GPT4": "The response provided aligns well with the reference answer, covering the key points about the purpose of the \ud83e\udde8 Diffusers tutorials. It mentions the gentle introduction to diffusion models and the aim to help users understand the library fundamentals. Additionally, it expands on the reference answer by explaining the modular nature of the library and the goal of enabling users to build their own diffusion systems. The response also touches on the beginner-friendly aspect of the tutorials and the practical outcome of being able to generate things using a pipeline. The response is factual and accurate, providing a clear and comprehensive explanation that is in line with the reference answer."}, {"question": "What is the default setting for the `allow_flagging` parameter in Gradio's `Interface`?\n", "true_answer": "\"manual\"", "source_doc": "gradio-app/gradio/blob/main/guides/cn/07_other-tutorials/using-flagging.md", "generated_answer": "The default setting for the `allow_flagging` parameter in Gradio's `Interface` is \"manual\". This means that by default, a \"Flag\" button will be displayed underneath the output components, and users can click this button to flag input and output data for further review. The flagged data will be saved to a CSV log file by default, and if the demo involves file data, separate folders will be created to store those flagged files. The user can", "retrieved_docs": ["There are [four parameters](https://gradio.app/docs/#interface-header) in `gradio.Interface` that control how flagging works. We will go over them in greater detail.\n\n- `allow_flagging`: this parameter can be set to either `\"manual\"` (default), `\"auto\"`, or `\"never\"`.\n  - `manual`: users will see a button to flag, and samples are only flagged when the button is clicked.\n  - `auto`: users will not see a button to flag, but every sample will be flagged automatically.\n  - `never`: users will not see a button to flag, and no sample will be flagged.\n- `flagging_options`: this parameter can be either `None` (default) or a list of strings.\n  - If `None`, then the user simply clicks on the **Flag** button and no additional options are shown.\n  - If a list of strings are provided, then the user sees several buttons, corresponding to each of the strings that are provided. For example, if the value of this parameter is `[\"Incorrect\", \"Ambiguous\"]`, then buttons labeled **Flag as Incorrect** and **Flag as Ambiguous** appear. This only applies if `allow_flagging` is `\"manual\"`.\n  - The chosen option is then logged along with the input and output.\n- `flagging_dir`: this parameter takes a string.\n  - It represents what to name the directory where flagged data is stored.\n- `flagging_callback`: this parameter takes an instance of a subclass of the `FlaggingCallback` class\n  - Using this parameter allows you to write custom code that gets run when the flag button is clicked\n  - By default, this is set to an instance of `gr.CSVLogger`\n  - One example is setting it to an instance of `gr.HuggingFaceDatasetSaver` which can allow you to pipe any flagged data into a HuggingFace Dataset. (See more below.)\n\n## What happens to flagged data?\n\nWithin the directory provided by the `flagging_dir` argument, a CSV file will log the flagged data.", "![annotated](https://github.com/gradio-app/gradio/blob/main/guides/assets/annotated.png?raw=true)\n\nIf you're using the `Blocks` API instead, you can insert text, markdown, or HTML anywhere using the `gr.Markdown(...)` or `gr.HTML(...)` components, with descriptive content inside the `Component` constructor.\n\nAnother useful keyword argument is `label=`, which is present in every `Component`. This modifies the label text at the top of each `Component`. You can also add the `info=` keyword argument to form elements like `Textbox` or `Radio` to provide further information on their usage.\n\n```python\ngr.Number(label='Age', info='In years, must be greater than 0')\n```\n\n## Flagging\n\nBy default, an `Interface` will have \"Flag\" button. When a user testing your `Interface` sees input with interesting output, such as erroneous or unexpected model behaviour, they can flag the input for you to review. Within the directory provided by the `flagging_dir=` argument to the `Interface` constructor, a CSV file will log the flagged inputs. If the interface involves file data, such as for Image and Audio components, folders will be created to store those flagged data as well.\n\nFor example, with the calculator interface shown above, we would have the flagged data stored in the flagged directory shown below:\n\n```directory\n+-- calculator.py\n+-- flagged/\n|   +-- logs.csv\n```\n\n_flagged/logs.csv_\n\n```csv\nnum1,operation,num2,Output\n5,add,7,12\n6,subtract,1.5,4.5\n```\n\nWith the sepia interface shown earlier, we would have the flagged data stored in the flagged directory shown below:", "HF_TOKEN = os.getenv('HF_TOKEN')\nhf_writer = gr.HuggingFaceDatasetSaver(HF_TOKEN, \"crowdsourced-calculator-demo\")\n\niface = gr.Interface(\n    calculator,\n    [\"number\", gr.Radio([\"add\", \"subtract\", \"multiply\", \"divide\"]), \"number\"],\n    \"number\",\n    description=\"Check out the crowd-sourced dataset at: [https://huggingface.co/datasets/aliabd/crowdsourced-calculator-demo](https://huggingface.co/datasets/aliabd/crowdsourced-calculator-demo)\",\n    allow_flagging=\"manual\",\n    flagging_options=[\"wrong sign\", \"off by one\", \"other\"],\n    flagging_callback=hf_writer\n)\n\niface.launch()\n```\n\nNotice that we define our own\ninstance of `gradio.HuggingFaceDatasetSaver` using our Hugging Face token and\nthe name of a dataset we'd like to save samples to. In addition, we also set `allow_flagging=\"manual\"`\nbecause on Hugging Face Spaces, `allow_flagging` is set to `\"never\"` by default. Here's our demo:\n\n<gradio-app space=\"gradio/calculator-flagging-crowdsourced/\"></gradio-app>\n\nYou can now see all the examples flagged above in this [public Hugging Face dataset](https://huggingface.co/datasets/aliabd/crowdsourced-calculator-demo).\n\n![flagging callback hf](https://github.com/gradio-app/gradio/blob/main/guides/assets/flagging-callback-hf.png?raw=true)", "Using Flagging\n\nRelated spaces: https://huggingface.co/spaces/gradio/calculator-flagging-crowdsourced, https://huggingface.co/spaces/gradio/calculator-flagging-options, https://huggingface.co/spaces/gradio/calculator-flag-basic\nTags: FLAGGING, DATA\n\n## Introduction\n\nWhen you demo a machine learning model, you might want to collect data from users who try the model, particularly data points in which the model is not behaving as expected. Capturing these \"hard\" data points is valuable because it allows you to improve your machine learning model and make it more reliable and robust.\n\nGradio simplifies the collection of this data by including a **Flag** button with every `Interface`. This allows a user or tester to easily send data back to the machine where the demo is running. In this Guide, we discuss more about how to use the flagging feature, both with `gradio.Interface` as well as with `gradio.Blocks`.\n\n## The **Flag** button in `gradio.Interface`\n\nFlagging with Gradio's `Interface` is especially easy. By default, underneath the output components, there is a button marked **Flag**. When a user testing your model sees input with interesting output, they can click the flag button to send the input and output data back to the machine where the demo is running. The sample is saved to a CSV log file (by default). If the demo involves images, audio, video, or other types of files, these are saved separately in a parallel directory and the paths to these files are saved in the CSV file.\n\nThere are [four parameters](https://gradio.app/docs/#interface-header) in `gradio.Interface` that control how flagging works. We will go over them in greater detail.", "## What happens to flagged data?\n\nWithin the directory provided by the `flagging_dir` argument, a CSV file will log the flagged data.\n\nHere's an example: The code below creates the calculator interface embedded below it:\n\n```python\nimport gradio as gr\n\n\ndef calculator(num1, operation, num2):\n    if operation == \"add\":\n        return num1 + num2\n    elif operation == \"subtract\":\n        return num1 - num2\n    elif operation == \"multiply\":\n        return num1 * num2\n    elif operation == \"divide\":\n        return num1 / num2\n\n\niface = gr.Interface(\n    calculator,\n    [\"number\", gr.Radio([\"add\", \"subtract\", \"multiply\", \"divide\"]), \"number\"],\n    \"number\",\n    allow_flagging=\"manual\"\n)\n\niface.launch()\n```\n\n<gradio-app space=\"gradio/calculator-flag-basic/\"></gradio-app>\n\nWhen you click the flag button above, the directory where the interface was launched will include a new flagged subfolder, with a csv file inside it. This csv file includes all the data that was flagged.\n\n```directory\n+-- flagged/\n|   +-- logs.csv\n```\n\n_flagged/logs.csv_\n\n```csv\nnum1,operation,num2,Output,timestamp\n5,add,7,12,2022-01-31 11:40:51.093412\n6,subtract,1.5,4.5,2022-01-31 03:25:32.023542\n```\n\nIf the interface involves file data, such as for Image and Audio components, folders will be created to store those flagged data as well. For example an `image` input to `image` output interface will create the following structure.", "- Updated image upload component to accept all image formats, including lossless formats like .webp by [@fienestar](https://github.com/fienestar) in [PR 3225](https://github.com/gradio-app/gradio/pull/3225)\n- Adds a disabled mode to the `gr.Button` component by setting `interactive=False` by [@abidlabs](https://github.com/abidlabs) in [PR 3266](https://github.com/gradio-app/gradio/pull/3266) and [PR 3288](https://github.com/gradio-app/gradio/pull/3288)\n- Adds visual feedback to the when the Flag button is clicked, by [@abidlabs](https://github.com/abidlabs) in [PR 3289](https://github.com/gradio-app/gradio/pull/3289)\n- Adds ability to set `flagging_options` display text and saved flag separately by [@abidlabs](https://github.com/abidlabs) in [PR 3289](https://github.com/gradio-app/gradio/pull/3289)\n- Allow the setting of `brush_radius` for the `Image` component both as a default and via `Image.update()` by [@pngwn](https://github.com/pngwn) in [PR 3277](https://github.com/gradio-app/gradio/pull/3277)\n- Added `info=` argument to form components to enable extra context provided to users, by [@aliabid94](https://github.com/aliabid94) in [PR 3291](https://github.com/gradio-app/gradio/pull/3291)", "- Updated image upload component to accept all image formats, including lossless formats like .webp by [@fienestar](https://github.com/fienestar) in [PR 3225](https://github.com/gradio-app/gradio/pull/3225)\n- Adds a disabled mode to the `gr.Button` component by setting `interactive=False` by [@abidlabs](https://github.com/abidlabs) in [PR 3266](https://github.com/gradio-app/gradio/pull/3266) and [PR 3288](https://github.com/gradio-app/gradio/pull/3288)\n- Adds visual feedback to the when the Flag button is clicked, by [@abidlabs](https://github.com/abidlabs) in [PR 3289](https://github.com/gradio-app/gradio/pull/3289)\n- Adds ability to set `flagging_options` display text and saved flag separately by [@abidlabs](https://github.com/abidlabs) in [PR 3289](https://github.com/gradio-app/gradio/pull/3289)\n- Allow the setting of `brush_radius` for the `Image` component both as a default and via `Image.update()` by [@pngwn](https://github.com/pngwn) in [PR 3277](https://github.com/gradio-app/gradio/pull/3277)\n- Added `info=` argument to form components to enable extra context provided to users, by [@aliabid94](https://github.com/aliabid94) in [PR 3291](https://github.com/gradio-app/gradio/pull/3291)"], "eval_score_GPT4": "5", "eval_feedback_GPT4": "The response correctly identifies the default setting for the `allow_flagging` parameter as \"manual\". The explanation provided aligns with the functionality of the flagging feature in Gradio's `Interface`, which is to allow users to flag input and output data for review. The details about the CSV log file and the storage of flagged files are additional accurate information that supports the correct response. Based on the score rubric, the response is completely correct, accurate, and factual."}, {"question": "Where can the full code for the Stable Diffusion demo be found?\n", "true_answer": "https://hf.co/spaces/stabilityai/stable-diffusion/tree/main", "source_doc": "gradio-app/gradio/blob/main/demo/stable-diffusion/DESCRIPTION.md", "generated_answer": "The full code for the Stable Diffusion demo can be found at this link: https://hf.co/spaces/stabilityai/stable-diffusion/tree/main. Alternatively, you can find the original codebase for Stable Diffusion v1.0 at [CompVis/stable-diffusion](https://github.com/CompVis/stable-diffusion) and Stable Diffusion v2.0 at [Stability-AI/stabled", "retrieved_docs": ["ote: This is a simplified version of the code needed to create the Stable Diffusion demo. See full code here: https://hf.co/spaces/stabilityai/stable-diffusion/tree/main", "You can find the original codebase for Stable Diffusion v1.0 at [CompVis/stable-diffusion](https://github.com/CompVis/stable-diffusion) and Stable Diffusion v2.0 at [Stability-AI/stablediffusion](https://github.com/Stability-AI/stablediffusion) as well as their original scripts for various tasks. Additional official checkpoints for the different Stable Diffusion versions and tasks can be found on the [CompVis](https://huggingface.co/CompVis), [Runway](https://huggingface.co/runwayml), and [Stability AI](https://huggingface.co/stabilityai) Hub organizations. Explore these organizations to find the best checkpoint for your use-case!\n\nThe table below summarizes the available Stable Diffusion pipelines, their supported tasks, and an interactive demo:", "The original codebase can be found at [Xiang-cd/DiffEdit-stable-diffusion](https://github.com/Xiang-cd/DiffEdit-stable-diffusion), and you can try it out in this [demo](https://blog.problemsolversguild.com/technical/research/2022/11/02/DiffEdit-Implementation.html).\n\nThis pipeline was contributed by [clarencechen](https://github.com/clarencechen). \u2764\ufe0f\n\n## Tips", "Gradio Demo: stable-diffusion\n### Note: This is a simplified version of the code needed to create the Stable Diffusion demo. See full code here: https://hf.co/spaces/stabilityai/stable-diffusion/tree/main\n        \n\n\n```\n!pip install -q gradio diffusers transformers nvidia-ml-py3 ftfy torch\n```\n\n\n```\nimport gradio as gr\nimport torch\nfrom diffusers import StableDiffusionPipeline\nfrom PIL import Image\nimport os\n\nauth_token = os.getenv(\"auth_token\")\nmodel_id = \"CompVis/stable-diffusion-v1-4\"\ndevice = \"cpu\"\npipe = StableDiffusionPipeline.from_pretrained(\n    model_id, use_auth_token=auth_token, revision=\"fp16\", torch_dtype=torch.float16\n)\npipe = pipe.to(device)\n\n\ndef infer(prompt, samples, steps, scale, seed):\n    generator = torch.Generator(device=device).manual_seed(seed)\n    images_list = pipe(\n        [prompt] * samples,\n        num_inference_steps=steps,\n        guidance_scale=scale,\n        generator=generator,\n    )\n    images = []\n    safe_image = Image.open(r\"unsafe.png\")\n    for i, image in enumerate(images_list[\"sample\"]):\n        if images_list[\"nsfw_content_detected\"][i]:\n            images.append(safe_image)\n        else:\n            images.append(image)\n    return images\n\n\nblock = gr.Blocks()\n\nwith block:\n    with gr.Group():\n        with gr.Row():\n            text = gr.Textbox(\n                label=\"Enter your prompt\",\n                max_lines=1,\n                placeholder=\"Enter your prompt\",\n                container=False,\n            )\n            btn = gr.Button(\"Generate image\")\n        gallery = gr.Gallery(\n            label=\"Generated images\",\n            show_label=False,\n            elem_id=\"gallery\",\n            columns=[2],\n            height=\"auto\",\n        )", "If you don't meet the requirements to run Stable Diffusion locally, or prefer a more streamlined solution, there are many ways to run Stable Diffusion online.\n\nFree solutions include many [spaces](https://huggingface.co/spaces) here on \ud83e\udd17 Hugging Face, such as the [Stable Diffusion 2.1 Demo](https://huggingface.co/spaces/stabilityai/stable-diffusion) or the [camemduru webui](https://huggingface.co/spaces/camenduru/webui). You can find a list of additional online services [here](https://github.com/AUTOMATIC1111/stable-diffusion-webui/wiki/Online-Services). You can even use \ud83e\udd17 [Diffusers](https://huggingface.co/docs/diffusers/index) to write your own free solution! You can find a simple code example to get started [here](https://colab.research.google.com/drive/1HebngGyjKj7nLdXfj6Qi0N1nh7WvD74z?usp=sharing).\n\n*Note:* Parts of this series will use advanced features such as image2image, which may not be available on all online services.\n\n### Generating Concept Art <a name=\"generating\"></a>\n\nLet's generate some concept art. The steps are simple:\n\n1. Type what you want.\n2. Click generate.\n\n<figure class=\"image text-center\">\n  <img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/124_ml-for-games/sd-demo.png\" alt=\"Stable Diffusion Demo Space\">\n</figure>", "### Stable Diffusion Comparisons\n\nThis Community Pipeline enables the comparison between the 4 checkpoints that exist for Stable Diffusion. They can be found through the following links:\n1. [Stable Diffusion v1.1](https://huggingface.co/CompVis/stable-diffusion-v1-1)\n2. [Stable Diffusion v1.2](https://huggingface.co/CompVis/stable-diffusion-v1-2)\n3. [Stable Diffusion v1.3](https://huggingface.co/CompVis/stable-diffusion-v1-3)\n4. [Stable Diffusion v1.4](https://huggingface.co/CompVis/stable-diffusion-v1-4)\n\n```python\nfrom diffusers import DiffusionPipeline\nimport matplotlib.pyplot as plt\n\npipe = DiffusionPipeline.from_pretrained('CompVis/stable-diffusion-v1-4', custom_pipeline='suvadityamuk/StableDiffusionComparison')\npipe.enable_attention_slicing()\npipe = pipe.to('cuda')\nprompt = \"an astronaut riding a horse on mars\"\noutput = pipe(prompt)\n\nplt.subplots(2,2,1)\nplt.imshow(output.images[0])\nplt.title('Stable Diffusion v1.1')\nplt.axis('off')\nplt.subplots(2,2,2)\nplt.imshow(output.images[1])\nplt.title('Stable Diffusion v1.2')\nplt.axis('off')\nplt.subplots(2,2,3)\nplt.imshow(output.images[2])\nplt.title('Stable Diffusion v1.3')\nplt.axis('off')\nplt.subplots(2,2,4)\nplt.imshow(output.images[3])\nplt.title('Stable Diffusion v1.4')\nplt.axis('off')\n\nplt.show()\n```", "!--Copyright 2023 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\n\nhttp://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License.\n-->\n\n# Image variation\n\nThe Stable Diffusion model can also generate variations from an input image. It uses a fine-tuned version of a Stable Diffusion model by [Justin Pinkney](https://www.justinpinkney.com/) from [Lambda](https://lambdalabs.com/).\n\nThe original codebase can be found at [LambdaLabsML/lambda-diffusers](https://github.com/LambdaLabsML/lambda-diffusers#stable-diffusion-image-variations) and additional official checkpoints for image variation can be found at [lambdalabs/sd-image-variations-diffusers](https://huggingface.co/lambdalabs/sd-image-variations-diffusers).\n\n<Tip>\n\nMake sure to check out the Stable Diffusion [Tips](./overview#tips) section to learn how to explore the tradeoff between scheduler speed and quality, and how to reuse pipeline components efficiently!\n\n</Tip>\n\n## StableDiffusionImageVariationPipeline\n\n[[autodoc]] StableDiffusionImageVariationPipeline\n\t- all\n\t- __call__\n\t- enable_attention_slicing\n\t- disable_attention_slicing\n\t- enable_xformers_memory_efficient_attention\n\t- disable_xformers_memory_efficient_attention\n\n## StableDiffusionPipelineOutput\n\n[[autodoc]] pipelines.stable_diffusion.StableDiffusionPipelineOutput"], "eval_score_GPT4": "5", "eval_feedback_GPT4": "The response provides the correct link to the full code for the Stable Diffusion demo as requested in the instruction. It matches the reference answer and includes additional useful information about where to find the original codebase for different versions of Stable Diffusion. The response is completely correct, accurate, and factual."}, {"question": "What transformation does the FNet model use to replace the self-attention layer in a BERT model?\n", "true_answer": "Fourier transform", "source_doc": "huggingface/transformers/blob/main/docs/source/en/model_doc/fnet.md", "generated_answer": "The FNet model replaces the self-attention sublayer in a Transformer encoder with a simple linear transformation called \"linear mixers\" that mix input tokens, along with standard nonlinearities in feed-forward layers. Additionally, the FNet model uses a standard, unparameterized Fourier Transform to replace the self-attention sublayer, which achieves 92-97% of the accuracy of BERT counterparts on the GLUE benchmark but trains", "retrieved_docs": ["*We show that Transformer encoder architectures can be sped up, with limited accuracy costs, by replacing the\nself-attention sublayers with simple linear transformations that \"mix\" input tokens. These linear mixers, along with\nstandard nonlinearities in feed-forward layers, prove competent at modeling semantic relationships in several text\nclassification tasks. Most surprisingly, we find that replacing the self-attention sublayer in a Transformer encoder\nwith a standard, unparameterized Fourier Transform achieves 92-97% of the accuracy of BERT counterparts on the GLUE\nbenchmark, but trains 80% faster on GPUs and 70% faster on TPUs at standard 512 input lengths. At longer input lengths,\nour FNet model is significantly faster: when compared to the \"efficient\" Transformers on the Long Range Arena\nbenchmark, FNet matches the accuracy of the most accurate models, while outpacing the fastest models across all\nsequence lengths on GPUs (and across relatively shorter lengths on TPUs). Finally, FNet has a light memory footprint\nand is particularly efficient at smaller model sizes; for a fixed speed and accuracy budget, small FNet models\noutperform Transformer counterparts.*\n\nThis model was contributed by [gchhablani](https://huggingface.co/gchhablani). The original code can be found [here](https://github.com/google-research/google-research/tree/master/f_net).\n\n## Usage tips\n\nThe model was trained without an attention mask as it is based on Fourier Transform. The model was trained with \nmaximum sequence length 512 which includes pad tokens. Hence, it is highly recommended to use the same maximum \nsequence length for fine-tuning and inference.\n\n## Resources\n\n- [Text classification task guide](../tasks/sequence_classification)\n- [Token classification task guide](../tasks/token_classification)\n- [Question answering task guide](../tasks/question_answering)\n- [Masked language modeling task guide](../tasks/masked_language_modeling)\n- [Multiple choice task guide](../tasks/multiple_choice)\n\n## FNetConfig\n\n[[autodoc]] FNetConfig\n\n## FNetTokenizer", "!--Copyright 2021 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\n\nhttp://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License.\n\n\u26a0\ufe0f Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\nrendered properly in your Markdown viewer.\n\n-->\n\n# FNet\n\n## Overview\n\nThe FNet model was proposed in [FNet: Mixing Tokens with Fourier Transforms](https://arxiv.org/abs/2105.03824) by\nJames Lee-Thorp, Joshua Ainslie, Ilya Eckstein, Santiago Ontanon. The model replaces the self-attention layer in a BERT\nmodel with a fourier transform which returns only the real parts of the transform. The model is significantly faster\nthan the BERT model because it has fewer parameters and is more memory efficient. The model achieves about 92-97%\naccuracy of BERT counterparts on GLUE benchmark, and trains much faster than the BERT model. The abstract from the\npaper is the following:", "# Load the pre-trained model\ntransformer_model = AutoModelForSequenceClassification.from_pretrained(\n   \"cardiffnlp/twitter-roberta-base-sentiment-latest\"\n)\n```\n\nThis should download the model, which is now ready to be used.\n\nUsing the hidden representation for some text can be tricky at first, mainly because we could tackle this with many different approaches. Below is the approach we chose.\n\nFirst, we tokenize the text. Tokenizing means splitting the text into tokens (a sequence of specific characters that can also be words) and replacing each with a number. Then, we send the tokenized text to the transformer model, which outputs a hidden representation (output of the self attention layers which are often used as input to the classification layers) for each word. Finally, we average the representations for each word to get a text-level representation.\n\nThe result is a matrix of shape (number of examples, hidden size). The hidden size is the number of dimensions in the hidden representation. For BERT, the hidden size is 768. The hidden representation is a vector of numbers that represents the text that can be used for many different tasks. In this case, we will use it for classification with [XGBoost](https://github.com/dmlc/xgboost) afterwards.\n\n```python\nimport numpy as np\nimport tqdm\n# Function that transforms a list of texts to their representation\n# learned by the transformer.\ndef text_to_tensor(\n   list_text_X_train: list,\n   transformer_model: AutoModelForSequenceClassification,\n   tokenizer: AutoTokenizer,\n   device: str,\n) -> np.ndarray:\n   # Tokenize each text in the list one by one\n   tokenized_text_X_train_split = []\n   tokenized_text_X_train_split = [\n       tokenizer.encode(text_x_train, return_tensors=\"pt\")\n       for text_x_train in list_text_X_train\n   ]", "2.  Second, BERT\\'s *bi-directional* self-attention layers have to be\n    changed to *uni-directional* self-attention layers to comply with\n    auto-regressive generation. Because both the bi-directional and the\n    uni-directional self-attention layer are based on the same *key*,\n    *query* and *value* projection weights, the decoder\\'s\n    self-attention layer weights can be initialized with BERT\\'s\n    self-attention layer weights. *E.g.* the query, key and value weight\n    parameters of the decoder\\'s uni-directional self-attention layer\n    are initialized with those of BERT\\'s bi-directional self-attention\n    layer \\\\(\\theta_{\\text{BERT}}^{\\text{self-attn}, 2} = \\{\\mathbf{W}_{\\text{BERT}, k}^{\\text{self-attn}, 2}, \\mathbf{W}_{\\text{BERT}, v}^{\\text{self-attn}, 2}, \\mathbf{W}_{\\text{BERT}, q}^{\\text{self-attn}, 2} \\} \\to \\theta_{\\text{dec}}^{\\text{self-attn}, 2} = \\{\\mathbf{W}_{\\text{dec}, k}^{\\text{self-attn}, 2}, \\mathbf{W}_{\\text{dec}, v}^{\\text{self-attn}, 2}, \\mathbf{W}_{\\text{dec}, q}^{\\text{self-attn}, 2} \\}. \\\\) However, in *uni-directional* self-attention each token only\n    attends to all previous tokens, so that the decoder\\'s\n    self-attention layers yield different output vectors than BERT\\'s\n    self-attention layers even though they share the same weights.\n    Compare *e.g.*, the decoder\\'s causally connected graph in the right\n    box versus BERT\\'s fully connected graph in the left box.", "### Encoder[[nlp-encoder]]\n\n[BERT](model_doc/bert) is an encoder-only Transformer that randomly masks certain tokens in the input to avoid seeing other tokens, which would allow it to \"cheat\". The pretraining objective is to predict the masked token based on the context. This allows BERT to fully use the left and right contexts to help it learn a deeper and richer representation of the inputs. However, there was still room for improvement in BERT's pretraining strategy. [RoBERTa](model_doc/roberta) improved upon this by introducing a new pretraining recipe that includes training for longer and on larger batches, randomly masking tokens at each epoch instead of just once during preprocessing, and removing the next-sentence prediction objective. \n\nThe dominant strategy to improve performance is to increase the model size. But training large models is computationally expensive. One way to reduce computational costs is using a smaller model like [DistilBERT](model_doc/distilbert). DistilBERT uses [knowledge distillation](https://arxiv.org/abs/1503.02531) - a compression technique - to create a smaller version of BERT while keeping nearly all of its language understanding capabilities. \n\nHowever, most Transformer models continued to trend towards more parameters, leading to new models focused on improving training efficiency. [ALBERT](model_doc/albert) reduces memory consumption by lowering the number of parameters in two ways: separating the larger vocabulary embedding into two smaller matrices and allowing layers to share parameters. [DeBERTa](model_doc/deberta) added a disentangled attention mechanism where the word and its position are separately encoded in two vectors. The attention is computed from these separate vectors instead of a single vector containing the word and position embeddings. [Longformer](model_doc/longformer) also focused on making attention more efficient, especially for processing documents with longer sequence lengths. It uses a combination of local windowed attention (attention only calculated from fixed window size around each token) and global attention (only for specific task tokens like `[CLS]` for classification) to create a sparse attention matrix instead of a full attention matrix.\n\n### Decoder[[nlp-decoder]]", "Let's dive in this representation. It contains one vector per word that was passed through the encoder. Each of these vector is a numerical representation of the word in question. The dimension of that vector is defined by the architecture of the model, for the base BERT model, it is 768. These representations contain the value of a word; but contextualized. For example, the vector attributed to the word \"to\", isn't the representation of only the \"to\" word. It also takes into account the words around it, which we call the \u201ccontext\u201d.As in, it looks to the left context, the word on the left of the one we're studying (here the word \"Welcome\") and the context on the right (here the word \"NYC\") and outputs a value for the word, within its context. It is therefore a contextualized value. One could say that the vector of 768 values holds the \"meaning\" of that word in the text. How it does this is thanks to the self-attention mechanism. The self-attention mechanism relates to different positions (or different words) in a single sequence, in order to compute a representation of that sequence. As we've seen before, this means that the resulting representation of a word has been affected by other words in the sequence. We won't dive into the specifics here, but we'll offer some further readings if you want to get a better understanding at what happens under the hood. So when should one use an encoder? Encoders can be used as standalone models in a wide variety of tasks. For example BERT, arguably the most famous transformer model, is a standalone encoder model and at the time of release, beat the state of the art in many sequence classification tasks, question answering tasks, and masked language modeling, to only cite a few. The idea is that encoders are very powerful at extracting vectors that carry meaningful information about a sequence. This vector can then be handled down the road by additional layers of neurons to make sense of them", "### 3. Machine Learning is evolving at a neck-breaking speed\nResearch in the field of machine learning, and especially neural networks, evolves extremely fast. A model that was state-of-the-art a year ago might be outdated today. We don't know which attention mechanism, position embedding, or architecture will be the best in a year. Therefore, we cannot define standard logical patterns that apply to all models. \n\nAs an example, two years ago, one might have defined BERT's self attention layer as the standard attention layer used by all Transformers models. Logically, a \"standard\" attention function could have been moved into a central `attention.py` file. But then came attention layers that added relative positional embeddings in each attention layer (T5), multiple different forms of chunked attention (Reformer, Longformer, BigBird), and separate attention mechanism for position and word embeddings (DeBERTa), etc... Every time we would have to have asked ourselves whether the \"standard\" attention function should be adapted or whether it would have been better to add a new attention function to `attention.py`. But then how do we name it? `attention_with_positional_embd`, `reformer_attention`, `deberta_attention`? \n\nIt's dangerous to give logical components of machine learning models general names because the perception of what this component stands for might change or become outdated very quickly. E.g., does chunked attention corresponds to GPTNeo's, Reformer's, or BigBird's chunked attention? Is the attention layer a self-attention layer, a cross-attentional layer, or does it include both? However, if we name attention layers by their model's name, we should directly put the attention function in the corresponding modeling file.\n\n### 4. Machine Learning models are static\nThe Transformers library is a unified and polished collection of machine learning models that different research teams have created. Every machine learning model is usually accompanied by a paper and its official GitHub repository. Once a machine learning model is published, it is rarely adapted or changed afterward."], "eval_score_GPT4": "5", "eval_feedback_GPT4": "The response correctly identifies that the FNet model uses a Fourier Transform to replace the self-attention sublayer in a Transformer encoder, which aligns with the reference answer. However, the response includes additional information about \"linear mixers\" and performance metrics on the GLUE benchmark, which are not part of the reference answer but do not detract from the correctness of the response regarding the Fourier Transform. Based on the score rubric, the response is completely correct, accurate, and factual in the context of the question asked."}, {"question": "What type of test should typically accompany a bug fix in Gradio's testing strategy?\n", "true_answer": "Dynamic code test", "source_doc": "gradio-app/gradio/blob/main/test-strategy.md", "generated_answer": "The context provided does not explicitly state what type of test should typically accompany a bug fix in Gradio's testing strategy. However, based on industry best practices and Gradio's previous testing strategies, it is likely that regression tests should accompany a bug fix to ensure that the fix does not introduce any new issues or regressions in existing functionality. Gradio's testing strategy may also include unit tests, integration tests, and end-to-end tests, depending on the nature of", "retrieved_docs": ["### Bug Fixes:\n\n- Updated the minimum FastApi used in tests to version 0.87 by [@freddyaboulton](https://github.com/freddyaboulton) in [PR 2647](https://github.com/gradio-app/gradio/pull/2647)\n- Fixed bug where interfaces with examples could not be loaded with `gr.Interface.load` by [@freddyaboulton](https://github.com/freddyaboulton) [PR 2640](https://github.com/gradio-app/gradio/pull/2640)\n- Fixed bug where the `interactive` property of a component could not be updated by [@freddyaboulton](https://github.com/freddyaboulton) in [PR 2639](https://github.com/gradio-app/gradio/pull/2639)\n- Fixed bug where some URLs were not being recognized as valid URLs and thus were not\n  loading correctly in various components by [@abidlabs](https://github.com/abidlabs) in [PR 2659](https://github.com/gradio-app/gradio/pull/2659)\n\n### Documentation Changes:\n\n- Fix some typos in the embedded demo names in \"05_using_blocks_like_functions.md\" by [@freddyaboulton](https://github.com/freddyaboulton) in [PR 2656](https://github.com/gradio-app/gradio/pull/2656)\n\n### Testing and Infrastructure Changes:\n\nNo changes to highlight.\n\n### Breaking Changes:\n\nNo changes to highlight.\n\n### Full Changelog:", "### Bug Fixes:\n\n- Updated the minimum FastApi used in tests to version 0.87 by [@freddyaboulton](https://github.com/freddyaboulton) in [PR 2647](https://github.com/gradio-app/gradio/pull/2647)\n- Fixed bug where interfaces with examples could not be loaded with `gr.Interface.load` by [@freddyaboulton](https://github.com/freddyaboulton) [PR 2640](https://github.com/gradio-app/gradio/pull/2640)\n- Fixed bug where the `interactive` property of a component could not be updated by [@freddyaboulton](https://github.com/freddyaboulton) in [PR 2639](https://github.com/gradio-app/gradio/pull/2639)\n- Fixed bug where some URLs were not being recognized as valid URLs and thus were not\n  loading correctly in various components by [@abidlabs](https://github.com/abidlabs) in [PR 2659](https://github.com/gradio-app/gradio/pull/2659)\n\n### Documentation Changes:\n\n- Fix some typos in the embedded demo names in \"05_using_blocks_like_functions.md\" by [@freddyaboulton](https://github.com/freddyaboulton) in [PR 2656](https://github.com/gradio-app/gradio/pull/2656)\n\n### Testing and Infrastructure Changes:\n\nNo changes to highlight.\n\n### Breaking Changes:\n\nNo changes to highlight.\n\n### Full Changelog:", "### Fixes\n\n- [#6046](https://github.com/gradio-app/gradio/pull/6046) [`dbb7de5e0`](https://github.com/gradio-app/gradio/commit/dbb7de5e02c53fee05889d696d764d212cb96c74) - fix tests. Thanks [@pngwn](https://github.com/pngwn)!\n\n## 0.3.0-beta.6\n\n### Features", "### Fixes\n\n- [#6046](https://github.com/gradio-app/gradio/pull/6046) [`dbb7de5e0`](https://github.com/gradio-app/gradio/commit/dbb7de5e02c53fee05889d696d764d212cb96c74) - fix tests.  Thanks [@pngwn](https://github.com/pngwn)!\n\n## 0.1.0-beta.6\n\n### Features", "### Fixes\n\n- [#6046](https://github.com/gradio-app/gradio/pull/6046) [`dbb7de5e0`](https://github.com/gradio-app/gradio/commit/dbb7de5e02c53fee05889d696d764d212cb96c74) - fix tests. Thanks [@pngwn](https://github.com/pngwn)!\n\n## 0.1.0-beta.6\n\n### Features\n\n- [#5960](https://github.com/gradio-app/gradio/pull/5960) [`319c30f3f`](https://github.com/gradio-app/gradio/commit/319c30f3fccf23bfe1da6c9b132a6a99d59652f7) - rererefactor frontend files. Thanks [@pngwn](https://github.com/pngwn)!\n- [#5938](https://github.com/gradio-app/gradio/pull/5938) [`13ed8a485`](https://github.com/gradio-app/gradio/commit/13ed8a485d5e31d7d75af87fe8654b661edcca93) - V4: Use beta release versions for '@gradio' packages. Thanks [@freddyaboulton](https://github.com/freddyaboulton)!\n\n## 0.0.7\n\n### Patch Changes\n\n- Updated dependencies []:\n  - @gradio/utils@0.1.2\n\n## 0.0.6\n\n### Features", "### Fixes\n\n- [#6046](https://github.com/gradio-app/gradio/pull/6046) [`dbb7de5e0`](https://github.com/gradio-app/gradio/commit/dbb7de5e02c53fee05889d696d764d212cb96c74) - fix tests. Thanks [@pngwn](https://github.com/pngwn)!\n\n## 0.4.0-beta.6\n\n### Features\n\n- [#5960](https://github.com/gradio-app/gradio/pull/5960) [`319c30f3f`](https://github.com/gradio-app/gradio/commit/319c30f3fccf23bfe1da6c9b132a6a99d59652f7) - rererefactor frontend files. Thanks [@pngwn](https://github.com/pngwn)!\n- [#5938](https://github.com/gradio-app/gradio/pull/5938) [`13ed8a485`](https://github.com/gradio-app/gradio/commit/13ed8a485d5e31d7d75af87fe8654b661edcca93) - V4: Use beta release versions for '@gradio' packages. Thanks [@freddyaboulton](https://github.com/freddyaboulton)!\n\n## 0.4.3\n\n### Patch Changes", "### Fixes\n\n- [#6046](https://github.com/gradio-app/gradio/pull/6046) [`dbb7de5e0`](https://github.com/gradio-app/gradio/commit/dbb7de5e02c53fee05889d696d764d212cb96c74) - fix tests. Thanks [@pngwn](https://github.com/pngwn)!\n\n## 0.1.0-beta.5\n\n### Features\n\n- [#5960](https://github.com/gradio-app/gradio/pull/5960) [`319c30f3f`](https://github.com/gradio-app/gradio/commit/319c30f3fccf23bfe1da6c9b132a6a99d59652f7) - rererefactor frontend files. Thanks [@pngwn](https://github.com/pngwn)!\n- [#5938](https://github.com/gradio-app/gradio/pull/5938) [`13ed8a485`](https://github.com/gradio-app/gradio/commit/13ed8a485d5e31d7d75af87fe8654b661edcca93) - V4: Use beta release versions for '@gradio' packages. Thanks [@freddyaboulton](https://github.com/freddyaboulton)!\n\n## 0.0.8\n\n### Patch Changes"], "eval_score_GPT4": "2", "eval_feedback_GPT4": "Feedback: The response provided does not mention \"dynamic code test,\" which is the correct answer according to the reference answer. Instead, it suggests regression tests and other types of tests such as unit tests, integration tests, and end-to-end tests, which are not specified in the reference answer. Therefore, the response is mostly incorrect and not factual based on the reference answer provided."}, {"question": "How can you force mixed precision training when initializing the Accelerator in \ud83e\udd17 Accelerate?\n", "true_answer": "By passing `fp16=True` to the Accelerator init.", "source_doc": "huggingface/blog/blob/main/accelerate-library.md", "generated_answer": "To force mixed precision training when initializing the Accelerator in \ud83e\udd17 Accelerate, you can set the `fp16` flag to `True` in the `Accelerator` initialization, as shown below:\n\n```python\nfrom accelerate import Accelerator\n\naccelerator = Accelerator(fp16=True)\n```\n\nThis will enable mixed precision training using 16-bit floating-point numbers", "retrieved_docs": ["Then we will need an optimizer. As usual we use the classic `AdamW`, which is like Adam, but with a fix in the way weight decay is applied:\n\n```py\nfrom torch.optim import AdamW\n\noptimizer = AdamW(model.parameters(), lr=2e-5)\n```\n\nOnce we have all those objects, we can send them to the `accelerator.prepare()` method. Remember that if you want to train on TPUs in a Colab notebook, you will need to move all of this code into a training function, and that shouldn't execute any cell that instantiates an `Accelerator`. We can force mixed-precision training by passing `fp16=True` to the `Accelerator` (or, if you are executing the code as a script, just make sure to fill in the \ud83e\udd17 Accelerate `config` appropriately).\n\n```py\nfrom accelerate import Accelerator\n\naccelerator = Accelerator(fp16=True)\nmodel, optimizer, train_dataloader, eval_dataloader = accelerator.prepare(\n    model, optimizer, train_dataloader, eval_dataloader\n)\n```\n\nAs you should know from the previous sections, we can only use the `train_dataloader` length to compute the number of training steps after it has gone through the `accelerator.prepare()` method. We use the same linear schedule as in the previous sections:\n\n```py\nfrom transformers import get_scheduler\n\nnum_train_epochs = 3\nnum_update_steps_per_epoch = len(train_dataloader)\nnum_training_steps = num_train_epochs * num_update_steps_per_epoch\n\nlr_scheduler = get_scheduler(\n    \"linear\",\n    optimizer=optimizer,\n    num_warmup_steps=0,\n    num_training_steps=num_training_steps,\n)\n```", "The full example training loop with \ud83e\udd17 Accelerate is only a handful of lines of code long:\n\n```py\nfrom accelerate import Accelerator\nfrom torch.utils.data.dataloader import DataLoader\n\ndataloader = DataLoader(ds, batch_size=training_args.per_device_train_batch_size)\n\nif training_args.gradient_checkpointing:\n    model.gradient_checkpointing_enable()\n\naccelerator = Accelerator(fp16=training_args.fp16)\nmodel, optimizer, dataloader = accelerator.prepare(model, adam_bnb_optim, dataloader)\n\nmodel.train()\nfor step, batch in enumerate(dataloader, start=1):\n    loss = model(**batch).loss\n    loss = loss / training_args.gradient_accumulation_steps\n    accelerator.backward(loss)\n    if step % training_args.gradient_accumulation_steps == 0:\n        optimizer.step()\n        optimizer.zero_grad()\n```\n\nFirst we wrap the dataset in a [`DataLoader`](https://pytorch.org/docs/stable/data.html#torch.utils.data.DataLoader). \nThen we can enable gradient checkpointing by calling the model's [`~PreTrainedModel.gradient_checkpointing_enable`] method. \nWhen we initialize the [`Accelerator`](https://huggingface.co/docs/accelerate/package_reference/accelerator#accelerate.Accelerator) \nwe can specify if we want to use mixed precision training and it will take care of it for us in the [`prepare`] call. \nDuring the [`prepare`](https://huggingface.co/docs/accelerate/package_reference/accelerator#accelerate.Accelerator.prepare) \ncall the dataloader will also be distributed across workers should we use multiple GPUs. We use the same [8-bit optimizer](#8-bit-adam) from the earlier example.", "upercharge your Pytorch training loop with Hugging Face Accelerate. There are multiple setups on which you can run your training: it could be on CPU, GPUs, TPUs. Distributed on one machine with several devices, or several machines (often called nodes) each with multiple devices. On top of that there are new tweaks to make your training faster or more memory efficient, like mixed precision and DeepSpeed. Each of those setups or training tweaks, requires you to change the code of your training loop in one way or another and to learn a new API. All those setups are handled by the Trainer API, and there are several third-party libraries that can also help you with that. The problem with those is that they can feel like a black box and that it might not be easy to implement the tweak to the training loop you need. Accelerate has been designed specifically to let you retain full control over your training loop and be as non-intrusive as possible. With just four lines to add to your training loop (here shown on the code of the training loop from the \"Raw training loop\" video), Accelerate will handle all the setups and training tweaks mentioned on the first slide. It's only one API to learn and master instead of ten different ones. More specifically, you have to import and instantiate an accelerator object, that will handle all the necessary code for your  specific setup. Then you have to send it the model, optimizer and dataloaders you are using in the prepare method, which is the main method to remember. Accelerate handles device placement, so you don't need to put your batch on the specific device you are using. Finally, you have to replace the loss.backward line by accelerate.backward(loss), and that's all you need! Accelerate also handles distributed evaluation. You can still use a classic evaluation loop such as the one we saw in the \"Raw training loop\" video, in which case all processes will each perform the full evaluation", "### fp16\n\nThe main advantage of mixed precision training comes from saving the activations in half precision (fp16). \nAlthough the gradients are also computed in half precision they are converted back to full precision for the optimization \nstep so no memory is saved here. \nWhile mixed precision training results in faster computations, it can also lead to more GPU memory being utilized, especially for small batch sizes.\nThis is because the model is now present on the GPU in both 16-bit and 32-bit precision (1.5x the original model on the GPU).\n\nTo enable mixed precision training, set the `fp16` flag to `True`:\n\n```py\ntraining_args = TrainingArguments(per_device_train_batch_size=4, fp16=True, **default_args)\n```\n\nIf you prefer to use \ud83e\udd17 Accelerate, find the \ud83e\udd17 Accelerate example [further in this guide](#using-accelerate). \n\n### BF16\n\nIf you have access to an Ampere or newer hardware you can use bf16 for mixed precision training and evaluation. While \nbf16 has a worse precision than fp16, it has a much bigger dynamic range. In fp16 the biggest number you can have \nis `65535` and any number above that will result in an overflow. A bf16 number can be as large as `3.39e+38` (!) which \nis about the same as fp32 - because both have 8-bits used for the numerical range.\n\nYou can enable BF16 in the \ud83e\udd17 Trainer with:\n\n```python\ntraining_args = TrainingArguments(bf16=True, **default_args)\n```\n\n### TF32", "For example, to speedup training with mixed precision using the bf16 format, add the `--mixed_precision` parameter to the training command:\n\n```bash\naccelerate launch train_unconditional.py \\\n  --mixed_precision=\"bf16\"\n```\n\nSome basic and important parameters to specify include:\n\n- `--dataset_name`: the name of the dataset on the Hub or a local path to the dataset to train on\n- `--output_dir`: where to save the trained model\n- `--push_to_hub`: whether to push the trained model to the Hub\n- `--checkpointing_steps`: frequency of saving a checkpoint as the model trains; this is useful if training is interrupted, you can continue training from that checkpoint by adding `--resume_from_checkpoint` to your training command\n\nBring your dataset, and let the training script handle everything else!\n\n## Training script\n\nThe code for preprocessing the dataset and the training loop is found in the [`main()`](https://github.com/huggingface/diffusers/blob/096f84b05f9514fae9f185cbec0a4d38fbad9919/examples/unconditional_image_generation/train_unconditional.py#L275) function. If you need to adapt the training script, this is where you'll need to make your changes.\n\nThe `train_unconditional` script [initializes a `UNet2DModel`](https://github.com/huggingface/diffusers/blob/096f84b05f9514fae9f185cbec0a4d38fbad9919/examples/unconditional_image_generation/train_unconditional.py#L356) if you don't provide a model configuration. You can configure the UNet here if you'd like:", "```bash\ncd examples/t2i_adapter\npip install -r requirements.txt\n```\n\n<Tip>\n\n\ud83e\udd17 Accelerate is a library for helping you train on multiple GPUs/TPUs or with mixed-precision. It'll automatically configure your training setup based on your hardware and environment. Take a look at the \ud83e\udd17 Accelerate [Quick tour](https://huggingface.co/docs/accelerate/quicktour) to learn more.\n\n</Tip>\n\nInitialize an \ud83e\udd17 Accelerate environment:\n\n```bash\naccelerate config\n```\n\nTo setup a default \ud83e\udd17 Accelerate environment without choosing any configurations:\n\n```bash\naccelerate config default\n```\n\nOr if your environment doesn't support an interactive shell, like a notebook, you can use:\n\n```bash\nfrom accelerate.utils import write_basic_config\n\nwrite_basic_config()\n```\n\nLastly, if you want to train a model on your own dataset, take a look at the [Create a dataset for training](create_dataset) guide to learn how to create a dataset that works with the training script.\n\n<Tip>\n\nThe following sections highlight parts of the training script that are important for understanding how to modify it, but it doesn't cover every aspect of the script in detail. If you're interested in learning more, feel free to read through the [script](https://github.com/huggingface/diffusers/blob/main/examples/t2i_adapter/train_t2i_adapter_sdxl.py) and let us know if you have any questions or concerns.\n\n</Tip>\n\n## Script parameters", "```bash\ncd examples/instruct_pix2pix\npip install -r requirements.txt\n```\n\n<Tip>\n\n\ud83e\udd17 Accelerate is a library for helping you train on multiple GPUs/TPUs or with mixed-precision. It'll automatically configure your training setup based on your hardware and environment. Take a look at the \ud83e\udd17 Accelerate [Quick tour](https://huggingface.co/docs/accelerate/quicktour) to learn more.\n\n</Tip>\n\nInitialize an \ud83e\udd17 Accelerate environment:\n\n```bash\naccelerate config\n```\n\nTo setup a default \ud83e\udd17 Accelerate environment without choosing any configurations:\n\n```bash\naccelerate config default\n```\n\nOr if your environment doesn't support an interactive shell, like a notebook, you can use:\n\n```bash\nfrom accelerate.utils import write_basic_config\n\nwrite_basic_config()\n```\n\nLastly, if you want to train a model on your own dataset, take a look at the [Create a dataset for training](create_dataset) guide to learn how to create a dataset that works with the training script.\n\n<Tip>\n\nThe following sections highlight parts of the training script that are important for understanding how to modify it, but it doesn't cover every aspect of the script in detail. If you're interested in learning more, feel free to read through the [script](https://github.com/huggingface/diffusers/blob/main/examples/instruct_pix2pix/train_instruct_pix2pix.py) and let us know if you have any questions or concerns.\n\n</Tip>\n\n## Script parameters"], "eval_score_GPT4": "5", "eval_feedback_GPT4": "The response provided is completely correct, accurate, and factual. It aligns with the reference answer by indicating that the `fp16` flag should be set to `True` in the `Accelerator` initialization to enable mixed precision training. The response also includes an example code snippet that correctly demonstrates how to initialize the Accelerator with mixed precision, which further supports the accuracy and factuality of the response."}, {"question": "What is the range of parameters for the LLaMA models?\n", "true_answer": "7B to 65B parameters", "source_doc": "huggingface/transformers/blob/main/docs/source/en/model_doc/llama.md", "generated_answer": "The LLaMA model family proposed in [LLaMA: Open and Efficient Foundation Language Models](https://arxiv.org/abs/2302.13971) by Hugo Touvron et al. Consists of foundation language models ranging from 7B to 65B parameters. Specifically, the paper introduces three model sizes: 7B, 13B, and 65B parameters. The biggest model", "retrieved_docs": ["## Why Llama 2?\n\nThe Llama 2 release introduces a family of pretrained and fine-tuned LLMs, ranging in scale from 7B to 70B parameters (7B, 13B, 70B). The pretrained models come with significant improvements over the Llama 1 models, including being trained on 40% more tokens, having a much longer context length (4k tokens \ud83e\udd2f), and using grouped-query attention for fast inference of the 70B model\ud83d\udd25!\n\nHowever, the most exciting part of this release is the fine-tuned models (Llama 2-Chat), which have been optimized for dialogue applications using [Reinforcement Learning from Human Feedback (RLHF)](https://huggingface.co/blog/rlhf). Across a wide range of helpfulness and safety benchmarks, the Llama 2-Chat models perform better than most open models and achieve comparable performance to ChatGPT according to human evaluations. You can read the paper [here](https://huggingface.co/papers/2307.09288).\n\n![mqa](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/llama-rlhf.png)\n\n_image from [Llama 2: Open Foundation and Fine-Tuned Chat Models](https://scontent-fra3-2.xx.fbcdn.net/v/t39.2365-6/10000000_6495670187160042_4742060979571156424_n.pdf?_nc_cat=104&ccb=1-7&_nc_sid=3c67a6&_nc_ohc=GK8Rh1tm_4IAX8b5yo4&_nc_ht=scontent-fra3-2.xx&oh=00_AfDtg_PRrV6tpy9UmiikeMRuQgk6Rej7bCPOkXZQVmUKAg&oe=64BBD830)_", "### What is Llama 2?\n\nLlama 2 is a family of LLMs from Meta, trained on 2 trillion tokens. Llama 2 comes in three sizes - 7B, 13B, and 70B parameters - and introduces key improvements like longer context length, commercial licensing, and optimized chat abilities through reinforcement learning compared to Llama (1). If you want to learn more about Llama 2 check out this [blog post](https://huggingface.co/blog/llama2). \n\n### What is GPTQ?\n\nGPTQ is a post-training quantziation method to compress LLMs, like GPT. GPTQ compresses GPT (decoder) models by reducing the number of bits needed to store each weight in the model, from 32 bits down to just 3-4 bits. This means the model takes up much less memory and can run on less Hardware, e.g. Single GPU for 13B Llama2 models. GPTQ analyzes each layer of the model separately and approximates the weights to preserve the overall accuracy. If you want to learn more and how to use it, check out [Optimize open LLMs using GPTQ and Hugging Face Optimum](https://www.philschmid.de/gptq-llama).\n\n## Benchmark\n\nTo benchmark the real-world performance of Llama 2, we tested 3 model sizes (7B, 13B, 70B parameters) on four different instance types with four different load levels, resulting in 60 different configurations:\n\n- Models: We evaluated all currently available model sizes, including 7B, 13B, and 70B.\n- Concurrent Requests: We tested configurations with 1, 5, 10, and 20 concurrent requests to determine the performance on different usage scenarios.\n- Instance Types: We evaluated different GPU instances, including g5.2xlarge, g5.12xlarge, g5.48xlarge powered by NVIDIA A10G GPUs, and p4d.24xlarge powered by NVIDIA A100 40GB GPU.\n- Quantization: We compared performance with and without quantization. We used GPTQ 4-bit as a quantization technique.", "The first model family in this series was the [LLaMA](https://huggingface.co/papers/2302.13971) family, released by Meta AI. The explicit objective of the researchers was to train a set of models of various sizes with the best possible performances for a given computing budget. For one of the first times, the research team explicitly decided to consider not only the training budget but also the inference cost (for a given performance objective, how much does it cost to run inference with the model). In this perspective, they decided to train smaller models on even more data and for more steps than was usually done, thereby reaching higher performances at a smaller model size (the trade-off being training compute efficiency). The biggest model in the Llama 1 family is a 65B parameters model trained on 1.4T tokens, while the smaller models (resp. 6 and 13B parameters) were trained on 1T tokens. The small 13B LLaMA model outperformed GPT-3 on most benchmarks, and the biggest LLaMA model was state of the art when it came out. The weights were released with a non-commercial license though, limiting the adoption by the community. \n\nThe [Pythia](https://huggingface.co/papers/2304.01373) models were released by the open-source non-profit lab Eleuther AI, and were a [suite of LLMs](https://huggingface.co/collections/EleutherAI/pythia-scaling-suite-64fb5dfa8c21ebb3db7ad2e1) of different sizes, trained on completely public data, provided to help researchers to understand the different steps of LLM training.", "llama_model = get_peft_model(llama_model, llama_peft_config)\nllama_model.print_trainable_parameters()\n\n```\n\nThe number of trainable parameters reprents only 0.12% of the Llama 2 model parameters:\n```\ntrainable params: 8,404,992 || all params: 6,615,748,608 || trainable%: 0.1270452143516515\n```\n\nAt this point, we defined the tokenized dataset for training as well as the LLMs setup with LoRa layers. The following section will introduce how to launch training using the HuggingFace `Trainer` class. \n\n\n## Setup the trainer\n\n### Evaluation Metrics\n\nFirst, we define the performance metrics we will use to compare the three models: F1 score, recall, precision and accuracy:\n\n```python\nimport evaluate\nimport numpy as np\n\ndef compute_metrics(eval_pred):\n    # All metrics are already predefined in the HF `evaluate` package\n    precision_metric = evaluate.load(\"precision\")\n    recall_metric = evaluate.load(\"recall\")\n    f1_metric= evaluate.load(\"f1\")\n    accuracy_metric = evaluate.load(\"accuracy\")\n\n    logits, labels = eval_pred # eval_pred is the tuple of predictions and labels returned by the model\n    predictions = np.argmax(logits, axis=-1)\n    precision = precision_metric.compute(predictions=predictions, references=labels)[\"precision\"]\n    recall = recall_metric.compute(predictions=predictions, references=labels)[\"recall\"]\n    f1 = f1_metric.compute(predictions=predictions, references=labels)[\"f1\"]\n    accuracy = accuracy_metric.compute(predictions=predictions, references=labels)[\"accuracy\"]\n    # The trainer is expecting a dictionary where the keys are the metrics names and the values are the scores. \n    return {\"precision\": precision, \"recall\": recall, \"f1-score\": f1, 'accuracy': accuracy}\n```", "!--Copyright 2022 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\n\nhttp://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License.\n\n\u26a0\ufe0f Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\nrendered properly in your Markdown viewer.\n\n-->\n\n# LLaMA\n\n## Overview\n\nThe LLaMA model was proposed in [LLaMA: Open and Efficient Foundation Language Models](https://arxiv.org/abs/2302.13971) by Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timoth\u00e9e Lacroix, Baptiste Rozi\u00e8re, Naman Goyal, Eric Hambro, Faisal Azhar, Aurelien Rodriguez, Armand Joulin, Edouard Grave, Guillaume Lample. It is a collection of foundation language models ranging from 7B to 65B parameters.\n\nThe abstract from the paper is the following:\n\n*We introduce LLaMA, a collection of foundation language models ranging from 7B to 65B parameters. We train our models on trillions of tokens, and show that it is possible to train state-of-the-art models using publicly available datasets exclusively, without resorting to proprietary and inaccessible datasets. In particular, LLaMA-13B outperforms GPT-3 (175B) on most benchmarks, and LLaMA-65B is competitive with the best models, Chinchilla-70B and PaLM-540B. We release all our models to the research community. *", "## Efficient training strategies\n\nEven training the smallest LLaMA model requires an enormous amount of memory. Some quick math: in bf16, every parameter uses 2 bytes (in fp32 4 bytes) in addition to 8 bytes used, e.g., in the Adam optimizer (see the [performance docs](https://huggingface.co/docs/transformers/perf_train_gpu_one#optimizer) in Transformers for more info). So a 7B parameter model would use `(2+8)*7B=70GB` just to fit in memory and would likely need more when you compute intermediate values such as attention scores. So you couldn\u2019t train the model even on a single 80GB A100 like that. You can use some tricks, like more efficient optimizers of half-precision training, to squeeze a bit more into memory, but you\u2019ll run out sooner or later.\n\nAnother option is to use Parameter-Efficient Fine-Tuning (PEFT) techniques, such as the [`peft`](https://github.com/huggingface/peft) library, which can perform Low-Rank Adaptation (LoRA) on a model loaded in 8-bit. \n\n![](https://huggingface.co/datasets/trl-internal-testing/example-images/resolve/main/blog/stackllama/lora-animated.gif)   \n*Low-Rank Adaptation of linear layers: extra parameters (in orange) are added next to the frozen layer (in blue), and the resulting encoded hidden states are added together with the hidden states of the frozen layer.*\n\nLoading the model in 8bit reduces the memory footprint drastically since you only need one byte per parameter for the weights (e.g. 7B LlaMa is 7GB in memory). Instead of training the original weights directly, LoRA adds small adapter layers on top of some specific layers (usually the attention layers); thus, the number of trainable parameters is drastically reduced.", "*we\u2019re currently running evaluation of the Llama 2 70B (non chatty version). This table will be updated with the results.\n\n\n## Demo\n\nYou can easily try the Big Llama 2 Model (70 billion parameters!) in [this Space](https://huggingface.co/spaces/ysharma/Explore_llamav2_with_TGI) or in the playground embedded below:\n\n<script type=\"module\" src=\"https://gradio.s3-us-west-2.amazonaws.com/3.37.0/gradio.js\"> </script>\n<gradio-app theme_mode=\"light\" space=\"ysharma/Explore_llamav2_with_TGI\"></gradio-app>\n\nUnder the hood, this playground uses Hugging Face's [Text Generation Inference](https://github.com/huggingface/text-generation-inference), the same technology that powers [HuggingChat](https://huggingface.co/chat/), and which we'll share more in the following sections.\n\n\n## Inference\nIn this section, we\u2019ll go through different approaches to running inference of the Llama2 models. Before using these models, make sure you have requested access to one of the models in the official [Meta Llama 2](https://huggingface.co/meta-llama) repositories. \n\n**Note: Make sure to also fill the official Meta form. Users are provided access to the repository once both forms are filled after few hours.**\n\n### Using transformers\n\nWith transformers [release 4.31](https://github.com/huggingface/transformers/releases/tag/v4.31.0), one can already use Llama 2 and leverage all the tools within the HF ecosystem, such as:"], "eval_score_GPT4": "4", "eval_feedback_GPT4": "The response correctly identifies the range of parameters for the LLaMA models as being from 7B to 65B parameters, which is in line with the reference answer. It also provides additional information about the specific model sizes introduced in the paper, which is accurate and factual. However, the response is incomplete as it ends abruptly with \"The biggest model\" without finishing the sentence. Despite this, the main question about the range of parameters is answered correctly."}, {"question": "What is the purpose of tokenizers in the NLP pipeline?\n", "true_answer": "To translate text into data that can be processed by the model.", "source_doc": "huggingface/course/blob/main/chapters/en/chapter2/4.mdx", "generated_answer": "The purpose of tokenizers in the NLP pipeline is to convert text inputs into numerical data that can be processed by neural network models. Tokenizers serve as a core component of the NLP pipeline by translating raw text into numerical inputs for the model. Models can only process numbers, so tokenizers need to convert text inputs into tokens, which are then mapped to their respective IDs in the vocabulary of the tokenizer. This process involves several steps, including normalization, pre-token", "retrieved_docs": ["FrameworkSwitchCourse {fw} />\n\n# Tokenizers[[tokenizers]]\n\n{#if fw === 'pt'}\n\n<CourseFloatingBanner chapter={2}\n  classNames=\"absolute z-10 right-0 top-0\"\n  notebooks={[\n    {label: \"Google Colab\", value: \"https://colab.research.google.com/github/huggingface/notebooks/blob/master/course/en/chapter2/section4_pt.ipynb\"},\n    {label: \"Aws Studio\", value: \"https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/master/course/en/chapter2/section4_pt.ipynb\"},\n]} />\n\n{:else}\n\n<CourseFloatingBanner chapter={2}\n  classNames=\"absolute z-10 right-0 top-0\"\n  notebooks={[\n    {label: \"Google Colab\", value: \"https://colab.research.google.com/github/huggingface/notebooks/blob/master/course/en/chapter2/section4_tf.ipynb\"},\n    {label: \"Aws Studio\", value: \"https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/master/course/en/chapter2/section4_tf.ipynb\"},\n]} />\n\n{/if}\n\n<Youtube id=\"VFp38yj8h3A\"/>\n\nTokenizers are one of the core components of the NLP pipeline. They serve one purpose: to translate text into data that can be processed by the model. Models can only process numbers, so tokenizers need to convert our text inputs to numerical data. In this section, we'll explore exactly what happens in the tokenization pipeline. \n\nIn NLP tasks, the data that is generally processed is raw text. Here's an example of such text:", "Then we'll look at the tokenizer API, which is the other main component of the `pipeline()` function. Tokenizers take care of the first and last processing steps, handling the conversion from text to numerical inputs for the neural network, and the conversion back to text when it is needed. Finally, we'll show you how to handle sending multiple sentences through a model in a prepared batch, then wrap it all up with a closer look at the high-level `tokenizer()` function.\n\n<Tip>\n\u26a0\ufe0f In order to benefit from all features available with the Model Hub and \ud83e\udd17 Transformers, we recommend <a href=\"https://huggingface.co/join\">creating an account</a>.\n</Tip>", ". To enable this, the fast tokenizers store additional information at each step of their internal pipeline. That internal pipeline consists of normalization, where we apply some cleaning to the text, like lowercasing or removing the accents;() pre-tokenization, which is where we split the texts into words;() then we apply the model of the tokenizer, which is where the words are splits into tokens,() before finally doing the post-processing, where special tokens are added. From the beginning to the end of the pipeline, the tokenizer keeps track of each span of text that corresponds to each word, then each token. We will see how useful it is when we tackle the following tasks: when doing masked language modeling, one variation that gets state-of-the-art results is to mask all the tokens of a given word instead of randomly chosen tokens. This will require us to use the word IDs we saw. When doing token classification, we'll need to convert the labels we have on words, to labels on each tokens. As for the offset mappings, it will be super useful when we need to convert token positions in a sentence into a span of text, which we will need to know when looking at question answering or when grouping the tokens corresponding to the same entity in token classification. To have a look at these tasks, check the videos linked below!", "## What is a Tokenizer\n\nA Tokenizer works as a pipeline, it processes some raw text as input and outputs an `Encoding`.\nThe various steps of the pipeline are:\n\n1. The `Normalizer`: in charge of normalizing the text. Common examples of normalization are\n   the [unicode normalization standards](https://unicode.org/reports/tr15/#Norm_Forms), such as `NFD` or `NFKC`.\n   More details about how to use the `Normalizers` are available on the\n   [Hugging Face blog](https://huggingface.co/docs/tokenizers/components#normalizers)\n2. The `PreTokenizer`: in charge of creating initial words splits in the text. The most common way of\n   splitting text is simply on whitespace.\n3. The `Model`: in charge of doing the actual tokenization. An example of a `Model` would be\n   `BPE` or `WordPiece`.\n4. The `PostProcessor`: in charge of post-processing the `Encoding` to add anything relevant\n   that, for example, a language model would need, such as special tokens.\n\n### Loading a pretrained tokenizer from the Hub\n```rust\nuse tokenizers::tokenizer::{Result, Tokenizer};\n\nfn main() -> Result<()> {\n    # #[cfg(feature = \"http\")]\n    # {\n        let tokenizer = Tokenizer::from_pretrained(\"bert-base-cased\", None)?;\n\n        let encoding = tokenizer.encode(\"Hey there!\", false)?;\n        println!(\"{:?}\", encoding.get_tokens());\n    # }\n    Ok(())\n}\n```\n\n### Deserialization and tokenization example\n\n```rust\nuse tokenizers::tokenizer::{Result, Tokenizer, EncodeInput};\nuse tokenizers::models::bpe::BPE;", "he tokenizer pipeline. In this video, we'll look at how a tokenizer converts raw text to numbers that a Transformer model can make sense of, like when we execute this code. Here is a quick overview of what happens inside the tokenizer object: first the text is split into tokens, which are words, parts of words, or punctuation symbols. Then the tokenizer adds potential special tokens and converts each token to their unique respective ID as defined by the tokenizer's vocabulary. As we'll see it doesn't actually happen in this order, but viewing it like this is better for understanding what happens. The first step is to split our input text into tokens with the tokenize method. To do this, the tokenizer may first perform some operations like lowercasing all words, then follow a set of rules to split the result in small chunks of text. Most of the Transformers models use a subword tokenization algorithm, which means that one given word can be split in several tokens, like tokenize here. Look at the \"Tokenization algorithms\" videos linked below for more information! The ## prefix we see in front of ize is the convention used by BERT to indicate this token is not the beginning of a word. Other tokenizers may use different conventions however: for instance ALBERT tokenizers will add a long underscore in front of all the tokens that had a space before them, which is a convention used by sentencepiece tokenizers. The second step of the tokenization pipeline is to map those tokens to their respective IDs as defined by the vocabulary of the tokenizer.  This is why we need to download a file when we instantiate a tokenizer with the from_pretrained method: we have to make sure we use the same mapping as when the model was pretrained. To do this, we use the convert_tokens_to_ids method. You may have noticed that we don't have the exact same result as in our first slide \u2014 or not, as this looks like a list of random numbers, in which case allow me to refresh your memory", "The role of the model is to split your \"words\" into tokens, using the\nrules it has learned. It's also responsible for mapping those tokens to\ntheir corresponding IDs in the vocabulary of the model.\n\nThis model is passed along when intializing the\n`Tokenizer` so you already know how to\ncustomize this part. Currently, the \ud83e\udd17 Tokenizers library supports:\n\n-   `models.BPE`\n-   `models.Unigram`\n-   `models.WordLevel`\n-   `models.WordPiece`\n\nFor more details about each model and its behavior, you can check\n[here](components#models)\n\n## Post-Processing\n\nPost-processing is the last step of the tokenization pipeline, to\nperform any additional transformation to the\n`Encoding` before it's returned, like\nadding potential special tokens.\n\nAs we saw in the quick tour, we can customize the post processor of a\n`Tokenizer` by setting the\ncorresponding attribute. For instance, here is how we can post-process\nto make the inputs suitable for the BERT model:", "he tokenization pipeline involves several steps that convert raw text into numbers. In this video, we will see what happens during the pre-tokenization step. The pre-tokenization operation is the operation performed after the normalization of the text and before the application of the tokenization algorithm. This step consists in applying rules that do not need to be learned to perform a first division of the text. Let's look at how several tokenizers pre_tokenize this example. The gpt 2 pretokenization divides the text on spaces and some punctuation - but the apostrophe is not a division criterion for example. We also notice that spaces have been replaced by a capital G with a dot above. Albert's pre-tokenization divides the text at the level of spaces, adds a space at the beginning of the sentence and replaces spaces with a special underscore. Finally, Bert's pre-tokenization divides the text at the level of punctuation and spaces. Unlike the previous tokenizers, spaces are not transformed and integrated to the tokens produced with this pre-tokenizer. Through these 3 examples, we could observe the two main types of operations brought by the pre-tokenization: some changes on the text and the division of the string into tokens that can be associated to words. Finally, the \"backend_tokenizer\" of the fast tokenizers also allows to test the pre-tokenization operation very easily thanks to its \"pre_tokenize_str\" method. We notice that the output of this operation is composed of both tokens and offsets which allow to link the token to its position in the text given in input of the method. This operation defines the largest tokens that can be produced by the tokenization or in other words the barriers of the sub-tokens which will be produced then."], "eval_score_GPT4": "4", "eval_feedback_GPT4": "Feedback: The response provided is mostly correct, accurate, and factual. It explains the role of tokenizers in converting text into numerical data for processing by neural network models, which aligns with the reference answer's point about translating text into data for the model. However, the response goes into more detail than the reference answer by discussing the tokenizer's role as a core component and the process it involves, such as normalization and pre-tokenization, which were not mentioned in the reference answer. Despite the additional information, the core idea that tokenizers translate text into a form that can be processed by the model is conveyed, which is the main point of the reference answer."}, {"question": "What is the purpose of the Safety Checker in the Diffusers library?\n", "true_answer": "The Safety Checker checks and compares the class probability of a set of hard-coded harmful concepts in the embedding space against an image after it has been generated to mitigate the risk of generating harmful content.", "source_doc": "huggingface/diffusers/blob/main/docs/source/en/conceptual/ethical_guidelines.md", "generated_answer": "The purpose of the Safety Checker in the Diffusers library is to flag inappropriate content generated during inference by diffusion models. It is a component that can be incorporated into pretrained models by the model creators if they choose to do so. The Safety Checker compares the class probability of a set of harmful concepts in the embedding space against an image after it has been generated, and intentionally hides the harmful concepts to prevent reverse engineering of the checker. The Diffusers library", "retrieved_docs": ["- **Encouraging safety in deployment**\n\n  - [**Safe Stable Diffusion**](https://huggingface.co/docs/diffusers/main/en/api/pipelines/stable_diffusion/stable_diffusion_safe): It mitigates the well-known issue that models, like Stable Diffusion, that are trained on unfiltered, web-crawled datasets tend to suffer from inappropriate degeneration. Related paper: [Safe Latent Diffusion: Mitigating Inappropriate Degeneration in Diffusion Models](https://arxiv.org/abs/2211.05105).\n\n  - [**Safety Checker**](https://github.com/huggingface/diffusers/blob/main/src/diffusers/pipelines/stable_diffusion/safety_checker.py): It checks and compares the class probability of a set of hard-coded harmful concepts in the embedding space against an image after it has been generated. The harmful concepts are intentionally hidden to prevent reverse engineering of the checker.\n\n- **Staged released on the Hub**: in particularly sensitive situations, access to some repositories should be restricted. This staged release is an intermediary step that allows the repository\u2019s authors to have more control over its use.\n\n- **Licensing**: [OpenRAILs](https://huggingface.co/blog/open_rail), a new type of licensing, allow us to ensure free access while having a set of restrictions that ensure more responsible use.", "- **Transparency**: we are committed to being transparent in managing PRs, explaining our choices to users, and making technical decisions.\n\n- **Consistency**: we are committed to guaranteeing our users the same level of attention in project management, keeping it technically stable and consistent.\n\n- **Simplicity**: with a desire to make it easy to use and exploit the Diffusers library, we are committed to keeping the project\u2019s goals lean and coherent.\n\n- **Accessibility**: the Diffusers project helps lower the entry bar for contributors who can help run it even without technical expertise. Doing so makes research artifacts more accessible to the community.\n\n- **Reproducibility**: we aim to be transparent about the reproducibility of upstream code, models, and datasets when made available through the Diffusers library.\n\n- **Responsibility**: as a community and through teamwork, we hold a collective responsibility to our users by anticipating and mitigating this technology's potential risks and dangers.\n\n\n## Examples of implementations: Safety features and Mechanisms\n\nThe team works daily to make the technical and non-technical tools available to deal with the potential ethical and social risks associated with diffusion technology. Moreover, the community's input is invaluable in ensuring these features' implementation and raising awareness with us.\n\n- [**Community tab**](https://huggingface.co/docs/hub/repositories-pull-requests-discussions): it enables the community to discuss and better collaborate on a project.\n\n- **Bias exploration and evaluation**: the Hugging Face team provides a [space](https://huggingface.co/spaces/society-ethics/DiffusionBiasExplorer) to demonstrate the biases in Stable Diffusion interactively. In this sense, we support and encourage bias explorers and evaluations.\n\n- **Encouraging safety in deployment**", "The library has three main components:\n\n- State-of-the-art diffusion pipelines for inference with just a few lines of code. There are many pipelines in \ud83e\udd17 Diffusers, check out the table in the pipeline [overview](api/pipelines/overview) for a complete list of available pipelines and the task they solve.\n- Interchangeable [noise schedulers](api/schedulers/overview) for balancing trade-offs between generation speed and quality.\n- Pretrained [models](api/models) that can be used as building blocks, and combined with schedulers, for creating your own end-to-end diffusion systems.", "In addition to that, we also support specific hardware and formats like ONNX, the `mps` PyTorch device for Apple Silicon computers, Core ML, and others.\n\nTo learn more about how we optimize inference with \ud83e\udd17\u00a0Diffusers, check out the [docs](https://huggingface.co/docs/diffusers/optimization/opt_overview)!\n\n## Ethics and safety\n\nGenerative models are cool, but they also have the ability to produce harmful and NSFW content. To help users interact with these models responsibly and ethically, we\u2019ve added a [`safety_checker`](https://github.com/huggingface/diffusers/blob/main/src/diffusers/pipelines/stable_diffusion/safety_checker.py) component that flags inappropriate content generated during inference. Model creators can choose to incorporate this component into their models if they want.\n\nIn addition, generative models can also be used to produce disinformation. Earlier this year, the [Balenciaga Pope](https://www.theverge.com/2023/3/27/23657927/ai-pope-image-fake-midjourney-computer-generated-aesthetic) went viral for how realistic the image was despite it being fake. This underscores the importance and need for a mechanism to distinguish between generated and human content. That\u2019s why we\u2019ve added an invisible watermark for images generated by the SDXL model, which helps users be better informed.\n\nThe development of these features is guided by our [ethical charter](https://huggingface.co/docs/diffusers/main/en/conceptual/ethical_guidelines), which you can find in our documentation.\n\n## Support for LoRA", "```python\nfrom diffusers import DiffusionPipeline\n\nrepo_id = \"runwayml/stable-diffusion-v1-5\"\nstable_diffusion = DiffusionPipeline.from_pretrained(repo_id, safety_checker=None, use_safetensors=True)\n\"\"\"\nYou have disabled the safety checker for <class 'diffusers.pipelines.stable_diffusion.pipeline_stable_diffusion.StableDiffusionPipeline'> by passing `safety_checker=None`. Ensure that you abide by the conditions of the Stable Diffusion license and do not expose unfiltered results in services or applications open to the public. Both the diffusers team and Hugging Face strongly recommend keeping the safety filter enabled in all public-facing circumstances, disabling it only for use cases that involve analyzing network behavior or auditing its results. For more information, please have a look at https://github.com/huggingface/diffusers/pull/254 .\n\"\"\"\n```\n\n### Reuse components across pipelines\n\nYou can also reuse the same components in multiple pipelines to avoid loading the weights into RAM twice. Use the [`~DiffusionPipeline.components`] method to save the components:\n\n```python\nfrom diffusers import StableDiffusionPipeline, StableDiffusionImg2ImgPipeline\n\nmodel_id = \"runwayml/stable-diffusion-v1-5\"\nstable_diffusion_txt2img = StableDiffusionPipeline.from_pretrained(model_id, use_safetensors=True)\n\ncomponents = stable_diffusion_txt2img.components\n```\n\nThen you can pass the `components` to another pipeline without reloading the weights into RAM:\n\n```py\nstable_diffusion_img2img = StableDiffusionImg2ImgPipeline(**components)\n```", "\ud83e\udd17 Diffusers is the go-to library for state-of-the-art pretrained diffusion models for generating images, audio, and even 3D structures of molecules. Whether you're looking for a simple inference solution or training your own diffusion models, \ud83e\udd17 Diffusers is a modular toolbox that supports both. Our library is designed with a focus on [usability over performance](https://huggingface.co/docs/diffusers/conceptual/philosophy#usability-over-performance), [simple over easy](https://huggingface.co/docs/diffusers/conceptual/philosophy#simple-over-easy), and [customizability over abstractions](https://huggingface.co/docs/diffusers/conceptual/philosophy#tweakable-contributorfriendly-over-abstraction).\n\n\ud83e\udd17 Diffusers offers three core components:\n\n- State-of-the-art [diffusion pipelines](https://huggingface.co/docs/diffusers/api/pipelines/overview) that can be run in inference with just a few lines of code.\n- Interchangeable noise [schedulers](https://huggingface.co/docs/diffusers/api/schedulers/overview) for different diffusion speeds and output quality.\n- Pretrained [models](https://huggingface.co/docs/diffusers/api/models/overview) that can be used as building blocks, and combined with schedulers, for creating your own end-to-end diffusion systems.\n\n## Installation\n\nWe recommend installing \ud83e\udd17 Diffusers in a virtual environment from PyPI or Conda. For more details about installing [PyTorch](https://pytorch.org/get-started/locally/) and [Flax](https://flax.readthedocs.io/en/latest/#installation), please refer to their official documentation.\n\n### PyTorch\n\nWith `pip` (official package):\n\n```bash\npip install --upgrade diffusers[torch]\n```\n\nWith `conda` (maintained by the community):", "Using \ud83e\udde8 `diffusers` at Hugging Face\n\nDiffusers is the go-to library for state-of-the-art pretrained diffusion models for generating images, audio, and even 3D structures of molecules. Whether you\u2019re looking for a simple inference solution or want to train your own diffusion model, Diffusers is a modular toolbox that supports both. The library is designed with a focus on usability over performance, simple over easy, and customizability over abstractions.\n\n## Exploring Diffusers in the Hub\n\nThere are over 10,000 `diffusers` compatible pipelines on the Hub which you can find by filtering at the left of [the models page](https://huggingface.co/models?library=diffusers&sort=downloads). Diffusion systems are typically composed of multiple components such as text encoder, UNet, VAE, and scheduler. Even though they are not standalone models, the pipeline abstraction makes it easy to use them for inference or training.\n\nYou can find diffusion pipelines for many different tasks:\n\n* Generating images from natural language text prompts ([text-to-image](https://huggingface.co/models?library=diffusers&pipeline_tag=text-to-image&sort=downloads)).\n* Transforming images using natural language text prompts ([image-to-image](https://huggingface.co/models?library=diffusers&pipeline_tag=image-to-image&sort=downloads)).\n* Generating videos from natural language descriptions ([text-to-video](https://huggingface.co/models?library=diffusers&pipeline_tag=text-to-video&sort=downloads)).\n\n\nYou can try out the models directly in the browser if you want to test them out without downloading them, thanks to the in-browser widgets! \n\n<div class=\"flex justify-center\">\n<img class=\"hidden dark:block\" src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/refs%2Fpr%2F35/hub/libraries-diffusers_widget.png\"/>\n</div>"], "eval_score_GPT4": "4", "eval_feedback_GPT4": "Feedback: The response provided is mostly correct and factual. It correctly identifies the Safety Checker as a component used to flag inappropriate content generated by diffusion models, which aligns with the reference answer. It also accurately describes the process of comparing class probability of harmful concepts in the embedding space against a generated image. However, the response includes additional information about the Safety Checker being optional for model creators and its function to prevent reverse engineering, which is not mentioned in the reference answer. This additional information is not incorrect, but it is not required to answer the question about the purpose of the Safety Checker. Therefore, the response is not completely correct as per the reference answer but still provides a mostly accurate description of the Safety Checker's purpose."}, {"question": "What Python class allows you to retrieve Discussions and Pull Requests from a given repository on the Hugging Face Hub?\n", "true_answer": "HfApi", "source_doc": "huggingface/huggingface_hub/blob/main/docs/source/en/guides/community.md", "generated_answer": "The `HfApi` class in the `huggingface_hub` library allows you to retrieve Discussions and Pull Requests from a given repository on the Hugging Face Hub.", "retrieved_docs": ["!--\u26a0\ufe0f Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\nrendered properly in your Markdown viewer.\n-->\n\n# Interact with Discussions and Pull Requests \n\nThe `huggingface_hub` library provides a Python interface to interact with Pull Requests and Discussions on the Hub.\nVisit [the dedicated documentation page](https://huggingface.co/docs/hub/repositories-pull-requests-discussions)\nfor a deeper view of what Discussions and Pull Requests on the Hub are, and how they work under the hood.\n\n## Retrieve Discussions and Pull Requests from the Hub\n\nThe `HfApi` class allows you to retrieve Discussions and Pull Requests on a given repo:\n\n```python\n>>> from huggingface_hub import get_repo_discussions\n>>> for discussion in get_repo_discussions(repo_id=\"bigscience/bloom\"):\n...     print(f\"{discussion.num} - {discussion.title}, pr: {discussion.is_pull_request}\")\n\n# 11 - Add Flax weights, pr: True\n# 10 - Update README.md, pr: True\n# 9 - Training languages in the model card, pr: True\n# 8 - Update tokenizer_config.json, pr: True\n# 7 - Slurm training script, pr: False\n[...]\n```\n\n`HfApi.get_repo_discussions` supports filtering by author, type (Pull Request or Discussion) and status (`open` or `closed`):\n\n```python\n>>> from huggingface_hub import get_repo_discussions\n>>> for discussion in get_repo_discussions(\n...    repo_id=\"bigscience/bloom\",\n...    author=\"ArthurZ\",\n...    discussion_type=\"pull_request\",\n...    discussion_status=\"open\",\n... ):\n...     print(f\"{discussion.num} - {discussion.title} by {discussion.author}, pr: {discussion.is_pull_request}\")", "## Create and edit a Discussion or Pull Request programmatically\n\nThe [`HfApi`] class also offers ways to create and edit Discussions and Pull Requests.\nYou will need an [access token](https://huggingface.co/docs/hub/security-tokens) to create and edit Discussions\nor Pull Requests.\n\nThe simplest way to propose changes on a repo on the Hub is via the [`create_commit`] API: just \nset the `create_pr` parameter to `True`. This parameter is also available on other methods that wrap [`create_commit`]:\n\n    * [`upload_file`]\n    * [`upload_folder`]\n    * [`delete_file`]\n    * [`delete_folder`]\n    * [`metadata_update`]\n\n```python\n>>> from huggingface_hub import metadata_update\n\n>>> metadata_update(\n...     repo_id=\"username/repo_name\",\n...     metadata={\"tags\": [\"computer-vision\", \"awesome-model\"]},\n...     create_pr=True,\n... )\n```\n\nYou can also use [`HfApi.create_discussion`] (respectively [`HfApi.create_pull_request`]) to create a Discussion (respectively a Pull Request) on a repo.\nOpening a Pull Request this way can be useful if you need to work on changes locally. Pull Requests opened this way will be in `\"draft\"` mode.\n\n```python\n>>> from huggingface_hub import create_discussion, create_pull_request\n\n>>> create_discussion(\n...     repo_id=\"username/repo-name\",\n...     title=\"Hi from the huggingface_hub library!\",\n...     token=\"<insert your access token here>\",\n... )\nDiscussionWithDetails(...)", "The `push_to_hub()` method is backed by the [`huggingface_hub`](https://github.com/huggingface/huggingface_hub) Python package, which offers a direct API to the Hugging Face Hub. It's integrated within \ud83e\udd17 Transformers and several other machine learning libraries, like [`allenlp`](https://github.com/allenai/allennlp). Although we focus on the \ud83e\udd17 Transformers integration in this chapter, integrating it into your own code or library is simple.\n\nJump to the last section to see how to upload files to your newly created repository!\n\n## Using the `huggingface_hub` Python library[[using-the-huggingfacehub-python-library]]\n\nThe `huggingface_hub` Python library is a package which offers a set of tools for the model and datasets hubs. It provides simple methods and classes for common tasks like \ngetting information about repositories on the hub and managing them. It provides simple APIs that work on top of git to manage those repositories' content and to integrate the Hub\nin your projects and libraries.\n\nSimilarly to using the `push_to_hub` API, this will require you to have your API token saved in your cache. In order to do this, you will need to use the `login` command from the CLI, as mentioned in the previous section (again, make sure to prepend these commands with the `!` character if running in Google Colab):\n\n```bash\nhuggingface-cli login\n```\n\nThe `huggingface_hub` package offers several methods and classes which are useful for our purpose. Firstly, there are a few methods to manage repository creation, deletion, and others:\n\n```python no-format\nfrom huggingface_hub import (\n    # User management\n    login,\n    logout,\n    whoami,\n\n    # Repository creation and management\n    create_repo,\n    delete_repo,\n    update_repo_visibility,", "--\ntitle: Introducing Pull Requests and Discussions \ud83e\udd73\nthumbnail: /blog/assets/76_community_update/thumbnail.png\n---\n\n# Introducing Pull Requests and Discussions \ud83e\udd73\n\n![Pull requests and discussions on the hub](assets/76_community_update/community-update.png)\n\nWe are thrilled to announce the release of our latest collaborative features: pull requests and discussions on the Hugging Face Hub!\n\nPull requests and discussions are available today under the [community tab](https://huggingface.co/gpt2/discussions) for all repository types: models, datasets, and Spaces. Any member of the community can create and participate in discussions and pull requests, facilitating collaborations not only within teams, but also with everyone else in the community!\n\nIt's the biggest update ever done to the Hub, and we can't wait to see the community members start collaborating with it \ud83e\udd29.\n\nThe new \"Community\" tab also aligns with proposals in ethical ML throughout the years. Feedback and iterations have a central place in the development of ethical machine learning software. We really believe having it in the community's toolset will unlock new kinds of positive patterns in ML, collaborations, and progress.\n\nSome example use cases for discussions and pull requests:\n\n- Propose suggestions in model cards to improve disclosures of ethical biases.\n- Let users flag concerning generations of a given Space demo.\n- Provide a venue through which model and dataset authors can have a direct discussion with community members.\n- Allow others to improve your repositories! For example, users might want to provide TensorFlow weights!\n\n## Discussions\n\n![Discussions on the Hugging Face Hub](assets/76_community_update/new-discussion.png)\n\n[Discussions](https://huggingface.co/gpt2/discussions?type=discussion) allow community members ask and answer questions as well as share their ideas and suggestions directly with the repository owners and the community. Anyone can create and participate in discussions in the community tab of a repository.\n\n## Pull requests\n\n![Pull requests on the Hugging Face Hub](assets/76_community_update/new-pr.png)", "Pull requests and Discussions\n\nHub Pull requests and Discussions allow users to do community contributions to repositories. Pull requests and discussions work the same for all the repo types.\n\nAt a high level, the aim is to build a simpler version of other git hosts' (like GitHub's) PRs and Issues:\n- no forks are involved: contributors push to a special `ref` branch directly on the source repo.\n- there's no hard distinction between discussions and PRs: they are essentially the same so they are displayed in the same lists.\n- they are streamlined for ML (i.e. models/datasets/spaces repos), not arbitrary repos.\n\n_Note, Pull Requests and discussions can be enabled or disabled from the [repository settings](./repositories-settings#disabling-discussions-pull-requests)_\n\n## List\n\nBy going to the community tab in any repository, you can see all Discussions and Pull requests. You can also filter to only see the ones that are open.\n\n\n<div class=\"flex justify-center\">\n<img class=\"block dark:hidden\" src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/hub/discussions-list.png\"/>\n<img class=\"hidden dark:block\" src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/hub/discussions-list-dark.png\"/>\n</div>\n\n## View\n\nThe Discussion page allows you to see the comments from different users. If it's a Pull Request, you can see all the changes by going to the Files changed tab.", "<figure class=\"image table text-center m-0 w-full\">\n  <medium-zoom background=\"rgba(0,0,0,.7)\" alt=\"Pull requests and discussions on a model\" src=\"assets/92_introducing_private_hub/pull-requests-and-discussions.png\"></medium-zoom>\n  <figcaption>Pull requests and discussions on a model</figcaption>\n</figure>\n\nThe Hub allows users to create [Organizations](https://huggingface.co/docs/hub/organizations), that is, team accounts to manage models, datasets, and spaces collaboratively. An organization\u2019s repositories will be featured on the organization\u2019s page and admins can set roles to control access to these repositories. Every member of the organization can contribute to models, datasets and spaces given the right permissions. Here at Hugging Face, we believe having the right tools to collaborate drastically accelerates machine learning development! \ud83d\udd25 \n\n<figure class=\"image table text-center m-0 w-full\">\n  <medium-zoom background=\"rgba(0,0,0,.7)\" alt=\"Organization in the Hub for BigScience\" src=\"assets/92_introducing_private_hub/organizations.png\"></medium-zoom>\n  <figcaption>Organization in the Hub for <a href=\"https://huggingface.co/bigscience\">BigScience</a></figcaption>\n</figure>\n\nNow that we have covered the basics, let's dive into the specific characteristics of models, datasets and spaces hosted on the Hugging Face Hub.\n\n### Models", "### Advanced programmatic repository management \n\nThe `Repository` class helps manage both offline Git repositories and Hugging\nFace Hub repositories. Using the `Repository` class requires `git` and `git-lfs`\nto be installed.\n\nInstantiate a `Repository` object by calling it with a path to a local Git\nclone/repository:\n\n```python\n>>> from huggingface_hub import Repository\n>>> repo = Repository(\"<path>/<to>/<folder>\")\n```\n\nThe `Repository` takes a `clone_from` string as parameter. This can stay as\n`None` for offline management, but can also be set to any URL pointing to a Git\nrepo to clone that repository in the specified directory:\n\n```python\n>>> repo = Repository(\"huggingface-hub\", clone_from=\"https://github.com/huggingface/huggingface_hub\")\n```\n\nThe `clone_from` method can also take any Hugging Face model ID as input, and\nwill clone that repository:\n\n```python\n>>> repo = Repository(\"w2v2\", clone_from=\"facebook/wav2vec2-large-960h-lv60\")\n```\n\nIf the repository you're cloning is one of yours or one of your organisation's,\nthen having the ability to commit and push to that repository is important. In\norder to do that, you should make sure to be logged-in using `huggingface-cli\nlogin`, and to have the `token` parameter set to `True` (the default)\nwhen  instantiating the `Repository` object:\n\n```python\n>>> repo = Repository(\"my-model\", clone_from=\"<user>/<model_id>\", token=True)\n```\n\nThis works for models, datasets and spaces repositories; but you will need to\nexplicitely specify the type for the last two options:"], "eval_score_GPT4": "5", "eval_feedback_GPT4": "Feedback: The response correctly identifies the `HfApi` class as the tool used to retrieve Discussions and Pull Requests from a given repository on the Hugging Face Hub. It provides additional context by mentioning the `huggingface_hub` library, which is accurate and enhances the factual content of the answer. The response aligns with the reference answer and meets the criteria for a score of 5."}, {"question": "What is the name of the new library introduced by Hugging Face for hosting scikit-learn models?\n", "true_answer": "Skops", "source_doc": "huggingface/blog/blob/main/skops.md", "generated_answer": "The question asks about the name of the new library introduced by Hugging Face for hosting scikit-learn models. However, the provided context does not mention any new library for hosting scikit-learn models. The context only discusses the integration of third-party libraries with the Hugging Face Hub, which currently supports dozens of libraries in the Open Source ecosystem, including scikit-learn. To answer the question, there is no new library mentioned for hosting scikit-learn", "retrieved_docs": ["Integrate your library with the Hub\n\nThe Hugging Face Hub aims to facilitate sharing machine learning models, checkpoints, and artifacts. This endeavor includes integrating the Hub into many of the amazing third-party libraries in the community. Some of the ones already integrated include [spaCy](https://spacy.io/usage/projects#huggingface_hub), [AllenNLP](https://allennlp.org/), and [timm](https://rwightman.github.io/pytorch-image-models/), among many others. Integration means users can download and upload files to the Hub directly from your library. We hope you will integrate your library and join us in democratizing artificial intelligence for everyone.\n\nIntegrating the Hub with your library provides many benefits, including:\n\n- Free model hosting for you and your users.\n- Built-in file versioning - even for huge files - made possible by [Git-LFS](https://git-lfs.github.com/).\n- All public models are powered by the [Inference API](https://huggingface.co/docs/api-inference/index).\n- In-browser widgets allow users to interact with your hosted models directly.\n\nThis tutorial will help you integrate the Hub into your library so your users can benefit from all the features offered by the Hub.\n\nBefore you begin, we recommend you create a [Hugging Face account](https://huggingface.co/join) from which you can manage your repositories and files. \n\nIf you need help with the integration, feel free to open an [issue](https://github.com/huggingface/huggingface_hub/issues/new/choose), and we would be more than happy to help you.\n\n## Installation\n\n1. Install the `huggingface_hub` library with pip in your environment:\n\n   ```bash\n   python -m pip install huggingface_hub\n   ```\n\n2. Once you have successfully installed the `huggingface_hub` library, log in to your Hugging Face account:", "The literature on Diffusion-based models is developing at a rapid pace which is why we partnered with [Jonathan Whitaker](https://github.com/johnowhitaker) to develop a course on it. The course is free, and you can check it out [here](https://github.com/huggingface/diffusion-models-class).\n\n## Support for third-party libraries\n\nCentral to the Hugging Face ecosystem is the [Hugging Face Hub](https://huggingface.co/docs/hub), which lets people collaborate effectively on Machine Learning. As mentioned earlier, we not only support models from \ud83e\udd17 Transformers on the Hub but also models from other third-party libraries. To this end, we provide [several utilities](https://huggingface.co/docs/hub/models-adding-libraries) so that you can integrate your own library with the Hub. One of the primary advantages of doing this is that it becomes very easy to share artifacts (such as models and datasets) with the community, thereby making it easier for your users to try out your models.\n\nWhen you have your models hosted on the Hub, you can also [add custom inference widgets](https://github.com/huggingface/api-inference-community) for them. Inference widgets allow users to quickly check out the models. This helps with improving user engagement.\n\n<div align=\"center\">\n<img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/cv_state/task_widget_generation.png\"/>\n</div>\n\n## Spaces for computer vision demos", "### Would you like to integrate your library to the Hub?\n\nThis integration is made possible by the [`huggingface_hub`](https://github.com/huggingface/huggingface_hub) library. If you want to add your library to the Hub, we have a [guide](https://huggingface.co/docs/hub/models-adding-libraries) for you! Or simply tag someone from the Hugging Face team.\n\nA shout out to the Hugging Face team for all the work on this integration, in particular [@osanseviero](https://twitter.com/osanseviero) \ud83e\udd99.\n\nThank you fastlearners and hugging learners \ud83e\udd17.", "!--\u26a0\ufe0f Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\nrendered properly in your Markdown viewer.\n-->\n\n# Integrate any ML framework with the Hub\n\nThe Hugging Face Hub makes hosting and sharing models with the community easy. It supports\n[dozens of libraries](https://huggingface.co/docs/hub/models-libraries) in the Open Source ecosystem. We are always\nworking on expanding this support to push collaborative Machine Learning forward. The `huggingface_hub` library plays a\nkey role in this process, allowing any Python script to easily push and load files.\n\nThere are four main ways to integrate a library with the Hub:\n1. **Push to Hub:** implement a method to upload a model to the Hub. This includes the model weights, as well as\n   [the model card](https://huggingface.co/docs/huggingface_hub/how-to-model-cards) and any other relevant information\n   or data necessary to run the model (for example, training logs). This method is often called `push_to_hub()`.\n2. **Download from Hub:** implement a method to load a model from the Hub. The method should download the model\n   configuration/weights and load the model. This method is often called `from_pretrained` or `load_from_hub()`.\n3. **Inference API:** use our servers to run inference on models supported by your library for free.\n4. **Widgets:** display a widget on the landing page of your models on the Hub. It allows users to quickly try a model\n   from the browser.\n\nIn this guide, we will focus on the first two topics. We will present the two main approaches you can use to integrate\na library, with their advantages and drawbacks. Everything is summarized at the end of the guide to help you choose\nbetween the two. Please keep in mind that these are only guidelines that you are free to adapt to you requirements.", "Now that we have covered the basics, let's dive into the specific characteristics of models, datasets and spaces hosted on the Hugging Face Hub.\n\n### Models\n\n[Transfer learning](https://www.youtube.com/watch?v=BqqfQnyjmgg&ab_channel=HuggingFace) has changed the way companies approach machine learning problems. Traditionally, companies needed to train models from scratch, which requires a lot of time, data, and resources. Now machine learning teams can use a pre-trained model and [fine-tune it for their own use case](https://huggingface.co/course/chapter3/1?fw=pt) in a fast and cost-effective way. This dramatically accelerates the process of getting accurate and performant models.\n\nOn the Hub, you can find 60,000+ state-of-the-art open source pre-trained models for NLP, computer vision, speech, time-series, biology, reinforcement learning, chemistry and more. You can use the search bar or filter by tasks, libraries, licenses and other tags to find the right model for your particular use case:\n\n<figure class=\"image table text-center m-0 w-full\">\n  <medium-zoom background=\"rgba(0,0,0,.7)\" alt=\"60,000+ models available on the Hub\" src=\"assets/92_introducing_private_hub/models.png\"></medium-zoom>\n  <figcaption>60,000+ models available on the Hub</figcaption>\n</figure>\n\nThese models span 180 languages and support up to 25 ML libraries (including Transformers, Keras, spaCy, Timm and others), so there is a lot of flexibility in terms of the type of models, languages and libraries.", "Models\n\nThe Hugging Face Hub hosts many models for a [variety of machine learning tasks](https://huggingface.co/tasks). Models are stored in repositories, so they benefit from [all the features](./repositories) possessed by every repo on the Hugging Face Hub. Additionally, model repos have attributes that make exploring and using models as easy as possible. These docs will take you through everything you'll need to know to find models on the Hub, upload your models, and make the most of everything the Model Hub offers!\n\n## Contents\n\n- [The Model Hub](./models-the-hub)\n- [Model Cards](./model-cards)\n  - [CO<sub>2</sub> emissions](./model-cards-co2)\n  - [Gated models](./models-gated)\n- [Libraries](./models-libraries)\n- [Uploading Models](./models-uploading)\n- [Downloading Models](./models-downloading)\n- [Widgets](./models-widgets)\n  - [Widget Examples](./models-widgets-examples)\n- [Inference API](./models-inference)\n- [Frequently Asked Questions](./models-faq)\n- [Advanced Topics](./models-advanced)\n  - [Integrating libraries with the Hub](./models-adding-libraries)\n  - [Tasks](./models-tasks)", "elcome to the Hugging Face Course! This course has been designed to teach you all about the Hugging Face ecosystem: how to use the dataset and model hub as well as all our open source libraries. Here is the Table of Contents. As you can see, it's divided in three sections which become progressively more advanced. At this stage, the first two sections have been released. The first will teach you the basics of how to use a Transformer model, fine-tune it on your own dataset and share the result with the community. The second will dive deeper into our libraries and teach you how to tackle any NLP task. We are actively working on the last one and hope to have it ready for you for the spring of 2022. The first chapter requires no technical knowledge and is a good introduction to learn what Transformers models can do and how they could be of use to you or your company. The next chapters require a good knowledge of Python and some basic knowledge of Machine Learning and Deep Learning. If you don't know what a training and validation set is or what gradient descent means, you should look at an introductory course such as the ones published by deeplearning.ai or fast.ai. It's also best if you have some basics in one Deep Learning Framework (PyTorch or TensorFlow). Each part of the material introduced in this course has a version in both those frameworks, so you will be able to pick the one you are most comfortable with. This is the team that developed this course. I'll now let each of the speakers introduce themselves briefly."], "eval_score_GPT4": "1", "eval_feedback_GPT4": "Feedback: The response does not provide the correct name of the new library introduced by Hugging Face for hosting scikit-learn models. The reference answer states that the name of the library is \"Skops,\" which is not mentioned in the response. Instead, the response discusses the integration of third-party libraries with the Hugging Face Hub without specifying the name of the new library. Therefore, the response is completely incorrect and not factual as per the reference answer and the score rubric."}, {"question": "What is the purpose of Textual Inversion?\n", "true_answer": "Textual Inversion is a training method for personalizing models by learning new text embeddings from a few example images.", "source_doc": "huggingface/diffusers/blob/main/docs/source/en/api/loaders/textual_inversion.md", "generated_answer": "Textual Inversion is a training method for personalizing text-to-image diffusion models by learning new text embeddings from a few example images. Its purpose is to allow users to teach a pre-trained text-to-image diffusion model about new concepts to generate outputs involving the concept(s) of interest. The resulting embeddings, which are extremely small (a few KBs), can be loaded into the text encoder of the model, and a special token can be loaded to", "retrieved_docs": ["## Textual Inversion\n\n[Paper](https://arxiv.org/abs/2208.01618)\n\n[Textual Inversion](../training/text_inversion) fine-tunes a model to teach it about a new concept. I.e. a few pictures of a style of artwork can be used to generate images in that style.\n\n## ControlNet\n\n[Paper](https://arxiv.org/abs/2302.05543)\n\n[ControlNet](../api/pipelines/controlnet) is an auxiliary network which adds an extra condition.\nThere are 8 canonical pre-trained ControlNets trained on different conditionings such as edge detection, scribbles,\ndepth maps, and semantic segmentations.\n\n## Prompt Weighting\n\n[Prompt weighting](../using-diffusers/weighted_prompts) is a simple technique that puts more attention weight on certain parts of the text\ninput.\n\n## Custom Diffusion\n\n[Paper](https://arxiv.org/abs/2212.04488)\n\n[Custom Diffusion](../training/custom_diffusion) only fine-tunes the cross-attention maps of a pre-trained\ntext-to-image diffusion model. It also allows for additionally performing Textual Inversion. It supports\nmulti-concept training by design. Like DreamBooth and Textual Inversion, Custom Diffusion is also used to\nteach a pre-trained text-to-image diffusion model about new concepts to generate outputs involving the\nconcept(s) of interest.\n\n## Model Editing\n\n[Paper](https://arxiv.org/abs/2303.08084)\n\nThe [text-to-image model editing pipeline](../api/pipelines/model_editing) helps you mitigate some of the incorrect implicit assumptions a pre-trained text-to-image\ndiffusion model might make about the subjects present in the input prompt. For example, if you prompt Stable Diffusion to generate images for \"A pack of roses\", the roses in the generated images\nare more likely to be red. This pipeline helps you change that assumption.\n\n## DiffEdit", "Textual Inversion lets you personalize a Stable Diffusion model on your own images with just 3-5 samples. With this tool, you can train a model on a concept, and then share the concept with the rest of the community! \n\n![image info](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/diffusers-2nd-month/textual_inversion.jpeg)\n\nIn just a couple of days, the community shared over 200 concepts! Check them out!\n\n* [Organization](https://huggingface.co/sd-concepts-library) with the concepts. \n* [Navigator Colab](https://colab.research.google.com/github/huggingface/notebooks/blob/main/diffusers/stable_diffusion_textual_inversion_library_navigator.ipynb): Browse visually and use over 150 concepts created by the community.\n* [Training Colab](https://colab.research.google.com/github/huggingface/notebooks/blob/main/diffusers/sd_textual_inversion_training.ipynb): Teach Stable Diffusion a new concept and share it with the rest of the community.\n* [Inference Colab](https://colab.research.google.com/github/huggingface/notebooks/blob/main/diffusers/stable_conceptualizer_inference.ipynb): Run Stable Diffusion with the learned concepts.\n\n\n## Experimental inpainting pipeline\n\nInpainting allows to provide an image, then select an area in the image (or provide a mask), and use Stable Diffusion to replace the mask. Here is an example:\n\n<figure class=\"image table text-center m-0 w-full\">\n  <img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/diffusers-2nd-month/inpainting.png\" alt=\"Example inpaint of owl being generated from an initial image and a prompt\"/>\n</figure>", "# Textual Inversion fine-tuning example\n\n[Textual inversion](https://arxiv.org/abs/2208.01618) is a method to personalize text2image models like stable diffusion on your own images using just 3-5 examples.\nThe `textual_inversion.py` script shows how to implement the training procedure and adapt it for stable diffusion.\n\n## Training with Intel Extension for PyTorch\n\nIntel Extension for PyTorch provides the optimizations for faster training and inference on CPUs. You can leverage the training example \"textual_inversion.py\". Follow the [instructions](https://github.com/huggingface/diffusers/tree/main/examples/textual_inversion) to get the model and [dataset](https://huggingface.co/sd-concepts-library/dicoo2) before running the script.\n\nThe example supports both single node and multi-node distributed training:\n\n### Single node training\n\n```bash\nexport MODEL_NAME=\"CompVis/stable-diffusion-v1-4\"\nexport DATA_DIR=\"path-to-dir-containing-dicoo-images\"\n\npython textual_inversion.py \\\n  --pretrained_model_name_or_path=$MODEL_NAME \\\n  --train_data_dir=$DATA_DIR \\\n  --learnable_property=\"object\" \\\n  --placeholder_token=\"<dicoo>\" --initializer_token=\"toy\" \\\n  --seed=7 \\\n  --resolution=512 \\\n  --train_batch_size=1 \\\n  --gradient_accumulation_steps=1 \\\n  --max_train_steps=3000 \\\n  --learning_rate=2.5e-03 --scale_lr \\\n  --output_dir=\"textual_inversion_dicoo\"\n```\n\nNote: Bfloat16 is available on Intel Xeon Scalable Processors Cooper Lake or Sapphire Rapids. You may not get performance speedup without Bfloat16 support.\n\n### Multi-node distributed training", "# Textual Inversion fine-tuning example\n\n[Textual inversion](https://arxiv.org/abs/2208.01618) is a method to personalize text2image models like stable diffusion on your own images using just 3-5 examples.\nThe `textual_inversion.py` script shows how to implement the training procedure and adapt it for stable diffusion.\n\n## Running on Colab\n\nColab for training\n[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/huggingface/notebooks/blob/main/diffusers/sd_textual_inversion_training.ipynb)\n\nColab for inference\n[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/huggingface/notebooks/blob/main/diffusers/stable_conceptualizer_inference.ipynb)\n\n## Running locally with PyTorch\n### Installing the dependencies\n\nBefore running the scripts, make sure to install the library's training dependencies:\n\n**Important**\n\nTo make sure you can successfully run the latest versions of the example scripts, we highly recommend **installing from source** and keeping the install up to date as we update the example scripts frequently and install some example-specific requirements. To do this, execute the following steps in a new virtual environment:\n```bash\ngit clone https://github.com/huggingface/diffusers\ncd diffusers\npip install .\n```\n\nThen cd in the example folder and run:\n```bash\npip install -r requirements.txt\n```\n\nAnd initialize an [\ud83e\udd17 Accelerate](https://github.com/huggingface/accelerate/) environment with:\n\n```bash\naccelerate config\n```\n\n### Cat toy example", "# Textual Inversion fine-tuning example\n\n[Textual inversion](https://arxiv.org/abs/2208.01618) is a method to personalize text2image models like stable diffusion on your own images using just 3-5 examples.\nThe `textual_inversion.py` script shows how to implement the training procedure and adapt it for stable diffusion.\n\n## Running on Colab\n\nColab for training\n[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/huggingface/notebooks/blob/main/diffusers/sd_textual_inversion_training.ipynb)\n\nColab for inference\n[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/huggingface/notebooks/blob/main/diffusers/stable_conceptualizer_inference.ipynb)\n\n## Running locally with PyTorch\n### Installing the dependencies\n\nBefore running the scripts, make sure to install the library's training dependencies:\n\n**Important**\n\nTo make sure you can successfully run the latest versions of the example scripts, we highly recommend **installing from source** and keeping the install up to date as we update the example scripts frequently and install some example-specific requirements. To do this, execute the following steps in a new virtual environment:\n```bash\ngit clone https://github.com/huggingface/diffusers\ncd diffusers\npip install .\n```\n\nThen cd in the example folder  and run\n```bash\npip install -r requirements.txt\n```\n\nAnd initialize an [\ud83e\udd17Accelerate](https://github.com/huggingface/accelerate/) environment with:\n\n```bash\naccelerate config\n```\n\n\n### Cat toy example", "Distillation for quantization on Textual Inversion models to personalize text2image\n\n[Textual inversion](https://arxiv.org/abs/2208.01618) is a method to personalize text2image models like stable diffusion on your own images._By using just 3-5 images new concepts can be taught to Stable Diffusion and the model personalized on your own images_\nThe `textual_inversion.py` script shows how to implement the training procedure and adapt it for stable diffusion.\nWe have enabled distillation for quantization in `textual_inversion.py` to do quantization aware training as well as distillation on the model generated by Textual Inversion method.\n\n## Installing the dependencies\n\nBefore running the scripts, make sure to install the library's training dependencies:\n\n```bash\npip install -r requirements.txt\n```\n\n## Prepare Datasets\n\nOne picture which is from the huggingface datasets [sd-concepts-library/dicoo2](https://huggingface.co/sd-concepts-library/dicoo2) is needed, and save it to the `./dicoo` directory. The picture is shown below:\n\n<a href=\"https://huggingface.co/sd-concepts-library/dicoo2/blob/main/concept_images/1.jpeg\">\n    <img src=\"https://huggingface.co/sd-concepts-library/dicoo2/resolve/main/concept_images/1.jpeg\" width = \"300\" height=\"300\">\n</a>\n\n## Get a FP32 Textual Inversion model\n\nUse the following command to fine-tune the Stable Diffusion model on the above dataset to obtain the FP32 Textual Inversion model.\n\n```bash\nexport MODEL_NAME=\"CompVis/stable-diffusion-v1-4\"\nexport DATA_DIR=\"./dicoo\"", "!--Copyright 2023 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\n\nhttp://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License.\n-->\n\n# Textual Inversion\n\nTextual Inversion is a training method for personalizing models by learning new text embeddings from a few example images. The file produced from training is extremely small (a few KBs) and the new embeddings can be loaded into the text encoder.\n\n[`TextualInversionLoaderMixin`] provides a function for loading Textual Inversion embeddings from Diffusers and Automatic1111 into the text encoder and loading a special token to activate the embeddings.\n\n<Tip>\n\nTo learn more about how to load Textual Inversion embeddings, see the [Textual Inversion](../../using-diffusers/loading_adapters#textual-inversion) loading guide.\n\n</Tip>\n\n## TextualInversionLoaderMixin\n\n[[autodoc]] loaders.textual_inversion.TextualInversionLoaderMixin"], "eval_score_GPT4": "4", "eval_feedback_GPT4": "Feedback: The response provided is mostly correct, accurate, and factual. It correctly identifies Textual Inversion as a training method for personalizing text-to-image diffusion models and explains its purpose in teaching a pre-trained model about new concepts using a few example images. The response also correctly mentions the creation of new text embeddings and the practical aspect of their small size. However, the response is slightly more detailed than the reference answer, which could be seen as providing additional context rather than being strictly inaccurate."}, {"question": "What is the recommended multiple of batch size for fp16 data type on an A100 GPU?\n", "true_answer": "64", "source_doc": "huggingface/transformers/blob/main/docs/source/en/perf_train_gpu_one.md", "generated_answer": "Based on the benchmarks provided, the recommended multiple of batch size for fp16 data type on an A100 GPU is 11. This is determined through the \"Generating Images from Text with Stable Diffusion\" benchmark, where the first two iterations with batch size 1 are discarded due to OOM errors. After optimizing various default values and offload features, the effective throughput is measured for different batch sizes. The benchmark results show that the", "retrieved_docs": ["# using bitsandbytes fp4/fp16 scheme\nCUDA_VISIBLE_DEVICES=0 python benchmark_gptq.py --model meta-llama/Llama-2-13b-chat-hf --sweep --num-batches 4 --task text-generation --bitsandbytes --generate\n```\n\nHere are results obtained on a single NVIDIA A100-SXM4-80GB GPU. We use a prompt length of 512, and generate exactly 512 new tokens. Each generation is repeated for 4 batches, and metrics are averaged over the number of batches and generation length.\n\nAdditional benchmarks could be done in the act-order case.\n\nFrom the bencharmk, it appears that Exllama kernel is the best-in-class for GPTQ, although it is rather slow for larger batch sizes. The memory savings are not exactly of x4 although weights are in int4. This can be explained by the possible static buffers used by the kernels, the CUDA context (taken into account in the measurements), and the KV cache that is still in fp16.\n\nBitsandbytes uses the fp4 scheme, with the compute in fp16.\n\n### Batch size = 1", "The following table displays the throughputs we got for first-gen Gaudi, Gaudi2 and Nvidia A100 80GB GPUs:\n\n<center>\n\n|   | First-gen Gaudi (BS=32) | Gaudi2 (BS=32) | Gaudi2 (BS=64) | A100 (BS=32) | A100 (BS=64) |\n|:-:|:-----------------------:|:--------------:|:--------------:|:-------:|:---------------------:|\n| Throughput (samples/s) | 520.2 | 1580.2 | 1835.8 | 981.6 | 1082.6 |\n| Speedup | x1.0 | x3.04 | x3.53 | x1.89 | x2.08 |\n\n</center>\n\n*BS* is the batch size per device. The Gaudi runs were performed in mixed precision (bf16/fp32) and the A100 runs in fp16. All runs were *distributed* runs on *8 devices*.\n\n\n### Generating Images from Text with Stable Diffusion\n\nOne of the main new features of \ud83e\udd17 Optimum Habana release 1.3 is [the support for Stable Diffusion](https://huggingface.co/docs/optimum/habana/usage_guides/stable_diffusion). It is now very easy to generate images from text on Gaudi. Unlike with \ud83e\udd17 Diffusers on GPUs, images are generated by batches. Due to model compilation times, the first two batches will be slower than the following iterations. In this benchmark, these first two iterations were discarded to compute the throughputs for both first-gen Gaudi and Gaudi2.", "First of all set batch size to 1 (you can always use gradient accumulation for any desired effective batch size).\n\n1. Enable `--gradient_checkpointing 1` (HF Trainer) or directly `model.gradient_checkpointing_enable()` - if OOM then\n2. Try ZeRO stage 2 first. if OOM then\n3. Try ZeRO stage 2 + `offload_optimizer` - if OOM then\n4. Switch to ZeRO stage 3 - if OOM then\n5. Enable `offload_param` to `cpu` - if OOM then\n6. Enable `offload_optimizer` to `cpu` - if OOM then\n\n7. If you still can't fit a batch size of 1 first check various default values and lower them if you can. For example, if you use `generate` and you don't use a wide search beam make it narrower as it'd take a lot of memory.\n\n8. Definitely use mixed half-precision over fp32 - so bf16 on Ampere and higher GPUs and fp16 on older gpu architectures.\n\n9. If you still OOM you could add more hardware or enable ZeRO-Infinity - that is switch offloads `offload_param` and  `offload_optimizer` to `nvme`. You need to make sure it's a very fast nvme. As an anecdote I was able to infer BLOOM-176B on a tiny GPU using ZeRO-Infinity except it was extremely slow. But it worked!\n\nYou can, of course, work through these steps in reverse by starting with the most GPU memory efficient config and then going backwards. Or try bi-secting it.\n\nOnce you have your batch size 1 not leading to OOM, measure your effective throughput.\n\nNext try to increase the batch size to as large as you can, since the higher the batch size the more efficient the GPUs are as they perform the best when matrices they multiply are huge.\n\nNow the performance optimization game starts. You can turn off some offload features or step down in ZeRO stages and increase/decrease batch size and again measure your effective throughput. Rinse and repeat until satisfied.", "Table 2: Benchmarking FSDP on GPT-2 XL (1.5B) model\n\nFrom Table 2, we can observe that DDP (w and w/o fp16) isn\u2019t even able to run with batch size of 1 and results in CUDA OOM error. FSDP with Zero-Stage 3 is able to be run on 2 GPUs with batch size of 5 (effective batch size =10 (5 X 2)). FSDP with CPU offload can further increase the max batch size to 14 per GPU when using 2 GPUs. **FSDP with CPU offload enables training GPT-2 1.5B model on a single GPU with a batch size of 10**. This enables ML practitioners with minimal compute resources to train such large models, thereby democratizing large model training.\n\n## Capabilities and limitations of the FSDP Integration\n\nLet\u2019s dive into the current support that Accelerate provides for FSDP integration and the known limitations.\n\n**Required PyTorch version for FSDP support**: PyTorch Nightly (or 1.12.0 if you read this after it has been released) as the model saving with FSDP activated is only available with recent fixes.\n\n**Configuration through CLI:**\n\n1. **Sharding Strategy**: [1] FULL_SHARD, [2] SHARD_GRAD_OP\n2. **Min Num Params**: FSDP's minimum number of parameters for Default Auto Wrapping.\n3. **Offload Params**: Decides Whether to offload parameters and gradients to CPU.\n\nFor more control, users can leverage the `FullyShardedDataParallelPlugin` wherein they can specify `auto_wrap_policy`, `backward_prefetch` and `ignored_modules`.\n\nAfter creating an instance of this class, users can pass it when creating the Accelerator object.", "**A higher throughput is preferred.**\n\n## No batching\n\nHere are the results with `batch_size=1`.\n\n| Absolute values             | Latency | Memory  |\n|-----------------------------|---------|---------|\n| no optimization             |   10.48 | 5025.0M |\n| bettertransformer only      |    7.70 | 4974.3M |\n| offload + bettertransformer |    8.90 | 2040.7M |\n| offload + bettertransformer + fp16            |    8.10 | 1010.4M |\n\n| Relative value              | Latency | Memory |\n|-----------------------------|---------|--------|\n| no optimization             |      0% |     0% |\n| bettertransformer only      |    -27% |    -1% |\n| offload + bettertransformer |    -15% |   -59% |\n| offload + bettertransformer + fp16            |    -23% |   -80% |\n\n### Comment\n\nAs expected, CPU offload greatly reduces memory footprint while slightly increasing latency.\n\nHowever, combined with bettertransformer and `fp16`, we get the best of both worlds, huge latency and memory decrease!\n\n## Batch size set to 8\nAnd here are the benchmark results but with `batch_size=8` and throughput measurement.\n\nNote that since `bettertransformer` is a free optimization because it does exactly the same operation and has the same memory footprint as the non-optimized model while being faster, the benchmark was run with **this optimization enabled by default**.", "BF16 has no such problem, you can easily do `10_000*10_000=100_000_000` and it's no problem.\n\nOf course, since BF16 and FP16 have the same size of 2 bytes, one doesn't get a free lunch and one pays with really bad precision when using BF16. However, if you remember the training using stochastic gradient descent and its variations is a sort of stumbling walk, so if you don't get the perfect direction immediately it's no problem, you will correct yourself in the next steps.\n\nRegardless of whether one uses BF16 or FP16 there is also a copy of weights which is always in FP32 - this is what gets updated by the optimizer. So the 16-bit formats are only used for the computation, the optimizer updates the FP32 weights with full precision and then casts them into the 16-bit format for the next iteration.\n\nAll PyTorch components have been updated to ensure that they perform any accumulation in FP32, so no loss happening there.\n\nOne crucial issue is gradient accumulation, and it's one of the main features of pipeline parallelism as the gradients from each microbatch processing get accumulated. It's crucial to implement gradient accumulation in FP32 to keep the training precise, and this is what `BF16Optimizer` does.\n\nBesides other improvements we believe that using BF16 mixed precision training turned a potential nightmare into a relatively smooth process which can be observed from the following lm loss graph:\n\n![176B-fail](assets/86_bloom_megatron_deepspeed/176b-lm-loss.png)\n\n## Fused CUDA Kernels\n\nThe GPU performs two things. It can copy data to/from memory and perform computations on that data. While the GPU is busy copying the GPU's computations units idle. If we want to efficiently utilize the GPU we want to minimize the idle time.", "Bitsandbytes uses the fp4 scheme, with the compute in fp16.\n\n### Batch size = 1\n\n|quantization |act_order|bits|group_size|kernel|Load time (s)|Per-token latency (ms)|Throughput (tok/s)|Peak memory (MB)|\n|-----|---------|----|----------|------|-------------|----------------------|------------------|----------------|\n|None|None     |None|None      |None  |26.0         |36.958                |27.058            |29152.98        |\n| gptq | False | 4 | 128 | exllamav2 | 36.07 | 32.25 | 31.01 | 11313.75 |\n|gptq |False    |4   |128       |exllama|36.2         |33.711                |29.663            |10484.34        |\n|gptq |False    |4   |128       |autogptq-cuda-old|36.2         |46.44                 |21.53             |10344.62        |\n|bitsandbytes|None     |None|None      |None  |37.64        |52.00                 |19.23             |11018.36       |\n\n### Batch size = 2"], "eval_score_GPT4": "1", "eval_feedback_GPT4": "The response provided does not align with the reference answer, which states that the recommended multiple of batch size for fp16 data type on an A100 GPU is 64. The response incorrectly states that the recommended multiple is 11, which is not supported by the reference answer. Additionally, the response does not complete its explanation, leaving the sentence unfinished. This indicates a lack of accuracy and factualness as per the score rubric."}, {"question": "How do you run a Gradio Blocks app in reload mode using a Python IDE?\n", "true_answer": "Run `gradio run.py` in the terminal.", "source_doc": "gradio-app/gradio/blob/main/guides/09_other-tutorials/developing-faster-with-reload-mode.md", "generated_answer": "To run a Gradio Blocks app in reload mode using a Python IDE, follow these steps:\n\n1. Make sure your app is written using Gradio Blocks.\n2. Save your app as a Python file (e.g., `app.py`).\n3. Open your Python IDE (e.g., VS Code, PyCharm) and open your app file.\n4. Run your app using the `gradio run.py", "retrieved_docs": ["Developing Faster with Auto-Reloading\n\n**Prerequisite**: This Guide requires you to know about Blocks. Make sure to [read the Guide to Blocks first](https://gradio.app/guides/quickstart/#blocks-more-flexibility-and-control).\n\nThis guide covers auto reloading, reloading in a Python IDE, and using gradio with Jupyter Notebooks.\n\n## Why Auto-Reloading?\n\nWhen you are building a Gradio demo, particularly out of Blocks, you may find it cumbersome to keep re-running your code to test your changes.\n\nTo make it faster and more convenient to write your code, we've made it easier to \"reload\" your Gradio apps instantly when you are developing in a **Python IDE** (like VS Code, Sublime Text, PyCharm, or so on) or generally running your Python code from the terminal. We've also developed an analogous \"magic command\" that allows you to re-run cells faster if you use **Jupyter Notebooks** (or any similar environment like Colab).\n\nThis short Guide will cover both of these methods, so no matter how you write Python, you'll leave knowing how to build Gradio apps faster.\n\n## Python IDE Reload \ud83d\udd25\n\nIf you are building Gradio Blocks using a Python IDE, your file of code (let's name it `run.py`) might look something like this:\n\n```python\nimport gradio as gr\n\nwith gr.Blocks() as demo:\n    gr.Markdown(\"# Greetings from Gradio!\")\n    inp = gr.Textbox(placeholder=\"What is your name?\")\n    out = gr.Textbox()\n\n    inp.change(fn=lambda x: f\"Welcome, {x}!\",\n               inputs=inp,\n               outputs=out)\n\nif __name__ == \"__main__\":\n    demo.launch()\n```", "```python\nimport gradio as gr\n\nwith gr.Blocks() as my_demo:\n    gr.Markdown(\"# Greetings from Gradio!\")\n    inp = gr.Textbox(placeholder=\"What is your name?\")\n    out = gr.Textbox()\n\n    inp.change(fn=lambda x: f\"Welcome, {x}!\",\n               inputs=inp,\n               outputs=out)\n\nif __name__ == \"__main__\":\n    my_demo.launch()\n```\n\nThen you would launch it in reload mode like this: `gradio run.py my_demo`.\n\nBy default, the Gradio use UTF-8 encoding for scripts. **For reload mode**, If you are using encoding formats other than UTF-8 (such as cp1252), make sure you've done like this:\n\n1. Configure encoding declaration of python script, for example: `# -*- coding: cp1252 -*-`\n2. Confirm that your code editor has identified that encoding format. \n3. Run like this: `gradio run.py --encoding cp1252`\n\n\ud83d\udd25 If your application accepts command line arguments, you can pass them in as well. Here's an example:\n\n```python\nimport gradio as gr\nimport argparse\n\nparser = argparse.ArgumentParser()\nparser.add_argument(\"--name\", type=str, default=\"User\")\nargs, unknown = parser.parse_known_args()\n\nwith gr.Blocks() as demo:\n    gr.Markdown(f\"# Greetings {args.name}!\")\n    inp = gr.Textbox()\n    out = gr.Textbox()\n\n    inp.change(fn=lambda x: x, inputs=inp, outputs=out)\n\nif __name__ == \"__main__\":\n    demo.launch()\n```", "Now, run your code. If you've written the Python code in a file named, for example, `app.py`, then you would run `python app.py` from the terminal.\n\nThe demo below will open in a browser on [http://localhost:7860](http://localhost:7860) if running from a file. If you are running within a notebook, the demo will appear embedded within the notebook.\n\n$demo_hello_world_4\n\nType your name in the textbox on the left, drag the slider, and then press the Submit button. You should see a friendly greeting on the right.\n\nTip: When developing locally, you can run your Gradio app in <strong>hot reload mode</strong>, which automatically reloads the Gradio app whenever you make changes to the file. To do this, simply type in <code>gradio</code> before the name of the file instead of <code>python</code>. In the example above, you would type: `gradio app.py` in your terminal. Learn more about hot reloading in the <a href=\"https://www.gradio.app/guides/developing-faster-with-reload-mode\">Hot Reloading Guide</a>.\n\n\n**Understanding the `Interface` Class**\n\nYou'll notice that in order to make your first demo, you created an instance of the `gr.Interface` class. The `Interface` class is designed to create demos for machine learning models which accept one or more inputs, and return one or more outputs. \n\nThe `Interface` class has three core arguments:\n\n- `fn`: the function to wrap a user interface (UI) around\n- `inputs`: the Gradio component(s) to use for the input. The number of components should match the number of arguments in your function.\n- `outputs`: the Gradio component(s) to use for the output. The number of components should match the number of return values from your function.", "if __name__ == \"__main__\":\n    demo.launch()\n```\n\nThe problem is that anytime that you want to make a change to your layout, events, or components, you have to close and rerun your app by writing `python run.py`.\n\nInstead of doing this, you can run your code in **reload mode** by changing 1 word: `python` to `gradio`:\n\nIn the terminal, run `gradio run.py`. That's it!\n\nNow, you'll see that after you'll see something like this:\n\n```bash\nWatching: '/Users/freddy/sources/gradio/gradio', '/Users/freddy/sources/gradio/demo/'\n\nRunning on local URL:  http://127.0.0.1:7860\n```\n\nThe important part here is the line that says `Watching...` What's happening here is that Gradio will be observing the directory where `run.py` file lives, and if the file changes, it will automatically rerun the file for you. So you can focus on writing your code, and your Gradio demo will refresh automatically \ud83e\udd73\n\n\u26a0\ufe0f Warning: the `gradio` command does not detect the parameters passed to the `launch()` methods because the `launch()` method is never called in reload mode. For example, setting `auth`, or `show_error` in `launch()` will not be reflected in the app.\n\nThere is one important thing to keep in mind when using the reload mode: Gradio specifically looks for a Gradio Blocks/Interface demo called `demo` in your code. If you have named your demo something else, you will need to pass in the name of your demo as the 2nd parameter in your code. So if your `run.py` file looked like this:\n\n```python\nimport gradio as gr", "Type your name in the textbox on the left, drag the slider, and then press the Submit button. You should see a friendly greeting on the right.\n\n> [!TIP]\n > When developing locally, you can run your Gradio app in <strong>hot reload mode</strong>, which automatically reloads the Gradio app whenever you make changes to the file. To do this, simply type in <code>gradio</code> before the name of the file instead of <code>python</code>. In the example above, you would type: `gradio app.py` in your terminal. Learn more about hot reloading in the <a href=\"https://www.gradio.app/guides/developing-faster-with-reload-mode\">Hot Reloading Guide</a>.\n\n\n**Understanding the `Interface` Class**\n\nYou'll notice that in order to make your first demo, you created an instance of the `gr.Interface` class. The `Interface` class is designed to create demos for machine learning models which accept one or more inputs, and return one or more outputs. \n\nThe `Interface` class has three core arguments:\n\n- `fn`: the function to wrap a user interface (UI) around\n- `inputs`: the Gradio component(s) to use for the input. The number of components should match the number of arguments in your function.\n- `outputs`: the Gradio component(s) to use for the output. The number of components should match the number of return values from your function.\n\nThe `fn` argument is very flexible -- you can pass *any* Python function that you want to wrap with a UI. In the example above, we saw a relatively simple function, but the function could be anything from a music generator to a tax calculator to the prediction function of a pretrained machine learning model.", "* `elem_id` and `elem_classes` allow Gradio app developers to target your component with custom CSS and JavaScript from the Python `Blocks` class.\n\n* `scale` and `min_width` allow Gradio app developers to control how much space your component takes up in the UI.\n\n* `loading_status` is used to display a loading status over the component when it is the output of an event.\n\n* `mode` is how the parent Gradio app tells your component whether the `interactive` or `static` version should be displayed.\n\n* `gradio`: The `gradio` object is created by the parent Gradio app. It stores some application-level configuration that will be useful in your component, like internationalization. You must use it to dispatch events from your component.\n\nA minimal `Index.svelte` file would look like:\n\n```typescript\n<script lang=\"ts\">\n\timport type { LoadingStatus } from \"@gradio/statustracker\";\n    import { Block } from \"@gradio/atoms\";\n\timport { StatusTracker } from \"@gradio/statustracker\";\n\timport type { Gradio } from \"@gradio/utils\";\n\n\texport let gradio: Gradio<{\n\t\tevent_1: never;\n\t\tevent_2: never;\n\t}>;\n\n    export let value = \"\";\n\texport let elem_id = \"\";\n\texport let elem_classes: string[] = [];\n\texport let scale: number | null = null;\n\texport let min_width: number | undefined = undefined;\n\texport let loading_status: LoadingStatus | undefined = undefined;\n    export let mode: \"static\" | \"interactive\";\n</script>", "You can build very custom and complex applications using `gr.Blocks()`. For example, the popular image generation [Automatic1111 Web UI](https://github.com/AUTOMATIC1111/stable-diffusion-webui) is built using Gradio Blocks. We dive deeper into the `gr.Blocks` on our series on [building with Blocks](https://www.gradio.app/guides/blocks-and-event-listeners).\n\n\n### The Gradio Python & JavaScript Ecosystem\n\nThat's the gist of the core `gradio` Python library, but Gradio is actually so much more! Its an entire ecosystem of Python and JavaScript libraries that let you build machine learning applications, or query them programmatically, in Python or JavaScript. Here are other related parts of the Gradio ecosystem:\n\n* [Gradio Python Client](https://www.gradio.app/guides/getting-started-with-the-python-client) (`gradio_client`): query any Gradio app programmatically in Python.\n* [Gradio JavaScript Client](https://www.gradio.app/guides/getting-started-with-the-js-client) (`@gradio/client`): query any Gradio app programmatically in JavaScript.\n* [Gradio-Lite](https://www.gradio.app/guides/gradio-lite) (`@gradio/lite`): write Gradio apps in Python that run entirely in the browser (no server needed!), thanks to Pyodide. \n* [Hugging Face Spaces](https://huggingface.co/spaces): the most popular place to host Gradio applications \u2014 for free!\n\n## What's Next?\n\nKeep learning about Gradio sequentially using the Gradio Guides, which include explanations as well as example code and embedded interactive demos. Next up: [key features about Gradio demos](https://www.gradio.app/guides/key-features)."], "eval_score_GPT4": "3", "eval_feedback_GPT4": "Feedback: The response provided is somewhat correct as it does mention saving the app as a Python file and running it from an IDE, which is part of the process. However, it fails to mention the specific command to enable reload mode, which is crucial for the task. The reference answer suggests using `gradio run.py` in the terminal, but the response does not clarify how to enable reload mode or if the command provided is to be used in the IDE's terminal. The response is also incomplete because it ends with \"Run your app using the `gradio run.py\" without finishing the sentence or providing the full command. Therefore, the response is not completely accurate or factual in the context of the instruction to run in reload mode."}, {"question": "What command is used to install the development version of the \ud83e\udd17 Transformers library in a Python virtual environment?\n", "true_answer": "pip install \"transformers[sentencepiece]\"", "source_doc": "huggingface/course/blob/main/chapters/en/chapter0/1.mdx", "generated_answer": "The command to install the development version of the \ud83e\udd17 Transformers library in a Python virtual environment is:\n\n```bash\npip install git+https://github.com/huggingface/transformers\n```\n\nThis command installs the bleeding edge `main` version of the library, which may not always be stable, but is useful for staying up-to-date with the latest developments, such as fixing bugs that have been resolved", "retrieved_docs": ["From inside this directory, create a virtual environment using the Python `venv` module:\n\n```\npython -m venv .env\n```\n\nYou should now have a directory called *.env* in your otherwise empty folder:\n\n```\nls -a\n```\n\n```out\n.      ..    .env\n```\n\nYou can jump in and out of your virtual environment with the `activate` and `deactivate` scripts:\n\n```\n# Activate the virtual environment\nsource .env/bin/activate\n\n# Deactivate the virtual environment\nsource .env/bin/deactivate\n```\n\nYou can make sure that the environment is activated by running the `which python` command: if it points to the virtual environment, then you have successfully activated it!\n\n```\nwhich python\n```\n\n```out\n/home/<user>/transformers-course/.env/bin/python\n```\n\n### Installing dependencies[[installing-dependencies]]\n\nAs in the previous section on using Google Colab instances, you'll now need to install the packages required to continue. Again, you can install the development version of \ud83e\udd17 Transformers using the `pip` package manager:\n\n```\npip install \"transformers[sentencepiece]\"\n```\n\nYou're now all set up and ready to go!", "First, create a virtual environment with the version of Python you're going to use and activate it.\n\nThen, you will need to install at least one of Flax, PyTorch, or TensorFlow.\nPlease refer to [TensorFlow installation page](https://www.tensorflow.org/install/), [PyTorch installation page](https://pytorch.org/get-started/locally/#start-locally) and/or [Flax](https://github.com/google/flax#quick-install) and [Jax](https://github.com/google/jax#installation) installation pages regarding the specific installation command for your platform.\n\nWhen one of those backends has been installed, \ud83e\udd17 Transformers can be installed using pip as follows:\n\n```bash\npip install transformers\n```\n\nIf you'd like to play with the examples or need the bleeding edge of the code and can't wait for a new release, you must [install the library from source](https://huggingface.co/docs/transformers/installation#installing-from-source).\n\n### With conda\n\nSince Transformers version v4.0.0, we now have a conda channel: `huggingface`.\n\n\ud83e\udd17 Transformers can be installed using conda as follows:\n\n```shell script\nconda install -c huggingface transformers\n```\n\nFollow the installation pages of Flax, PyTorch or TensorFlow to see how to install them with conda.\n\n> **_NOTE:_**  On Windows, you may be prompted to activate Developer Mode in order to benefit from caching. If this is not an option for you, please let us know in [this issue](https://github.com/huggingface/huggingface_hub/issues/1062).\n\n## Model architectures", "<Tip warning={true}>\n\nYou must keep the `transformers` folder if you want to keep using the library.\n\n</Tip>\n\nNow you can easily update your clone to the latest version of \ud83e\udd17 Transformers with the following command:\n\n```bash\ncd ~/transformers/\ngit pull\n```\n\nYour Python environment will find the `main` version of \ud83e\udd17 Transformers on the next run.\n\n## Install with conda\n\nInstall from the conda channel `huggingface`:\n\n```bash\nconda install -c huggingface transformers\n```\n\n## Cache setup\n\nPretrained models are downloaded and locally cached at: `~/.cache/huggingface/hub`. This is the default directory given by the shell environment variable `TRANSFORMERS_CACHE`. On Windows, the default directory is given by `C:\\Users\\username\\.cache\\huggingface\\hub`. You can change the shell environment variables shown below - in order of priority - to specify a different cache directory:\n\n1. Shell environment variable (default): `HUGGINGFACE_HUB_CACHE` or `TRANSFORMERS_CACHE`.\n2. Shell environment variable: `HF_HOME`.\n3. Shell environment variable: `XDG_CACHE_HOME` + `/huggingface`.\n\n<Tip>\n\n\ud83e\udd17 Transformers will use the shell environment variables `PYTORCH_TRANSFORMERS_CACHE` or `PYTORCH_PRETRAINED_BERT_CACHE` if you are coming from an earlier iteration of this library and have set those environment variables, unless you specify the shell environment variable `TRANSFORMERS_CACHE`.\n\n</Tip>\n\n## Offline mode\n\nRun \ud83e\udd17 Transformers in a firewalled or offline environment with locally cached files by setting the environment variable `TRANSFORMERS_OFFLINE=1`.\n\n<Tip>\n\nAdd [\ud83e\udd17 Datasets](https://huggingface.co/docs/datasets/) to your offline training workflow with the environment variable `HF_DATASETS_OFFLINE=1`.\n\n</Tip>", "Since those choices are not always obvious when in doubt feel free to ask on Discord or even better post your question on the forum, as was \ndone, *e.g.* [here](https://discuss.huggingface.co/t/spanish-asr-fine-tuning-wav2vec2/4586).\n\n## How to install relevant libraries\n\nThe following libraries are required to fine-tune a speech model with \ud83e\udd17 Transformers and \ud83e\udd17 Datasets in PyTorch.\n\n- [PyTorch](https://pytorch.org/)\n- [Transformers](https://github.com/huggingface/transformers)\n- [Datasets](https://github.com/huggingface/datasets)\n\nWe recommend installing the above libraries in a [virtual environment](https://docs.python.org/3/library/venv.html). \nIf you're unfamiliar with Python virtual environments, check out the [user guide](https://packaging.python.org/guides/installing-using-pip-and-virtual-environments/). Create a virtual environment with the version of Python you're going\nto use and activate it.\n\nYou should be able to run the command:\n\n```bash\npython3 -m venv <your-venv-name>\n```\n\nYou can activate your venv by running\n\n```bash\nsource ~/<your-venv-name>/bin/activate\n```\n\nTo begin with please make sure you have PyTorch and CUDA correctly installed. \nThe following command should return ``True``:\n\n```bash\npython -c \"import torch; print(torch.cuda.is_available())\"\n```\n\nIf the above command doesn't print ``True``, in the first step, please follow the\ninstructions [here](https://pytorch.org/) to install PyTorch with CUDA.", "## Install from source\n\nInstall \ud83e\udd17 Transformers from source with the following command:\n\n```bash\npip install git+https://github.com/huggingface/transformers\n```\n\nThis command installs the bleeding edge `main` version rather than the latest `stable` version. The `main` version is useful for staying up-to-date with the latest developments. For instance, if a bug has been fixed since the last official release but a new release hasn't been rolled out yet. However, this means the `main` version may not always be stable. We strive to keep the `main` version operational, and most issues are usually resolved within a few hours or a day. If you run into a problem, please open an [Issue](https://github.com/huggingface/transformers/issues) so we can fix it even sooner!\n\nCheck if \ud83e\udd17 Transformers has been properly installed by running the following command:\n\n```bash\npython -c \"from transformers import pipeline; print(pipeline('sentiment-analysis')('I love you'))\"\n```\n\n## Editable install\n\nYou will need an editable install if you'd like to:\n\n* Use the `main` version of the source code.\n* Contribute to \ud83e\udd17 Transformers and need to test changes in the code.\n\nClone the repository and install \ud83e\udd17 Transformers with the following commands:\n\n```bash\ngit clone https://github.com/huggingface/transformers.git\ncd transformers\npip install -e .\n```\n\nThese commands will link the folder you cloned the repository to and your Python library paths. Python will now look inside the folder you cloned to in addition to the normal library paths. For example, if your Python packages are typically installed in `~/anaconda3/envs/main/lib/python3.7/site-packages/`, Python will also search the folder you cloned to: `~/transformers/`.\n\n<Tip warning={true}>\n\nYou must keep the `transformers` folder if you want to keep using the library.\n\n</Tip>", "Start by creating a virtual environment in your project directory:\n\n```bash\npython -m venv .env\n```\n\nActivate the virtual environment. On Linux and MacOs:\n\n```bash\nsource .env/bin/activate\n```\nActivate Virtual environment on Windows\n\n```bash\n.env/Scripts/activate\n```\n\nNow you're ready to install \ud83e\udd17 Transformers with the following command:\n\n```bash\npip install transformers\n```\n\nFor CPU-support only, you can conveniently install \ud83e\udd17 Transformers and a deep learning library in one line. For example, install \ud83e\udd17 Transformers and PyTorch with:\n\n```bash\npip install 'transformers[torch]'\n```\n\n\ud83e\udd17 Transformers and TensorFlow 2.0:\n\n```bash\npip install 'transformers[tf-cpu]'\n```\n\n<Tip warning={true}>\n\nM1 / ARM Users\n    \nYou will need to install the following before installing TensorFLow 2.0\n```\nbrew install cmake\nbrew install pkg-config\n```\n\n</Tip>\n\n\ud83e\udd17 Transformers and Flax:\n\n```bash\npip install 'transformers[flax]'\n```\n\nFinally, check if \ud83e\udd17 Transformers has been properly installed by running the following command. It will download a pretrained model:\n\n```bash\npython -c \"from transformers import pipeline; print(pipeline('sentiment-analysis')('we love you'))\"\n```\n\nThen print out the label and score:\n\n```bash\n[{'label': 'POSITIVE', 'score': 0.9998704791069031}]\n```\n\n## Install from source\n\nInstall \ud83e\udd17 Transformers from source with the following command:\n\n```bash\npip install git+https://github.com/huggingface/transformers\n```", "!---\nCopyright 2022 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\");\nyou may not use this file except in compliance with the License.\nYou may obtain a copy of the License at\n\n    http://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software\ndistributed under the License is distributed on an \"AS IS\" BASIS,\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\nSee the License for the specific language governing permissions and\nlimitations under the License.\n\n\u26a0\ufe0f Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\nrendered properly in your Markdown viewer.\n\n-->\n\n# Installation\n\nInstall \ud83e\udd17 Transformers for whichever deep learning library you're working with, setup your cache, and optionally configure \ud83e\udd17 Transformers to run offline.\n\n\ud83e\udd17 Transformers is tested on Python 3.6+, PyTorch 1.1.0+, TensorFlow 2.0+, and Flax. Follow the installation instructions below for the deep learning library you are using:\n\n* [PyTorch](https://pytorch.org/get-started/locally/) installation instructions.\n* [TensorFlow 2.0](https://www.tensorflow.org/install/pip) installation instructions.\n* [Flax](https://flax.readthedocs.io/en/latest/) installation instructions.\n\n## Install with pip\n\nYou should install \ud83e\udd17 Transformers in a [virtual environment](https://docs.python.org/3/library/venv.html). If you're unfamiliar with Python virtual environments, take a look at this [guide](https://packaging.python.org/guides/installing-using-pip-and-virtual-environments/). A virtual environment makes it easier to manage different projects, and avoid compatibility issues between dependencies.\n\nStart by creating a virtual environment in your project directory:"], "eval_score_GPT4": "4", "eval_feedback_GPT4": "Feedback: The response provides a command that is indeed used to install a version of the \ud83e\udd17 Transformers library directly from the repository, which could be considered a development version. However, the reference answer specifies the installation of the library with the 'sentencepiece' extra, which is not included in the evaluated response. The response is mostly correct as it does install the library from the repository, which is a form of the development version, but it lacks the specificity of including 'sentencepiece' as indicated in the reference answer."}, {"question": "How can you install the Hugging Face Unity API in your Unity project?\n", "true_answer": "To install the Hugging Face Unity API in your Unity project, go to `Window` -> `Package Manager`, click `+` and select `Add Package from git URL`, then enter `https://github.com/huggingface/unity-api.git`.", "source_doc": "huggingface/blog/blob/main/unity-api.md", "generated_answer": "To install the Hugging Face Unity API in your Unity project, follow these steps:\n\n1. Open your Unity project.\n2. Go to `Window` > `Package Manager`.\n3. Click the `+` button and select `Add Package from git URL`.\n4. Enter `https://github.com/huggingface/unity-api.git` and click `Add`.\n5. Once the package is installed, the Hugging Face", "retrieved_docs": ["--\ntitle: \"How to Install and Use the Hugging Face Unity API\"\nthumbnail: /blog/assets/124_ml-for-games/unity-api-thumbnail.png\nauthors:\n- user: dylanebert\n---\n\n# How to Install and Use the Hugging Face Unity API\n\n<!-- {authors} --> \n\nThe [Hugging Face Unity API](https://github.com/huggingface/unity-api) is an easy-to-use integration of the [Hugging Face Inference API](https://huggingface.co/inference-api), allowing developers to access and use Hugging Face AI models in their Unity projects. In this blog post, we'll walk through the steps to install and use the Hugging Face Unity API.\n\n## Installation\n\n1. Open your Unity project\n2. Go to `Window` -> `Package Manager`\n3. Click `+` and select `Add Package from git URL`\n4. Enter `https://github.com/huggingface/unity-api.git`\n5. Once installed, the Unity API wizard should pop up. If not, go to `Window` -> `Hugging Face API Wizard`\n\n<figure class=\"image text-center\">\n  <img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/124_ml-for-games/packagemanager.gif\">\n</figure>", "6. Enter your API key. Your API key can be created in your [Hugging Face account settings](https://huggingface.co/settings/tokens).\n7. Test the API key by clicking `Test API key` in the API Wizard.\n8. Optionally, change the model endpoints to change which model to use. The model endpoint for any model that supports the inference API can be found by going to the model on the Hugging Face website, clicking `Deploy` -> `Inference API`, and copying the url from the `API_URL` field.\n9. Configure advanced settings if desired. For up-to-date information, visit the project repository at `https://github.com/huggingface/unity-api`\n10. To see examples of how to use the API, click `Install Examples`. You can now close the API Wizard.\n\n<figure class=\"image text-center\">\n  <img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/124_ml-for-games/apiwizard.png\">\n</figure> \n\nNow that the API is set up, you can make calls from your scripts to the API. Let's look at an example of performing a Sentence Similarity task:\n\n```\nusing HuggingFace.API;\n\n/* other code */\n\n// Make a call to the API\nvoid Query() {\n    string inputText = \"I'm on my way to the forest.\";\n    string[] candidates = {\n        \"The player is going to the city\",\n        \"The player is going to the wilderness\",\n        \"The player is wandering aimlessly\"\n    };\n    HuggingFaceAPI.SentenceSimilarity(inputText, OnSuccess, OnError, candidates);\n}\n\n// If successful, handle the result\nvoid OnSuccess(float[] result) {\n    foreach(float value in result) {\n        Debug.Log(value);\n    }\n}\n\n// Otherwise, handle the error\nvoid OnError(string error) {\n    Debug.LogError(error);\n}\n\n/* other code */\n```", "// Otherwise, handle the error\nvoid OnError(string error) {\n    Debug.LogError(error);\n}\n\n/* other code */\n```\n\n## Supported Tasks and Custom Models\n\nThe Hugging Face Unity API also currently supports the following tasks:\n\n- [Conversation](https://huggingface.co/tasks/conversational)\n- [Text Generation](https://huggingface.co/tasks/text-generation)\n- [Text to Image](https://huggingface.co/tasks/text-to-image)\n- [Text Classification](https://huggingface.co/tasks/text-classification)\n- [Question Answering](https://huggingface.co/tasks/question-answering)\n- [Translation](https://huggingface.co/tasks/translation)\n- [Summarization](https://huggingface.co/tasks/summarization)\n- [Speech Recognition](https://huggingface.co/tasks/automatic-speech-recognition)\n\nUse the corresponding methods provided by the `HuggingFaceAPI` class to perform these tasks.\n\nTo use your own custom model hosted on Hugging Face, change the model endpoint in the API Wizard.\n\n## Usage Tips\n\n1. Keep in mind that the API makes calls asynchronously, and returns a response or error via callbacks.\n2. Address slow response times or performance issues by changing model endpoints to lower resource models.\n\n## Conclusion\n\nThe Hugging Face Unity API offers a simple way to integrate AI models into your Unity projects. We hope you found this tutorial helpful. If you have any questions or would like to get more involved in using Hugging Face for Games, join the [Hugging Face Discord](https://hf.co/join/discord)!", "--\ntitle: \"AI Speech Recognition in Unity\"\nthumbnail: /blog/assets/124_ml-for-games/unity-asr-thumbnail.png\nauthors:\n- user: dylanebert\n---\n\n# AI Speech Recognition in Unity\n\n\n[![Open Source AI Game Jam](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/124_ml-for-games/gamejambanner.png)](https://itch.io/jam/open-source-ai-game-jam)\n\n## Introduction\n\nThis tutorial guides you through the process of implementing state-of-the-art Speech Recognition in your Unity game using the Hugging Face Unity API. This feature can be used for giving commands, speaking to an NPC, improving accessibility, or any other functionality where converting spoken words to text may be useful.\n\nTo try Speech Recognition in Unity for yourself, check out the [live demo in itch.io](https://individualkex.itch.io/speech-recognition-demo).\n\n### Prerequisites\n\nThis tutorial assumes basic knowledge of Unity. It also requires you to have installed the [Hugging Face Unity API](https://github.com/huggingface/unity-api). For instructions on setting up the API, check out our [earlier blog post](https://huggingface.co/blog/unity-api).\n\n## Steps\n\n### 1. Set up the Scene\n\nIn this tutorial, we'll set up a very simple scene where the player can start and stop a recording, and the result will be converted to text.\n\nBegin by creating a Unity project, then creating a Canvas with four UI elements:\n\n1. **Start Button**: This will start the recording.\n2. **Stop Button**: This will stop the recording.\n3. **Text (TextMeshPro)**: This is where the result of the speech recognition will be displayed.\n\n### 2. Set up the Script", "## Step 10: Enable Git-LFS for Large File Storage\n\nNavigate to your repository. Use the following commands to track large build files.\n\n```\ngit lfs install\ngit lfs track Build/* \n```\n\n## Step 11: Push your Changes\n\nFinally, use the following Git commands to push your changes:\n\n```\ngit add .\ngit commit -m \"Add Unity WebGL build files\"\ngit push\n```\n\n## Done!\n\nCongratulations! Refresh your Space. You should now be able to play your game in a Hugging Face Space.\n\nWe hope you found this tutorial helpful. If you have any questions or would like to get more involved in using Hugging Face for Games, join the [Hugging Face Discord](https://hf.co/join/discord)!", "--\ntitle: \"How to host a Unity game in a Space\"\nthumbnail: /blog/assets/124_ml-for-games/unity-in-spaces-thumbnail.png\nauthors:\n- user: dylanebert\n---\n\n# How to host a Unity game in a Space\n\n<!-- {authors} --> \n\n\nDid you know you can host a Unity game in a Hugging Face Space? No? Well, you can!\n\nHugging Face Spaces are an easy way to build, host, and share demos. While they are typically used for Machine Learning demos, \nthey can also host playable Unity games. Here are some examples:\n- [Huggy](https://huggingface.co/spaces/ThomasSimonini/Huggy)\n- [Farming Game](https://huggingface.co/spaces/dylanebert/FarmingGame) \n- [Unity API Demo](https://huggingface.co/spaces/dylanebert/UnityDemo)\n\nHere's how you can host your own Unity game in a Space.\n\n## Step 1: Create a Space using the Static HTML template\n\nFirst, navigate to [Hugging Face Spaces](https://huggingface.co/new-space) to create a space.\n\n<figure class=\"image text-center\">\n  <img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/124_ml-for-games/games-in-spaces/1.png\">\n</figure> \n\nSelect the \"Static HTML\" template, give your Space a name, and create it.\n\n<figure class=\"image text-center\">\n  <img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/124_ml-for-games/games-in-spaces/2.png\">\n</figure> \n\n## Step 2: Use Git to Clone the Space\n\nClone your newly created Space to your local machine using Git. You can do this by running the following command in your terminal or command prompt:", "Then, in the Player Settings panel, switch the WebGL template to Hugging Face. To do so, in Player Settings, click \"Resolution and Presentation\", then select the Hugging Face WebGL template.\n\n<figure class=\"image text-center\">\n  <img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/124_ml-for-games/games-in-spaces/6.png\">\n</figure> \n\n## Step 7: Change the Compression Format to Disabled\n\nIn the Player Settings panel, navigate to the \"Publishing Settings\" section and change the Compression Format to \"Disabled\".\n\n<figure class=\"image text-center\">\n  <img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/124_ml-for-games/games-in-spaces/7.png\">\n</figure> \n\n## Step 8: Build your Project\n\nReturn to the Build Settings window and click the \"Build\" button. Choose a location to save your build files, and Unity will build the project for WebGL.\n\n<figure class=\"image text-center\">\n  <img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/124_ml-for-games/games-in-spaces/8.png\">\n</figure> \n\n## Step 9: Copy the Contents of the Build Folder\n\nAfter the build process is finished, navigate to the folder containing your build files. Copy the files in the build folder to the repository you cloned in [Step 2](#step-2-use-git-to-clone-the-space).\n<figure class=\"image text-center\">\n  <img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/124_ml-for-games/games-in-spaces/9.png\">\n</figure> \n\n## Step 10: Enable Git-LFS for Large File Storage"], "eval_score_GPT4": "4", "eval_feedback_GPT4": "Feedback: The response provided is mostly correct, accurate, and factual. It follows the same steps as the reference answer, providing a clear and correct sequence of actions to install the Hugging Face Unity API. However, the response is incomplete as it ends abruptly without finishing the last sentence. This leaves out any final steps or confirmation that the installation was successful, which is implied in the reference answer by its brevity and conclusiveness."}, {"question": "What is the pretraining objective of the Wav2Vec2 context network?\n", "true_answer": "The pretraining objective of the Wav2Vec2 context network is a contrastive task where the model has to predict the true quantized speech representation of the masked prediction from a set of false ones.", "source_doc": "huggingface/transformers/blob/main/docs/source/en/tasks_explained.md", "generated_answer": "The pretraining objective of the Wav2Vec2 context network is a novel contrastive learning objective that learns powerful speech representations by randomly masking feature vectors and passing them through a transformer network to learn contextualized speech representations. This objective is similar to BERT's masked language modeling objective, where words are masked and replaced with special tokens, and the model predicts the original words based on the context. In the case of Wav2Vec2, feature vectors are", "retrieved_docs": ["Let\\'s start by creating the tokenizer responsible for decoding the\nmodel\\'s predictions.\n\n### Create Wav2Vec2CTCTokenizer\n\nThe [pretrained Wav2Vec2 checkpoint](https://huggingface.co/facebook/wav2vec2-base) maps the speech signal to a\nsequence of context representations as illustrated in the figure above.\nA fine-tuned Wav2Vec2 checkpoint needs to map this sequence of context\nrepresentations to its corresponding transcription so that a linear\nlayer has to be added on top of the transformer block (shown in yellow).\nThis linear layer is used to classifies each context representation to a\ntoken class analogous how, *e.g.*, after pretraining a linear layer is\nadded on top of BERT\\'s embeddings for further classification - *cf.*\nwith *\\\"BERT\\\"* section of this [blog post](https://huggingface.co/blog/warm-starting-encoder-decoder).\n\nThe output size of this layer corresponds to the number of tokens in the\nvocabulary, which does **not** depend on Wav2Vec2\\'s pretraining task,\nbut only on the labeled dataset used for fine-tuning. So in the first\nstep, we will take a look at Timit and define a vocabulary based on the\ndataset\\'s transcriptions.\n\nLet\\'s start by loading the dataset and taking a look at its structure.\n\n```python\nfrom datasets import load_dataset, load_metric\n\ntimit = load_dataset(\"timit_asr\")\n\nprint(timit)\n```", "Using Connectionist Temporal Classification (CTC), pre-trained\nWav2Vec2-like checkpoints are extremely easy to fine-tune on downstream\nspeech recognition tasks. In a nutshell, fine-tuning pre-trained\nWav2Vec2 checkpoints works as follows:\n\nA single randomly initialized linear layer is stacked on top of the\npre-trained checkpoint and trained to classify raw audio input to a\nsequence of letters. It does so by:\n\n1.  extracting audio representations from the raw audio (using CNN\n    layers),\n2.  processing the sequence of audio representations with a stack of\n    transformer layers, and,\n3.  classifying the processed audio representations into a sequence of\n    output letters.\n\nPreviously audio classification models required an additional language\nmodel (LM) and a dictionary to transform the sequence of classified audio\nframes to a coherent transcription. Wav2Vec2's architecture is based on\ntransformer layers, thus giving each processed audio representation\ncontext from all other audio representations. In addition, Wav2Vec2\nleverages the [CTC algorithm](https://distill.pub/2017/ctc/) for\nfine-tuning, which solves the problem of alignment between a varying\n\"input audio length\"-to-\"output text length\" ratio.\n\nHaving contextualized audio classifications and no alignment problems,\nWav2Vec2 does not require an external language model or dictionary to\nyield acceptable audio transcriptions.\n\nAs can be seen in Appendix C of the [official\npaper](https://arxiv.org/abs/2006.11477), Wav2Vec2 gives impressive\ndownstream performances on [LibriSpeech](https://huggingface.co/datasets/librispeech_asr) without using a language model at\nall. However, from the appendix, it also becomes clear that using Wav2Vec2\nin combination with a language model can yield a significant\nimprovement, especially when the model was trained on only 10 minutes of\ntranscribed audio.", "--\ntitle: \"Fine-Tune Wav2Vec2 for English ASR in Hugging Face with \ud83e\udd17 Transformers\"\nthumbnail: /blog/assets/15_fine_tune_wav2vec2/wav2vec2.png\nauthors:\n- user: patrickvonplaten\n---\n\n# Fine-Tune Wav2Vec2 for English ASR with \ud83e\udd17 Transformers\n\n\n<a target=\"_blank\" href=\"https://colab.research.google.com/github/patrickvonplaten/notebooks/blob/master/Fine_tuning_Wav2Vec2_for_English_ASR.ipynb\">\n    <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n</a>\n\nWav2Vec2 is a pretrained model for Automatic Speech Recognition (ASR)\nand was released in [September\n2020](https://ai.facebook.com/blog/wav2vec-20-learning-the-structure-of-speech-from-raw-audio/)\nby Alexei Baevski, Michael Auli, and Alex Conneau.\n\nUsing a novel contrastive pretraining objective, Wav2Vec2 learns\npowerful speech representations from more than 50.000 hours of unlabeled\nspeech. Similar, to [BERT\\'s masked language\nmodeling](http://jalammar.github.io/illustrated-bert/), the model learns\ncontextualized speech representations by randomly masking feature\nvectors before passing them to a transformer network.\n\n![wav2vec2\\_structure](https://raw.githubusercontent.com/patrickvonplaten/scientific_images/master/wav2vec2.png)", "For the first time, it has been shown that pretraining, followed by\nfine-tuning on very little labeled speech data achieves competitive\nresults to state-of-the-art ASR systems. Using as little as 10 minutes\nof labeled data, Wav2Vec2 yields a word error rate (WER) of less than 5%\non the clean test set of\n[LibriSpeech](https://huggingface.co/datasets/librispeech_asr) - *cf.*\nwith Table 9 of the [paper](https://arxiv.org/pdf/2006.11477.pdf).\n\nIn this notebook, we will give an in-detail explanation of how\nWav2Vec2\\'s pretrained checkpoints can be fine-tuned on any English ASR\ndataset. Note that in this notebook, we will fine-tune Wav2Vec2 without\nmaking use of a language model. It is much simpler to use Wav2Vec2\nwithout a language model as an end-to-end ASR system and it has been\nshown that a standalone Wav2Vec2 acoustic model achieves impressive\nresults. For demonstration purposes, we fine-tune the \\\"base\\\"-sized\n[pretrained checkpoint](https://huggingface.co/facebook/wav2vec2-base)\non the rather small [Timit](https://huggingface.co/datasets/timit_asr)\ndataset that contains just 5h of training data.\n\nWav2Vec2 is fine-tuned using Connectionist Temporal Classification\n(CTC), which is an algorithm that is used to train neural networks for\nsequence-to-sequence problems and mainly in Automatic Speech Recognition\nand handwriting recognition.\n\nI highly recommend reading the blog post [Sequence Modeling with CTC\n(2017)](https://distill.pub/2017/ctc/) very well-written blog post by\nAwni Hannun.", "This model was contributed by [patrickvonplaten](https://huggingface.co/patrickvonplaten).\n\n## Usage tips\n\n- Wav2Vec2 is a speech model that accepts a float array corresponding to the raw waveform of the speech signal.\n- Wav2Vec2 model was trained using connectionist temporal classification (CTC) so the model output has to be decoded\n  using [`Wav2Vec2CTCTokenizer`].\n\n## Resources\n\nA list of official Hugging Face and community (indicated by \ud83c\udf0e) resources to help you get started with Wav2Vec2. If you're interested in submitting a resource to be included here, please feel free to open a Pull Request and we'll review it! The resource should ideally demonstrate something new instead of duplicating an existing resource.\n\n<PipelineTag pipeline=\"audio-classification\"/>\n\n- A notebook on how to [leverage a pretrained Wav2Vec2 model for emotion classification](https://colab.research.google.com/github/m3hrdadfi/soxan/blob/main/notebooks/Emotion_recognition_in_Greek_speech_using_Wav2Vec2.ipynb). \ud83c\udf0e\n- [`Wav2Vec2ForCTC`] is supported by this [example script](https://github.com/huggingface/transformers/tree/main/examples/pytorch/audio-classification) and [notebook](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/audio_classification.ipynb).\n- [Audio classification task guide](../tasks/audio_classification)\n\n<PipelineTag pipeline=\"automatic-speech-recognition\"/>", "Wav2Vec2 was pretrained on the audio data of\n[LibriSpeech](https://huggingface.co/datasets/librispeech_asr) and\nLibriVox which both were sampling with 16kHz. Our fine-tuning dataset,\n[Timit](hhtps://huggingface.co/datasets/timit_asr), was luckily also\nsampled with 16kHz. If the fine-tuning dataset would have been sampled\nwith a rate lower or higher than 16kHz, we first would have had to up or\ndownsample the speech signal to match the sampling rate of the data used\nfor pretraining.\n\nA Wav2Vec2 feature extractor object requires the following parameters to\nbe instantiated:", "### Create `Wav2Vec2FeatureExtractor`\n\nSpeech is a continuous signal and to be treated by computers, it first has to be discretized, which is usually called **sampling**. The sampling rate hereby plays an important role in that it defines how many data points of the speech signal are measured per second. Therefore, sampling with a higher sampling rate results in a better approximation of the *real* speech signal but also necessitates more values per second.\n\nA pretrained checkpoint expects its input data to have been sampled more or less from the same distribution as the data it was trained on. The same speech signals sampled at two different rates have a very different distribution, *e.g.*, doubling the sampling rate results in twice as many data points. Thus,\nbefore fine-tuning a pretrained checkpoint of an ASR model, it is crucial to verify that the sampling rate of the data that was used to pretrain the model matches the sampling rate of the dataset used to fine-tune the model.\n\nA `Wav2Vec2FeatureExtractor` object requires the following parameters to be instantiated:\n\n-   `feature_size`: Speech models take a sequence of feature vectors as an input. While the length of this sequence obviously varies, the feature size should not. In the case of Wav2Vec2, the feature size is 1 because the model was trained on the raw speech signal \\\\( {}^2 \\\\).\n-   `sampling_rate`: The sampling rate at which the model is trained on.\n-   `padding_value`: For batched inference, shorter inputs need to be padded with a specific value\n-   `do_normalize`: Whether the input should be *zero-mean-unit-variance* normalized or not. Usually, speech models perform better when normalizing the input\n-   `return_attention_mask`: Whether the model should make use of an `attention_mask` for batched inference. In general, XLS-R models checkpoints should **always** use the `attention_mask`.\n\n```python\nfrom transformers import Wav2Vec2FeatureExtractor"], "eval_score_GPT4": "4", "eval_feedback_GPT4": "The response correctly identifies the pretraining objective of the Wav2Vec2 context network as a contrastive learning objective and makes an accurate comparison to BERT's masked language modeling. However, it does not mention the specific task of predicting the true quantized speech representation from a set of false ones, which is a key detail in the reference answer. Therefore, while the response is mostly correct, it lacks complete accuracy due to the omission of this detail."}, {"question": "What is the default checkpoint used by the sentiment analysis pipeline in the Transformers library?\n", "true_answer": "distilbert base uncased finetuned sst2 english", "source_doc": "huggingface/course/blob/main/subtitles/en/raw/chapter2/02_inside-pipeline-pt.md", "generated_answer": "The default checkpoint used by the sentiment analysis pipeline in the Transformers library is \"distilbert-base-uncased-finetuned-sst-2-english\". This information can be found in the context provided, specifically in the code snippets for creating the sentiment analysis pipeline and loading the tokenizer. The checkpoint is downloaded and cached by the `AutoTokenizer` class when it is instantiated with the checkpoint name.", "retrieved_docs": ["hat happens inside the pipeline function? In this video, we will look at what actually happens when we use the pipeline function of the Transformers library. More specifically, we will look at the sentiment analysis pipeline, and how it went from the two following sentences to the positive labels with their respective scores. As we have seen in the pipeline presentation, there are three stages in the pipeline. First, we convert the raw texts to numbers the model can make sense of, using a tokenizer. Then, those numbers go through the model, which outputs logits. Finally, the post-processing steps transforms those logits into labels and scores. Let's look in detail at those three steps, and how to replicate them using the Transformers library, beginning with the first stage, tokenization. The tokenization process has several steps. First, the text is split into small chunks called tokens. They can be words, parts of words or punctuation symbols. Then the tokenizer will had some special tokens (if the model expect them). Here the model uses expects a CLS token at the beginning and a SEP token at the end of the sentence to classify. Lastly, the tokenizer matches each token to its unique ID in the vocabulary of the pretrained model. To load such a tokenizer, the Transformers library provides the AutoTokenizer API. The most important method of this class is from_pretrained, which will download and cache the configuration and the vocabulary associated to a given checkpoint. Here, the checkpoint used by default for the sentiment analysis pipeline is distilbert base uncased finetuned sst2 english. We instantiate a tokenizer associated with that checkpoint, then feed it the two sentences. Since those two sentences are not of the same size, we will need to pad the shortest one to be able to build an array. This is done by the tokenizer with the option padding=True. With truncation=True, we ensure that any sentence longer than the maximum the model can handle is truncated", "hat happens inside the pipeline function? In this video, we will look at what actually happens when we use the pipeline function of the Transformers library. More specifically, we will look at the sentiment analysis pipeline, and how it went from the two following sentences to the positive labels with their respective scores. As we have seen in the pipeline presentation, there are three stages in the pipeline. First, we convert the raw texts to numbers the model can make sense of, using a tokenizer. Then, those numbers go through the model, which outputs logits. Finally, the post-processing steps transforms those logits into labels and scores. Let's look in detail at those three steps, and how to replicate them using the Transformers library, beginning with the first stage, tokenization. The tokenization process has several steps. First, the text is split into small chunks called tokens. They can be words, parts of words or punctuation symbols. Then the tokenizer will had some special tokens (if the model expect them). Here the model uses expects a CLS token at the beginning and a SEP token at the end of the sentence to classify. Lastly, the tokenizer matches each token to its unique ID in the vocabulary of the pretrained model. To load such a tokenizer, the Transformers library provides the AutoTokenizer API. The most important method of this class is from_pretrained, which will download and cache the configuration and the vocabulary associated to a given checkpoint. Here, the checkpoint used by default for the sentiment analysis pipeline is distilbert base uncased finetuned sst2 english. We instantiate a tokenizer associated with that checkpoint, then feed it the two sentences. Since those two sentences are not of the same size, we will need to pad the shortest one to be able to build an array. This is done by the tokenizer with the option padding=True. With truncation=True, we ensure that any sentence longer than the maximum the model can handle is truncated", "Let's quickly go over each of these.\n\n## Preprocessing with a tokenizer[[preprocessing-with-a-tokenizer]]\n\nLike other neural networks, Transformer models can't process raw text directly, so the first step of our pipeline is to convert the text inputs into numbers that the model can make sense of. To do this we use a *tokenizer*, which will be responsible for:\n\n- Splitting the input into words, subwords, or symbols (like punctuation) that are called *tokens*\n- Mapping each token to an integer\n- Adding additional inputs that may be useful to the model\n\nAll this preprocessing needs to be done in exactly the same way as when the model was pretrained, so we first need to download that information from the [Model Hub](https://huggingface.co/models). To do this, we use the `AutoTokenizer` class and its `from_pretrained()` method. Using the checkpoint name of our model, it will automatically fetch the data associated with the model's tokenizer and cache it (so it's only downloaded the first time you run the code below).\n\nSince the default checkpoint of the `sentiment-analysis` pipeline is `distilbert-base-uncased-finetuned-sst-2-english` (you can see its model card [here](https://huggingface.co/distilbert-base-uncased-finetuned-sst-2-english)), we run the following:\n\n```python\nfrom transformers import AutoTokenizer\n\ncheckpoint = \"distilbert-base-uncased-finetuned-sst-2-english\"\ntokenizer = AutoTokenizer.from_pretrained(checkpoint)\n```\n\nOnce we have the tokenizer, we can directly pass our sentences to it and we'll get back a dictionary that's ready to feed to our model! The only thing left to do is to convert the list of input IDs to tensors.", "Start by creating an instance of [`pipeline`] and specifying a task you want to use it for. In this guide, you'll use the [`pipeline`] for sentiment analysis as an example:\n\n```py\n>>> from transformers import pipeline\n\n>>> classifier = pipeline(\"sentiment-analysis\")\n```\n\nThe [`pipeline`] downloads and caches a default [pretrained model](https://huggingface.co/distilbert-base-uncased-finetuned-sst-2-english) and tokenizer for sentiment analysis. Now you can use the `classifier` on your target text:\n\n```py\n>>> classifier(\"We are very happy to show you the \ud83e\udd17 Transformers library.\")\n[{'label': 'POSITIVE', 'score': 0.9998}]\n```\n\nIf you have more than one input, pass your inputs as a list to the [`pipeline`] to return a list of dictionaries:\n\n```py\n>>> results = classifier([\"We are very happy to show you the \ud83e\udd17 Transformers library.\", \"We hope you don't hate it.\"])\n>>> for result in results:\n...     print(f\"label: {result['label']}, with score: {round(result['score'], 4)}\")\nlabel: POSITIVE, with score: 0.9998\nlabel: NEGATIVE, with score: 0.5309\n```\n\nThe [`pipeline`] can also iterate over an entire dataset for any task you like. For this example, let's choose automatic speech recognition as our task:\n\n```py\n>>> import torch\n>>> from transformers import pipeline\n\n>>> speech_recognizer = pipeline(\"automatic-speech-recognition\", model=\"facebook/wav2vec2-base-960h\")\n```", "he pipeline function. The pipeline function is the most high-level API of the Transformers library. It regroups together all the steps to go from raw texts to usable predictions. The model used is at the core of a pipeline, but the pipeline also include all the necessary pre-processing (since the model does not expect texts, but numbers) as well as some post-processing to make the output of the model human-readable. Let's look at a first example with the sentiment analysis pipeline. This pipeline performs text classification on a given input, and determines if it's positive or negative. Here, it attributed the positive label on the given text, with a confidence of 95%. You can pass multiple texts to the same pipeline, which will be processed and passed through the model together, as a batch. The output is a list of individual results, in the same order as the input texts. Here we find the same label and score for the first text, and the second text is judged positive with a confidence of 99.99%. The zero-shot classification pipeline is a more general text-classification pipeline: it allows you to provide the labels you want. Here we want to classify our input text along the labels \"education\", \"politics\" and \"business\". The pipeline successfully recognizes it's more about education than the other labels, with a confidence of 84%. Moving on to other tasks, the text generation pipeline will auto-complete a given prompt. The output is generated with a bit of randomness, so it changes each time you call the generator object on a given prompt. Up until now, we have used the pipeline API with the default model associated to each task, but you can use it with any model that has been pretrained or fine-tuned on this task. Going on the model hub (huggingface.co/models), you can filter the available models by task", "There are more than [215 sentiment analysis models](https://huggingface.co/models?pipeline_tag=text-classification&sort=downloads&search=sentiment) publicly available on the Hub and integrating them with Python just takes 5 lines of code:\n\n\n```python\npip install -q transformers\nfrom transformers import pipeline\nsentiment_pipeline = pipeline(\"sentiment-analysis\")\ndata = [\"I love you\", \"I hate you\"]\nsentiment_pipeline(data)\n```\n\nThis code snippet uses the [pipeline class](https://huggingface.co/docs/transformers/main_classes/pipelines) to make predictions from models available in the Hub. It uses the [default model for sentiment analysis](https://huggingface.co/distilbert-base-uncased-finetuned-sst-2-english?text=I+like+you.+I+love+you) to analyze the list of texts `data` and it outputs the following results:\n\n```python\n[{'label': 'POSITIVE', 'score': 0.9998},\n {'label': 'NEGATIVE', 'score': 0.9991}]\n```\n\nYou can use a specific sentiment analysis model that is better suited to your language or use case by providing the name of the model. For example, if you want a sentiment analysis model for tweets, you can specify the [model id](https://huggingface.co/finiteautomata/bertweet-base-sentiment-analysis):\n\n```python\nspecific_model = pipeline(model=\"finiteautomata/bertweet-base-sentiment-analysis\")\nspecific_model(data)\n```\n\nYou can test these models with your own data using this [Colab notebook](https://colab.research.google.com/drive/1G4nvWf6NtytiEyiIkYxs03nno5ZupIJn?usp=sharing):", "By default, this pipeline selects a particular pretrained model that has been fine-tuned for sentiment analysis in English. The model is downloaded and cached when you create the `classifier` object. If you rerun the command, the cached model will be used instead and there is no need to download the model again.\n\nThere are three main steps involved when you pass some text to a pipeline:\n\n1. The text is preprocessed into a format the model can understand.\n2. The preprocessed inputs are passed to the model.\n3. The predictions of the model are post-processed, so you can make sense of them.\n\n\nSome of the currently [available pipelines](https://huggingface.co/transformers/main_classes/pipelines.html) are:\n\n- `feature-extraction` (get the vector representation of a text)\n- `fill-mask`\n- `ner` (named entity recognition)\n- `question-answering`\n- `sentiment-analysis`\n- `summarization`\n- `text-generation`\n- `translation`\n- `zero-shot-classification`\n\nLet's have a look at a few of these!\n\n## Zero-shot classification[[zero-shot-classification]]\n\nWe'll start by tackling a more challenging task where we need to classify texts that haven't been labelled. This is a common scenario in real-world projects because annotating text is usually time-consuming and requires domain expertise. For this use case, the `zero-shot-classification` pipeline is very powerful: it allows you to specify which labels to use for the classification, so you don't have to rely on the labels of the pretrained model. You've already seen how the model can classify a sentence as positive or negative using those two labels \u2014 but it can also classify the text using any other set of labels you like.\n\n```python\nfrom transformers import pipeline\n\nclassifier = pipeline(\"zero-shot-classification\")\nclassifier(\n    \"This is a course about the Transformers library\",\n    candidate_labels=[\"education\", \"politics\", \"business\"],\n)\n```"], "eval_score_GPT4": "5", "eval_feedback_GPT4": "Feedback: The response provided is completely correct, accurate, and factual. It matches the reference answer, providing the exact default checkpoint used by the sentiment analysis pipeline in the Transformers library. The additional explanation about how the checkpoint is downloaded and cached by the `AutoTokenizer` class is accurate and relevant, although not necessary to fulfill the instruction."}, {"question": "What is the purpose of the notebook \"How to use DeepSpeed to train models with billions of parameters on Habana Gaudi\"?\n", "true_answer": "To show how to use DeepSpeed to pre-train/fine-tune the 1.6B-parameter GPT2-XL for causal language modeling on Habana Gaudi.", "source_doc": "huggingface/optimum/blob/main/docs/source/notebooks.md", "generated_answer": "The purpose of the notebook \"How to use DeepSpeed to train models with billions of parameters on Habana Gaudi\" is to demonstrate how to use DeepSpeed, a library for speed and scale in distributed training of large models with billions of parameters, to pre-train/fine-tune the 1.6B-parameter GPT2-XL for causal language modeling on Habana Gaudi, a second-generation AI hardware accelerator designed by", "retrieved_docs": ["| [How to use DeepSpeed to train models with billions of parameters on Habana Gaudi](https://github.com/huggingface/optimum-habana/blob/main/notebooks/AI_HW_Summit_2022.ipynb) | Show how to use DeepSpeed to pre-train/fine-tune the 1.6B-parameter GPT2-XL for causal language modeling on Habana Gaudi. |  [![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/huggingface/optimum-habana/blob/main/notebooks/AI_HW_Summit_2022.ipynb) | [![Open in AWS Studio](https://studiolab.sagemaker.aws/studiolab.svg)](https://studiolab.sagemaker.aws/import/github/huggingface/optimum-habana/blob/main/notebooks/AI_HW_Summit_2022.ipynb) |", "| [How to use DeepSpeed to train models with billions of parameters on Habana Gaudi](https://github.com/huggingface/optimum-habana/blob/main/notebooks/AI_HW_Summit_2022.ipynb) | Show how to use DeepSpeed to pre-train/fine-tune the 1.6B-parameter GPT2-XL for causal language modeling on Habana Gaudi. |  [![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/huggingface/optimum-habana/blob/main/notebooks/AI_HW_Summit_2022.ipynb) | [![Open in AWS Studio](https://studiolab.sagemaker.aws/studiolab.svg)](https://studiolab.sagemaker.aws/import/github/huggingface/optimum-habana/blob/main/notebooks/AI_HW_Summit_2022.ipynb) |", "To go further, we are looking forward to using HPU graphs for training models even faster and to presenting how to use DeepSpeed ZeRO-3 on Gaudi2 to accelerate the training of your LLMs. Stay tuned!\n\nIf you are interested in accelerating your Machine Learning training and inference workflows using the latest AI hardware accelerators and software libraries, check out our [Expert Acceleration Program](https://huggingface.co/support). To learn more about Habana solutions, [read about our partnership and contact them here](https://huggingface.co/hardware/habana). To learn more about Hugging Face efforts to make AI hardware accelerators easy to use, check out our [Hardware Partner Program](https://huggingface.co/hardware).\n\n\n### Related Topics\n\n- [Faster Training and Inference: Habana Gaudi-2 vs Nvidia A100 80GB](https://huggingface.co/blog/habana-gaudi-2-benchmark)\n- [Fast Inference on Large Language Models: BLOOMZ on Habana Gaudi2 Accelerator](https://huggingface.co/blog/habana-gaudi-2-bloom)", "Such large models raise new challenges in terms of memory and speed for both [training](https://huggingface.co/blog/bloom-megatron-deepspeed) and [inference](https://huggingface.co/blog/bloom-inference-optimization). Even in 16-bit precision, one instance requires 352 GB to fit! You will probably struggle to find any device with so much memory at the moment, but state-of-the-art hardware like Habana Gaudi2 does make it possible to perform inference on BLOOM and BLOOMZ models with low latencies.\n\n\n## Habana Gaudi2\n\n[Gaudi2](https://habana.ai/training/gaudi2/) is the second-generation AI hardware accelerator designed by Habana Labs. A single server contains 8 accelerator devices (called Habana Processing Units, or HPUs) with 96GB of memory each, which provides room to make very large models fit in. However, hosting the model is not very interesting if the computation is slow. Fortunately, Gaudi2 shines on that aspect: it differs from GPUs in that its architecture enables the accelerator to perform General Matrix Multiplication (GeMM) and other operations in parallel, which speeds up deep learning workflows. These features make Gaudi2 a great candidate for LLM training and inference.\n\nHabana's SDK, SynapseAI\u2122, supports PyTorch and DeepSpeed for accelerating LLM training and inference. The [SynapseAI graph compiler](https://docs.habana.ai/en/latest/Gaudi_Overview/SynapseAI_Software_Suite.html#graph-compiler-and-runtime) will optimize the execution of the operations accumulated in the graph (e.g. operator fusion, data layout management, parallelization, pipelining and memory management, and graph-level optimizations).", "### Fine-tuning T5-3B\n\nWith 96 GB of memory per device, Gaudi2 enables running much bigger models. For instance, we managed to fine-tune T5-3B (containing 3 billion parameters) with gradient checkpointing being the only applied memory optimization. This is not possible on first-gen Gaudi.\n[Here](https://huggingface.co/regisss/t5-3b-summarization-gaudi-2/tensorboard?scroll=1#scalars) are the logs of this run where the model was fine-tuned on the CNN DailyMail dataset for text summarization using [this script](https://github.com/huggingface/optimum-habana/tree/main/examples/summarization).\n\nThe results we achieved are presented in the table below. **Gaudi2 is x2.44 faster than A100 80GB.** We observe that we cannot fit a batch size larger than 1 on Gaudi2 here. This is due to the memory space taken by the graph where operations are accumulated during the first iteration of the run. Habana is working on optimizing the memory footprint in future releases of SynapseAI. We are looking forward to expanding this benchmark using newer versions of Habana's SDK and also using [DeepSpeed](https://www.deepspeed.ai/) to see if the same trend holds.\n\n<center>\n\n|   | First-gen Gaudi | Gaudi2 (BS=1) | A100 (BS=16) |\n|:-:|:-------:|:--------------:|:------------:|\n| Throughput (samples/s) | N/A | 19.7 | 8.07 |\n| Speedup | / | x2.44 | x1.0 |\n\n</center>\n\n*BS* is the batch size per device. Gaudi2 and A100 runs were performed in fp32 with gradient checkpointing enabled. All runs were *distributed* runs on *8 devices*.\n\n\n## Conclusion", "!--\u26a0\ufe0f Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\nrendered properly in your Markdown viewer.\n-->\n\n# DeepSpeed\n\n[DeepSpeed](https://www.deepspeed.ai/) is a library designed for speed and scale for distributed training of large models with billions of parameters. At its core is the Zero Redundancy Optimizer (ZeRO) that shards optimizer states (ZeRO-1), gradients (ZeRO-2), and parameters (ZeRO-3) across data parallel processes. This drastically reduces memory usage, allowing you to scale your training to billion parameter models. To unlock even more memory efficiency, ZeRO-Offload reduces GPU compute and memory by leveraging CPU resources during optimization.\n\nBoth of these features are supported in \ud83e\udd17 Accelerate, and you can use them with \ud83e\udd17 PEFT. This guide will help you learn how to use our DeepSpeed [training script](https://github.com/huggingface/peft/blob/main/examples/conditional_generation/peft_lora_seq2seq_accelerate_ds_zero3_offload.py). You'll configure the script to train a large model for conditional generation with ZeRO-3 and ZeRO-Offload.\n\n<Tip>\n\n\ud83d\udca1 To help you get started, check out our example training scripts for [causal language modeling](https://github.com/huggingface/peft/blob/main/examples/causal_language_modeling/peft_lora_clm_accelerate_ds_zero3_offload.py) and [conditional generation](https://github.com/huggingface/peft/blob/main/examples/conditional_generation/peft_lora_seq2seq_accelerate_ds_zero3_offload.py). You can adapt these scripts for your own applications or even use them out of the box if your task is similar to the one in the scripts.\n\n</Tip>\n\n## Configuration", "DeepSpeed is an open-source deep learning optimization library that is integrated with \ud83e\udd17 Transformers and \ud83e\udd17 Accelerate.\nIt provides a wide range of features and optimizations designed to improve the efficiency and scalability of large-scale \ndeep learning training.\n\nIf your model fits onto a single GPU and you have enough space to fit a small batch size, you don't need to use DeepSpeed\nas it'll only slow things down. However, if the model doesn't fit onto a single GPU or you can't fit a small batch, you can \nleverage DeepSpeed ZeRO + CPU Offload, or NVMe Offload for much larger models. In this case, you need to separately\n[install the library](main_classes/deepspeed#installation), then follow one of the guides to create a configuration file \nand launch DeepSpeed: \n \n* For an in-depth guide on DeepSpeed integration with [`Trainer`], review [the corresponding documentation](main_classes/deepspeed), specifically the \n[section for a single GPU](main_classes/deepspeed#deployment-with-one-gpu). Some adjustments are required to use DeepSpeed in a notebook; please take a look at the [corresponding guide](main_classes/deepspeed#deployment-in-notebooks).\n* If you prefer to use \ud83e\udd17 Accelerate, refer to [\ud83e\udd17 Accelerate DeepSpeed guide](https://huggingface.co/docs/accelerate/en/usage_guides/deepspeed).\n\n## Using torch.compile\n\nPyTorch 2.0 introduced a new compile function that doesn't require any modification to existing PyTorch code but can \noptimize your code by adding a single line of code: `model = torch.compile(model)`.\n\nIf using [`Trainer`], you only need `to` pass the `torch_compile` option in the [`TrainingArguments`]: \n\n```python\ntraining_args = TrainingArguments(torch_compile=True, **default_args)\n```"], "eval_score_GPT4": "4", "eval_feedback_GPT4": "The response provided aligns well with the reference answer, mentioning the use of DeepSpeed for training large models and specifying the 1.6B-parameter GPT2-XL for causal language modeling on Habana Gaudi. However, the response is incomplete as it ends abruptly and does not finish the explanation of the Habana Gaudi hardware. Despite this, the key elements of the purpose are present, and the response is mostly correct, accurate, and factual, but not entirely complete."}, {"question": "What command line module does PyTorch provide to run a script on multiple GPUs?\n", "true_answer": "torchrun", "source_doc": "huggingface/blog/blob/main/pytorch-ddp-accelerate-transformers.md", "generated_answer": "PyTorch provides a convenient `torchrun` command line module to run a script on multiple GPUs. The command is as follows:\n\n```bash\ntorchrun --nproc_per_node=2 --nnodes=1 example_script.py\n```\n\nIn this example, `--nproc_per_node=2` specifies that each node (machine) will use 2 GPUs, and `--nnodes=1` spec", "retrieved_docs": ["Below is an example of our training setup, refactored as a function, with this capability:\n\n> Note: Here rank is the overall rank of the current GPU compared to all the other GPUs available, meaning they have a rank of `0 -> n-1`\n\n```python\nfrom torch.nn.parallel import DistributedDataParallel as DDP\n\ndef train(model, rank, world_size):\n    setup(rank, world_size)\n    model = model.to(rank)\n    ddp_model = DDP(model, device_ids=[rank])\n    optimizer = optim.AdamW(ddp_model.parameters(), lr=1e-3)\n    # Train for one epoch\n    model.train()\n    for batch_idx, (data, target) in enumerate(train_loader):\n        data, target = data.to(device), target.to(device)\n        output = model(data)\n        loss = F.nll_loss(output, target)\n        loss.backward()\n        optimizer.step()\n        optimizer.zero_grad()\n    cleanup()\n```\n\nThe optimizer needs to be declared based on the model *on the specific device* (so `ddp_model` and not `model`) for all of the gradients to properly be calculated.\n\nLastly, to run the script PyTorch has a convenient `torchrun` command line module that can help. Just pass in the number of nodes it should use as well as the script to run and you are set:\n\n```bash\ntorchrun --nproc_per_node=2 --nnodes=1 example_script.py\n```\n\nThe above will run the training script on two GPUs that live on a single machine and this is the barebones for performing only distributed training with PyTorch.\n\nNow let's talk about Accelerate, a library aimed to make this process more seameless and also help with a few best practices\n\n## \ud83e\udd17 Accelerate", "This is it. `pytorch_model.bin` will now contain the full fp32 model weights consolidated from multiple GPUs.\n\nThe script will automatically be able to handle either a ZeRO-2 or ZeRO-3 checkpoint.\n\n`python zero_to_fp32.py -h` will give you usage details.\n\nThe script will auto-discover the deepspeed sub-folder using the contents of the file `latest`, which in the current\nexample will contain `global_step1`.\n\nNote: currently the script requires 2x general RAM of the final fp32 model weights.\n\n\n### ZeRO-3 and Infinity Nuances\n\nZeRO-3 is quite different from ZeRO-2 because of its param sharding feature.\n\nZeRO-Infinity further extends ZeRO-3 to support NVMe memory and multiple other speed and scalability improvements.\n\nWhile all the efforts were made for things to just work without needing any special changes to your models, in certain\ncircumstances you may find the following information to be needed.\n\n\n\n#### Constructing Massive Models\n\nDeepSpeed/ZeRO-3 can handle models with Trillions of parameters which may not fit onto the existing RAM. In such cases,\nbut also if you want the initialization to happen much faster, initialize the model using *deepspeed.zero.Init()*\ncontext manager (which is also a function decorator), like so:\n\n```python\nfrom transformers import T5ForConditionalGeneration, T5Config\nimport deepspeed\n\nwith deepspeed.zero.Init():\n    config = T5Config.from_pretrained(\"t5-small\")\n    model = T5ForConditionalGeneration(config)\n```\n\nAs you can see this gives you a randomly initialized model.", "```bash\nCUDA_VISIBLE_DEVICES=0,2 torchrun trainer-program.py ...\n```\n\nOnly the 2 physical GPUs (0 and 2) are \"visible\" to PyTorch and these are mapped to `cuda:0` and `cuda:1` respectively. You can also reverse the order of the GPUs to use 2 first. Now, the mapping is `cuda:1` for GPU 0 and `cuda:0` for GPU 2.\n\n```bash\nCUDA_VISIBLE_DEVICES=2,0 torchrun trainer-program.py ...\n```\n\nYou can also set the `CUDA_VISIBLE_DEVICES` environment variable to an empty value to create an environment without GPUs.\n\n```bash\nCUDA_VISIBLE_DEVICES= python trainer-program.py ...\n```\n\n<Tip warning={true}>\n\nAs with any environment variable, they can be exported instead of being added to the command line. However, this is not recommended because it can be confusing if you forget how the environment variable was setup and you end up using the wrong GPUs. Instead, it is common practice to set the environment variable for a specific training run on the same command line.\n\n</Tip>\n\n`CUDA_DEVICE_ORDER` is an alternative environment variable you can use to control how the GPUs are ordered. You can either order them by:\n\n1. PCIe bus ID's that matches the order of [`nvidia-smi`](https://developer.nvidia.com/nvidia-system-management-interface) and [`rocm-smi`](https://rocm.docs.amd.com/projects/rocm_smi_lib/en/latest/.doxygen/docBin/html/index.html) for NVIDIA and AMD GPUs respectively\n\n```bash\nexport CUDA_DEVICE_ORDER=PCI_BUS_ID\n```\n\n2. GPU compute ability\n\n```bash\nexport CUDA_DEVICE_ORDER=FASTEST_FIRST\n```", "## PyTorch Distributed Data Parallelism\n\nAs the name implies, `torch.distributed` is meant to work on *distributed* setups. This can include multi-node, where you have a number of machines each with a single GPU, or multi-gpu where a single system has multiple GPUs, or some combination of both.\n\nTo convert our above code to work within a distributed setup, a few setup configurations must first be defined, detailed in the [Getting Started with DDP Tutorial](https://pytorch.org/tutorials/intermediate/ddp_tutorial.html)\n\nFirst a `setup` and a `cleanup` function must be declared. This will open up a processing group that all of the compute processes can communicate through\n\n> Note: for this section of the tutorial it should be assumed these are sent in python script files. Later on a launcher using Accelerate will be discussed that removes this necessity\n\n```python\nimport os\nimport torch.distributed as dist\n\ndef setup(rank, world_size):\n    \"Sets up the process group and configuration for PyTorch Distributed Data Parallelism\"\n    os.environ[\"MASTER_ADDR\"] = 'localhost'\n    os.environ[\"MASTER_PORT\"] = \"12355\"\n\n    # Initialize the process group\n    dist.init_process_group(\"gloo\", rank=rank, world_size=world_size)\n\ndef cleanup():\n    \"Cleans up the distributed environment\"\n    dist.destroy_process_group()\n```\n\nThe last piece of the puzzle is *how do I send my data and model to another GPU?*\n\nThis is where the `DistributedDataParallel` module comes into play. It will copy your model onto each GPU, and when `loss.backward()` is called the backpropagation is performed and the resulting gradients across all these copies of the model will be averaged/reduced. This ensures each device has the same weights post the optimizer step.\n\nBelow is an example of our training setup, refactored as a function, with this capability:", "### Script to compare pre-training with PyTorch on 8 GPU V100's\n\nFor comparison you can run the same pre-training with PyTorch on GPU. Note that we have to make use of `gradient_accumulation` \nbecause the maximum batch size that fits on a single V100 GPU is 32 instead of 128.\nHaving created the tokenzier and configuration in `norwegian-roberta-base`, we create the following symbolic links:\n\n```bash\nln -s ~/transformers/examples/pytorch/language-modeling/run_mlm.py ./\n```\n\n, set some environment variables:\n\n```bash\nexport NUM_GPUS=8\nexport TOKENIZERS_PARALLELISM=0\nexport MODEL_DIR=\"./norwegian-roberta-base\"\nmkdir -p ${MODEL_DIR}\n```\n\n, and can start training as follows:", "Scripts\n\nA train, validation, inference, and checkpoint cleaning script included in the github root folder. Scripts are not currently packaged in the pip release.\n\nThe training and validation scripts evolved from early versions of the [PyTorch Imagenet Examples](https://github.com/pytorch/examples). I have added significant functionality over time, including CUDA specific performance enhancements based on\n[NVIDIA's APEX Examples](https://github.com/NVIDIA/apex/tree/master/examples).\n\n## Training Script\n\nThe variety of training args is large and not all combinations of options (or even options) have been fully tested. For the training dataset folder, specify the folder to the base that contains a `train` and `validation` folder.\n\nTo train an SE-ResNet34 on ImageNet, locally distributed, 4 GPUs, one process per GPU w/ cosine schedule, random-erasing prob of 50% and per-pixel random value:\n\n```bash\n./distributed_train.sh 4 /data/imagenet --model seresnet34 --sched cosine --epochs 150 --warmup-epochs 5 --lr 0.4 --reprob 0.5 --remode pixel --batch-size 256 --amp -j 4\n```\n\n<Tip>\n    It is recommended to use PyTorch 1.9+ w/ PyTorch native AMP and DDP instead of APEX AMP. --amp defaults to native AMP as of timm ver 0.4.3.  --apex-amp will force use of APEX components if they are installed.\n</Tip>\n\n\n## Validation / Inference Scripts\n\nValidation and inference scripts are similar in usage. One outputs metrics on a validation set and the other outputs topk class ids in a csv. Specify the folder containing validation images, not the base as in training script. \n\nTo validate with the model's pretrained weights (if they exist):", "You can use a launcher of your choice here. You can continue using the pytorch launcher:\n\n```bash\ntorch.distributed.run --nproc_per_node=2 your_program.py <normal cl args> --deepspeed ds_config.json\n```\nor use the launcher provided by `deepspeed`:\n\n```bash\ndeepspeed --num_gpus=2 your_program.py <normal cl args> --deepspeed ds_config.json\n```\n\nAs you can see the arguments aren't the same, but for most needs either of them works. The\nfull details on how to configure various nodes and GPUs can be found [here](https://www.deepspeed.ai/getting-started/#resource-configuration-multi-node).\n\nWhen you use the `deepspeed` launcher and you want to use all available gpus you can just omit the `--num_gpus` flag.\n\nHere is an example of running `run_translation.py` under DeepSpeed deploying all available GPUs:\n\n```bash\ndeepspeed examples/pytorch/translation/run_translation.py \\\n--deepspeed tests/deepspeed/ds_config_zero3.json \\\n--model_name_or_path t5-small --per_device_train_batch_size 1 \\\n--output_dir output_dir --overwrite_output_dir --fp16 \\\n--do_train --max_train_samples 500 --num_train_epochs 1 \\\n--dataset_name wmt16 --dataset_config \"ro-en\" \\\n--source_lang en --target_lang ro\n```"], "eval_score_GPT4": "5", "eval_feedback_GPT4": "The response correctly identifies `torchrun` as the command line module provided by PyTorch to run a script on multiple GPUs. The example command given is accurate and demonstrates the use of `torchrun` with the appropriate flags for specifying the number of processes per node and the number of nodes. This matches the reference answer, which states \"torchrun\" as the correct command line module. The response is factual and provides additional context that is relevant and correct."}, {"question": "What is the most popular vision transformer model on the Hugging Face Model Hub for image classification?\n", "true_answer": "google/vit-base-patch16-224", "source_doc": "gradio-app/gradio/blob/main/guides/06_integrating-other-frameworks/image-classification-with-vision-transformers.md", "generated_answer": "The most popular vision transformer model on the Hugging Face Model Hub for image classification is `google/vit-base-patch16-224` as mentioned in Step 1 of the tutorial. This model has been trained on ImageNet images at a resolution of 224x224 pixels and is designed for image classification tasks. It is currently the most popular model in this category on the Hub at the time of writing.", "retrieved_docs": ["Image Classification with Vision Transformers\n\nRelated spaces: https://huggingface.co/spaces/abidlabs/vision-transformer\nTags: VISION, TRANSFORMERS, HUB\n\n## Introduction\n\nImage classification is a central task in computer vision. Building better classifiers to classify what object is present in a picture is an active area of research, as it has applications stretching from facial recognition to manufacturing quality control.\n\nState-of-the-art image classifiers are based on the _transformers_ architectures, originally popularized for NLP tasks. Such architectures are typically called vision transformers (ViT). Such models are perfect to use with Gradio's _image_ input component, so in this tutorial we will build a web demo to classify images using Gradio. We will be able to build the whole web application in a **single line of Python**, and it will look like the demo on the bottom of the page.\n\nLet's get started!\n\n### Prerequisites\n\nMake sure you have the `gradio` Python package already [installed](/getting_started).\n\n## Step 1 \u2014 Choosing a Vision Image Classification Model\n\nFirst, we will need an image classification model. For this tutorial, we will use a model from the [Hugging Face Model Hub](https://huggingface.co/models?pipeline_tag=image-classification). The Hub contains thousands of models covering dozens of different machine learning tasks.\n\nExpand the Tasks category on the left sidebar and select \"Image Classification\" as our task of interest. You will then see all of the models on the Hub that are designed to classify images.\n\nAt the time of writing, the most popular one is `google/vit-base-patch16-224`, which has been trained on ImageNet images at a resolution of 224x224 pixels. We will use this model for our demo.\n\n## Step 2 \u2014 Loading the Vision Transformer Model with Gradio", "- [Image Classification with Vision Transformer](https://github.com/huggingface/notebooks/blob/master/sagemaker/09_image_classification_vision_transformer/sagemaker-notebook.ipynb)\n- [Deploy one of the 10,000+ Hugging Face Transformers to Amazon SageMaker for Inference](https://github.com/huggingface/notebooks/blob/master/sagemaker/11_deploy_model_from_hf_hub/deploy_transformer_model_from_hf_hub.ipynb)\n- [Deploy a Hugging Face Transformer model from S3 to SageMaker for inference](https://github.com/huggingface/notebooks/blob/master/sagemaker/10_deploy_model_from_s3/deploy_transformer_model_from_s3.ipynb)", "- [Image Classification with Vision Transformer](https://github.com/huggingface/notebooks/blob/main/sagemaker/09_image_classification_vision_transformer/sagemaker-notebook.ipynb)\n- [Deploy one of the 10 000+ Hugging Face Transformers to Amazon SageMaker for Inference](https://github.com/huggingface/notebooks/blob/main/sagemaker/11_deploy_model_from_hf_hub/deploy_transformer_model_from_hf_hub.ipynb)\n- [Deploy a Hugging Face Transformer model from S3 to SageMaker for inference](https://github.com/huggingface/notebooks/blob/main/sagemaker/10_deploy_model_from_s3/deploy_transformer_model_from_s3.ipynb)", "The abstract from the paper is the following:\n\n*While the Transformer architecture has become the de-facto standard for natural language processing tasks, its\napplications to computer vision remain limited. In vision, attention is either applied in conjunction with\nconvolutional networks, or used to replace certain components of convolutional networks while keeping their overall\nstructure in place. We show that this reliance on CNNs is not necessary and a pure transformer applied directly to\nsequences of image patches can perform very well on image classification tasks. When pre-trained on large amounts of\ndata and transferred to multiple mid-sized or small image recognition benchmarks (ImageNet, CIFAR-100, VTAB, etc.),\nVision Transformer (ViT) attains excellent results compared to state-of-the-art convolutional networks while requiring\nsubstantially fewer computational resources to train.*\n\nThis model was contributed by [nielsr](https://huggingface.co/nielsr). The original code (written in JAX) can be\nfound [here](https://github.com/google-research/vision_transformer).\n\n## Resources\n\nA list of official Hugging Face and community (indicated by \ud83c\udf0e) resources to help you get started with ViT Hybrid.\n\n<PipelineTag pipeline=\"image-classification\"/>\n\n- [`ViTHybridForImageClassification`] is supported by this [example script](https://github.com/huggingface/transformers/tree/main/examples/pytorch/image-classification) and [notebook](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/image_classification.ipynb).\n- See also: [Image classification task guide](../tasks/image_classification)\n\nIf you're interested in submitting a resource to be included here, please feel free to open a Pull Request and we'll review it! The resource should ideally demonstrate something new instead of duplicating an existing resource.\n\n## ViTHybridConfig\n\n[[autodoc]] ViTHybridConfig", "The Hugging Face Hub[[the-hugging-face-hub]]\n\n<CourseFloatingBanner\n    chapter={4}\n    classNames=\"absolute z-10 right-0 top-0\"\n/>\n\nThe [Hugging Face Hub](https://huggingface.co/) \u2013- our main website \u2013- is a central platform that enables anyone to discover, use, and contribute new state-of-the-art models and datasets. It hosts a wide variety of models, with more than 10,000 publicly available. We'll focus on the models in this chapter, and take a look at the datasets in Chapter 5.\n\nThe models in the Hub are not limited to \ud83e\udd17 Transformers or even NLP. There are models from [Flair](https://github.com/flairNLP/flair) and [AllenNLP](https://github.com/allenai/allennlp) for NLP, [Asteroid](https://github.com/asteroid-team/asteroid) and [pyannote](https://github.com/pyannote/pyannote-audio) for speech, and [timm](https://github.com/rwightman/pytorch-image-models) for vision, to name a few. \n\nEach of these models is hosted as a Git repository, which allows versioning and reproducibility. Sharing a model on the Hub means opening it up to the community and making it accessible to anyone looking to easily use it, in turn eliminating their need to train a model on their own and simplifying sharing and usage. \n\nAdditionally, sharing a model on the Hub automatically deploys a hosted Inference API for that model. Anyone in the community is free to test it out directly on the model's page, with custom inputs and appropriate widgets.\n\nThe best part is that sharing and using any public model on the Hub is completely free! [Paid plans](https://huggingface.co/pricing) also exist if you wish to share models privately.\n\nThe video below shows how to navigate the Hub.\n\n<Youtube id=\"XvSGPZFEjDY\"/>", "Making Poplar compatible with these widely used, third-party systems allows developers to easily port their models from their other compute platforms and start taking advantage of the IPU\u2019s advanced AI capabilities.\n\n## Optimising Transformers for Production\n\nTransformers have completely transformed (pun intended) the field of AI. Models such as BERT are widely used by Graphcore customers in a huge array of applications, across NLP and beyond. These multi-talented models can perform feature extraction, text generation, sentiment analysis, translation and many more functions.\n\nAlready, Hugging Face plays host to hundreds of Transformers, from the French-language CamemBERT to ViT which applies lessons learned in NLP to computer vision. The Transformers library is downloaded an average of 2 million times every month and demand is growing.\n\nWith a user base of more than 50,000 developers \u2013 Hugging Face has seen the fastest ever adoption of an open-source project.\n\nNow, with its Hardware Partner Program, Hugging Face is connecting the ultimate Transformer toolset with today's most advanced AI hardware.\n\nUsing Optimum, a new open-source library and toolkit, developers will be able to access hardware-optimized models certified by Hugging Face.\n\nThese are being developed in a collaboration between Graphcore and Hugging Face, with the first IPU-optimized models appearing on Optimum later this year. Ultimately, these will cover a wide range of applications, from vision and speech to translation and text generation.\n\nHugging Face CEO Cl\u00e9ment Delangue said: \u201cDevelopers all want access to the latest and greatest hardware \u2013 like the Graphcore IPU, but there\u2019s always that question of whether they\u2019ll have to learn new code or processes. With Optimum and the Hugging Face Hardware Program, that\u2019s just not an issue. It\u2019s essentially plug-and-play\".\n\n## SOTA Models meet SOTA Hardware\n\nPrior to the announcement of the Hugging Face Partnership, we had demonstrated the power of the IPU to accelerate state-of-the-art Transformer models with a special Graphcore-optimised implementation of Hugging Face BERT using Pytorch.", "- Image classification\n- Image segmentation\n- (Zero-shot) object detection\n- Video classification\n- Depth estimation\n- Image-to-image synthesis\n- Unconditional image generation\n- Zero-shot image classification\n\nEach of these tasks comes with at least 10 model checkpoints on the Hub for you to explore. Furthermore, we support [tasks](https://huggingface.co/tasks) that lie at the intersection of vision and language such as:\n\n- Image-to-text (image captioning, OCR)\n- Text-to-image\n- Document question-answering\n- Visual question-answering\n\nThese tasks entail not only state-of-the-art Transformer-based architectures such as [ViT](https://huggingface.co/docs/transformers/model_doc/vit), [Swin](https://huggingface.co/docs/transformers/model_doc/swin), [DETR](https://huggingface.co/docs/transformers/model_doc/detr) but also *pure convolutional* architectures like [ConvNeXt](https://huggingface.co/docs/transformers/model_doc/convnext), [ResNet](https://huggingface.co/docs/transformers/model_doc/resnet), [RegNet](https://huggingface.co/docs/transformers/model_doc/regnet), and more! Architectures like ResNets are still very much relevant for a myriad of industrial use cases and hence the support of these non-Transformer architectures in \ud83e\udd17 Transformers.\n\nIt\u2019s also important to note that the models on the Hub are not just from the Transformers library but also from other third-party libraries. For example, even though we support tasks like unconditional image generation on the Hub, we don\u2019t have any models supporting that task in Transformers yet (such as [this](https://huggingface.co/ceyda/butterfly_cropped_uniq1K_512)). Supporting all ML tasks, whether they are solved with Transformers or a third-party library is a part of our mission to foster a collaborative open-source Machine Learning ecosystem."], "eval_score_GPT4": "5", "eval_feedback_GPT4": "Feedback: The response correctly identifies `google/vit-base-patch16-224` as the most popular vision transformer model on the Hugging Face Model Hub for image classification, which matches the reference answer. The additional details provided about the model being trained on ImageNet images and designed for image classification tasks are accurate and factual, enhancing the quality of the response."}, {"question": "What is the command to upload an ESPnet model to a Hugging Face repository?\n", "true_answer": "./run.sh --stage 15 --skip_upload_hf false --hf_repo username/model_repo", "source_doc": "huggingface/hub-docs/blob/main/docs/hub/espnet.md", "generated_answer": "To upload an ESPnet model to a Hugging Face repository, you can use the `--push_to_hub` argument with the `run.sh` script provided by ESPnet. Here's an example command:\n\n```bash\n./run.sh --stage 15 --skip_upload_hf false --hf_repo username/model_repo --push_to_hub\n```\n\nThis will upload the final model at the end of training", "retrieved_docs": ["Using ESPnet at Hugging Face\n\n`espnet` is an end-to-end toolkit for speech processing, including automatic speech recognition, text to speech, speech enhancement, dirarization and other tasks.\n\n## Exploring ESPnet in the Hub\n\nYou can find hundreds of `espnet` models by filtering at the left of the [models page](https://huggingface.co/models?library=espnet&sort=downloads). \n\nAll models on the Hub come up with useful features:\n1. An automatically generated model card with a description, a training configuration, licenses and more.\n2. Metadata tags that help for discoverability and contain information such as license, language and datasets.\n3. An interactive widget you can use to play out with the model directly in the browser.\n4. An Inference API that allows to make inference requests.\n\n<div class=\"flex justify-center\">\n<img class=\"block dark:hidden\" src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/hub/libraries-espnet_widget.png\"/>\n<img class=\"hidden dark:block\" src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/hub/libraries-espnet_widget-dark.png\"/>\n</div>\n\n## Using existing models\n\nFor a full guide on loading pre-trained models, we recommend checking out the [official guide](https://github.com/espnet/espnet_model_zoo)). \n\nIf you're interested in doing inference, different classes for different tasks have a `from_pretrained` method that allows loading models from the Hub. For example:\n* `Speech2Text` for Automatic Speech Recognition.\n* `Text2Speech` for Text to Speech.\n* `SeparateSpeech` for Audio Source Separation.\n\nHere is an inference example:\n\n```py\nimport soundfile\nfrom espnet2.bin.tts_inference import Text2Speech", "Here is an inference example:\n\n```py\nimport soundfile\nfrom espnet2.bin.tts_inference import Text2Speech\n\ntext2speech = Text2Speech.from_pretrained(\"model_name\")\nspeech = text2speech(\"foobar\")[\"wav\"]\nsoundfile.write(\"out.wav\", speech.numpy(), text2speech.fs, \"PCM_16\")\n```\n\nIf you want to see how to load a specific model, you can click `Use in ESPnet` and you will be given a working snippet that you can load it! \n\n<div class=\"flex justify-center\">\n<img class=\"block dark:hidden\" src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/hub/libraries-espnet_snippet.png\"/>\n<img class=\"hidden dark:block\" src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/hub/libraries-espnet_snippet-dark.png\"/>\n</div>\n\n## Sharing your models\n\n`ESPnet` outputs a `zip` file that can be uploaded to Hugging Face easily. For a full guide on sharing  models, we recommend checking out the [official guide](https://github.com/espnet/espnet_model_zoo#register-your-model)).\n\nThe `run.sh` script allows to upload a given model to a Hugging Face repository.\n\n```bash\n./run.sh --stage 15 --skip_upload_hf false --hf_repo username/model_repo\n```\n\n## Additional resources", ". So we're going to add the PushToHubCallback, and it takes just a few arguments. So the first argument is the temporary directory that files are going to be saved to before they're uploaded to the Hub. The second argument is the tokenizer, and the third argument here is the keyword argument hub_model_id. So that's the name it's going to be saved under on the HuggingFace Hub. You can also upload to an organization account just by adding the organization name before the repository name with a slash like this. So you probably don't have permissions to upload to the Hugging Face organization, if you do please file a bug and let us know extremely urgently. But if you do have access to your own organization then you can use that same approach to upload models to their account instead of to your own personal set of models. So, once you've made your callback you simply add it to the callbacks list when you're called model.fit() and everything is uploaded for you from there, and there's nothing else to worry about. The second way to upload a model, though, is to call model.push_to_hub(). So this is more of a once-off method - it's not called regularly during training. You can just call this manually whenever you want to upload a model to the hub. So we recommend running this after the end of training, just to make sure that you have a commit message just to guarantee that this was the final version of the model at the end of training. And it just makes sure that you're working with the definitive end-of-training model and not accidentally using a model that's from a checkpoint somewhere along the way", "### Upload the trained/fine-tuned model to the Hub\n\nAll the example scripts support automatic upload of your final model to the [Model Hub](https://huggingface.co/models) by adding a `--push_to_hub` argument. It will then create a repository with your username slash the name of the folder you are using as `output_dir`. For instance, `\"sgugger/test-mrpc\"` if your username is `sgugger` and you are working in the folder `~/tmp/test-mrpc`.\n\nTo specify a given repository name, use the `--hub_model_id` argument. You will need to specify the whole repository name (including your username), for instance `--hub_model_id sgugger/finetuned-bert-mrpc`. To upload to an organization you are a member of, just use the name of that organization instead of your username: `--hub_model_id huggingface/finetuned-bert-mrpc`.\n\nA few notes on this integration:\n\n- you will need to be logged in to the Hugging Face website locally for it to work, the easiest way to achieve this is to run `huggingface-cli login` and then type your username and password when prompted. You can also pass along your authentication token with the `--hub_token` argument.\n- the `output_dir` you pick will either need to be a new folder or a local clone of the distant repository you are using.\n\n## Distributed training and mixed precision\n\nAll the PyTorch scripts mentioned above work out of the box with distributed training and mixed precision, thanks to\nthe [Trainer API](https://huggingface.co/transformers/main_classes/trainer.html). To launch one of them on _n_ GPUs,\nuse the following command:\n\n```bash\ntorchrun \\\n    --nproc_per_node number_of_gpu_you_have path_to_script.py \\\n\t--all_arguments_of_the_script\n```", "On the Hugging Face Hub, you\u2019ll be able to create or discover the following ML assets:\n\n- [Models](https://huggingface.co/models): hosting the latest state-of-the-art models for NLP, computer vision, speech, time-series, biology, reinforcement learning, chemistry and more.\n- [Datasets](https://huggingface.co/datasets): featuring a wide variety of data for different domains, modalities and languages.\n- [Spaces](https://huggingface.co/spaces): interactive apps for showcasing ML models directly in your browser.\n\nEach model, dataset or space uploaded to the Hub is a [Git-based repository](https://huggingface.co/docs/hub/repositories), which are version-controlled places that can contain all your files. You can use the traditional git commands to pull, push, clone, and/or manipulate your files. You can see the commit history for your models, datasets and spaces, and see who did what and when.\n\n<figure class=\"image table text-center m-0 w-full\">\n  <medium-zoom background=\"rgba(0,0,0,.7)\" alt=\"Commit history on a machine learning model\" src=\"assets/92_introducing_private_hub/commit-history.png\"></medium-zoom>\n  <figcaption>Commit history on a model</figcaption>\n</figure>\n\nThe Hugging Face Hub is also a central place for feedback and development in machine learning. Teams use [pull requests and discussions](https://huggingface.co/docs/hub/repositories-pull-requests-discussions) to support peer reviews on models, datasets, and spaces, improve collaboration and accelerate their ML work.", "### Download Model Repository Directly\n\nHugging Face repositories can be downloaded directly using `git clone`:\n\n```\ngit clone git@hf.co:<Name of HuggingFace Repo> # example: git clone git@hf.co:bigscience/bloom\n```\n\n## Using Downloaded Models with Sample-Factory\n\nAfter downloading the model, you can run the models in the repo with the enjoy script corresponding to your environment. For example, if you are downloading a `mujoco-ant` model, it can be run with:\n\n```\npython -m sf_examples.mujoco.enjoy_mujoco --algo=APPO --env=mujoco_ant --experiment=<repo_name> --train_dir=./train_dir\n```\n\nNote, you may have to specify the `--train_dir` if your local train_dir has a different path than the one in the `cfg.json`\n\n## Sharing your models\n### Using push_to_hub\n\nIf you want to upload without generating evaluation metrics or a replay video, you can use the `push_to_hub` script:\n\n```\npython -m sample_factory.huggingface.push_to_hub -r <hf_username>/<hf_repo_name> -d <experiment_dir_path>\n```\n\nThe command line arguments are:\n\n- `-r`: The repo_id to save on HF Hub. This is the same as `hf_repository` in the enjoy script and must be in the form `<hf_username>/<hf_repo_name>`\n- `-d`: The full path to your experiment directory to upload\n\n\n### Using enjoy.py\n\nYou can upload your models to the Hub using your environment's `enjoy` script with the `--push_to_hub` flag. Uploading using `enjoy` can also generate evaluation metrics and a replay video.", "As you can see, it is pretty simple to upload model weights and training logs to the model hub. Since the repository \nhas git version control, you & your team probably already have the necessary skills to collaborate. Thanks \nto `git-lfs` being integrated into the hub, model weights and other larger file can just as easily be uploaded \nand changed. Finally, at Hugging Face, we believe that the model hub is a great platform to share your project \nwhile you are still working on it:\n\n- Bugs in training scripts can be found and corrected by anybody participating in the event\n- Loss curves can be analyzed directly on the model page\n- Model weights can be accessed and analyzed by everybody from the model repository\n\nIf you are not using a transformers model, don't worry - you should still be able to make use of the hub's functionalities!\nThe [huggingface_hub](https://github.com/huggingface/huggingface_hub) allows you to upload essentially any JAX/Flax model to the hub with \njust a couple of lines of code. *E.g.* assuming you want to call your model simply `flax-model-dummy`, you can upload it to the hub with \njust three lines of code:\n\n\n```python\nfrom flax import serialization\nfrom jax import random\nfrom flax import linen as nn\nfrom huggingface_hub import Repository\n\nmodel = nn.Dense(features=5)\n\nkey1, key2 = random.split(random.PRNGKey(0))\nx = random.normal(key1, (10,))\nparams = model.init(key2, x)\n\nbytes_output = serialization.to_bytes(params)\n\nrepo = Repository(\"flax-model\", clone_from=\"flax-community/flax-model-dummy\", token=True)\nwith repo.commit(\"My cool Flax model :)\"):\n    with open(\"flax_model.msgpack\", \"wb\") as f:\n        f.write(bytes_output)\n\n# Repo is created and available here: https://huggingface.co/flax-community/flax-model-dummy\n```"], "eval_score_GPT4": "4", "eval_feedback_GPT4": "Feedback: The response provided is mostly correct and factual. It includes the necessary arguments such as `--stage 15`, `--skip_upload_hf false`, and `--hf_repo username/model_repo` which align with the reference answer. However, the response includes an additional argument `--push_to_hub` that is not present in the reference answer. This additional argument does not make the response incorrect, as it is a plausible part of the command to push the model to the Hugging Face repository, but it does introduce an element that is not verified by the reference answer. Therefore, the response is not completely correct based on the reference answer provided."}, {"question": "What file should be added to a model repository to install custom Python dependencies for Inference Endpoints?\n", "true_answer": "requirements.txt", "source_doc": "huggingface/hf-endpoints-documentation/blob/main/docs/source/guides/custom_dependencies.mdx", "generated_answer": "The file that should be added to a model repository to install custom Python dependencies for Inference Endpoints is a \"requirements.txt\" file. This file should contain a list of the required Python dependencies that are not included in the Inference Endpoints' base image. When the model repository is pushed to the Hugging Face Hub, Inference Endpoints checks for the presence of this file and installs the listed dependencies during the creation of the Endpoint and Image artifacts. For more information", "retrieved_docs": ["Add custom Dependencies\n\nInference Endpoints\u2019 base image includes all required libraries to run inference on \ud83e\udd17 Transformers models, but it also supports custom dependencies. This is useful if you want to:\n\n* [customize your inference pipeline](/docs/inference-endpoints/guides/custom_handler) and need additional Python dependencies\n* run a model which requires special dependencies like the newest or a fixed version of a library (for example, `tapas` (`torch-scatter`)).\n\nTo add custom dependencies, add a `requirements.txt` [file](https://huggingface.co/philschmid/distilbert-onnx-banking77/blob/main/requirements.txt) with the Python dependencies you want to install in your model repository on the Hugging Face Hub. When your Endpoint and Image artifacts are created, Inference Endpoints checks if the model repository contains a `requirements.txt ` file and installs the dependencies listed within.\n\n```bash\noptimum[onnxruntime]==1.2.3\nmkl-include\nmkl\n```\n\nCheck out the `requirements.txt` files in the following model repositories for examples:\n\n* [Optimum and onnxruntime](https://huggingface.co/philschmid/distilbert-onnx-banking77/blob/main/requirements.txt)\n* [diffusers](https://huggingface.co/philschmid/stable-diffusion-v1-4-endpoints/blob/main/requirements.txt)\n\nFor more information, take a look at how you can create and install dependencies when you [use your own custom container](/docs/inference-endpoints/guides/custom_container) for inference.", "Inference Endpoints Version\n\nHugging Face Inference Endpoints comes with a default serving container which is used for all [supported Transformers and Sentence-Transformers tasks](/docs/inference-endpoints/supported_tasks) and for [custom inference handler](/docs/inference-endpoints/guides/custom_handler) and implement batching.\nBelow you will find information about the installed packages and versions used.\n\nYou can always upgrade installed packages and a custom packages by adding a `requirements.txt` file to your model repository. Read more in [Add custom Dependencies](/docs/inference-endpoints/guides/custom_dependencies).\n\n## Installed packages & version\n\nThe installed packages are split into `general`, `CPU` & `GPU` packages. The `general` packages are installed in all containers, the `CPU` and `GPU` packages are only installed in the corresponding containers.\nThe Hugging Face Inference Runtime has separate versions for `PyTorch` and `TensorFlow` for `CPU` and `GPU`, which are used based on the selected `framework` when creating an Inference Endpoint. The `TensorFlow` and `PyTorch` flavors are grouped together in the list below.\n\n### General\n\n- `Python`: `3.9.13`\n\n### CPU\n\n- `transformers[sklearn,sentencepiece,audio,vision]`: `4.27.2`\n- `diffusers`: `0.14.0`\n- `accelerate`: `0.17.1`\n- `sentence_transformers`: `latest`\n- `pandas`: `latest`\n- `pytorch`: `1.13.1`\n- `torchvision`: `0.14.1`\n- `tensorflow`: `2.9.1`\n\n### GPU", "The `inference.py` file contains your custom inference module, and the `requirements.txt` file contains additional dependencies that should be added. The custom module can override the following methods:  \n\n* `model_fn(model_dir)` overrides the default method for loading a model. The return value `model` will be used in `predict` for predictions. `predict` receives argument the `model_dir`, the path to your unzipped `model.tar.gz`.\n* `transform_fn(model, data, content_type, accept_type)` overrides the default transform function with your custom implementation. You will need to implement your own `preprocess`, `predict` and `postprocess` steps in the `transform_fn`. This method can't be combined with `input_fn`, `predict_fn` or `output_fn` mentioned below.\n* `input_fn(input_data, content_type)` overrides the default method for preprocessing. The return value `data` will be used in `predict` for predictions. The inputs are:\n  - `input_data` is the raw body of your request.\n  - `content_type` is the content type from the request header.\n* `predict_fn(processed_data, model)` overrides the default method for predictions. The return value `predictions` will be used in `postprocess`. The input is `processed_data`, the result from `preprocess`.\n* `output_fn(prediction, accept)` overrides the default method for postprocessing. The return value `result` will be the response of your request (e.g.`JSON`). The inputs are:\n  - `predictions` is the result from `predict`.\n  - `accept` is the return accept type from the HTTP Request, e.g. `application/json`.\n\nHere is an example of a custom inference module with `model_fn`, `input_fn`, `predict_fn`, and `output_fn`:  \n\n```python\nfrom sagemaker_huggingface_inference_toolkit import decoder_encoder", "- A path to the file to upload.\n- The final path in the repository.\n- The repository you wish to push the files to.\n\nFor example:\n\n```python\n>>> from huggingface_hub import upload_file\n>>> upload_file(\n...    path_or_fileobj=\"/home/lysandre/dummy-test/README.md\", \n...    path_in_repo=\"README.md\", \n...    repo_id=\"lysandre/test-model\"\n... )\n'https://huggingface.co/lysandre/test-model/blob/main/README.md'\n```\n\nIf you need to upload more than one file, look at the [utilities offered by the `Repository` class](https://huggingface.co/docs/huggingface_hub/package_reference/repository).\n\nOnce again, if you check your Hugging Face account, you should see the file inside your repository.\n\nLastly, it is important to add a model card so users understand how to use your model. See [here](./model-cards) for more details about how to create a model card.\n\n## Set up the Inference API\n\nOur Inference API powers models uploaded to the Hub through your library.\n\n### Create an Inference API Docker image \n\nAll third-party libraries are Dockerized, so you can install the dependencies you'll need for your library to work correctly. Add your library to the existing Docker images by navigating to the [Docker images folder](https://github.com/huggingface/api-inference-community/tree/main/docker_images).", "Create custom Inference Handler\n\nHugging Face Endpoints supports all of the Transformers and Sentence-Transformers tasks and can support custom tasks, including custom pre- & post-processing. The customization can be done through a [handler.py](https://huggingface.co/philschmid/distilbert-onnx-banking77/blob/main/handler.py) file in your model repository on the Hugging Face Hub.\n\nThe [handler.py](https://huggingface.co/philschmid/distilbert-onnx-banking77/blob/main/handler.py) needs to implement the [EndpointHandler](https://huggingface.co/philschmid/distilbert-onnx-banking77/blob/main/handler.py) class with a `__init__` and a `__call__` method.\n\nIf you want to use custom dependencies, e.g. [optimum](https://raw.githubusercontent.com/huggingface/optimum), the dependencies must be listed in a `requirements.txt` as described above in \u201cadd custom dependencies.\u201d\n\n## Custom Handler Examples\n\nThere are already several public examples on the [Hugging Face Hub](https://huggingface.co/models?other=endpoints-template) where you can take insipiration or directly use them. The repositories are tagged with `endpoints-template` and can be found under this [link](https://huggingface.co/models?other=endpoints-template).\n\nIncluded examples are for:", "## User defined code and modules\n\nThe Hugging Face Inference Toolkit allows the user to override the default methods of the `HuggingFaceHandlerService`. You will need to create a folder named `code/` with an `inference.py` file in it. See [here](#create-a-model-artifact-for-deployment) for more details on how to archive your model artifacts. For example:  \n\n```bash\nmodel.tar.gz/\n|- pytorch_model.bin\n|- ....\n|- code/\n  |- inference.py\n  |- requirements.txt \n```\n\nThe `inference.py` file contains your custom inference module, and the `requirements.txt` file contains additional dependencies that should be added. The custom module can override the following methods:", "### 2. Create EndpointHandler (CP)\n\nAfter we have set up our environment, we can start creating your custom handler. The custom handler is a Python class (`EndpointHandler`) inside a `handler.py` file in our repository. The `EndpointHandler` needs to implement an `__init__` and a `__call__` method.\n\n- The `__init__` method will be called when starting the Endpoint and will receive 1 argument, a string with the path to your model weights. This allows you to load your model correctly.\n- The `__call__` method will be called on every request and receive a dictionary with your request body as a python dictionary. It will always contain the `inputs` key.\n\nThe first step is to create our `handler.py` in the local clone of our repository.\n\n```\n!cd distilbert-base-uncased-emotion && touch handler.py\n```\n\nIn there, you define your `EndpointHandler` class with the `__init__` and `__call__ `method.\n\n```python\nfrom typing import Dict, List, Any\n\nclass EndpointHandler():\n    def __init__(self, path=\"\"):\n        # Preload all the elements you are going to need at inference.\n        # pseudo:\n        # self.model= load_model(path)\n\n    def __call__(self, data: Dict[str, Any]) -> List[Dict[str, Any]]:\n        \"\"\"\n       data args:\n            inputs (:obj: `str` | `PIL.Image` | `np.array`)\n            kwargs\n      Return:\n            A :obj:`list` | `dict`: will be serialized and returned\n        \"\"\"\n\n        # pseudo\n        # self.model(input)\n```\n\n### 3. Customize EndpointHandler"], "eval_score_GPT4": "5", "eval_feedback_GPT4": "The response provided is completely correct, accurate, and factual. It not only mentions the correct file, \"requirements.txt,\" but also explains its purpose and the process that involves the file when pushing to the Hugging Face Hub, which aligns with the reference answer."}, {"question": "How many images are needed to teach new concepts to Stable Diffusion using Textual Inversion?\n", "true_answer": "3-5 images", "source_doc": "huggingface/diffusers/blob/main/examples/research_projects/intel_opts/textual_inversion_dfq/README.md", "generated_answer": "According to the context provided, Textual Inversion allows for teaching new concepts to Stable Diffusion using just 3-5 images. (Source: 0)", "retrieved_docs": ["Textual Inversion lets you personalize a Stable Diffusion model on your own images with just 3-5 samples. With this tool, you can train a model on a concept, and then share the concept with the rest of the community! \n\n![image info](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/diffusers-2nd-month/textual_inversion.jpeg)\n\nIn just a couple of days, the community shared over 200 concepts! Check them out!\n\n* [Organization](https://huggingface.co/sd-concepts-library) with the concepts. \n* [Navigator Colab](https://colab.research.google.com/github/huggingface/notebooks/blob/main/diffusers/stable_diffusion_textual_inversion_library_navigator.ipynb): Browse visually and use over 150 concepts created by the community.\n* [Training Colab](https://colab.research.google.com/github/huggingface/notebooks/blob/main/diffusers/sd_textual_inversion_training.ipynb): Teach Stable Diffusion a new concept and share it with the rest of the community.\n* [Inference Colab](https://colab.research.google.com/github/huggingface/notebooks/blob/main/diffusers/stable_conceptualizer_inference.ipynb): Run Stable Diffusion with the learned concepts.\n\n\n## Experimental inpainting pipeline\n\nInpainting allows to provide an image, then select an area in the image (or provide a mask), and use Stable Diffusion to replace the mask. Here is an example:\n\n<figure class=\"image table text-center m-0 w-full\">\n  <img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/diffusers-2nd-month/inpainting.png\" alt=\"Example inpaint of owl being generated from an initial image and a prompt\"/>\n</figure>", "Distillation for quantization on Textual Inversion models to personalize text2image\n\n[Textual inversion](https://arxiv.org/abs/2208.01618) is a method to personalize text2image models like stable diffusion on your own images._By using just 3-5 images new concepts can be taught to Stable Diffusion and the model personalized on your own images_\nThe `textual_inversion.py` script shows how to implement the training procedure and adapt it for stable diffusion.\nWe have enabled distillation for quantization in `textual_inversion.py` to do quantization aware training as well as distillation on the model generated by Textual Inversion method.\n\n## Installing the dependencies\n\nBefore running the scripts, make sure to install the library's training dependencies:\n\n```bash\npip install -r requirements.txt\n```\n\n## Prepare Datasets\n\nOne picture which is from the huggingface datasets [sd-concepts-library/dicoo2](https://huggingface.co/sd-concepts-library/dicoo2) is needed, and save it to the `./dicoo` directory. The picture is shown below:\n\n<a href=\"https://huggingface.co/sd-concepts-library/dicoo2/blob/main/concept_images/1.jpeg\">\n    <img src=\"https://huggingface.co/sd-concepts-library/dicoo2/resolve/main/concept_images/1.jpeg\" width = \"300\" height=\"300\">\n</a>\n\n## Get a FP32 Textual Inversion model\n\nUse the following command to fine-tune the Stable Diffusion model on the above dataset to obtain the FP32 Textual Inversion model.\n\n```bash\nexport MODEL_NAME=\"CompVis/stable-diffusion-v1-4\"\nexport DATA_DIR=\"./dicoo\"", "# Textual Inversion fine-tuning example\n\n[Textual inversion](https://arxiv.org/abs/2208.01618) is a method to personalize text2image models like stable diffusion on your own images using just 3-5 examples.\nThe `textual_inversion.py` script shows how to implement the training procedure and adapt it for stable diffusion.\n\n## Training with Intel Extension for PyTorch\n\nIntel Extension for PyTorch provides the optimizations for faster training and inference on CPUs. You can leverage the training example \"textual_inversion.py\". Follow the [instructions](https://github.com/huggingface/diffusers/tree/main/examples/textual_inversion) to get the model and [dataset](https://huggingface.co/sd-concepts-library/dicoo2) before running the script.\n\nThe example supports both single node and multi-node distributed training:\n\n### Single node training\n\n```bash\nexport MODEL_NAME=\"CompVis/stable-diffusion-v1-4\"\nexport DATA_DIR=\"path-to-dir-containing-dicoo-images\"\n\npython textual_inversion.py \\\n  --pretrained_model_name_or_path=$MODEL_NAME \\\n  --train_data_dir=$DATA_DIR \\\n  --learnable_property=\"object\" \\\n  --placeholder_token=\"<dicoo>\" --initializer_token=\"toy\" \\\n  --seed=7 \\\n  --resolution=512 \\\n  --train_batch_size=1 \\\n  --gradient_accumulation_steps=1 \\\n  --max_train_steps=3000 \\\n  --learning_rate=2.5e-03 --scale_lr \\\n  --output_dir=\"textual_inversion_dicoo\"\n```\n\nNote: Bfloat16 is available on Intel Xeon Scalable Processors Cooper Lake or Sapphire Rapids. You may not get performance speedup without Bfloat16 support.\n\n### Multi-node distributed training", "!--Copyright 2023 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\n\nhttp://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License.\n-->\n\n# Textual inversion\n\n[[open-in-colab]]\n\nThe [`StableDiffusionPipeline`] supports textual inversion, a technique that enables a model like Stable Diffusion to learn a new concept from just a few sample images. This gives you more control over the generated images and allows you to tailor the model towards specific concepts. You can get started quickly with a collection of community created concepts in the [Stable Diffusion Conceptualizer](https://huggingface.co/spaces/sd-concepts-library/stable-diffusion-conceptualizer).\n\nThis guide will show you how to run inference with textual inversion using a pre-learned concept from the Stable Diffusion Conceptualizer. If you're interested in teaching a model new concepts with textual inversion, take a look at the [Textual Inversion](../training/text_inversion) training guide.\n\nImport the necessary libraries:\n\n```py\nimport torch\nfrom diffusers import StableDiffusionPipeline\nfrom diffusers.utils import make_image_grid\n```\n\n## Stable Diffusion 1 and 2\n\nPick a Stable Diffusion checkpoint and a pre-learned concept from the [Stable Diffusion Conceptualizer](https://huggingface.co/spaces/sd-concepts-library/stable-diffusion-conceptualizer):\n\n```py\npretrained_model_name_or_path = \"runwayml/stable-diffusion-v1-5\"\nrepo_id_embeds = \"sd-concepts-library/cat-toy\"\n```", "# Textual Inversion fine-tuning example\n\n[Textual inversion](https://arxiv.org/abs/2208.01618) is a method to personalize text2image models like stable diffusion on your own images using just 3-5 examples.\nThe `textual_inversion.py` script shows how to implement the training procedure and adapt it for stable diffusion.\n\n## Running on Colab\n\nColab for training\n[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/huggingface/notebooks/blob/main/diffusers/sd_textual_inversion_training.ipynb)\n\nColab for inference\n[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/huggingface/notebooks/blob/main/diffusers/stable_conceptualizer_inference.ipynb)\n\n## Running locally with PyTorch\n### Installing the dependencies\n\nBefore running the scripts, make sure to install the library's training dependencies:\n\n**Important**\n\nTo make sure you can successfully run the latest versions of the example scripts, we highly recommend **installing from source** and keeping the install up to date as we update the example scripts frequently and install some example-specific requirements. To do this, execute the following steps in a new virtual environment:\n```bash\ngit clone https://github.com/huggingface/diffusers\ncd diffusers\npip install .\n```\n\nThen cd in the example folder and run:\n```bash\npip install -r requirements.txt\n```\n\nAnd initialize an [\ud83e\udd17 Accelerate](https://github.com/huggingface/accelerate/) environment with:\n\n```bash\naccelerate config\n```\n\n### Cat toy example", "# Textual Inversion fine-tuning example\n\n[Textual inversion](https://arxiv.org/abs/2208.01618) is a method to personalize text2image models like stable diffusion on your own images using just 3-5 examples.\nThe `textual_inversion.py` script shows how to implement the training procedure and adapt it for stable diffusion.\n\n## Running on Colab\n\nColab for training\n[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/huggingface/notebooks/blob/main/diffusers/sd_textual_inversion_training.ipynb)\n\nColab for inference\n[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/huggingface/notebooks/blob/main/diffusers/stable_conceptualizer_inference.ipynb)\n\n## Running locally with PyTorch\n### Installing the dependencies\n\nBefore running the scripts, make sure to install the library's training dependencies:\n\n**Important**\n\nTo make sure you can successfully run the latest versions of the example scripts, we highly recommend **installing from source** and keeping the install up to date as we update the example scripts frequently and install some example-specific requirements. To do this, execute the following steps in a new virtual environment:\n```bash\ngit clone https://github.com/huggingface/diffusers\ncd diffusers\npip install .\n```\n\nThen cd in the example folder  and run\n```bash\npip install -r requirements.txt\n```\n\nAnd initialize an [\ud83e\udd17Accelerate](https://github.com/huggingface/accelerate/) environment with:\n\n```bash\naccelerate config\n```\n\n\n### Cat toy example", "# [Deprecated] Multi Token Textual Inversion\n\n**IMPORTART: This research project is deprecated. Multi Token Textual Inversion is now supported natively in [the official textual inversion example](https://github.com/huggingface/diffusers/tree/main/examples/textual_inversion#running-locally-with-pytorch).**\n\nThe author of this project is [Isamu Isozaki](https://github.com/isamu-isozaki) - please make sure to tag the author for issue and PRs as well as @patrickvonplaten.\n\nWe add multi token support to textual inversion. I added\n1. num_vec_per_token for the number of used to reference that token\n2. progressive_tokens for progressively training the token from 1 token to 2 token etc\n3. progressive_tokens_max_steps for the max number of steps until we start full training\n4. vector_shuffle to shuffle vectors\n\nFeel free to add these options to your training! In practice num_vec_per_token around 10+vector shuffle works great!\n\n## Textual Inversion fine-tuning example\n\n[Textual inversion](https://arxiv.org/abs/2208.01618) is a method to personalize text2image models like stable diffusion on your own images using just 3-5 examples.\nThe `textual_inversion.py` script shows how to implement the training procedure and adapt it for stable diffusion.\n\n## Running on Colab\n\nColab for training\n[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/huggingface/notebooks/blob/main/diffusers/sd_textual_inversion_training.ipynb)"], "eval_score_GPT4": "5", "eval_feedback_GPT4": "The response provided is completely correct, accurate, and factual. It aligns perfectly with the reference answer, stating that 3-5 images are needed to teach new concepts to Stable Diffusion using Textual Inversion."}, {"question": "What is the maximum size of a model checkpoint before it is automatically sharded in Transformers version 4.18.0?\n", "true_answer": "10GB", "source_doc": "huggingface/transformers/blob/main/docs/source/en/big_models.md", "generated_answer": "The maximum size of a model checkpoint before it is automatically sharded in Transformers version 4.18.0 is 10GB.", "retrieved_docs": ["</Tip>\n\nIn this guide, we explore the solutions Transformers offer to deal with this issue. Note that this is an area of active development, so the APIs explained here may change slightly in the future.\n\n## Sharded checkpoints\n\nSince version 4.18.0, model checkpoints that end up taking more than 10GB of space are automatically sharded in smaller pieces. In terms of having one single checkpoint when you do `model.save_pretrained(save_dir)`, you will end up with several partial checkpoints (each of which being of size < 10GB) and an index that maps parameter names to the files they are stored in.\n\nYou can control the maximum size before sharding with the `max_shard_size` parameter, so for the sake of an example, we'll use a normal-size models with a small shard size: let's take a traditional BERT model.\n\n```py\nfrom transformers import AutoModel\n\nmodel = AutoModel.from_pretrained(\"bert-base-cased\")\n```\n\nIf you save it using [`~PreTrainedModel.save_pretrained`], you will get a new folder with two files: the config of the model and its weights:\n\n```py\n>>> import os\n>>> import tempfile\n\n>>> with tempfile.TemporaryDirectory() as tmp_dir:\n...     model.save_pretrained(tmp_dir)\n...     print(sorted(os.listdir(tmp_dir)))\n['config.json', 'pytorch_model.bin']\n```\n\nNow let's use a maximum shard size of 200MB:", "```py\n>>> from transformers.modeling_utils import load_sharded_checkpoint\n\n>>> with tempfile.TemporaryDirectory() as tmp_dir:\n...     model.save_pretrained(tmp_dir, max_shard_size=\"200MB\")\n...     load_sharded_checkpoint(model, tmp_dir)\n```\n\n## Low memory loading\n\nSharded checkpoints reduce the memory usage during step 2 of the workflow mentioned above, but in order to use that model in a low memory setting, we recommend leveraging our tools based on the Accelerate library.\n\nPlease read the following guide for more information: [Large model loading using Accelerate](./main_classes/model#large-model-loading)", "## PreTrainedModel\n\n[[autodoc]] PreTrainedModel\n    - push_to_hub\n    - all\n\n<a id='from_pretrained-torch-dtype'></a>\n\n### Large model loading\n\nIn Transformers 4.20.0, the [`~PreTrainedModel.from_pretrained`] method has been reworked to accommodate large models using [Accelerate](https://huggingface.co/docs/accelerate/big_modeling). This requires Accelerate >= 0.9.0 and PyTorch >= 1.9.0. Instead of creating the full model, then loading the pretrained weights inside it (which takes twice the size of the model in RAM, one for the randomly initialized model, one for the weights), there is an option to create the model as an empty shell, then only materialize its parameters when the pretrained weights are loaded.\n\nThis option can be activated with `low_cpu_mem_usage=True`. The model is first created on the Meta device (with empty weights) and the state dict is then loaded inside it (shard by shard in the case of a sharded checkpoint). This way the maximum RAM used is the full size of the model only.\n\n```py\nfrom transformers import AutoModelForSeq2SeqLM\n\nt0pp = AutoModelForSeq2SeqLM.from_pretrained(\"bigscience/T0pp\", low_cpu_mem_usage=True)\n```\n\nMoreover, you can directly place the model on different devices if it doesn't fully fit in RAM (only works for inference for now). With `device_map=\"auto\"`, Accelerate will determine where to put each layer to maximize the use of your fastest devices (GPUs) and offload the rest on the CPU, or even the hard drive if you don't have enough GPU RAM (or CPU RAM). Even if the model is split across several devices, it will run as you would normally expect.", "Before you begin, make sure you have all the necessary libraries installed. \n\n```bash\npip install -q bitsandbytes sentencepiece accelerate transformers\n```\n\n<Tip>\nTo run the following examples with a non-quantized version of the model checkpoint you will need at least 20GB of GPU memory.\n</Tip>\n\n## Loading the model\n\nLet's start by loading the model's 9 billion parameters checkpoint: \n\n```py\n>>> checkpoint = \"HuggingFaceM4/idefics-9b\"\n```\n\nJust like for other Transformers models, you need to load a processor and the model itself from the checkpoint. \nThe IDEFICS processor wraps a [`LlamaTokenizer`] and IDEFICS image processor into a single processor to take care of \npreparing text and image inputs for the model.\n\n```py\n>>> import torch\n\n>>> from transformers import IdeficsForVisionText2Text, AutoProcessor\n\n>>> processor = AutoProcessor.from_pretrained(checkpoint)\n\n>>> model = IdeficsForVisionText2Text.from_pretrained(checkpoint, torch_dtype=torch.bfloat16, device_map=\"auto\")\n```\n\nSetting `device_map` to `\"auto\"` will automatically determine how to load and store the model weights in the most optimized \nmanner given existing devices.\n\n### Quantized model\n\nIf high-memory GPU availability is an issue, you can load the quantized version of the model. To load the model and the \nprocessor in 4bit precision, pass a `BitsAndBytesConfig` to the `from_pretrained` method and the model will be compressed \non the fly while loading.\n\n```py\n>>> import torch\n>>> from transformers import IdeficsForVisionText2Text, AutoProcessor, BitsAndBytesConfig", "You can also use the [`~StableDiffusionXLPipeline.from_single_file`] method to load a model checkpoint stored in a single file format (`.ckpt` or `.safetensors`) from the Hub or locally:\n\n```py\nfrom diffusers import StableDiffusionXLPipeline\nimport torch\n\npipeline = StableDiffusionXLPipeline.from_single_file(\n    \"https://huggingface.co/stabilityai/sdxl-turbo/blob/main/sd_xl_turbo_1.0_fp16.safetensors\", torch_dtype=torch.float16)\npipeline = pipeline.to(\"cuda\")\n```\n\n## Text-to-image\n\nFor text-to-image, pass a text prompt. By default, SDXL Turbo generates a 512x512 image, and that resolution gives the best results. You can try setting the `height` and `width` parameters to 768x768 or 1024x1024, but you should expect quality degradations when doing so.\n\nMake sure to set `guidance_scale` to 0.0 to disable, as the model was trained without it. A single inference step is enough to generate high quality images. \nIncreasing the number of steps to 2, 3 or 4 should improve image quality.\n\n```py\nfrom diffusers import AutoPipelineForText2Image\nimport torch\n\npipeline_text2image = AutoPipelineForText2Image.from_pretrained(\"stabilityai/sdxl-turbo\", torch_dtype=torch.float16, variant=\"fp16\")\npipeline_text2image = pipeline_text2image.to(\"cuda\")\n\nprompt = \"A cinematic shot of a baby racoon wearing an intricate italian priest robe.\"\n\nimage = pipeline_text2image(prompt=prompt, guidance_scale=0.0, num_inference_steps=1).images[0]\nimage\n```", "You can also instantiate the checkpoint using the model architecture directly:\n\n{#if fw === 'pt'}\n```py\nfrom transformers import CamembertTokenizer, CamembertForMaskedLM\n\ntokenizer = CamembertTokenizer.from_pretrained(\"camembert-base\")\nmodel = CamembertForMaskedLM.from_pretrained(\"camembert-base\")\n```\n\nHowever, we recommend using the [`Auto*` classes](https://huggingface.co/transformers/model_doc/auto.html?highlight=auto#auto-classes) instead, as these are by design architecture-agnostic. While the previous code sample limits users to checkpoints loadable in the CamemBERT architecture, using the `Auto*` classes makes switching checkpoints simple:\n\n```py\nfrom transformers import AutoTokenizer, AutoModelForMaskedLM\n\ntokenizer = AutoTokenizer.from_pretrained(\"camembert-base\")\nmodel = AutoModelForMaskedLM.from_pretrained(\"camembert-base\")\n```\n{:else}\n```py\nfrom transformers import CamembertTokenizer, TFCamembertForMaskedLM\n\ntokenizer = CamembertTokenizer.from_pretrained(\"camembert-base\")\nmodel = TFCamembertForMaskedLM.from_pretrained(\"camembert-base\")\n```\n\nHowever, we recommend using the [`TFAuto*` classes](https://huggingface.co/transformers/model_doc/auto.html?highlight=auto#auto-classes) instead, as these are by design architecture-agnostic. While the previous code sample limits users to checkpoints loadable in the CamemBERT architecture, using the `TFAuto*` classes makes switching checkpoints simple:\n\n```py\nfrom transformers import AutoTokenizer, TFAutoModelForMaskedLM", "## Converting custom checkpoints \n\n<Tip>\n\nFalcon models were initially added to the Hugging Face Hub as custom code checkpoints. However, Falcon is now fully\nsupported in the Transformers library. If you fine-tuned a model from a custom code checkpoint, we recommend converting\nyour checkpoint to the new in-library format, as this should give significant improvements to stability and\nperformance, especially for generation, as well as removing the need to use `trust_remote_code=True`!\n\n</Tip>\n\nYou can convert custom code checkpoints to full Transformers checkpoints using the `convert_custom_code_checkpoint.py` \nscript located in the\n[Falcon model directory](https://github.com/huggingface/transformers/tree/main/src/transformers/models/falcon)\nof the Transformers library. To use this script, simply call it with \n`python convert_custom_code_checkpoint.py --checkpoint_dir my_model`. This will convert your checkpoint in-place, and\nyou can immediately load it from the directory afterwards with e.g. `from_pretrained()`. If your model hasn't been\nuploaded to the Hub, we recommend making a backup before attempting the conversion, just in case!\n\n\n## FalconConfig\n\n[[autodoc]] FalconConfig\n    - all\n\n## FalconModel\n\n[[autodoc]] FalconModel\n    - forward\n\n## FalconForCausalLM\n\n[[autodoc]] FalconForCausalLM\n    - forward\n\n## FalconForSequenceClassification\n\n[[autodoc]] FalconForSequenceClassification\n    - forward\n\n## FalconForTokenClassification\n\n[[autodoc]] FalconForTokenClassification\n    - forward\n\n## FalconForQuestionAnswering\n\n[[autodoc]] FalconForQuestionAnswering\n    - forward"], "eval_score_GPT4": "5", "eval_feedback_GPT4": "Feedback: The response provided is completely correct, accurate, and factual based on the reference answer."}, {"question": "What is the purpose of Weights and Biases (W&B) for data scientists and machine learning scientists?\n", "true_answer": "To track their machine learning experiments at every stage, from training to production.", "source_doc": "gradio-app/gradio/blob/main/guides/06_integrating-other-frameworks/Gradio-and-Wandb-Integration.md", "generated_answer": "Weights and Biases (W&B) is a tool for data scientists and machine learning scientists to track their machine learning experiments at every stage, from training to production. It allows any metric to be aggregated over samples and shown in customizable and searchable dashboards. The purpose of W&B is to help data scientists and machine learning scientists address bias in their machine learning systems by providing a platform to monitor and analyze signs of bias at every level of ML development, from task specification", "retrieved_docs": ["Gradio and W&B Integration\n\nRelated spaces: https://huggingface.co/spaces/akhaliq/JoJoGAN\nTags: WANDB, SPACES\nContributed by Gradio team\n\n## Introduction\n\nIn this Guide, we'll walk you through:\n\n- Introduction of Gradio, and Hugging Face Spaces, and Wandb\n- How to setup a Gradio demo using the Wandb integration for JoJoGAN\n- How to contribute your own Gradio demos after tracking your experiments on wandb to the Wandb organization on Hugging Face\n\n\n## What is Wandb?\n\nWeights and Biases (W&B) allows data scientists and machine learning scientists to track their machine learning experiments at every stage, from training to production. Any metric can be aggregated over samples and shown in panels in a customizable and searchable dashboard, like below:\n\n<img alt=\"Screen Shot 2022-08-01 at 5 54 59 PM\" src=\"https://user-images.githubusercontent.com/81195143/182252755-4a0e1ca8-fd25-40ff-8c91-c9da38aaa9ec.png\">\n\n## What are Hugging Face Spaces & Gradio?\n\n### Gradio\n\nGradio lets users demo their machine learning models as a web app, all in a few lines of Python. Gradio wraps any Python function (such as a machine learning model's inference function) into a user interface and the demos can be launched inside jupyter notebooks, colab notebooks, as well as embedded in your own website and hosted on Hugging Face Spaces for free.\n\nGet started [here](https://gradio.app/getting_started)\n\n### Hugging Face Spaces", "--\ntitle: \"Let's talk about biases in machine learning! Ethics and Society Newsletter #2\" \nthumbnail: /blog/assets/122_ethics_soc_2/thumbnail-solstice.png\nauthors:\n- user: yjernite\n---\n\n# Machine Learning in development: Let's talk about bias!\n\n\n_Bias in ML is ubiquitous, and Bias in ML is complex; so complex in fact that no single technical intervention is likely to meaningfully address the problems it engenders. ML models, as sociotechnical systems, amplify social trends that may exacerbate inequities and harmful biases in ways that depend on their deployment context and are constantly evolving._\n\n_This means that developing ML systems with care requires vigilance and responding to feedback from those deployment contexts, which in turn we can facilitate by sharing lessons across contexts and developing tools to analyze signs of bias at every level of ML development._\n\n_This blog post from the [Ethics and Society regulars @\ud83e\udd17](https://huggingface.co/blog/ethics-soc-1) shares some of the lessons we have learned along with tools we have developed to support ourselves and others in our community\u2019s efforts to better address bias in Machine Learning. The first part is a broader reflection on bias and its context. If you\u2019ve already read it and are coming back specifically for the tools, feel free to jump to the [datasets](#i-am-curatingpicking-a-dataset-for-my-ml-system-how-can-i-address-bias) or [models](#i-am-trainingselecting-a-model-for-my-ml-system-how-can-i-address-bias)\nsection!_\n\n<p align=\"center\">\n <br>\n <img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/ethics_soc_2/img1.jpg\" alt=\"Selection of tools developed by HF team members to address bias in ML\" />\n    <em>Selection of tools developed by \ud83e\udd17 team members to address bias in ML</em>\n</p>", "- We ground the creation of these tools and artifacts in _responsibility_ for the impacts of what we do now and in the future. Prioritizing this has led to project designs that make machine learning systems more _auditable_ and _understandable_ \u2013 including for people with expertise outside of ML \u2013 such as [the education project](https://huggingface.co/blog/education) and our experimental [tools for ML data analysis that don't require coding](https://huggingface.co/spaces/huggingface/data-measurements-tool).", "Also, as we have mentioned in a [previous newsletter](https://huggingface.co/blog/ethics-soc-2#addressing-bias-throughout-the-ml-development-cycle), there is no one single way to develop machine learning systems, and any of the steps in the development and deployment process can present opportunities to tackle bias, from who is included at the start, to defining the task, to curating the dataset, training the model, and more. This also applies to multimodal models and the ways in which they are ultimately deployed or productionized in society, since the consequences of bias in multimodal models will depend on their downstream use. For instance, if a model is used in a human-in-the-loop setting for graphic design (such as those created by [RunwayML](https://runwayml.com/ai-magic-tools/text-to-image/)), the user has numerous occasions to detect and correct bias, for instance by changing the prompt or the generation options. However, if a model is used as part of a [tool to help forensic artists create police sketches of potential suspects](https://www.vice.com/en/article/qjk745/ai-police-sketches) (see image below), then the stakes are much higher, since this can reinforce stereotypes and racial biases in a high-risk setting.\n\n\n<p align=\"center\">\n <br>\n <img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/152_ethics_soc_4/forensic.png\" alt=\"Forensic AI Sketch artist tool developed using Dall-E 2.\" />\n</p>\n\n## Other updates\n\nWe are also continuing work on other fronts of ethics and society, including:", "**Machine Learning & Sensing Laboratory:** A University of Florida laboratory that develops machine learning methods for autonomously analyzing and understanding sensor data.\n\n#### **1. How has ML made a positive impact on Science**\nML has made a positive impact in a number of ways from helping to automate tedious and/or slow tasks or providing new ways to examine and look at various questions. One example from my work in ML for plant science is that we have developed ML approaches to automate plant root segmentation and characterization in imagery. This task was previously a bottleneck for plant scientists looking at root imagery. By automating this step through ML we can conduct these analyses at a much higher throughput and begin to use this data to investigate plant biology research questions at scale.\n\n#### **2. What are the biggest ML challenges within Scientific research?**\nThere are many challenges. One example is when using ML for Science research, we have to think carefully through the data collection and curation protocols. In some cases, the protocols we used for non-ML analysis are not appropriate or effective. The quality of the data and how representative it is of what we expect to see in the application can make a huge impact on the performance, reliability, and trustworthiness of an ML-based system.\n\n#### **3. What\u2019s a common mistake you see people make trying to integrate ML into Science?**\nRelated to the question above, one common mistake is misinterpreting results or performance to be a function of just the ML system and not also considering the data collection, curation, calibration, and normalization protocols.", "So, who\u2019s on the hook for machine biases in ML? These three cases illustrate one of the reasons why discussions about the responsibility of ML developers in addressing bias can get so complicated: depending on decisions made at other points in the ML system development process by other people, the biases in an ML dataset or model may land anywhere between being irrelevant to the application settings and directly leading to grievous harm. However, in all of these cases, **stronger biases in the model/dataset increase the risk of negative outcomes**. The European Union has started to develop frameworks that address this phenomenon in [recent regulatory efforts](https://ec.europa.eu/info/business-economy-euro/doing-business-eu/contract-rules/digital-contracts/liability-rules-artificial-intelligence_en): in short, a company that deploys an AI system based on a measurably biased model is liable for harm caused by the system.\n\nConceptualizing bias as a risk factor then allows us to better understand the **shared responsibility** for machine biases between developers at all stages. Bias can never be fully removed, not least because the definitions of social biases and the power dynamics that tie them to discrimination vary vastly across social contexts. However:\n\n\n\n1. Each stage of the development process, from task specification, dataset curation, and model training, to model integration and system deployment, can take steps to minimize the aspects of machine bias** that most directly depend on its choices** and technical decisions, and\n2. Clear communication and **information flow between the various ML development stages** can make the difference between making choices that build on top of each other to attenuate the negative potential of bias (multipronged approach to bias mitigation, as in deployment scenario 1 above) _versus_ making choices that compound this negative potential to exacerbate the risk of harm (as in deployment scenario 3).\n\nIn the next section, we review these various stages along with some of the tools that can help us address machine bias at each of them.\n\n\n## Addressing Bias throughout the ML Development Cycle\n\nReady for some practical advice yet? Here we go \ud83e\udd17", "## Limitations and Bias\n*Note any limitations or biases that the metric has.*\n\n\n## Citation\n```bibtex\n@article{scikit-learn,\n  title={Scikit-learn: Machine Learning in {P}ython},\n  author={Pedregosa, F. and Varoquaux, G. and Gramfort, A. and Michel, V.\n         and Thirion, B. and Grisel, O. and Blondel, M. and Prettenhofer, P.\n         and Weiss, R. and Dubourg, V. and Vanderplas, J. and Passos, A. and\n         Cournapeau, D. and Brucher, M. and Perrot, M. and Duchesnay, E.},\n  journal={Journal of Machine Learning Research},\n  volume={12},\n  pages={2825--2830},\n  year={2011}\n}\n```\n\n## Further References\n\n- This Hugging Face implementation uses [this scikit-learn implementation](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.matthews_corrcoef.html)"], "eval_score_GPT4": "4", "eval_feedback_GPT4": "Feedback: The response provided is mostly correct and factual as it aligns well with the reference answer. It correctly identifies W&B as a tool for tracking machine learning experiments and mentions its use in monitoring metrics, which is a part of the tracking process. However, the response introduces an additional aspect of addressing bias in machine learning systems, which is not mentioned in the reference answer. While this is a valid function of W&B, it is not the primary purpose highlighted in the reference answer. Therefore, the response is not completely accurate in the context of the reference answer provided."}, {"question": "What is the name of the open-source library created by Hugging Face to simplify Transformer acceleration?\n", "true_answer": "Optimum", "source_doc": "huggingface/blog/blob/main/intel.md", "generated_answer": "The name of the open-source library created by Hugging Face to simplify Transformer acceleration is Optimum.", "retrieved_docs": ["To target these needs, Hugging Face built two open-sourced libraries: __Accelerate__ and __Optimum__. While [\ud83e\udd17 Accelerate](https://huggingface.co/docs/accelerate/index) focuses on out-of-the-box distributed training, [\ud83e\udd17 Optimum](https://huggingface.co/docs/optimum/index), as an extension of transformers, accelerates model training and inference by leveraging the maximum efficiency of users\u2019 targeted hardware. Optimum integrated machine learning accelerators like ONNX Runtime and specialized hardware like [Intel's Habana Gaudi](https://huggingface.co/blog/habana-gaudi-2-benchmark), so users can benefit from considerable speedup in both training and inference. Besides, Optimum seamlessly integrates other Hugging Face\u2019s tools while inheriting the same ease of use as Transformers. Developers can easily adapt their work to achieve lower latency with less computing power.\n\n## ONNX Runtime Training\n\n[ONNX Runtime](https://onnxruntime.ai/) accelerates [large model training](https://onnxruntime.ai/docs/get-started/training-pytorch.html) to speed up throughput by up to 40% standalone, and 130% when composed with [DeepSpeed](https://www.deepspeed.ai/tutorials/zero/) for popular HuggingFace transformer based models. ONNX Runtime is already integrated as part of Optimum and enables faster training through Hugging Face\u2019s Optimum training framework.", "Making Poplar compatible with these widely used, third-party systems allows developers to easily port their models from their other compute platforms and start taking advantage of the IPU\u2019s advanced AI capabilities.\n\n## Optimising Transformers for Production\n\nTransformers have completely transformed (pun intended) the field of AI. Models such as BERT are widely used by Graphcore customers in a huge array of applications, across NLP and beyond. These multi-talented models can perform feature extraction, text generation, sentiment analysis, translation and many more functions.\n\nAlready, Hugging Face plays host to hundreds of Transformers, from the French-language CamemBERT to ViT which applies lessons learned in NLP to computer vision. The Transformers library is downloaded an average of 2 million times every month and demand is growing.\n\nWith a user base of more than 50,000 developers \u2013 Hugging Face has seen the fastest ever adoption of an open-source project.\n\nNow, with its Hardware Partner Program, Hugging Face is connecting the ultimate Transformer toolset with today's most advanced AI hardware.\n\nUsing Optimum, a new open-source library and toolkit, developers will be able to access hardware-optimized models certified by Hugging Face.\n\nThese are being developed in a collaboration between Graphcore and Hugging Face, with the first IPU-optimized models appearing on Optimum later this year. Ultimately, these will cover a wide range of applications, from vision and speech to translation and text generation.\n\nHugging Face CEO Cl\u00e9ment Delangue said: \u201cDevelopers all want access to the latest and greatest hardware \u2013 like the Graphcore IPU, but there\u2019s always that question of whether they\u2019ll have to learn new code or processes. With Optimum and the Hugging Face Hardware Program, that\u2019s just not an issue. It\u2019s essentially plug-and-play\".\n\n## SOTA Models meet SOTA Hardware\n\nPrior to the announcement of the Hugging Face Partnership, we had demonstrated the power of the IPU to accelerate state-of-the-art Transformer models with a special Graphcore-optimised implementation of Hugging Face BERT using Pytorch.", "### **NEW: Hardware Acceleration**\n\nHugging Face is [partnering with leading AI hardware accelerators](http://hf.co/hardware) such as Intel, Qualcomm and GraphCore to make state-of-the-art production performance accessible and extend training capabilities on SOTA hardware. As the first step in this journey, we [introduced a new Open Source library](https://huggingface.co/blog/hardware-partners-program): \ud83e\udd17 Optimum - the ML optimization toolkit for production performance \ud83c\udfce. Learn more in this [blog post](https://huggingface.co/blog/graphcore). \n\n### **NEW: Inference on SageMaker**\n\nWe launched a [new integration with AWS](https://huggingface.co/blog/deploy-hugging-face-models-easily-with-amazon-sagemaker) to make it easier than ever to deploy \ud83e\udd17 Transformers in SageMaker \ud83d\udd25. Pick up the code snippet right from the \ud83e\udd17 Hub model page! Learn more about how to leverage transformers in SageMaker in our [docs](https://huggingface.co/docs/sagemaker/inference) or check out these [video tutorials](https://youtube.com/playlist?list=PLo2EIpI_JMQtPhGR5Eo2Ab0_Vb89XfhDJ).\n\nFor questions reach out to us on the forum: [https://discuss.huggingface.co/c/sagemaker/17](https://discuss.huggingface.co/c/sagemaker/17)\n\n![Sagemaker](assets/27_summer_at_huggingface/sagemaker.png)\n\n### **NEW: AutoNLP In Your Browser**", "Optimizing their performance in the real world requires considerable time, effort and skills that are beyond the reach of many companies and organizations. In providing an open-source library of transformer models, Hugging Face has directly addressed these issues. Integrating IPUs with HuggingFace also allows developers to leverage not just the models, but also datasets available in the HuggingFace Hub.\n\nDevelopers can now use Graphcore systems to train 10 different types of state-of-the-art transformer models and access thousands of datasets with minimal coding complexity. With this partnership, we are providing users with the tools and ecosystem to easily download and fine-tune state-of-the-art pretrained models to various domains and downstream tasks.\n\n## Bringing Graphcore\u2019s latest hardware and software to the table\n\nWhile members of Hugging Face\u2019s ever-expanding user base have already been able to benefit from the speed, performance, and power- and cost-efficiency of IPU technology, a combination of recent hardware and software releases from Graphcore will unlock even more potential.\n\nOn the hardware front, the [Bow IPU](https://www.graphcore.ai/bow-processors) \u2014 announced in March and now shipping to customers \u2014 is the first processor in the world to use Wafer-on-Wafer (WoW) 3D stacking technology, taking the well-documented benefits of the IPU to the next level. Featuring ground-breaking advances in compute architecture and silicon implementation, communication and memory, each Bow IPU delivers up to 350 teraFLOPS of AI compute\u2014an impressive 40% increase in performance\u2014and up to 16% more power efficiency compared to the previous generation IPU. Importantly, Hugging Face Optimum users can switch seamlessly from previous generation IPUs to Bow processors, as no code changes are required.", "With 60,000+ stars on Github, 30,000+ models, and millions of monthly visits, Hugging Face is one of the fastest-growing projects in open source software history, and the go-to place for the machine learning community. \n\nWith its [Hardware Partner Program](https://huggingface.co/hardware), Hugging Face provides Gaudi\u2019s advanced deep learning hardware with the ultimate Transformer toolset. This partnership will enable rapid expansion of the Habana Gaudi training transformer model library, bringing Gaudi efficiency and ease of use to a wide array of customer use cases like natural language processing, computer vision, speech, and more. \n\n\u201c*We\u2019re excited to partner with Hugging Face and its many open-source developers to address the growing demand for transformer models that benefit from the efficiency, usability, and scalability of the Gaudi training platform*\u201d, said Sree Ganesan, head of software product management, Habana Labs. \n\n\u201cHabana Gaudi brings a new level of efficiency to deep learning model training, and we\u2019re super excited to make this performance easily accessible to Transformer users with minimal code changes through Optimum\u201d, said Jeff Boudier, product director at Hugging Face.\n\nTo learn how to get started training with Habana Gaudi, please visit [https://developer.habana.ai](https://developer.habana.ai). \n\nFor more info on the Hugging Face and Habana Gaudi collaboration, please visit [https://huggingface.co/Habana](https://huggingface.co/Habana).", "RWKV can also benefit from the acceleration techniques proposed by [`optimum`](https://github.com/huggingface/optimum) library in the near future.\nSome of these techniques are highlighted in the [`rwkv.cpp` repository](https://github.com/saharNooby/rwkv.cpp) or [`rwkv-cpp-cuda` repository](https://github.com/harrisonvanderbyl/rwkv-cpp-cuda).\n\n## Acknowledgements\n\nThe Hugging Face team would like to thank Bo and RWKV community for their time and for answering our questions about the architecture. We would also like to thank them for their help and support and we look forward to see more adoption of RWKV models in the HF ecosystem.\nWe also would like to acknowledge the work of [Johan Wind](https://twitter.com/johanwind) for his blogpost on RWKV, which helped us a lot to understand the architecture and its potential.\nAnd finally, we would like to highlight anf acknowledge the work of [ArEnSc](https://github.com/ArEnSc) for starting over the initial `transformers` PR.\nAlso big kudos to [Merve Noyan](https://huggingface.co/merve), [Maria Khalusova](https://huggingface.co/MariaK) and [Pedro Cuenca](https://huggingface.co/pcuenq) for kindly reviewing this blogpost to make it much better!\n\n## Citation\n\nIf you use RWKV for your work, please use [the following `cff` citation](https://github.com/BlinkDL/RWKV-LM/blob/main/CITATION.cff).", "--\ntitle: \"Graphcore and Hugging Face Launch New Lineup of IPU-Ready Transformers\"\nthumbnail: /blog/assets/77_graphcore-update/graphcore_update.png\nauthors:\n- user: sallydoherty\n  guest: true\n---\n\n# Graphcore and Hugging Face Launch New Lineup of IPU-Ready Transformers\n\n\n[Graphcore](https://huggingface.co/hardware/graphcore/) and Hugging Face have significantly expanded the range of Machine Learning modalities and tasks available in [Hugging Face Optimum](https://github.com/huggingface/optimum), an open-source library for Transformers performance optimization. Developers now have convenient access to a wide range of off-the-shelf Hugging Face Transformer models, optimised to deliver the best possible performance on Graphcore\u2019s IPU.\n\nIncluding the [BERT transformer model](https://www.graphcore.ai/posts/getting-started-with-hugging-face-transformers-for-ipus-with-optimum) made available shortly after [Optimum Graphcore launched](https://huggingface.co/blog/graphcore), developers can now access 10 models covering Natural Language Processing (NLP), Speech and Computer Vision, which come with IPU configuration files and ready-to-use pre-trained and fine-tuned model weights.\n\n## New Optimum models\n\n### Computer vision\n\n[ViT](https://huggingface.co/Graphcore/vit-base-ipu) (Vision Transformer) is a breakthrough in image recognition that uses the transformer mechanism as its main component. When images are input to ViT, they're divided into small patches similar to how words are processed in language systems. Each patch is encoded by the Transformer (Embedding) and then can be processed individually.\n\n### NLP"], "eval_score_GPT4": "5", "eval_feedback_GPT4": "The response provided is completely correct, accurate, and factual. It matches the reference answer exactly, indicating that the respondent has correctly identified \"Optimum\" as the open-source library created by Hugging Face to simplify Transformer acceleration. There are no inaccuracies or incorrect information in the response."}, {"question": "What parameter is used to ensure that elements in a row have the same height in Gradio?\n", "true_answer": "equal_height", "source_doc": "gradio-app/gradio/blob/main/guides/cn/03_building-with-blocks/02_controlling-layout.md", "generated_answer": "The `min_width` parameter is used to ensure that elements in a row have the same height in Gradio by setting the minimum width for each element. If there isn't sufficient space to satisfy all `min_width` values, the row will wrap. However, if you want to control the height of specific components, you can use the `height` parameter, which accepts either a number (in pixels) or a string with a CSS unit. This parameter is available for various components", "retrieved_docs": ["```python\nimport gradio as gr\n\ncss = \"\"\"\n.container {\n    height: 100vh;\n}\n\"\"\"\n\nwith gr.Blocks(css=css) as demo:\n    with gr.Column(elem_classes=[\"container\"]):\n        name = gr.Chatbot(value=[[\"1\", \"2\"]], height=\"70%\")\n\ndemo.launch()\n```\n\nIn this example, the Column layout component is given a height of 100% of the viewport height (100vh), and the Chatbot component inside it takes up 70% of the Column's height.\n\nYou can apply any valid CSS unit for these parameters. For a comprehensive list of CSS units, refer to [this guide](https://www.w3schools.com/cssref/css_units.php). We recommend you always consider responsiveness and test your interfaces on various screen sizes to ensure a consistent user experience.\n\n\n\n## Tabs and Accordions\n\nYou can also create Tabs using the `with gr.Tab('tab_name'):` clause. Any component created inside of a `with gr.Tab('tab_name'):` context appears in that tab. Consecutive Tab clauses are grouped together so that a single tab can be selected at one time, and only the components within that Tab's context are shown.\n\nFor example:\n\n$code_blocks_flipper\n$demo_blocks_flipper\n\nAlso note the `gr.Accordion('label')` in this example. The Accordion is a layout that can be toggled open or closed. Like `Tabs`, it is a layout element that can selectively hide or show content. Any components that are defined inside of a `with gr.Accordion('label'):` will be hidden or shown when the accordion's toggle icon is clicked.\n\nLearn more about [Tabs](https://gradio.app/docs/#tab) and [Accordions](https://gradio.app/docs/#accordion) in the docs.", "- Fix bug where `height` set in `Gallery.style` was not respected by the front-end by [@freddyaboulton](https://github.com/freddyaboulton) in [PR 3343](https://github.com/gradio-app/gradio/pull/3343)\n- Ensure markdown lists are rendered correctly by [@pngwn](https://github.com/pngwn) in [PR 3341](https://github.com/gradio-app/gradio/pull/3341)\n- Ensure that the initial empty value for `gr.Dropdown(Multiselect=True)` is an empty list and the initial value for `gr.Dropdown(Multiselect=False)` is an empty string by [@pngwn](https://github.com/pngwn) in [PR 3338](https://github.com/gradio-app/gradio/pull/3338)\n- Ensure uploaded images respect the shape property when the canvas is also enabled by [@pngwn](https://github.com/pngwn) in [PR 3351](https://github.com/gradio-app/gradio/pull/3351)\n- Ensure that Google Analytics works correctly when gradio apps are created with `analytics_enabled=True` by [@abidlabs](https://github.com/abidlabs) in [PR 3349](https://github.com/gradio-app/gradio/pull/3349)\n- Fix bug where files were being re-uploaded after updates by [@freddyaboulton](https://github.com/freddyaboulton) in [PR 3375](https://github.com/gradio-app/gradio/pull/3375)", "- Fix bug where `height` set in `Gallery.style` was not respected by the front-end by [@freddyaboulton](https://github.com/freddyaboulton) in [PR 3343](https://github.com/gradio-app/gradio/pull/3343)\n- Ensure markdown lists are rendered correctly by [@pngwn](https://github.com/pngwn) in [PR 3341](https://github.com/gradio-app/gradio/pull/3341)\n- Ensure that the initial empty value for `gr.Dropdown(Multiselect=True)` is an empty list and the initial value for `gr.Dropdown(Multiselect=False)` is an empty string by [@pngwn](https://github.com/pngwn) in [PR 3338](https://github.com/gradio-app/gradio/pull/3338)\n- Ensure uploaded images respect the shape property when the canvas is also enabled by [@pngwn](https://github.com/pngwn) in [PR 3351](https://github.com/gradio-app/gradio/pull/3351)\n- Ensure that Google Analytics works correctly when gradio apps are created with `analytics_enabled=True` by [@abidlabs](https://github.com/abidlabs) in [PR 3349](https://github.com/gradio-app/gradio/pull/3349)\n- Fix bug where files were being re-uploaded after updates by [@freddyaboulton](https://github.com/freddyaboulton) in [PR 3375](https://github.com/gradio-app/gradio/pull/3375)", "- ensure defaults height match for media inputs by [@pngwn](https://github.com/pngwn) in [PR 1236](https://github.com/gradio-app/gradio/pull/1236)\n- Default Label label value by [@radames](https://github.com/radames) in [PR 1239](https://github.com/gradio-app/gradio/pull/1239)\n- update-shortcut-syntax by [@FarukOzderim](https://github.com/FarukOzderim) in [PR 1234](https://github.com/gradio-app/gradio/pull/1234)\n- Update version.txt by [@FarukOzderim](https://github.com/FarukOzderim) in [PR 1244](https://github.com/gradio-app/gradio/pull/1244)\n- Layout bugs by [@pngwn](https://github.com/pngwn) in [PR 1246](https://github.com/gradio-app/gradio/pull/1246)\n- Update demo by [@FarukOzderim](https://github.com/FarukOzderim) in [PR 1253](https://github.com/gradio-app/gradio/pull/1253)\n- Button default name by [@FarukOzderim](https://github.com/FarukOzderim) in [PR 1243](https://github.com/gradio-app/gradio/pull/1243)", "- ensure defaults height match for media inputs by [@pngwn](https://github.com/pngwn) in [PR 1236](https://github.com/gradio-app/gradio/pull/1236)\n- Default Label label value by [@radames](https://github.com/radames) in [PR 1239](https://github.com/gradio-app/gradio/pull/1239)\n- update-shortcut-syntax by [@FarukOzderim](https://github.com/FarukOzderim) in [PR 1234](https://github.com/gradio-app/gradio/pull/1234)\n- Update version.txt by [@FarukOzderim](https://github.com/FarukOzderim) in [PR 1244](https://github.com/gradio-app/gradio/pull/1244)\n- Layout bugs by [@pngwn](https://github.com/pngwn) in [PR 1246](https://github.com/gradio-app/gradio/pull/1246)\n- Update demo by [@FarukOzderim](https://github.com/FarukOzderim) in [PR 1253](https://github.com/gradio-app/gradio/pull/1253)\n- Button default name by [@FarukOzderim](https://github.com/FarukOzderim) in [PR 1243](https://github.com/gradio-app/gradio/pull/1243)", "- `min_width` will set the minimum width the element will take. The Row will wrap if there isn't sufficient space to satisfy all `min_width` values.\n\nLearn more about Rows in the [docs](https://gradio.app/docs/#row).\n\n## Columns and Nesting\n\nComponents within a Column will be placed vertically atop each other. Since the vertical layout is the default layout for Blocks apps anyway, to be useful, Columns are usually nested within Rows. For example:\n\n$code_rows_and_columns\n$demo_rows_and_columns\n\nSee how the first column has two Textboxes arranged vertically. The second column has an Image and Button arranged vertically. Notice how the relative widths of the two columns is set by the `scale` parameter. The column with twice the `scale` value takes up twice the width.\n\nLearn more about Columns in the [docs](https://gradio.app/docs/#column).\n\n# Dimensions\n\nYou can control the height and width of various components, where the parameters are available. These parameters accept either a number (interpreted as pixels) or a string. Using a string allows the direct application of any CSS unit to the encapsulating Block element, catering to more specifc design requirements. When omitted, Gradio uses default dimensions suited for most use cases.\n\nBelow is an example illustrating the use of viewport width (vw):\n\n```python\nimport gradio as gr\n\nwith gr.Blocks() as demo:\n    im = gr.ImageEditor(\n        width=\"50vw\",\n    )\n\ndemo.launch()\n```\n\nWhen using percentage values for dimensions, you may want to define a parent component with an absolute unit (e.g. `px` or `vw`). This approach ensures that child components with relative dimensions are sized appropriately:\n\n\n```python\nimport gradio as gr\n\ncss = \"\"\"\n.container {\n    height: 100vh;\n}\n\"\"\"", "- [#5283](https://github.com/gradio-app/gradio/pull/5283) [`a7460557`](https://github.com/gradio-app/gradio/commit/a74605572dd0d6bb41df6b38b120d656370dd67d) - Add height parameter and scrolling to `gr.Dataframe`. Thanks [@abidlabs](https://github.com/abidlabs)!"], "eval_score_GPT4": "1", "eval_feedback_GPT4": "Feedback: The response provided is incorrect. The parameter mentioned, `min_width`, does not ensure that elements in a row have the same height in Gradio. The correct parameter for ensuring equal height of elements in a row is `equal_height`, as per the reference answer. The response incorrectly describes the functionality of `min_width` and introduces the `height` parameter, which is not relevant to the question about row element heights. Therefore, the response does not meet the criteria for correctness, accuracy, or factuality."}, {"question": "What is the command to install the latest version of Optimum with OpenVINO support?\n", "true_answer": "pip install --upgrade-strategy eager optimum[\"openvino\"]", "source_doc": "huggingface/diffusers/blob/main/docs/source/en/optimization/open_vino.md", "generated_answer": "To install the latest main dev version of Optimum with OpenVINO support, you can run the following command:\n\n```bash\npip install --upgrade-strategy eager optimum[\"openvino\"]\n```\n\nThis will ensure that the `optimum-intel` package is installed with the latest version, which includes OpenVINO support for accelerated inference on Intel processors.", "retrieved_docs": ["The `--upgrade-strategy eager` option is needed to ensure the different packages are upgraded to the latest possible version.\n\nTo install from source:\n\n```bash\npython -m pip install git+https://github.com/huggingface/optimum.git\n```\n\nFor the accelerator-specific features, append `optimum[accelerator_type]` to the above command:\n\n```bash\npython -m pip install optimum[onnxruntime]@git+https://github.com/huggingface/optimum.git\n```\n\n## Accelerated Inference\n\n\ud83e\udd17 Optimum provides multiple tools to export and run optimized models on various ecosystems: \n\n- [ONNX](https://huggingface.co/docs/optimum/exporters/onnx/usage_guides/export_a_model) / [ONNX Runtime](https://huggingface.co/docs/optimum/onnxruntime/usage_guides/models)\n- TensorFlow Lite\n- [OpenVINO](https://huggingface.co/docs/optimum/intel/inference)\n- Habana first-gen Gaudi / Gaudi2, more details [here](https://huggingface.co/docs/optimum/main/en/habana/usage_guides/accelerate_inference)\n\nThe [export](https://huggingface.co/docs/optimum/exporters/overview) and optimizations can be done both programmatically and with a command line.\n\n### Features summary", "!--Copyright 2023 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\n\nhttp://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License.\n-->\n\n# OpenVINO\n\n\ud83e\udd17 [Optimum](https://github.com/huggingface/optimum-intel) provides Stable Diffusion pipelines compatible with OpenVINO to perform inference on a variety of Intel processors (see the [full list](https://docs.openvino.ai/latest/openvino_docs_OV_UG_supported_plugins_Supported_Devices.html) of supported devices).\n\nYou'll need to install \ud83e\udd17 Optimum Intel with the `--upgrade-strategy eager` option to ensure [`optimum-intel`](https://github.com/huggingface/optimum-intel) is using the latest version:\n\n```bash\npip install --upgrade-strategy eager optimum[\"openvino\"]\n```\n\nThis guide will show you how to use the Stable Diffusion and Stable Diffusion XL (SDXL) pipelines with OpenVINO.\n\n## Stable Diffusion\n\nTo load and run inference, use the [`~optimum.intel.OVStableDiffusionPipeline`]. If you want to load a PyTorch model and convert it to the OpenVINO format on-the-fly, set `export=True`:\n\n```python\nfrom optimum.intel import OVStableDiffusionPipeline", "And run post-installation configuration:\n\n```bash\npython -m torch_ort.configure\n```\n\n### Install Optimum\n\nYou can install Optimum via pypi:\n\n```bash\npip install optimum\n```\n\nOr install from source:\n\n```bash\npip install git+https://github.com/huggingface/optimum.git\n```\n\nThis command installs the current main dev version of Optimum, which could include latest developments(new features, bug fixes). However, the\nmain version might not be very stable. If you run into any problem, please open an [issue](https://github.com/huggingface/optimum/issues) so\nthat we can fix it as soon as possible.\n\n## ORTTrainer\n\nThe [`ORTTrainer`] class inherits the [`Trainer`](https://huggingface.co/docs/transformers/main/en/main_classes/trainer#trainer)\nof Transformers. You can easily adapt the codes by replacing `Trainer` of transformers with `ORTTrainer` to take advantage of the acceleration\nempowered by ONNX Runtime. Here is an example of how to use `ORTTrainer` compared with `Trainer`:\n\n```diff\n-from transformers import Trainer, TrainingArguments\n+from optimum.onnxruntime import ORTTrainer, ORTTrainingArguments\n\n# Step 1: Define training arguments\n-training_args = TrainingArguments(\n+training_args = ORTTrainingArguments(\n    output_dir=\"path/to/save/folder/\",\n-   optim = \"adamw_hf\",\n+   optim=\"adamw_ort_fused\",\n    ...\n)\n\n# Step 2: Create your ONNX Runtime Trainer\n-trainer = Trainer(\n+trainer = ORTTrainer(\n    model=model,\n    args=training_args,\n    train_dataset=train_dataset,\n+   feature=\"text-classification\",\n    ...\n)", "--\ntitle: \"Accelerate your models with \ud83e\udd17 Optimum Intel and OpenVINO\"\nthumbnail: /blog/assets/113_openvino/thumbnail.png\nauthors:\n- user: echarlaix\n- user: juliensimon\n---\n\n# Accelerate your models with \ud83e\udd17 Optimum Intel and OpenVINO\n\n\n![image](assets/113_openvino/thumbnail.png)\n\nLast July, we [announced](https://huggingface.co/blog/intel) that Intel and Hugging Face would collaborate on building state-of-the-art yet simple hardware acceleration tools for Transformer models. \n\u200b\nToday, we are very happy to announce that we added Intel [OpenVINO](https://docs.openvino.ai/latest/index.html) to [Optimum Intel](https://github.com/huggingface/optimum-intel). You can now easily perform inference with OpenVINO Runtime on a variety of Intel processors  ([see](https://docs.openvino.ai/latest/openvino_docs_OV_UG_supported_plugins_Supported_Devices.html) the full list of supported devices) using Transformers models which can be hosted either on the Hugging Face hub or locally. You can also quantize your model with the OpenVINO Neural Network Compression Framework ([NNCF](https://github.com/openvinotoolkit/nncf)), and reduce its size and prediction latency in near minutes. \u200b\n\nThis first release is based on OpenVINO 2022.2 and enables inference for a large quantity of PyTorch models using our [`OVModels`](https://huggingface.co/docs/optimum/intel/inference). Post-training static quantization and quantization aware training can be applied on many encoder models (BERT, DistilBERT, etc.). More encoder models will be supported in the upcoming OpenVINO release. Currently the quantization of Encoder Decoder models is not enabled, however this restriction should be lifted with our integration of the next OpenVINO release.\n\n\u200bLet us show you how to get started in minutes!\u200b", "You can find the resulting [model](https://huggingface.co/echarlaix/vit-food101-int8) hosted on the Hugging Face hub. To load it, you can easily do as follows:\n```python\nfrom optimum.intel.openvino import OVModelForImageClassification\n\u200b\nov_model = OVModelForImageClassification.from_pretrained(\"echarlaix/vit-food101-int8\")\n```\n\n## Now it's your turn\n\u200b\nAs you can see, it's pretty easy to accelerate your models with \ud83e\udd17 Optimum Intel and OpenVINO. If you'd like to get started, please visit the [Optimum Intel](https://github.com/huggingface/optimum-intel) repository, and don't forget to give it a star \u2b50. You'll also find additional examples [there](https://huggingface.co/docs/optimum/intel/optimization_ov). If you'd like to dive deeper into OpenVINO, the Intel [documentation](https://docs.openvino.ai/latest/index.html) has you covered. \n\n\u200b\nGive it a try and let us know what you think. We'd love to hear your feedback on the Hugging Face [forum](https://discuss.huggingface.co/c/optimum), and please feel free to request features or file issues on [Github](https://github.com/huggingface/optimum-intel).\n\u200b\n\nHave fun with \ud83e\udd17 Optimum Intel, and thank you for reading.\n\n\u200b", "### OpenVINO\n\nBefore you begin, make sure you have all the necessary libraries installed :\n\n```bash\npip install --upgrade-strategy eager optimum[openvino,nncf]\n```\n\nIt is possible to export \ud83e\udd17 Transformers and Diffusers models to the OpenVINO format easily:\n\n```bash\noptimum-cli export openvino --model distilbert-base-uncased-finetuned-sst-2-english distilbert_sst2_ov\n```\n\nIf you add `--int8`, the weights will be quantized to INT8. Static quantization can also be applied on the activations using [NNCF](https://github.com/openvinotoolkit/nncf), more information can be found in the [documentation](https://huggingface.co/docs/optimum/main/en/intel/optimization_ov).\n\nTo load a model and run inference with OpenVINO Runtime, you can just replace your `AutoModelForXxx` class with the corresponding `OVModelForXxx` class. To load a PyTorch checkpoint and convert it to the OpenVINO format on-the-fly, you can set `export=True` when loading your model.\n\n```diff\n- from transformers import AutoModelForSequenceClassification\n+ from optimum.intel import OVModelForSequenceClassification\n  from transformers import AutoTokenizer, pipeline\n\n  model_id = \"distilbert-base-uncased-finetuned-sst-2-english\"\n  tokenizer = AutoTokenizer.from_pretrained(model_id)\n- model = AutoModelForSequenceClassification.from_pretrained(model_id)\n+ model = OVModelForSequenceClassification.from_pretrained(\"distilbert_sst2_ov\")\n\n  classifier = pipeline(\"text-classification\", model=model, tokenizer=tokenizer)\n  results = classifier(\"He's a dreadful magician.\")\n```", "## Optimum Intel\n\n### OpenVINO"], "eval_score_GPT4": "5", "eval_feedback_GPT4": "The response provided is completely correct, accurate, and factual. It matches the reference answer and includes the correct command to install the latest version of Optimum with OpenVINO support. The additional explanation about the `optimum-intel` package and the use of the `--upgrade-strategy eager` option is accurate and provides useful context without deviating from the correctness of the command."}]