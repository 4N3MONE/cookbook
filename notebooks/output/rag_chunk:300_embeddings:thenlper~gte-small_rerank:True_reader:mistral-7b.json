[{"question": "What architecture is the `tokenizers-linux-x64-musl` binary designed for?\n", "true_answer": "x86_64-unknown-linux-musl", "source_doc": "huggingface/tokenizers/blob/main/bindings/node/npm/linux-x64-musl/README.md", "generated_answer": "The `tokenizers-linux-x64-musl` binary is designed for the **x86_64-unknown-linux-musl** architecture. (Answer according to context provided.)", "retrieved_docs": ["`tokenizers-linux-x64-musl`\n\nThis is the **x86_64-unknown-linux-musl** binary for `tokenizers`", "`tokenizers-linux-arm64-musl`\n\nThis is the **aarch64-unknown-linux-musl** binary for `tokenizers`", "`tokenizers-linux-x64-gnu`\n\nThis is the **x86_64-unknown-linux-gnu** binary for `tokenizers`", "`tokenizers-linux-arm64-gnu`\n\nThis is the **aarch64-unknown-linux-gnu** binary for `tokenizers`", "`tokenizers-android-arm64`\n\nThis is the **aarch64-linux-android** binary for `tokenizers`", "`tokenizers-freebsd-x64`\n\nThis is the **x86_64-unknown-freebsd** binary for `tokenizers`", "`tokenizers-win32-x64-msvc`\n\nThis is the **x86_64-pc-windows-msvc** binary for `tokenizers`"], "eval_score_GPT4": "5", "eval_feedback_GPT4": "Feedback: The response provided is completely correct, accurate, and factual. It matches the reference answer exactly, indicating that the `tokenizers-linux-x64-musl` binary is designed for the x86_64-unknown-linux-musl architecture."}, {"question": "What is the purpose of the BLIP-Diffusion model?\n", "true_answer": "The BLIP-Diffusion model is designed for controllable text-to-image generation and editing.", "source_doc": "huggingface/diffusers/blob/main/docs/source/en/api/pipelines/blip_diffusion.md", "generated_answer": "The BLIP-Diffusion model, proposed in the paper \"BLIP-Diffusion: Pre-trained Subject Representation for Controllable Text-to-Image Generation and Editing,\" enables zero-shot subject-driven generation and control-guided zero-shot generation. Its purpose is to allow for the generation of images based on a given subject or control, without the need for additional training data for that specific subject or control. This is achieved through the use of the BLIP (", "retrieved_docs": ["BLIP is a model that is able to perform various multi-modal tasks including:\n- Visual Question Answering \n- Image-Text retrieval (Image-text matching)\n- Image Captioning\n\nThe abstract from the paper is the following:", "!--Copyright 2023 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\n\nhttp://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License.\n-->\n\n# BLIP-Diffusion\n\nBLIP-Diffusion was proposed in [BLIP-Diffusion: Pre-trained Subject Representation for Controllable Text-to-Image Generation and Editing](https://arxiv.org/abs/2305.14720). It enables zero-shot subject-driven generation and control-guided zero-shot generation. \n\n\nThe abstract from the paper is:", "Stable Diffusion models can be converted to different Core ML variants intended for different purposes:\n\n- The type of attention blocks used. The attention operation is used to \"pay attention\" to the relationship between different areas in the image representations and to understand how the image and text representations are related. Attention is compute- and memory-intensive, so different implementations exist that consider the hardware characteristics of different devices. For Core ML Stable Diffusion models, there are two attention variants:\n    * `split_einsum` ([introduced by Apple](https://machinelearning.apple.com/research/neural-engine-transformers)) is optimized for ANE devices, which is available in modern iPhones, iPads and M-series computers.\n    * The \"original\" attention (the base implementation used in `diffusers`) is only compatible with CPU/GPU and not ANE. It can be *faster* to run your model on CPU + GPU using `original` attention than ANE. See [this performance benchmark](https://huggingface.co/blog/fast-mac-diffusers#performance-benchmarks) as well as some [additional measures provided by the community](https://github.com/huggingface/swift-coreml-diffusers/issues/31) for additional details.", "--\ntitle: \"VQ-Diffusion\" \nthumbnail: /blog/assets/117_vq_diffusion/thumbnail.png\nauthors:\n- user: williamberman\n---\n\n# VQ-Diffusion\n\n\nVector Quantized Diffusion (VQ-Diffusion) is a conditional latent diffusion model developed by the University of Science and Technology of China and Microsoft. Unlike most commonly studied diffusion models, VQ-Diffusion's noising and denoising processes operate on a quantized latent space, i.e., the latent space is composed of a discrete set of vectors. Discrete diffusion models are less explored than their continuous counterparts and offer an interesting point of comparison with autoregressive (AR) models.\n\n- [Hugging Face model card](https://huggingface.co/microsoft/vq-diffusion-ithq)\n- [Hugging Face Spaces](https://huggingface.co/spaces/patrickvonplaten/vq-vs-stable-diffusion)\n- [Original Implementation](https://github.com/microsoft/VQ-Diffusion)\n- [Paper](https://arxiv.org/abs/2111.14822)\n\n### Demo\n\n\ud83e\udde8 Diffusers lets you run VQ-Diffusion with just a few lines of code.", "\ud83e\udde8 Diffusers Pipelines\n\nPipelines provide a simple way to run state-of-the-art diffusion models in inference.\nMost diffusion systems consist of multiple independently-trained models and highly adaptable scheduler\ncomponents - all of which are needed to have a functioning end-to-end diffusion system.", "# Diffusers\n\n\ud83e\udd17 Diffusers is the go-to library for state-of-the-art pretrained diffusion models for generating images, audio, and even 3D structures of molecules. Whether you're looking for a simple inference solution or want to train your own diffusion model, \ud83e\udd17 Diffusers is a modular toolbox that supports both. Our library is designed with a focus on [usability over performance](conceptual/philosophy#usability-over-performance), [simple over easy](conceptual/philosophy#simple-over-easy), and [customizability over abstractions](conceptual/philosophy#tweakable-contributorfriendly-over-abstraction).\n\nThe library has three main components:\n\n- State-of-the-art diffusion pipelines for inference with just a few lines of code. There are many pipelines in \ud83e\udd17 Diffusers, check out the table in the pipeline [overview](api/pipelines/overview) for a complete list of available pipelines and the task they solve.\n- Interchangeable [noise schedulers](api/schedulers/overview) for balancing trade-offs between generation speed and quality.\n- Pretrained [models](api/models) that can be used as building blocks, and combined with schedulers, for creating your own end-to-end diffusion systems.", "DDPO is a promising technique to align diffusion models with any reward function and we hope that with the release in TRL we can make it more accessible to the community!\n\n## Acknowledgements\n\nThanks to Chunte Lee for the thumbnail of this blog post."], "eval_score_GPT4": "4", "eval_feedback_GPT4": "Feedback: The response provided is mostly correct and factual. It correctly identifies the BLIP-Diffusion model as being used for the generation of images based on a given subject or control, aligning with the reference answer's mention of controllable text-to-image generation and editing. However, the response could be more concise and directly aligned with the reference answer by explicitly stating that the model is designed for \"controllable text-to-image generation and editing.\" The inclusion of \"zero-shot subject-driven generation\" and \"control-guided zero-shot generation\" adds relevant details about the model's capabilities without straying from accuracy. The response does not fully complete the explanation of how the BLIP-Diffusion model achieves its purpose, as it ends abruptly with \"the use of the BLIP (\", which suggests an incomplete thought. Despite this, the core purpose of the model is conveyed, which aligns with the reference answer."}, {"question": "How can a user claim authorship of a paper on the Hugging Face Hub?\n", "true_answer": "By clicking their name on the corresponding Paper page and clicking \"claim authorship\", then confirming the request in paper settings for admin team validation.", "source_doc": "huggingface/hub-docs/blob/main/docs/hub/paper-pages.md", "generated_answer": "To claim authorship of a paper on the Hugging Face Hub, a user should follow these steps:\n1. Visit the Paper page related to the paper in question.\n2. Click on their name in the corresponding Paper page.\n3. Click on \"claim authorship\" to automatically redirect to the paper settings page.\n4. Confirm the request on the paper settings page.\n5. The admin team will validate the request, and once confirmed, the Paper page will show", "retrieved_docs": ["## Claiming authorship to a Paper\n\nThe Hub will attempt to automatically match paper to users based on their email. \n\n<div class=\"flex justify-center\">\n<img class=\"block dark:hidden\" width=\"300\" src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/hub/papers-authors.png\"/>\n<img class=\"hidden dark:block\" width=\"300\" src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/hub/papers-authors-dark.png\"/>\n</div>\n\nIf your paper is not linked to your account, you can click in your name in the corresponding Paper page and click \"claim authorship\". This will automatically re-direct to your paper settings where you can confirm the request. The admin team will validate your request soon. Once confirmed, the Paper page will show as verified.", "* Visit the Paper page.\n* Filter for other models or datasets on the Hub that cite the same paper.\n\n<div class=\"flex justify-center\">\n<img class=\"block dark:hidden\" width=\"300\" src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/hub/datasets-arxiv.png\"/>\n<img class=\"hidden dark:block\" width=\"300\" src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/hub/datasets-arxiv-dark.png\"/>\n</div>\n\n## Claiming authorship to a Paper\n\nThe Hub will attempt to automatically match paper to users based on their email.", "## Authentication\n\nIn a lot of cases, you must be authenticated with a Hugging Face account to interact with\nthe Hub: download private repos, upload files, create PRs,...\n[Create an account](https://huggingface.co/join) if you don't already have one, and then sign in\nto get your [User Access Token](https://huggingface.co/docs/hub/security-tokens) from\nyour [Settings page](https://huggingface.co/settings/tokens). The User Access Token is\nused to authenticate your identity to the Hub.\n\n<Tip>\n\nTokens can have `read` or `write` permissions. Make sure to have a `write` access token if you want to create or edit a repository. Otherwise, it's best to generate a `read` token to reduce risk in case your token is inadvertently leaked.\n\n</Tip>\n\n### Login command\n\nThe easiest way to authenticate is to save the token on your machine. You can do that from the terminal using the [`login`] command:\n\n```bash\nhuggingface-cli login\n```", "Paper Pages\n\nPaper pages allow people to find artifacts related to a paper such as models, datasets and apps/demos (Spaces). Paper pages also enable the community to discuss about the paper.\n\n<div class=\"flex justify-center\">\n<img class=\"block dark:hidden\" width=\"300\" src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/hub/papers-discussions.png\"/>\n<img class=\"hidden dark:block\" width=\"300\" src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/hub/papers-discussions-dark.png\"/>\n</div>\n\n## Linking a Paper to a model, dataset or Space\n\nIf the repository card (`README.md`) includes a link to a paper on arXiv, the Hugging Face Hub will extract the arXiv ID and include it in the repository's tags. Clicking on the arxiv tag will let you:\n\n* Visit the Paper page.\n* Filter for other models or datasets on the Hub that cite the same paper.", "--\ntitle: \"Jupyter X Hugging Face\" \nthumbnail: /blog/assets/135_notebooks-hub/before_after_notebook_rendering.png\nauthors:\n- user: davanstrien\n- user: reach-vb \n- user: merve\n---\n\n# Jupyter X Hugging Face \n\n\n**We\u2019re excited to announce improved support for Jupyter notebooks hosted on the Hugging Face Hub!**\n\nFrom serving as an essential learning resource to being a key tool used for model development, Jupyter notebooks have become a key component across many areas of machine learning. Notebooks' interactive and visual nature lets you get feedback quickly as you develop models, datasets, and demos. For many, their first exposure to training machine learning models is via a Jupyter notebook, and many practitioners use notebooks as a critical tool for developing and communicating their work.", "Licenses\n\nYou are able to add a license to any repo that you create on the Hugging Face Hub to let other users know about the permissions that you want to attribute to your code or data. The license can be specified in your repository's `README.md` file, known as a *card* on the Hub, in the card's metadata section. Remember to seek out and respect a project's license if you're considering using their code or data.\n\nA full list of the available licenses is available here:\n\n<!-- IMPORTANT: do not remove or alter the \"region\" comments below -->", "--\ntitle: \"Ethics and Society Newsletter #1\" \nthumbnail: /blog/assets/103_ethics-soc-1/thumbnail.png\nauthors:\n- user: meg\n---\n\n# Ethics and Society Newsletter #1\n\n\n\nHello, world!\n\nOriginating as an open-source company, Hugging Face was founded on some key ethical values in tech: _collaboration_, _responsibility_, and _transparency_. To code in an open environment means having your code \u2013 and the choices within \u2013 viewable to the world, associated with your account and available for others to critique and add to.  As the research community began using the Hugging Face Hub to host models and data, the community directly integrated _reproducibility_ as another fundamental value of the company. And as the number of datasets and models on Hugging Face grew, those working at Hugging Face implemented [documentation requirements](https://huggingface.co/docs/hub/models-cards) and [free instructive courses](https://huggingface.co/course/chapter1/1), meeting the newly emerging values defined by the research community with complementary values around _auditability_ and _understanding_ the math, code, processes and people that lead to current technology."], "eval_score_GPT4": "4", "eval_feedback_GPT4": "Feedback: The response provided is mostly correct and factual. It outlines a clear sequence of steps that a user should follow to claim authorship of a paper on the Hugging Face Hub. The steps are accurate and reflect the process as described in the reference answer. However, the response includes an additional step about visiting the Paper page, which is implied but not explicitly stated in the reference answer. This does not detract from the factual accuracy of the response but adds a minor detail not present in the reference. Therefore, the response aligns closely with the reference answer but includes a slight elaboration."}, {"question": "What is the purpose of the /healthcheck endpoint in the Datasets server API?\n", "true_answer": "Ensure the app is running", "source_doc": "huggingface/datasets-server/blob/main/services/api/README.md", "generated_answer": "The purpose of the /healthcheck endpoint in the Datasets server API is to ensure that the app is running. This endpoint is used to check the availability and responsiveness of the Datasets server. When the endpoint is accessed, the server returns a response indicating that it is operational, which can be used to confirm that the server is functioning correctly. This endpoint is commonly used in monitoring and management tools to verify the health of the server and alert administrators if any issues arise. (Source: Ex", "retrieved_docs": ["Get dataset information\n\nDatasets Server provides an `/info` endpoint for exploring the general information about dataset, including such fields as description, citation, homepage, license and features.\n\nThe `/info` endpoint accepts two query parameters:\n\n- `dataset`: the dataset name\n- `config`: the configuration name", "## Endpoints\n\nSee https://huggingface.co/docs/datasets-server\n\n- /healthcheck: Ensure the app is running\n- /metrics: Return a list of metrics in the Prometheus format\n- /hub-cache: Return a dataset information as a Server-Sent Event (SSE) when a dataset is updated. If `?all=true` is passed in the parameters, and if the cache already has some entries, one SSE per cached dataset is sent to the client. Then, a SSE is sent when a dataset is inserted, modified or deleted. The event data is a JSON with the following structure. The `hub_cache` field is null for deleted entries, or when the response is an error. The `num_rows` value is `0` if it could not be determined.\n\n```typescript\n{\n    dataset: string;\n    hub_cache: null | {\n        preview: boolean;\n        viewer: boolean;\n        partial: boolean;\n        num_rows: int;\n    };\n}\n```", "Quickstart\n\n[[open-in-colab]]\n\nIn this quickstart, you'll learn how to use the Datasets Server's REST API to:\n\n- Check whether a dataset on the Hub is functional.\n- Return the configuration and splits of a dataset.\n- Preview the first 100 rows of a dataset.\n- Download slices of rows of a dataset.\n- Search a word in a dataset.\n- Access the dataset as parquet files.\n\n## API endpoints\n\nEach feature is served through an endpoint summarized in the table below:", "Datasets server SSE API\n\n> Server-sent events API for the Datasets server. It's used to update the Hub's backend cache.\n\n## Configuration\n\nThe service can be configured using environment variables. They are grouped by scope.\n\n### Common\n\nSee [../../libs/libcommon/README.md](../../libs/libcommon/README.md) for more information about the common configuration.\n\n## Endpoints\n\nSee https://huggingface.co/docs/datasets-server", "Datasets server - worker\n\n> Workers that pre-compute and cache the response to /splits, /first-rows, /parquet, /info and /size.\n\n## Configuration\n\nUse environment variables to configure the workers. The prefix of each environment variable gives its scope.\n\n### Uvicorn\n\nThe following environment variables are used to configure the Uvicorn server (`WORKER_UVICORN_` prefix). It is used for the /healthcheck and the /metrics endpoints:\n\n- `WORKER_UVICORN_HOSTNAME`: the hostname. Defaults to `\"localhost\"`.\n- `WORKER_UVICORN_NUM_WORKERS`: the number of uvicorn workers. Defaults to `2`.\n- `WORKER_UVICORN_PORT`: the port. Defaults to `8000`.\n\n### Prometheus", "Server infrastructure\n\nThe [Datasets Server](https://github.com/huggingface/datasets-server) has two main components that work together to return queries about a dataset instantly:\n\n- a user-facing web API for exploring and returning information about a dataset\n- a server runs the queries ahead of time and caches them in a database\n\nWhile most of the documentation is focused on the web API, the server is crucial because it performs all the time-consuming preprocessing and stores the results so the web API can retrieve and serve them to the user. This saves a user time because instead of generating the response every time it gets requested, Datasets Server can return the preprocessed results instantly from the cache.\n\nThere are three elements that keep the server running: the job queue, workers, and the cache.\n\n## Job queue\n\nThe job queue is a list of jobs stored in a Mongo database that should be completed by the workers. The jobs are practically identical to the endpoints the user uses; only the server runs the jobs ahead of time, and the user gets the results when they use the endpoint.\n\nThere are three jobs:", "Datasets server API - rows endpoint\n\n> /rows endpoint\n\n## Configuration\n\nThe service can be configured using environment variables. They are grouped by scope.\n\n### API service\n\nSee [../../libs/libapi/README.md](../../libs/libapi/README.md) for more information about the API configuration.\n\n### Common\n\nSee [../../libs/libcommon/README.md](../../libs/libcommon/README.md) for more information about the common configuration.\n\n## Endpoints\n\nSee https://huggingface.co/docs/datasets-server\n\n- /healthcheck: ensure the app is running\n- /metrics: return a list of metrics in the Prometheus format\n- /rows: get a slice of rows of a dataset split"], "eval_score_GPT4": "5", "eval_feedback_GPT4": "The response provided aligns well with the reference answer, stating that the purpose of the /healthcheck endpoint is to ensure that the app is running. It expands on the reference answer by explaining that the endpoint checks the availability and responsiveness of the Datasets server and that it is used in monitoring and management tools. The response is correct, accurate, and factual, providing a clear and detailed explanation of the purpose of the /healthcheck endpoint."}, {"question": "What is the default context window size for Local Attention in the LongT5 model?\n", "true_answer": "127 tokens", "source_doc": "huggingface/transformers/blob/main/docs/source/en/model_doc/longt5.md", "generated_answer": "The default context window size for Local Attention in the LongT5 model is `r=127` tokens to the left and right of a given token, as mentioned in the context provided. This is specified by the parameter `r` in the sparse sliding-window local attention operation used in Longformer self attention. However, the user can define a different window size for each layer by providing a list `config.attention_window` with a different value for each layer.", "retrieved_docs": ["## Longformer Self Attention\n\nLongformer self attention employs self attention on both a \"local\" context and a \"global\" context. Most tokens only\nattend \"locally\" to each other meaning that each token attends to its \\\\(\\frac{1}{2} w\\\\) previous tokens and\n\\\\(\\frac{1}{2} w\\\\) succeeding tokens with \\\\(w\\\\) being the window length as defined in\n`config.attention_window`. Note that `config.attention_window` can be of type `List` to define a\ndifferent \\\\(w\\\\) for each layer. A selected few tokens attend \"globally\" to all other tokens, as it is\nconventionally done for all tokens in `BertSelfAttention`.\n\nNote that \"locally\" and \"globally\" attending tokens are projected by different query, key and value matrices. Also note\nthat every \"locally\" attending token not only attends to tokens within its window \\\\(w\\\\), but also to all \"globally\"\nattending tokens so that global attention is *symmetric*.", "- [`LongT5ForConditionalGeneration`] is an extension of [`T5ForConditionalGeneration`] exchanging the traditional\nencoder *self-attention* layer with efficient either *local* attention or *transient-global* (*tglobal*) attention.\n- Unlike the T5 model, LongT5 does not use a task prefix. Furthermore, it uses a different pre-training objective\ninspired by the pre-training of [`PegasusForConditionalGeneration`].\n- LongT5 model is designed to work efficiently and very well on long-range *sequence-to-sequence* tasks where the\ninput sequence exceeds commonly used 512 tokens. It is capable of handling input sequences of a length up to 16,384 tokens.\n- For *Local Attention*, the sparse sliding-window local attention operation allows a given token to attend only `r`\ntokens to the left and right of it (with `r=127` by default). *Local Attention* does not introduce any new parameters\nto the model. The complexity of the mechanism is linear in input sequence length `l`: `O(l*r)`.", "The user can define which tokens attend \"locally\" and which tokens attend \"globally\" by setting the tensor\n`global_attention_mask` at run-time appropriately. All Longformer models employ the following logic for\n`global_attention_mask`:\n\n- 0: the token attends \"locally\",\n- 1: the token attends \"globally\".\n\nFor more information please also refer to [`~LongformerModel.forward`] method.\n\nUsing Longformer self attention, the memory and time complexity of the query-key matmul operation, which usually\nrepresents the memory and time bottleneck, can be reduced from \\\\(\\mathcal{O}(n_s \\times n_s)\\\\) to\n\\\\(\\mathcal{O}(n_s \\times w)\\\\), with \\\\(n_s\\\\) being the sequence length and \\\\(w\\\\) being the average window\nsize. It is assumed that the number of \"globally\" attending tokens is insignificant as compared to the number of\n\"locally\" attending tokens.\n\nFor more information, please refer to the official [paper](https://arxiv.org/pdf/2004.05150.pdf).\n\n\n## Training", "```python\n>>> # considering `window_size = 3`, we will consider 1 token to left & 1 to right of 'available'\n>>> # left token: 'now' ; right token: 'in'\n>>> sliding_tokens = [\"now\", \"available\", \"in\"]\n\n>>> # let's update our collection with the above tokens\n>>> key_tokens.append(sliding_tokens)\n```\n\n**Long-range dependencies:** For some tasks, it is crucial to capture long-range relationships between tokens. *E.g.*, in `question-answering the model needs to compare each token of the context to the whole question to be able to figure out which part of the context is useful for a correct answer. If most of the context tokens would just attend to other context tokens, but not to the question, it becomes much harder for the model to filter important context tokens from less important context tokens.\n\nNow, `BigBird` proposes two ways of allowing long-term attention dependencies while staying computationally efficient.", "Iz Beltagy, Matthew E. Peters, Arman Cohan\n\nLongformer addresses the memory bottleneck of transformers by replacing conventional self-attention with a combination of windowed/local/sparse (cf. [Sparse Transformers (2019)](https://arxiv.org/abs/1904.10509)) attention and global attention that scales linearly with the sequence length. As opposed to previous long-range transformer models (e.g. [Transformer-XL (2019)](https://arxiv.org/abs/1901.02860), [Reformer (2020)](https://arxiv.org/abs/2001.04451), [Adaptive Attention Span (2019)](https://arxiv.org/abs/1905.07799)), Longformer\u2019s self-attention layer is designed as a drop-in replacement for the standard self-attention, thus making it possible to leverage pre-trained checkpoints for further pre-training and/or fine-tuning on long sequence tasks.\n\nThe standard self-attention matrix (Figure a) scales quadratically with the input length:", "#### Main findings\n\n* The authors proposed the dilated windowed self-attention (Figure c) and showed that it yields better results on language modeling compared to just windowed/sparse self-attention (Figure b). The window sizes are increased through the layers. This pattern further outperforms previous architectures (such as Transformer-XL, or adaptive span attention) on downstream benchmarks.\n* Global attention allows the information to flow through the whole sequence and applying the global attention to task-motivated tokens (such as the tokens of the question in QA, CLS token for sentence classification) leads to stronger performance on downstream tasks. Using this global pattern, Longformer can be successfully applied to document-level NLP tasks in the transfer learning setting.\n* Standard pre-trained models can be adapted to long-range inputs by simply replacing the standard self-attention with the long-range self-attention proposed in this paper and then fine-tuning on the downstream task. This avoids costly pre-training specific to long-range inputs.\n\n#### Follow-up questions", "This enhanced local self-attention is better than the vanilla local self-attention architecture but still has a major drawback in that every input vector can only attend to a local context of predefined size. For NLP tasks that do not require the transformer model to learn long-range dependencies between the input vectors, which include arguably *e.g.* speech recognition, named entity recognition and causal language modeling of short sentences, this might not be a big issue. Many NLP tasks do require the model to learn long-range dependencies, so that local self-attention could lead to significant performance degradation, *e.g.* \n* *Question-answering*: the model has to learn the relationship between the question tokens and relevant answer tokens which will most likely not be in the same local range\n* *Multiple-Choice*: the model has to compare multiple answer token segments to each other which are usually separated by a significant length\n* *Summarization*: the model has to learn the relationship between a long sequence of context tokens and a shorter sequence of summary tokens, whereas the relevant relationships between context and summary can most likely not be captured by local self-attention\n* etc...\n\nLocal self-attention on its own is most likely not sufficient for the transformer model to learn the relevant relationships of input vectors (tokens) to each other."], "eval_score_GPT4": "5", "eval_feedback_GPT4": "Feedback: The response correctly identifies the default context window size for Local Attention in the LongT5 model as `r=127` tokens, which matches the reference answer. The additional information provided about the parameter `r` and the user's ability to define different window sizes does not detract from the accuracy of the response regarding the default setting."}, {"question": "What method is used to load a checkpoint for a task using `AutoPipeline`?\n", "true_answer": "from_pretrained()", "source_doc": "huggingface/diffusers/blob/main/docs/source/en/api/pipelines/auto_pipeline.md", "generated_answer": "To load a checkpoint for a task using `AutoPipeline`, you can use the `from_pretrained` method and pass the name of the checkpoint as an argument. The specific pipeline class to use will be automatically detected based on the task. For example, to load the [runwayml/stable-diffusion-v1-5](https://huggingface.co/runwayml/stable-diffusion-v1-5) checkpoint for text-", "retrieved_docs": ["<Tip>\n\nTake a look at the [AutoPipeline](../api/pipelines/auto_pipeline) reference to see which tasks are supported. Currently, it supports text-to-image, image-to-image, and inpainting.\n\n</Tip>\n\nThis tutorial shows you how to use an `AutoPipeline` to automatically infer the pipeline class to load for a specific task, given the pretrained weights.\n\n## Choose an AutoPipeline for your task\n\nStart by picking a checkpoint. For example, if you're interested in text-to-image with the [runwayml/stable-diffusion-v1-5](https://huggingface.co/runwayml/stable-diffusion-v1-5) checkpoint, use [`AutoPipelineForText2Image`]:\n\n```py\nfrom diffusers import AutoPipelineForText2Image\nimport torch", "## Use multiple pipelines\n\nFor some workflows or if you're loading many pipelines, it is more memory-efficient to reuse the same components from a checkpoint instead of reloading them which would unnecessarily consume additional memory. For example, if you're using a checkpoint for text-to-image and you want to use it again for image-to-image, use the [`~AutoPipelineForImage2Image.from_pipe`] method. This method creates a new pipeline from the components of a previously loaded pipeline at no additional memory cost.\n\nThe [`~AutoPipelineForImage2Image.from_pipe`] method detects the original pipeline class and maps it to the new pipeline class corresponding to the task you want to do. For example, if you load a `\"stable-diffusion\"` class pipeline for text-to-image:\n\n```py\nfrom diffusers import AutoPipelineForText2Image, AutoPipelineForImage2Image\nimport torch", "# AutoPipeline\n\n\ud83e\udd17 Diffusers is able to complete many different tasks, and you can often reuse the same pretrained weights for multiple tasks such as text-to-image, image-to-image, and inpainting. If you're new to the library and diffusion models though, it may be difficult to know which pipeline to use for a task. For example, if you're using the [runwayml/stable-diffusion-v1-5](https://huggingface.co/runwayml/stable-diffusion-v1-5) checkpoint for text-to-image, you might not know that you could also use it for image-to-image and inpainting by loading the checkpoint with the [`StableDiffusionImg2ImgPipeline`] and [`StableDiffusionInpaintPipeline`] classes respectively.\n\nThe `AutoPipeline` class is designed to simplify the variety of pipelines in \ud83e\udd17 Diffusers. It is a generic, *task-first* pipeline that lets you focus on the task. The `AutoPipeline` automatically detects the correct pipeline class to use, which makes it easier to load a checkpoint for a task without knowing the specific pipeline class name.\n\n<Tip>", "</Tip>\n\nLoading a checkpoint from TheLastBen is very similar. For example, to load the [TheLastBen/William_Eggleston_Style_SDXL](https://huggingface.co/TheLastBen/William_Eggleston_Style_SDXL) checkpoint:\n\n```py\nfrom diffusers import AutoPipelineForText2Image\nimport torch\n\npipeline = AutoPipelineForText2Image.from_pretrained(\"stabilityai/stable-diffusion-xl-base-1.0\", torch_dtype=torch.float16).to(\"cuda\")\npipeline.load_lora_weights(\"TheLastBen/William_Eggleston_Style_SDXL\", weight_name=\"wegg.safetensors\")\n\n# use by william eggleston in the prompt to trigger the LoRA\nprompt = \"a house by william eggleston, sunrays, beautiful, sunlight, sunrays, beautiful\"\nimage = pipeline(prompt=prompt).images[0]\nimage\n```\n\n## IP-Adapter", "<div class=\"flex justify-center\">\n    <img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/diffusers/autopipeline-inpaint.png\" alt=\"generated image of a tiger sitting on a bench\"/>\n</div>\n\nIf you try to load an unsupported checkpoint, it'll throw an error:\n\n```py\nfrom diffusers import AutoPipelineForImage2Image\nimport torch\n\npipeline = AutoPipelineForImage2Image.from_pretrained(\n    \"openai/shap-e-img2img\", torch_dtype=torch.float16, use_safetensors=True\n)\n\"ValueError: AutoPipeline can't find a pipeline linked to ShapEImg2ImgPipeline for None\"\n```\n\n## Use multiple pipelines", "```python\nfrom diffusers import AutoPipelineForText2Image\nimport torch\n\npipe = AutoPipelineForText2Image.from_pretrained(output_dir, torch_dtype=torch.float16)\npipe.enable_model_cpu_offload()\n\nprompt='A robot pokemon, 4k photo'\nimages = pipe(prompt=prompt).images\nimages[0].save(\"robot-pokemon.png\")\n```\n\nCheckpoints only save the unet, so to run inference from a checkpoint, just load the unet\n```python\nfrom diffusers import AutoPipelineForText2Image, UNet2DConditionModel\n\nmodel_path = \"path_to_saved_model\"\n\nunet = UNet2DConditionModel.from_pretrained(model_path + \"/checkpoint-<N>/unet\")\n\npipe = AutoPipelineForText2Image.from_pretrained(\"kandinsky-community/kandinsky-2-2-decoder\", unet=unet, torch_dtype=torch.float16)\npipe.enable_model_cpu_offload()", "You can also use the [`~StableDiffusionXLPipeline.from_single_file`] method to load a model checkpoint stored in a single file format (`.ckpt` or `.safetensors`) from the Hub or locally:\n\n```py\nfrom diffusers import StableDiffusionXLPipeline, StableDiffusionXLImg2ImgPipeline\nimport torch\n\npipeline = StableDiffusionXLPipeline.from_single_file(\n    \"https://huggingface.co/stabilityai/stable-diffusion-xl-base-1.0/blob/main/sd_xl_base_1.0.safetensors\", torch_dtype=torch.float16, variant=\"fp16\", use_safetensors=True\n).to(\"cuda\")"], "eval_score_GPT4": "4", "eval_feedback_GPT4": "The response correctly identifies the `from_pretrained` method as the way to load a checkpoint using `AutoPipeline`. It also accurately describes that the specific pipeline class will be automatically detected based on the task, which aligns with the reference answer. However, the response is incomplete as it trails off after \"for text-\" without finishing the sentence or providing a complete example. Despite this, the key information required to answer the question is present and correct."}, {"question": "What is the purpose of Diffusers library?\n", "true_answer": "To serve as a modular toolbox for both inference and training of state-of-the-art pretrained diffusion models across multiple modalities.", "source_doc": "huggingface/diffusers/blob/main/docs/source/en/conceptual/philosophy.md", "generated_answer": "The Diffusers library is designed to be a modular toolbox for state-of-the-art pretrained diffusion models for generating images, audio, and even 3D structures of molecules. Its purpose is to provide a simple and customizable solution for both inference and training of diffusion models, with a focus on usability over performance, simple over easy, and customizability over abstractions. The library offers interchangeable noise schedulers for balancing trade-offs between", "retrieved_docs": ["- **Simplicity**: with a desire to make it easy to use and exploit the Diffusers library, we are committed to keeping the project\u2019s goals lean and coherent.\n\n- **Accessibility**: the Diffusers project helps lower the entry bar for contributors who can help run it even without technical expertise. Doing so makes research artifacts more accessible to the community.\n\n- **Reproducibility**: we aim to be transparent about the reproducibility of upstream code, models, and datasets when made available through the Diffusers library.\n\n- **Responsibility**: as a community and through teamwork, we hold a collective responsibility to our users by anticipating and mitigating this technology's potential risks and dangers.\n\n\n## Examples of implementations: Safety features and Mechanisms\n\nThe team works daily to make the technical and non-technical tools available to deal with the potential ethical and social risks associated with diffusion technology. Moreover, the community's input is invaluable in ensuring these features' implementation and raising awareness with us.\n\n- [**Community tab**](https://huggingface.co/docs/hub/repositories-pull-requests-discussions): it enables the community to discuss and better collaborate on a project.", "Training examples of the Diffusers library should adhere to the following philosophy:\n- All the code necessary to run the examples should be found in a single Python file.\n- One should be able to run the example from the command line with `python <your-example>.py --args`.\n- Examples should be kept simple and serve as **an example** on how to use Diffusers for training. The purpose of example scripts is **not** to create state-of-the-art diffusion models, but rather to reproduce known training schemes without adding too much custom logic. As a byproduct of this point, our examples also strive to serve as good educational materials.", "Training examples of the Diffusers library should adhere to the following philosophy:\n- All the code necessary to run the examples should be found in a single Python file.\n- One should be able to run the example from the command line with `python <your-example>.py --args`.\n- Examples should be kept simple and serve as **an example** on how to use Diffusers for training. The purpose of example scripts is **not** to create state-of-the-art diffusion models, but rather to reproduce known training schemes without adding too much custom logic. As a byproduct of this point, our examples also strive to serve as good educational materials.", "# Diffusers\n\n\ud83e\udd17 Diffusers is the go-to library for state-of-the-art pretrained diffusion models for generating images, audio, and even 3D structures of molecules. Whether you're looking for a simple inference solution or want to train your own diffusion model, \ud83e\udd17 Diffusers is a modular toolbox that supports both. Our library is designed with a focus on [usability over performance](conceptual/philosophy#usability-over-performance), [simple over easy](conceptual/philosophy#simple-over-easy), and [customizability over abstractions](conceptual/philosophy#tweakable-contributorfriendly-over-abstraction).\n\nThe library has three main components:\n\n- State-of-the-art diffusion pipelines for inference with just a few lines of code. There are many pipelines in \ud83e\udd17 Diffusers, check out the table in the pipeline [overview](api/pipelines/overview) for a complete list of available pipelines and the task they solve.\n- Interchangeable [noise schedulers](api/schedulers/overview) for balancing trade-offs between generation speed and quality.\n- Pretrained [models](api/models) that can be used as building blocks, and combined with schedulers, for creating your own end-to-end diffusion systems.", "\ud83e\udd17 Diffusers is the go-to library for state-of-the-art pretrained diffusion models for generating images, audio, and even 3D structures of molecules. Whether you're looking for a simple inference solution or training your own diffusion models, \ud83e\udd17 Diffusers is a modular toolbox that supports both. Our library is designed with a focus on [usability over performance](https://huggingface.co/docs/diffusers/conceptual/philosophy#usability-over-performance), [simple over easy](https://huggingface.co/docs/diffusers/conceptual/philosophy#simple-over-easy), and [customizability over abstractions](https://huggingface.co/docs/diffusers/conceptual/philosophy#tweakable-contributorfriendly-over-abstraction).\n\n\ud83e\udd17 Diffusers offers three core components:", "Using \ud83e\udde8 `diffusers` at Hugging Face\n\nDiffusers is the go-to library for state-of-the-art pretrained diffusion models for generating images, audio, and even 3D structures of molecules. Whether you\u2019re looking for a simple inference solution or want to train your own diffusion model, Diffusers is a modular toolbox that supports both. The library is designed with a focus on usability over performance, simple over easy, and customizability over abstractions.\n\n## Exploring Diffusers in the Hub\n\nThere are over 10,000 `diffusers` compatible pipelines on the Hub which you can find by filtering at the left of [the models page](https://huggingface.co/models?library=diffusers&sort=downloads). Diffusion systems are typically composed of multiple components such as text encoder, UNet, VAE, and scheduler. Even though they are not standalone models, the pipeline abstraction makes it easy to use them for inference or training.\n\nYou can find diffusion pipelines for many different tasks:", "For large parts of the library, Diffusers adopts an important design principle of the [Transformers library](https://github.com/huggingface/transformers), which is to prefer copy-pasted code over hasty abstractions. This design principle is very opinionated and stands in stark contrast to popular design principles such as [Don't repeat yourself (DRY)](https://en.wikipedia.org/wiki/Don%27t_repeat_yourself).\nIn short, just like Transformers does for modeling files, Diffusers prefers to keep an extremely low level of abstraction and very self-contained code for pipelines and schedulers.\nFunctions, long code blocks, and even classes can be copied across multiple files which at first can look like a bad, sloppy design choice that makes the library unmaintainable.\n**However**, this design has proven to be extremely successful for Transformers and makes a lot of sense for community-driven, open-source machine learning libraries because:\n- Machine Learning is an extremely fast-moving field in which paradigms, model architectures, and algorithms are changing rapidly, which therefore makes it very difficult to define long-lasting code abstractions."], "eval_score_GPT4": "4", "eval_feedback_GPT4": "Feedback: The response provided is mostly correct, accurate, and factual. It correctly identifies the Diffusers library as a toolbox for diffusion models and mentions its use in generating images, audio, and 3D structures, which aligns with the reference answer's mention of multiple modalities. The response also accurately describes the library's focus on usability, simplicity, and customizability, which are implied in the reference answer's emphasis on a modular toolbox. However, the response does not explicitly mention that the library is for both inference and training, as stated in the reference answer, and it ends abruptly without completing the sentence about noise schedulers. Therefore, the response is not completely correct as it omits some details and contains a minor inaccuracy due to the incomplete sentence."}, {"question": "What method does the EulerAncestralDiscreteScheduler use for sampling?\n", "true_answer": "Ancestral sampling with Euler method steps.", "source_doc": "huggingface/diffusers/blob/main/docs/source/en/api/schedulers/euler_ancestral.md", "generated_answer": "The EulerAncestralDiscreteScheduler uses ancestral sampling with Euler method steps, as stated in the context provided. This is mentioned in the comments of the code snippets provided, as well as in the documentation for the scheduler itself. The original implementation of this scheduler is also attributed to Katherine Crowson in the comments.", "retrieved_docs": ["!--Copyright 2023 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\n\nhttp://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License.\n-->\n\n# EulerAncestralDiscreteScheduler\n\nA scheduler that uses ancestral sampling with Euler method steps. This is a fast scheduler which can often generate good outputs in 20-30 steps. The scheduler is based on the original [k-diffusion](https://github.com/crowsonkb/k-diffusion/blob/481677d114f6ea445aa009cf5bd7a9cdee909e47/k_diffusion/sampling.py#L72) implementation by [Katherine Crowson](https://github.com/crowsonkb/).", "# EulerDiscreteScheduler\n\nThe Euler scheduler (Algorithm 2) is from the [Elucidating the Design Space of Diffusion-Based Generative Models](https://huggingface.co/papers/2206.00364) paper by Karras et al. This is a fast scheduler which can often generate good outputs in 20-30 steps. The scheduler is based on the original [k-diffusion](https://github.com/crowsonkb/k-diffusion/blob/481677d114f6ea445aa009cf5bd7a9cdee909e47/k_diffusion/sampling.py#L51) implementation by [Katherine Crowson](https://github.com/crowsonkb/).\n\n\n## EulerDiscreteScheduler\n[[autodoc]] EulerDiscreteScheduler\n\n## EulerDiscreteSchedulerOutput\n[[autodoc]] schedulers.scheduling_euler_discrete.EulerDiscreteSchedulerOutput", "[`EulerDiscreteScheduler`] and [`EulerAncestralDiscreteScheduler`] can generate high quality results with as little as 30 steps.\n\n```python\nfrom diffusers import EulerDiscreteScheduler\n\npipeline.scheduler = EulerDiscreteScheduler.from_config(pipeline.scheduler.config)\n\ngenerator = torch.Generator(device=\"cuda\").manual_seed(8)\nimage = pipeline(prompt, generator=generator, num_inference_steps=30).images[0]\nimage\n```\n\n<p align=\"center\">\n    <br>\n    <img src=\"https://huggingface.co/datasets/patrickvonplaten/images/resolve/main/diffusers_docs/astronaut_euler_discrete.png\" width=\"400\"/>\n    <br>\n</p>\n\n\nand:\n\n```python\nfrom diffusers import EulerAncestralDiscreteScheduler\n\npipeline.scheduler = EulerAncestralDiscreteScheduler.from_config(pipeline.scheduler.config)", "```py\nfrom diffusers import EulerDiscreteScheduler\n\npipe.unload_lora_weights()\npipe.scheduler = EulerDiscreteScheduler.from_config(pipe.scheduler.config)\n```\n\nThen we can run inference as usual for SDXL. We\u2019ll gather results using varying number of steps:\n\n```py\nimages = []\nfor steps in (1, 4, 8, 15, 20, 25, 30, 50):\n    generator = torch.Generator(device=pipe.device).manual_seed(1337)\n    image = pipe(\n        prompt=prompt,\n        num_inference_steps=steps,\n        generator=generator,\n    ).images[0]\n    images.append(image)\n```", "## EulerAncestralDiscreteScheduler\n[[autodoc]] EulerAncestralDiscreteScheduler\n\n## EulerAncestralDiscreteSchedulerOutput\n[[autodoc]] schedulers.scheduling_euler_ancestral_discrete.EulerAncestralDiscreteSchedulerOutput", "# LMSDiscreteScheduler\n\n`LMSDiscreteScheduler` is a linear multistep scheduler for discrete beta schedules. The scheduler is ported from and created by [Katherine Crowson](https://github.com/crowsonkb/), and the original implementation can be found at [crowsonkb/k-diffusion](https://github.com/crowsonkb/k-diffusion/blob/481677d114f6ea445aa009cf5bd7a9cdee909e47/k_diffusion/sampling.py#L181).\n\n## LMSDiscreteScheduler\n[[autodoc]] LMSDiscreteScheduler\n\n## LMSDiscreteSchedulerOutput\n[[autodoc]] schedulers.scheduling_lms_discrete.LMSDiscreteSchedulerOutput", "| DPM2                | [`KDPM2DiscreteScheduler`]          |                                                                                                               |\n| DPM2 Karras         | [`KDPM2DiscreteScheduler`]          | init with `use_karras_sigmas=True`                                                                            |\n| DPM2 a              | [`KDPM2AncestralDiscreteScheduler`] |                                                                                                               |\n| DPM2 a Karras       | [`KDPM2AncestralDiscreteScheduler`] | init with `use_karras_sigmas=True`                                                                            |\n| DPM adaptive        | N/A                                 |                                                                                                               |\n| DPM fast            | N/A                                 |                                                                                                               |\n| Euler               | [`EulerDiscreteScheduler`]          |                                                                                                               |\n| Euler a             | [`EulerAncestralDiscreteScheduler`] |                                                                                                               |\n| Heun                | [`HeunDiscreteScheduler`]           |                                                                                                               |\n| LMS                 | [`LMSDiscreteScheduler`]            |                                                                                                               |\n| LMS Karras          | [`LMSDiscreteScheduler`]            | init with `use_karras_sigmas=True`                                                                            |"], "eval_score_GPT4": "5", "eval_feedback_GPT4": "The response correctly identifies that the EulerAncestralDiscreteScheduler uses ancestral sampling with Euler method steps. It provides additional context by mentioning the source of the information, such as code comments and documentation, and credits the original implementation to Katherine Crowson. This additional information supports the accuracy and factual nature of the response, aligning it with the reference answer."}, {"question": "What is the name of the large multimodal model that can solve image-text tasks and is based on Flamingo?\n", "true_answer": "IDEFICS", "source_doc": "huggingface/transformers/blob/main/docs/source/en/tasks/idefics.md", "generated_answer": "The name of the large multimodal model that can solve image-text tasks and is based on Flamingo is IDEFICS (I**mage-aware **D**ecoder **E**nhanced \u00e0 la **F**lamingo with **I**nterleaved **C**ross-attention**S**). It is an open-access reproduction of Flamingo, initially developed by DeepMind, and accepts arbitrary sequences of image and text inputs", "retrieved_docs": ["<p align=\"center\">\n    <img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/idefics/woodstock_ai.png\" width=\"600\" alt=\"Screenshot of IDEFICS generation for HF Woodstock of AI\"/>\n</p>\n\n## What is IDEFICS?\n\nIDEFICS is an 80 billion parameters multimodal model that accepts sequences of images and texts as input and generates coherent text as output. It can answer questions about images, describe visual content, create stories grounded in multiple images, etc.\n\nIDEFICS is an open-access reproduction of Flamingo and is comparable in performance with the original closed-source model across various image-text understanding benchmarks. It comes in two variants - 80 billion parameters and 9 billion parameters.\n\n<p align=\"center\">\n    <img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/idefics/Figure_Evals_IDEFICS.png\" width=\"600\" alt=\"Plot comparing the performance of Flamingo, OpenFlamingo and IDEFICS\"/>\n</p>", "[IDEFICS](../model_doc/idefics) is an open-access vision and language model based on [Flamingo](https://huggingface.co/papers/2204.14198), \na state-of-the-art visual language model initially developed by DeepMind. The model accepts arbitrary sequences of image \nand text inputs and generates coherent text as output. It can answer questions about images, describe visual content, \ncreate stories grounded in multiple images, and so on. IDEFICS comes in two variants - [80 billion parameters](https://huggingface.co/HuggingFaceM4/idefics-80b) \nand [9 billion parameters](https://huggingface.co/HuggingFaceM4/idefics-9b), both of which are available on the \ud83e\udd17 Hub. For each variant, you can also find fine-tuned instructed \nversions of the model adapted for conversational use cases.\n\nThis model is exceptionally versatile and can be used for a wide range of image and multimodal tasks. However, \nbeing a large model means it requires significant computational resources and infrastructure. It is up to you to decide whether \nthis approach suits your use case better than fine-tuning specialized models for each individual task.", "-->\n\n# Image tasks with IDEFICS\n\n[[open-in-colab]]\n\nWhile individual tasks can be tackled by fine-tuning specialized models, an alternative approach \nthat has recently emerged and gained popularity is to use large models for a diverse set of tasks without fine-tuning. \nFor instance, large language models can handle such NLP tasks as summarization, translation, classification, and more. \nThis approach is no longer limited to a single modality, such as text, and in this guide, we will illustrate how you can \nsolve image-text tasks with a large multimodal model called IDEFICS.", "Finally, models such as [MAPL](https://arxiv.org/abs/2210.07179) and [Flamingo](https://arxiv.org/abs/2204.14198) keep both the pre-trained vision encoder and language model frozen. Flamingo sets a new state-of-the-art in few-shot learning on a wide range of open-ended vision and language tasks by adding Perceiver Resampler modules on top of the pre-trained frozen vision model and inserting new cross-attention layers between existing pre-trained and frozen LM layers to condition the LM on visual data.\n\nA nifty advantage of the Frozen PrefixLM pre-training objective is it enables training with limited aligned image-text data, which is particularly useful for domains where aligned multi-modal datasets are not available.\n### 3) Multi-modal Fusing with Cross Attention", "## Multimodal\n\nMultimodal tasks require a model to process multiple data modalities (text, image, audio, video) to solve a particular problem. Image captioning is an example of a multimodal task where the model takes an image as input and outputs a sequence of text describing the image or some properties of the image. \n\nAlthough multimodal models work with different data types or modalities, internally, the preprocessing steps help the model convert all the data types into embeddings (vectors or list of numbers that holds meaningful information about the data). For a task like image captioning, the model learns relationships between image embeddings and text embeddings.\n\n### Document question answering\n\nDocument question answering is a task that answers natural language questions from a document. Unlike a token-level question answering task which takes text as input, document question answering takes an image of a document as input along with a question about the document and returns an answer. Document question answering can be used to parse structured documents and extract key information from it. In the example below, the total amount and change due can be extracted from a receipt.\n\n```py\n>>> from transformers import pipeline\n>>> from PIL import Image\n>>> import requests", "The abstract from the paper is the following:\n\n*Large multimodal models trained on natural documents, which interleave images and text, outperform models trained on image-text pairs on various multimodal benchmarks that require reasoning over one or multiple images to generate a text. However, the datasets used to train these models have not been released, and the collection process has not been fully specified. We introduce the OBELICS dataset, an open web-scale filtered dataset of interleaved image-text documents comprising 141 million web pages extracted from Common Crawl, 353 million associated images, and 115 billion text tokens. We describe the dataset creation process, present comprehensive filtering rules, and provide an analysis of the dataset's content. To show the viability of OBELISC, we train an 80 billion parameters vision and language model on the dataset and obtain competitive performance on various multimodal benchmarks. We release the code to reproduce the dataset along with the dataset itself.*\n\nThis model was contributed by [HuggingFaceM4](https://huggingface.co/HuggingFaceM4). The original code can be found [here](<INSERT LINK TO GITHUB REPO HERE>). (TODO: don't have a public link yet).\n\n\n<Tip warning={true}>", "# Introducing IDEFICS: An Open Reproduction of State-of-the-Art Visual Language Model\n\n\nWe are excited to release IDEFICS (**I**mage-aware **D**ecoder **E**nhanced \u00e0 la **F**lamingo with **I**nterleaved **C**ross-attention**S**), an open-access visual language model. IDEFICS is based on [Flamingo](https://huggingface.co/papers/2204.14198), a state-of-the-art visual language model initially developed by DeepMind, which has not been released publicly. Similarly to GPT-4, the model accepts arbitrary sequences of image and text inputs and produces text outputs. IDEFICS is built solely on publicly available data and models (LLaMA v1 and OpenCLIP) and comes in two variants\u2014the base version and the instructed version. Each variant is available at the 9 billion and 80 billion parameter sizes."], "eval_score_GPT4": "5", "eval_feedback_GPT4": "Feedback: The response correctly identifies the name of the large multimodal model as IDEFICS, which aligns with the reference answer. The additional information provided about the model being an open-access reproduction of Flamingo and accepting arbitrary sequences of image and text inputs is accurate and relevant, although not required by the instruction. The response is completely correct, accurate, and factual."}, {"question": "What is the purpose of the `gradio.Blocks` API?\n", "true_answer": "The `gradio.Blocks` API allows you to have full control over the data flows and layout of your application, enabling the building of complex, multi-step applications.", "source_doc": "huggingface/course/blob/main/chapters/en/chapter9/7.mdx", "generated_answer": "The purpose of the `gradio.Blocks` API is to provide a low-level, flexible API for building complex, multi-step machine learning applications with full control over the data flows and layout of the application. It allows for features such as grouping related demos into tabs, changing the layout of the demo, having multi-step interfaces with flexible data flows, and updating component properties or visibility based on user input. The `gradio.Blocks` API is designed to offer", "retrieved_docs": ["### Contributors Shoutout:\n\n- [@nhankiet](https://github.com/nhankiet) made their first contribution in [PR 1819](https://github.com/gradio-app/gradio/pull/1819)\n\n## 3.0\n\n###### \ud83d\udd25 Gradio 3.0 is the biggest update to the library, ever.\n\n### New Features:\n\n###### 1. Blocks \ud83e\uddf1\n\nBlocks is a new, low-level API that allows you to have full control over the data flows and layout of your application. It allows you to build very complex, multi-step applications. For example, you might want to:\n\n- Group together related demos as multiple tabs in one web app\n- Change the layout of your demo instead of just having all of the inputs on the left and outputs on the right\n- Have multi-step interfaces, in which the output of one model becomes the input to the next model, or have more flexible data flows in general\n- Change a component's properties (for example, the choices in a Dropdown) or its visibility based on user input\n\nHere's a simple example that creates the demo below it:", "### Contributors Shoutout:\n\n- [@nhankiet](https://github.com/nhankiet) made their first contribution in [PR 1819](https://github.com/gradio-app/gradio/pull/1819)\n\n## 3.0\n\n###### \ud83d\udd25 Gradio 3.0 is the biggest update to the library, ever.\n\n### New Features:\n\n###### 1. Blocks \ud83e\uddf1\n\nBlocks is a new, low-level API that allows you to have full control over the data flows and layout of your application. It allows you to build very complex, multi-step applications. For example, you might want to:\n\n- Group together related demos as multiple tabs in one web app\n- Change the layout of your demo instead of just having all of the inputs on the left and outputs on the right\n- Have multi-step interfaces, in which the output of one model becomes the input to the next model, or have more flexible data flows in general\n- Change a component's properties (for example, the choices in a Dropdown) or its visibility based on user input\n\nHere's a simple example that creates the demo below it:", "### Custom Demos with `gr.Blocks`\n\nGradio also offers a low-level approach for designing web apps with more flexible layouts and data flows with the `gr.Blocks` class. Blocks allows you to do things like control where components appear on the page, handle complex data flows (e.g. outputs can serve as inputs to other functions), and update properties/visibility of components based on user interaction \u2014 still all in Python. \n\nYou can build very custom and complex applications using `gr.Blocks()`. For example, the popular image generation [Automatic1111 Web UI](https://github.com/AUTOMATIC1111/stable-diffusion-webui) is built using Gradio Blocks. We dive deeper into the `gr.Blocks` on our series on [building with Blocks](https://www.gradio.app/guides/blocks-and-event-listeners).\n\n\n### The Gradio Python & JavaScript Ecosystem\n\nThat's the gist of the core `gradio` Python library, but Gradio is actually so much more! Its an entire ecosystem of Python and JavaScript libraries that let you build machine learning applications, or query them programmatically, in Python or JavaScript. Here are other related parts of the Gradio ecosystem:", "### 9. What does Gradio `Blocks` allow you to do?\n\n<Question\n\tchoices={[\n        {\n\t\t\ttext: \"Combine multiple demos into one web app\",\n\t\t\texplain: \"You can use the `with gradio.Tabs():` to add tabs for multiple demos\",\n\t\t\tcorrect: true\n        },\n        {\n\t\t\ttext: \"Assign event triggers such as clicked/changed/etc to `Blocks` components\",\n\t\t\texplain: \"When you assign an event, you pass in three parameters: fn: the function that should be called, inputs: the (list) of input component(s), and outputs: the (list) of output components that should be called.\",\n\t\t\tcorrect: true\n        },\n        {\n\t\t\ttext: \"Automatically determine which `Blocks` component should be interactive vs. static\",\n\t\t\texplain: \"Based on the event triggers you define, `Blocks` automatically figures out whether a component should accept user input or not.\",\n\t\t\tcorrect: true\n        },\n\t\t {\n\t\t\ttext: \"Create multi-step demos; meaning allowing you to reuse the output of one component as the input to the next\",\n\t\t\texplain: \"You can use a component for the input of one event trigger but the output of another.\",\n            correct: true\n        },\n\t]}\n/>", "<img class=\"max-w-full mx-auto my-6\" style=\"width: 54rem\" src=\"/blog/assets/68_gradio_blocks/mindseye-lite.jpg\">\n\n\nWhy did we create Blocks? Gradio demos are very easy to build, but what if you want more control over the layout of your demo, or more flexibility on how the data flows?  For example, you might want to:\n\n* Change the layout of your demo instead of just having all of the inputs on the left and outputs on the right\n* Have multi-step interfaces, in which the output of one model becomes the input to the next model, or have more flexible data flows in general\n* Change a component's properties (for example, the choices in a Dropdown) or its visibilty based on user input\n\nThe low-level Blocks API allows you to do all of this, right in Python.\n\nHere's an example of a Blocks demo that creates two simple demos and uses tabs to group them together:\n\n```python\nimport numpy as np\nimport gradio as gr\n\ndef flip_text(x):\n    return x[::-1]\n\ndef flip_image(x):\n    return np.fliplr(x)", "Introduction to Gradio Blocks[[introduction-to-gradio-blocks]]\n\n<CourseFloatingBanner chapter={9}\n  classNames=\"absolute z-10 right-0 top-0\"\n  notebooks={[\n    {label: \"Google Colab\", value: \"https://colab.research.google.com/github/huggingface/notebooks/blob/master/course/en/chapter9/section7.ipynb\"},\n    {label: \"Aws Studio\", value: \"https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/master/course/en/chapter9/section7.ipynb\"},\n]} />\n\nIn the previous sections we have explored and created demos using the `Interface` class. In this section we will introduce our **newly developed** low-level API called `gradio.Blocks`.\n\nNow, what's the difference between `Interface` and `Blocks`?\n\n- \u26a1 `Interface`: a high-level API that allows you to create a full machine learning demo simply by providing a list of inputs and outputs.", "You can build very custom and complex applications using `gr.Blocks()`. For example, the popular image generation [Automatic1111 Web UI](https://github.com/AUTOMATIC1111/stable-diffusion-webui) is built using Gradio Blocks. We dive deeper into the `gr.Blocks` on our series on [building with Blocks](https://www.gradio.app/guides/blocks-and-event-listeners).\n\n\n#### The Gradio Python & JavaScript Ecosystem\n\nThat's the gist of the core `gradio` Python library, but Gradio is actually so much more! Its an entire ecosystem of Python and JavaScript libraries that let you build machine learning applications, or query them programmatically, in Python or JavaScript. Here are other related parts of the Gradio ecosystem:"], "eval_score_GPT4": "4", "eval_feedback_GPT4": "Feedback: The response provided is mostly correct, accurate, and factual. It aligns well with the reference answer by mentioning the control over data flows and layout of the application, which is a key aspect of the `gradio.Blocks` API. Additionally, it expands on the reference answer by detailing specific features such as grouping demos into tabs, changing layouts, and having multi-step interfaces, which are all accurate functionalities of the `gradio.Blocks` API. However, the response is cut off and seems incomplete, which might leave out some additional information that could have been provided. Despite this, the information given is on point and does not deviate from the facts."}, {"question": "What is the purpose of the two-stage model proposed in the paper \"Hierarchical Text-Conditional Image Generation with CLIP Latents\"?\n", "true_answer": "The purpose of the two-stage model is to generate a CLIP image embedding given a text caption and then generate an image conditioned on the image embedding.", "source_doc": "huggingface/diffusers/blob/main/docs/source/en/api/pipelines/unclip.md", "generated_answer": "The purpose of the two-stage model proposed in the paper \"Hierarchical Text-Conditional Image Generation with CLIP Latents\" is to leverage the robust representations of images learned by contrastive models like CLIP for image generation. The model consists of a prior that generates a CLIP image embedding from a text caption, and a decoder that generates an image conditioned on the image embedding. This approach improves image diversity with minimal loss in photorealism and caption", "retrieved_docs": ["- Hierarchical Text-Conditional Image Generation with CLIP Latents (DALL-E 2) ([Ramesh et al., 2022](https://cdn.openai.com/papers/dall-e-2.pdf)): uses a prior to turn a text caption into a CLIP image embedding, after which a diffusion model decodes it into an image\n- Photorealistic Text-to-Image Diffusion Models with Deep Language Understanding (ImageGen) ([Saharia et al., 2022](https://arxiv.org/abs/2205.11487)): shows that combining a large pre-trained language model (e.g. T5) with cascaded diffusion works well for text-to-image synthesis", "The abstract from the paper is:\n\n*Contrastive models like CLIP have been shown to learn robust representations of images that capture both semantics and style. To leverage these representations for image generation, we propose a two-stage model: a prior that generates a CLIP image embedding given a text caption, and a decoder that generates an image conditioned on the image embedding. We show that explicitly generating image representations improves image diversity with minimal loss in photorealism and caption similarity. Our decoders conditioned on image representations can also produce variations of an image that preserve both its semantics and style, while varying the non-essential details absent from the image representation. Moreover, the joint embedding space of CLIP enables language-guided image manipulations in a zero-shot fashion. We use diffusion models for the decoder and experiment with both autoregressive and diffusion models for the prior, finding that the latter are computationally more efficient and produce higher-quality samples.*\n\n## PriorTransformer\n\n[[autodoc]] PriorTransformer\n\n## PriorTransformerOutput\n\n[[autodoc]] models.prior_transformer.PriorTransformerOutput", "The abstract from the paper is following:\n\n*Contrastive models like CLIP have been shown to learn robust representations of images that capture both semantics and style. To leverage these representations for image generation, we propose a two-stage model: a prior that generates a CLIP image embedding given a text caption, and a decoder that generates an image conditioned on the image embedding. We show that explicitly generating image representations improves image diversity with minimal loss in photorealism and caption similarity. Our decoders conditioned on image representations can also produce variations of an image that preserve both its semantics and style, while varying the non-essential details absent from the image representation. Moreover, the joint embedding space of CLIP enables language-guided image manipulations in a zero-shot fashion. We use diffusion models for the decoder and experiment with both autoregressive and diffusion models for the prior, finding that the latter are computationally more efficient and produce higher-quality samples.*\n\nYou can find lucidrains' DALL-E 2 recreation at [lucidrains/DALLE2-pytorch](https://github.com/lucidrains/DALLE2-pytorch).\n\n<Tip>", "The abstract from the paper is:\n\n*Contrastive models like CLIP have been shown to learn robust representations of images that capture both semantics and style. To leverage these representations for image generation, we propose a two-stage model: a prior that generates a CLIP image embedding given a text caption, and a decoder that generates an image conditioned on the image embedding. We show that explicitly generating image representations improves image diversity with minimal loss in photorealism and caption similarity. Our decoders conditioned on image representations can also produce variations of an image that preserve both its semantics and style, while varying the non-essential details absent from the image representation. Moreover, the joint embedding space of CLIP enables language-guided image manipulations in a zero-shot fashion. We use diffusion models for the decoder and experiment with both autoregressive and diffusion models for the prior, finding that the latter are computationally more efficient and produce higher-quality samples.*\n\n## Tips\n\nStable unCLIP takes  `noise_level` as input during inference which determines how much noise is added to the image embeddings. A higher `noise_level` increases variation in the final un-noised images. By default, we do not add any additional noise to the image embeddings (`noise_level = 0`).", "[Kandinsky 2.1](../api/pipelines/kandinsky) changes the architecture to include an image prior model ([`CLIP`](https://huggingface.co/docs/transformers/model_doc/clip)) to generate a mapping between text and image embeddings. The mapping provides better text-image alignment and it is used with the text embeddings during training, leading to higher quality results. Finally, Kandinsky 2.1 uses a [Modulating Quantized Vectors (MoVQ)](https://huggingface.co/papers/2209.09002) decoder - which adds a spatial conditional normalization layer to increase photorealism - to decode the latents into images.\n\n[Kandinsky 2.2](../api/pipelines/kandinsky_v22) improves on the previous model by replacing the image encoder of the image prior model with a larger CLIP-ViT-G model to improve quality. The image prior model was also retrained on images with different resolutions and aspect ratios to generate higher-resolution images and different image sizes.", "IF has two distinct advantages compared to existing text-to-image models\nlike Stable Diffusion:\n\n- The model operates directly in \"pixel space\" (*i.e.,* on\nuncompressed images) instead of running the denoising process in the\nlatent space such as [Stable Diffusion](http://hf.co/blog/stable_diffusion).\n- The model is trained on outputs of\n[T5-XXL](https://huggingface.co/google/t5-v1_1-xxl), a more powerful\ntext encoder than [CLIP](https://openai.com/research/clip), used by\nStable Diffusion as the text encoder.\n\nAs a result, IF is better at generating images with high-frequency\ndetails (*e.g.,* human faces and hands) and is the first open-source\nimage generation model that can reliably generate images with text.", "Generally, a text-to-image model consists of a text encoder that interprets text and a generative model that generates an image from its output.\n\nStable Diffusion uses CLIP, the language-image pre-training model from OpenAI, as its text encoder and a latent diffusion model, which is an improved version of the diffusion model, as the generative model. Stable Diffusion was trained mainly on the English subset of LAION-5B and can generate high-performance images simply by entering text prompts. In addition to its high performance, Stable Diffusion is also easy to use with inference running at a computing cost of about 10GB VRAM GPU.\n\n<p align=\"center\">\n<img src=\"https://raw.githubusercontent.com/patrickvonplaten/scientific_images/master/stable_diffusion.png\" alt=\"sd-pipeline\" width=\"300\"/>\n</p>\n\n*from [Stable Diffusion with \ud83e\udde8 Diffusers](https://huggingface.co/blog/stable_diffusion)*\n\n## Japanese Stable Diffusion\n\n### Why do we need Japanese Stable Diffusion?"], "eval_score_GPT4": "4", "eval_feedback_GPT4": "Feedback: The response provided is mostly correct and factual. It accurately describes the two-stage model's purpose, which involves generating a CLIP image embedding from a text caption and then generating an image based on that embedding. The response also correctly identifies the use of contrastive models like CLIP and mentions the improvement in image diversity and photorealism. However, the response could be more concise and to the point to match the reference answer perfectly."}, {"question": "What command is used to install the requirements for a research project using \ud83e\udd17 Transformers?\n", "true_answer": "pip install -r requirements.txt", "source_doc": "huggingface/transformers/blob/main/examples/research_projects/README.md", "generated_answer": "The command to install the requirements for a research project using \ud83e\udd17 Transformers is:\n\n```bash\npip install -r requirements.txt\n```\n\nThis command should be run inside the folder of the chosen research project. It will install the necessary dependencies specified in the `requirements.txt` file. Note that this command may need to be run again if the requirements change in the future.", "retrieved_docs": ["!---\nCopyright 2020 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\");\nyou may not use this file except in compliance with the License.\nYou may obtain a copy of the License at\n\n    http://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software\ndistributed under the License is distributed on an \"AS IS\" BASIS,\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\nSee the License for the specific language governing permissions and\nlimitations under the License.\n-->\n\n# Research projects\n\nThis folder contains various research projects using \ud83e\udd17 Transformers. They are not maintained and require a specific\nversion of \ud83e\udd17 Transformers that is indicated in the requirements file of each folder. Updating them to the most recent version of the library will require some work.\n\nTo use any of them, just run the command\n```\npip install -r requirements.txt\n```\ninside the folder of your choice.\n\nIf you need help with any of those, contact the author(s), indicated at the top of the `README` of each folder.", "Research projects\n\nThis folder contains various research projects using \ud83e\udde8 Diffusers.\nThey are not really maintained by the core maintainers of this library and often require a specific version of Diffusers that is indicated in the requirements file of each folder.\nUpdating them to the most recent version of the library will require some work.\n\nTo use any of them, just run the command\n\n```\npip install -r requirements.txt\n```\ninside the folder of your choice.\n\nIf you need help with any of those, please open an issue where you directly ping the author(s), as indicated at the top of the README of each folder.", "Check if \ud83e\udd17 Transformers has been properly installed by running the following command:\n\n```bash\npython -c \"from transformers import pipeline; print(pipeline('sentiment-analysis')('I love you'))\"\n```\n\n## Editable install\n\nYou will need an editable install if you'd like to:\n\n* Use the `main` version of the source code.\n* Contribute to \ud83e\udd17 Transformers and need to test changes in the code.\n\nClone the repository and install \ud83e\udd17 Transformers with the following commands:\n\n```bash\ngit clone https://github.com/huggingface/transformers.git\ncd transformers\npip install -e .\n```\n\nThese commands will link the folder you cloned the repository to and your Python library paths. Python will now look inside the folder you cloned to in addition to the normal library paths. For example, if your Python packages are typically installed in `~/anaconda3/envs/main/lib/python3.7/site-packages/`, Python will also search the folder you cloned to: `~/transformers/`.\n\n<Tip warning={true}>\n\nYou must keep the `transformers` folder if you want to keep using the library.", "Then print out the label and score:\n\n```bash\n[{'label': 'POSITIVE', 'score': 0.9998704791069031}]\n```\n\n## Install from source\n\nInstall \ud83e\udd17 Transformers from source with the following command:\n\n```bash\npip install git+https://github.com/huggingface/transformers\n```\n\nThis command installs the bleeding edge `main` version rather than the latest `stable` version. The `main` version is useful for staying up-to-date with the latest developments. For instance, if a bug has been fixed since the last official release but a new release hasn't been rolled out yet. However, this means the `main` version may not always be stable. We strive to keep the `main` version operational, and most issues are usually resolved within a few hours or a day. If you run into a problem, please open an [Issue](https://github.com/huggingface/transformers/issues) so we can fix it even sooner!\n\nCheck if \ud83e\udd17 Transformers has been properly installed by running the following command:", "In the following, we try to give you a general recipe that we found most useful when porting a model to \ud83e\udd17 Transformers.\n\nThe following list is a summary of everything that has to be done to add a model and can be used by you as a To-Do\nList:\n\n\u2610 (Optional) Understood the model's theoretical aspects<br>\n\u2610 Prepared \ud83e\udd17 Transformers dev environment<br>\n\u2610 Set up debugging environment of the original repository<br>\n\u2610 Created script that successfully runs the `forward()` pass using the original repository and checkpoint<br>\n\u2610 Successfully added the model skeleton to \ud83e\udd17 Transformers<br>\n\u2610 Successfully converted original checkpoint to \ud83e\udd17 Transformers checkpoint<br>\n\u2610 Successfully ran `forward()` pass in \ud83e\udd17 Transformers that gives identical output to original checkpoint<br>\n\u2610 Finished model tests in \ud83e\udd17 Transformers<br>\n\u2610 Successfully added tokenizer in \ud83e\udd17 Transformers<br>\n\u2610 Run end-to-end integration tests<br>\n\u2610 Finished docs<br>\n\u2610 Uploaded model weights to the Hub<br>\n\u2610 Submitted the pull request<br>\n\u2610 (Optional) Added a demo notebook", "## Important note\n\n**Important**\n\nTo make sure you can successfully run the latest versions of the example scripts, you have to **install the library from source** and install some example-specific requirements. To do this, execute the following steps in a new virtual environment:\n```bash\ngit clone https://github.com/huggingface/transformers\ncd transformers\npip install .\n```\nThen cd in the example folder of your choice and run\n```bash\npip install -r requirements.txt\n```\n\nTo browse the examples corresponding to released versions of \ud83e\udd17 Transformers, click on the line below and then on your desired version of the library:", "In the following, we try to give you a general recipe that we found most\nuseful when porting a model to \ud83e\udd17 Transformers.\n\nThe following list is a summary of everything that has to be done to add\na model and can be used by you as a To-Do List:\n\n1.  [ ] (Optional) Understood theoretical aspects\n\n2.  [ ] Prepared transformers dev environment\n\n3.  [ ] Set up debugging environment of the original repository\n\n4.  [ ] Created script that successfully runs forward pass using\n    original repository and checkpoint\n\n5.  [ ] Successfully opened a PR and added the model skeleton to Transformers\n\n6.  [ ] Successfully converted original checkpoint to Transformers\n    checkpoint\n\n7.  [ ] Successfully ran forward pass in Transformers that gives\n    identical output to original checkpoint\n\n8.  [ ] Finished model tests in Transformers\n\n9.  [ ] Successfully added Tokenizer in Transformers\n\n10. [ ] Run end-to-end integration tests\n\n11. [ ] Finished docs\n\n12. [ ] Uploaded model weights to the hub\n\n13. [ ] Submitted the pull request for review\n\n14. [ ] (Optional) Added a demo notebook"], "eval_score_GPT4": "5", "eval_feedback_GPT4": "The response provided is completely correct, accurate, and factual. It matches the reference answer and includes additional helpful context about where and when to run the command, which aligns with the correct usage of the command in question."}, {"question": "What task does the `roberta-large-mnli` checkpoint perform?\n", "true_answer": "Text classification", "source_doc": "huggingface/course/blob/main/chapters/en/chapter1/10.mdx", "generated_answer": "The `roberta-large-mnli` checkpoint performs the task of natural language inference, specifically classifying if two sentences are logically linked across three labels (contradiction, neutral, entailment). This is also known as the Multiple Natural Language Inference (MNLI) task. (Source: Extracted document 0, question provided in the context.)", "retrieved_docs": ["### 1. Explore the Hub and look for the `roberta-large-mnli` checkpoint. What task does it perform?\n\n\n<Question\n\tchoices={[\n\t\t{\n\t\t\ttext: \"Summarization\",\n\t\t\texplain: \"Look again on the <a href=\\\"https://huggingface.co/roberta-large-mnli\\\">roberta-large-mnli page</a>.\"\n\t\t},\n\t\t{\n\t\t\ttext: \"Text classification\",\n\t\t\texplain: \"More precisely, it classifies if two sentences are logically linked across three labels (contradiction, neutral, entailment) \u2014 a task also called <em>natural language inference</em>.\",\n\t\t\tcorrect: true\n\t\t},\n\t\t{\n\t\t\ttext: \"Text generation\",\n\t\t\texplain: \"Look again on the <a href=\\\"https://huggingface.co/roberta-large-mnli\\\">roberta-large-mnli page</a>.\"\n\t\t}\n\t]}\n/>\n\n### 2. What will the following code return?\n\n```py\nfrom transformers import pipeline\n\nner = pipeline(\"ner\", grouped_entities=True)\nner(\"My name is Sylvain and I work at Hugging Face in Brooklyn.\")\n```", "!-- DISABLE-FRONTMATTER-SECTIONS -->\n\n# End-of-chapter quiz[[end-of-chapter-quiz]]\n\n<CourseFloatingBanner\n    chapter={1}\n    classNames=\"absolute z-10 right-0 top-0\"\n/>\n\nThis chapter covered a lot of ground! Don't worry if you didn't grasp all the details; the next chapters will help you understand how things work under the hood.\n\nFirst, though, let's test what you learned in this chapter!\n\n\n### 1. Explore the Hub and look for the `roberta-large-mnli` checkpoint. What task does it perform?", "to implement BigBird.\n- The first goal should be to successfully run a forward pass using the RoBERTa checkpoint `bigbr_base/model.ckpt-0.data-00000-of-00001` and `bigbr_base/model.ckpt-0.index`.", "There are total **3 checkpoints** available in **\ud83e\udd17Hub** (at the point of writing this article): [`bigbird-roberta-base`](https://huggingface.co/google/bigbird-roberta-base), [`bigbird-roberta-large`](https://huggingface.co/google/bigbird-roberta-large), [`bigbird-base-trivia-itc`](https://huggingface.co/google/bigbird-base-trivia-itc). The first two checkpoints come from pretraining `BigBirdForPretraining` with `masked_lm loss`; while the last one corresponds to the checkpoint after finetuning `BigBirdForQuestionAnswering` on `trivia-qa` dataset.\n\nLet's have a look at minimal code you can write (in case you like to use your PyTorch trainer), to use \ud83e\udd17's BigBird model for fine-tuning your tasks.\n\n```python\n# let's consider our task to be question-answering as an example\n\nfrom transformers import BigBirdForQuestionAnswering, BigBirdTokenizer\nimport torch", "- [Data Processing](#data-processing)\n    - [Models](#models)\n        - [RoBERTa](#roberta)\n            - [Load RoBERTA Checkpoints for the Classification Task](#load-roberta-checkpoints-for-the-classification-task)\n            - [LoRA setup for RoBERTa classifier](#lora-setup-for-roberta-classifier)\n        - [Mistral](#mistral)\n            - [Load checkpoints for the classfication model](#load-checkpoints-for-the-classfication-model)\n            - [LoRA setup for Mistral 7B classifier](#lora-setup-for-mistral-7b-classifier)\n        - [Llama 2](#llama-2)\n            - [Load checkpoints for the classification mode](#load-checkpoints-for-the-classfication-mode)\n            - [LoRA setup for Llama 2 classifier](#lora-setup-for-llama-2-classifier)\n    - [Setup the trainer](#setup-the-trainer)\n        - [Evaluation Metrics](#evaluation-metrics)", "lr = 1e-4\nbatch_size = 8\nnum_epochs = 5\n\ntraining_args = TrainingArguments(\n    output_dir=\"roberta-large-lora-token-classification\",\n    learning_rate=lr,\n    lr_scheduler_type= \"constant\",\n    warmup_ratio= 0.1,\n    max_grad_norm= 0.3,\n    per_device_train_batch_size=batch_size,\n    per_device_eval_batch_size=batch_size,\n    num_train_epochs=num_epochs,\n    weight_decay=0.001,\n    evaluation_strategy=\"epoch\",\n    save_strategy=\"epoch\",\n    load_best_model_at_end=True,\n    report_to=\"wandb\",\n    fp16=False,\n    gradient_checkpointing=True,\n)\n```\n\nFinally, we define the RoBERTa trainer by providing the model, the training arguments and the tokenized datasets:", "Each of these tasks comes with at least 10 model checkpoints on the Hub for you to explore. Furthermore, we support [tasks](https://huggingface.co/tasks) that lie at the intersection of vision and language such as:\n\n- Image-to-text (image captioning, OCR)\n- Text-to-image\n- Document question-answering\n- Visual question-answering"], "eval_score_GPT4": "5", "eval_feedback_GPT4": "Feedback: The response accurately describes the task performed by the `roberta-large-mnli` checkpoint as natural language inference, which involves classifying the relationship between two sentences into three categories: contradiction, neutral, and entailment. This detailed explanation aligns well with the task of text classification mentioned in the reference answer, providing a more specific description of the type of text classification, which is the MNLI task. The response is therefore completely correct, accurate, and factual."}, {"question": "What service is replacing the Paid tier of the Inference API at Hugging Face?\n", "true_answer": "Inference Endpoints", "source_doc": "huggingface/blog/blob/main/pricing-update.md", "generated_answer": "The service replacing the Paid tier of the Inference API at Hugging Face is [Inference Endpoints](https://huggingface.co/inference-endpoints). The Inference API will still be available for free, but for faster and enterprise-grade inference, users are recommended to check out Inference Endpoints. This new service allows for dedicated inference on a cloud provider of your choice and does not require a subscription to use. However, a credit card is", "retrieved_docs": ["--\ntitle: Introducing our new pricing\nthumbnail: /blog/assets/114_pricing-update/thumbnail.png\nauthors:\n- user: sbrandeis\n- user: pierric\n---\n\n# Introducing our new pricing\n\n\nAs you might have noticed, our [pricing page](https://huggingface.co/pricing) has changed a lot recently.\n\nFirst of all, we are sunsetting the Paid tier of the Inference API service. The Inference API will still be available for everyone to use for free. But if you're looking for a fast, enterprise-grade inference as a service, we recommend checking out our brand new solution for this: [Inference Endpoints](https://huggingface.co/inference-endpoints).\n\nAlong with Inference Endpoints, we've recently introduced hardware upgrades for [Spaces](https://huggingface.co/spaces/launch), which allows running ML demos with the hardware of your choice. No subscription is required to use these services; you only need to add a credit card to your account from your [billing settings](https://huggingface.co/settings/billing). You can also attach a payment method to any of [your organizations](https://huggingface.co/settings/organizations).", "## Demos with the Hugging Face Inference API\n\nHugging Face has a free service called the [Inference API](https://huggingface.co/inference-api), which allows you to send HTTP requests to models in the Hub. For transformers or diffusers-based models, the API can be 2 to 10 times faster than running the inference yourself. The API is free (rate limited), and you can switch to dedicated [Inference Endpoints](https://huggingface.co/pricing) when you want to use it in production. Gradio integrates directly with the Hugging Face Inference API so that you can create a demo simply by specifying a model's name (e.g. `Helsinki-NLP/opus-mt-en-es`), like this:\n\n```python\nimport gradio as gr\n\ndemo = gr.load(\"Helsinki-NLP/opus-mt-en-es\", src=\"models\")\n\ndemo.launch()\n```\n\nFor any Hugging Face model supported in the Inference API, Gradio automatically infers the expected input and output and make the underlying server calls, so you don't have to worry about defining the prediction function.", "```python\n>>> from huggingface_hub import InferenceClient\n>>> client = InferenceClient(token=\"hf_***\")\n```\n\n<Tip>\n\nAuthentication is NOT mandatory when using the Inference API. However, authenticated users get a higher free-tier to\nplay with the service. Token is also mandatory if you want to run inference on your private models or on private\nendpoints.\n\n</Tip>\n\n## Supported tasks\n\n[`InferenceClient`]'s goal is to provide the easiest interface to run inference on Hugging Face models. It\nhas a simple API that supports the most common tasks. Here is a list of the currently supported tasks:", "Hugging Face PRO users now have access to exclusive API endpoints for a curated list of powerful models that benefit from ultra-fast inference powered by [text-generation-inference](https://github.com/huggingface/text-generation-inference). This is a benefit on top of the free inference API, which is available to all Hugging Face users to facilitate testing and prototyping on 200,000+ models. PRO users enjoy higher rate limits on these models, as well as exclusive access to some of the best models available today.\n\n## Contents", "--\ntitle: \"Hugging Face Platform on the AWS Marketplace: Pay with your AWS Account\"\nthumbnail: /blog/assets/158_aws_marketplace/thumbnail.jpg\nauthors:\n- user: philschmid\n- user: sbrandeis\n- user: jeffboudier\n---\n\n# Hugging Face Platform on the AWS Marketplace: Pay with your AWS Account\n\n\nThe [Hugging Face Platform](https://aws.amazon.com/marketplace/pp/prodview-n6vsyhdjkfng2) has landed on the AWS Marketplace. Starting today, you can subscribe to the Hugging Face Platform through AWS Marketplace to pay for your Hugging Face usage directly with your AWS account. This new integrated billing method makes it easy to manage payment for usage of all our managed services by all members of your organization, including Inference Endpoints, Spaces Hardware Upgrades, and AutoTrain to easily train, test and deploy the most popular machine learning models like Llama 2, StarCoder, or BERT.", "The model was later used in `afetharita` to extract addresses. The parsed addresses would be sent to a geocoding API to obtain longitude and latitude, and the geolocation would then be displayed on the front-end map. For inference, we have used Inference API, which is an API that hosts model for inference and is automatically enabled when the model is pushed to Hugging Face Hub. Using Inference API for serving has saved us from pulling the model, writing an app, building a docker image, setting up CI/CD, and deploying the model to a cloud instance, where it would be extra overhead work for the DevOps and cloud teams as well. Hugging Face teams have provided us with more replicas so that there would be no downtime and the application would be robust against a lot of traffic.\n\n![backend_pipeline](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/disaster-assets/production_pipeline.png)", "!--\u26a0\ufe0f Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\nrendered properly in your Markdown viewer.\n-->\n\n# Run Inference on servers\n\nInference is the process of using a trained model to make predictions on new data. As this process can be compute-intensive,\nrunning on a dedicated server can be an interesting option. The `huggingface_hub` library provides an easy way to call a\nservice that runs inference for hosted models. There are several services you can connect to:\n- [Inference API](https://huggingface.co/docs/api-inference/index): a service that allows you to run accelerated inference\non Hugging Face's infrastructure for free. This service is a fast way to get started, test different models, and\nprototype AI products.\n- [Inference Endpoints](https://huggingface.co/docs/inference-endpoints/index): a product to easily deploy models to production.\nInference is run by Hugging Face in a dedicated, fully managed infrastructure on a cloud provider of your choice."], "eval_score_GPT4": "5", "eval_feedback_GPT4": "The response correctly identifies \"Inference Endpoints\" as the service replacing the Paid tier of the Inference API at Hugging Face, which aligns with the reference answer. The additional details provided about the availability of the Inference API for free, the recommendation for faster and enterprise-grade inference, and the option for dedicated inference on a cloud provider of choice are accurate and factual, enhancing the response without deviating from the correct answer. The response does not contain any incorrect or inaccurate information."}, {"question": "What architectural feature does SqueezeBERT use instead of fully-connected layers for the Q, K, V, and FFN layers?\n", "true_answer": "Grouped convolutions", "source_doc": "huggingface/transformers/blob/main/docs/source/en/model_doc/squeezebert.md", "generated_answer": "SqueezeBERT uses [grouped convolutions](https://blog.yani.io/filter-group-tutorial) instead of fully-connected layers for the Q, K, V, and FFN layers, as described in the paper \"SqueezeBERT: What can computer vision teach NLP about efficient neural networks?\" by Forrest N. Iandola, Albert E. Shaw, Ravi Krishna, and Kurt W. Keutzer.", "retrieved_docs": ["-->\n\n# SqueezeBERT\n\n## Overview\n\nThe SqueezeBERT model was proposed in [SqueezeBERT: What can computer vision teach NLP about efficient neural networks?](https://arxiv.org/abs/2006.11316) by Forrest N. Iandola, Albert E. Shaw, Ravi Krishna, Kurt W. Keutzer. It's a\nbidirectional transformer similar to the BERT model. The key difference between the BERT architecture and the\nSqueezeBERT architecture is that SqueezeBERT uses [grouped convolutions](https://blog.yani.io/filter-group-tutorial)\ninstead of fully-connected layers for the Q, K, V and FFN layers.\n\nThe abstract from the paper is the following:", "This model was contributed by [forresti](https://huggingface.co/forresti).\n\n## Usage tips\n\n- SqueezeBERT is a model with absolute position embeddings so it's usually advised to pad the inputs on the right\n  rather than the left.\n- SqueezeBERT is similar to BERT and therefore relies on the masked language modeling (MLM) objective. It is therefore\n  efficient at predicting masked tokens and at NLU in general, but is not optimal for text generation. Models trained\n  with a causal language modeling (CLM) objective are better in that regard.\n- For best results when finetuning on sequence classification tasks, it is recommended to start with the\n  *squeezebert/squeezebert-mnli-headless* checkpoint.\n\n## Resources\n\n- [Text classification task guide](../tasks/sequence_classification)\n- [Token classification task guide](../tasks/token_classification)\n- [Question answering task guide](../tasks/question_answering)\n- [Masked language modeling task guide](../tasks/masked_language_modeling)\n- [Multiple choice task guide](../tasks/multiple_choice)\n\n## SqueezeBertConfig\n\n[[autodoc]] SqueezeBertConfig", "Finally, it is important to remember that *chunked linear layers* yield a mathematically equivalent output to conventional linear layers and can therefore be applied to all transformer linear layers. Making use of `config.chunk_size_feed_forward` therefore allows a better trade-off between memory and speed in certain use cases.\n\n---\n \\\\( {}^1 \\\\) For a simpler explanation, the layer norm layer which is normally applied to \\\\( \\mathbf{\\overline{Z}} \\\\) before being processed by the feed forward layers is omitted for now.\n\n \\\\( {}^2 \\\\) In `bert-base-uncased`, *e.g.* the intermediate dimension \\\\( d_{f} \\\\) is with 3072 four times larger than the output dimension \\\\( d_{h} \\\\).\n\n \\\\( {}^3 \\\\) As a reminder, the output `config.num_attention_heads` is assumed to be 1 for the sake of clarity and illustration in this notebook, so that the output of the self-attention layers can be assumed to be of size `config.hidden_size`.", "1. **[SqueezeBERT](https://huggingface.co/docs/transformers/model_doc/squeezebert)** (from Berkeley) released with the paper [SqueezeBERT: What can computer vision teach NLP about efficient neural networks?](https://arxiv.org/abs/2006.11316) by Forrest N. Iandola, Albert E. Shaw, Ravi Krishna, and Kurt W. Keutzer.\n1. **[SwiftFormer](https://huggingface.co/docs/transformers/model_doc/swiftformer)** (from MBZUAI) released with the paper [SwiftFormer: Efficient Additive Attention for Transformer-based Real-time Mobile Vision Applications](https://arxiv.org/abs/2303.15446) by Abdelrahman Shaker, Muhammad Maaz, Hanoona Rasheed, Salman Khan, Ming-Hsuan Yang, Fahad Shahbaz Khan.", "1. **[SqueezeBERT](https://huggingface.co/docs/transformers/model_doc/squeezebert)** (from Berkeley) released with the paper [SqueezeBERT: What can computer vision teach NLP about efficient neural networks?](https://arxiv.org/abs/2006.11316) by Forrest N. Iandola, Albert E. Shaw, Ravi Krishna, and Kurt W. Keutzer.\n1. **[SwiftFormer](https://huggingface.co/docs/transformers/model_doc/swiftformer)** (from MBZUAI) released with the paper [SwiftFormer: Efficient Additive Attention for Transformer-based Real-time Mobile Vision Applications](https://arxiv.org/abs/2303.15446) by Abdelrahman Shaker, Muhammad Maaz, Hanoona Rasheed, Salman Khan, Ming-Hsuan Yang, Fahad Shahbaz Khan.", "<!--\nType: model-index\nCollections:\n- Name: Gloun SENet\n  Paper:\n    Title: Squeeze-and-Excitation Networks\n    URL: https://paperswithcode.com/paper/squeeze-and-excitation-networks\nModels:\n- Name: gluon_senet154\n  In Collection: Gloun SENet\n  Metadata:\n    FLOPs: 26681705136\n    Parameters: 115090000\n    File Size: 461546622\n    Architecture:\n    - Convolution\n    - Dense Connections\n    - Global Average Pooling\n    - Max Pooling\n    - Softmax\n    - Squeeze-and-Excitation Block\n    Tasks:\n    - Image Classification\n    Training Data:\n    - ImageNet\n    ID: gluon_senet154\n    Crop Pct: '0.875'\n    Image Size: '224'\n    Interpolation: bicubic", "<!--\nType: model-index\nCollections:\n- Name: Gloun SENet\n  Paper:\n    Title: Squeeze-and-Excitation Networks\n    URL: https://paperswithcode.com/paper/squeeze-and-excitation-networks\nModels:\n- Name: gluon_senet154\n  In Collection: Gloun SENet\n  Metadata:\n    FLOPs: 26681705136\n    Parameters: 115090000\n    File Size: 461546622\n    Architecture:\n    - Convolution\n    - Dense Connections\n    - Global Average Pooling\n    - Max Pooling\n    - Softmax\n    - Squeeze-and-Excitation Block\n    Tasks:\n    - Image Classification\n    Training Data:\n    - ImageNet\n    ID: gluon_senet154\n    Crop Pct: '0.875'\n    Image Size: '224'\n    Interpolation: bicubic"], "eval_score_GPT4": "5", "eval_feedback_GPT4": "The response correctly identifies that SqueezeBERT uses grouped convolutions instead of fully-connected layers for the Q, K, V, and FFN layers. The answer is in line with the reference answer and provides additional context by citing the paper where this information is discussed, which supports the accuracy and factual nature of the response."}, {"question": "What type of license is the HuggingFace Team's software distributed under?\n", "true_answer": "Apache License, Version 2.0", "source_doc": "huggingface/simulate/blob/main/docs/source/tutorials/interaction.mdx", "generated_answer": "The HuggingFace Team's software is distributed under the Apache License, Version 2.0. This information can be found in the copyright statements at the beginning of each extracted document provided in the context. It should be noted that the software is distributed \"AS IS\" without any warranties or conditions, except as required by applicable law or agreed to in writing. The specific language governing permissions and limitations under the license can be found on the Apache website.", "retrieved_docs": ["!---\nCopyright 2020 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\");\nyou may not use this file except in compliance with the License.\nYou may obtain a copy of the License at\n\n    http://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software\ndistributed under the License is distributed on an \"AS IS\" BASIS,\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\nSee the License for the specific language governing permissions and\nlimitations under the License.\n-->\n\n# How to contribute to huggingface_hub, the GitHub repository?\n\nEveryone is welcome to contribute, and we value everybody's contribution. Code is not the only way to help the community.\nAnswering questions, helping others, reaching out and improving the documentations are immensely valuable to the community.\n\nIt also helps us if you spread the word: reference the library from blog posts\non the awesome projects it made possible, shout out on Twitter every time it has\nhelped you, or simply star the repo to say \"thank you\".", "!--Copyright 2023 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\n\nhttp://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License.\n-->\n\n# LMSDiscreteScheduler", "!--Copyright 2023 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\n\nhttp://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License.\n-->\n\n# Text-to-image", "!--Copyright 2022 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\n\nhttp://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License.\n-->\n\n# Scenes\n\nUnder construction \ud83d\udea7.", "!---\nCopyright 2020 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\");\nyou may not use this file except in compliance with the License.\nYou may obtain a copy of the License at\n\n    http://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software\ndistributed under the License is distributed on an \"AS IS\" BASIS,\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\nSee the License for the specific language governing permissions and\nlimitations under the License.\n-->", "!---\nCopyright 2020 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\");\nyou may not use this file except in compliance with the License.\nYou may obtain a copy of the License at\n\n    http://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software\ndistributed under the License is distributed on an \"AS IS\" BASIS,\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\nSee the License for the specific language governing permissions and\nlimitations under the License.\n-->", "!---\nCopyright 2020 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\");\nyou may not use this file except in compliance with the License.\nYou may obtain a copy of the License at\n\n    http://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software\ndistributed under the License is distributed on an \"AS IS\" BASIS,\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\nSee the License for the specific language governing permissions and\nlimitations under the License.\n-->"], "eval_score_GPT4": "5", "eval_feedback_GPT4": "The response correctly identifies the license as the Apache License, Version 2.0, which is in agreement with the reference answer. The additional information provided about the nature of the software distribution and where to find the license terms is accurate and factual, although not required by the instruction. The response goes beyond the reference answer but does not contradict it."}, {"question": "What are the two parameter-reduction techniques proposed in the ALBERT model to lower memory consumption and increase training speed?\n", "true_answer": "Splitting the embedding matrix into two smaller matrices and using repeating layers split among groups.", "source_doc": "huggingface/transformers/blob/main/docs/source/en/model_doc/albert.md", "generated_answer": "The ALBERT model proposes two parameter-reduction techniques to lower memory consumption and increase training speed:\n\n1. Separating the larger vocabulary embedding into two smaller matrices. This reduces the number of parameters in the embedding layer.\n\n2. Allowing layers to share parameters. This reduces the number of parameters in the repeating layers, which have a small memory footprint but similar computational cost as BERT-like architectures with the same number of hidden layers.", "retrieved_docs": ["However, most Transformer models continued to trend towards more parameters, leading to new models focused on improving training efficiency. [ALBERT](model_doc/albert) reduces memory consumption by lowering the number of parameters in two ways: separating the larger vocabulary embedding into two smaller matrices and allowing layers to share parameters. [DeBERTa](model_doc/deberta) added a disentangled attention mechanism where the word and its position are separately encoded in two vectors. The attention is computed from these separate vectors instead of a single vector containing the word and position embeddings. [Longformer](model_doc/longformer) also focused on making attention more efficient, especially for processing documents with longer sequence lengths. It uses a combination of local windowed attention (attention only calculated from fixed window size around each token) and global attention (only for specific task tokens like `[CLS]` for classification) to create a sparse attention matrix instead of a full attention matrix.\n\n### Decoder[[nlp-decoder]]", "You might want to use what is called **parameter efficient fine-tuning** (PEFT).\nThis technique first freezes up the parameters of your pretrained model of interest, then adds a number of new parameters on top of it, called the adapters. What you then fine-tune on your task are only the (lightweight) adapter weights, considerably smaller than the original model. You then just need to share your small adapter weights (and the base model)! You'll find a list of interesting approaches for PEFT [here](https://github.com/huggingface/peft).\n\n### Quantization: Models running everywhere\nWe've seen that well-performing models now come in all shapes and sizes\u2026 but even then, it doesn't mean that they are accessible to all! A 30B parameters model can require more than 66G of RAM just to load in memory (not even use), and not everyone in the community has the hardware necessary to do so.\n\nThat's where quantization comes in! Quantization is a special technique which reduces a model's size by changing the precision of its parameters. \n\nWhat does it mean?", "-->\n\n# ALBERT\n\n<div class=\"flex flex-wrap space-x-1\">\n<a href=\"https://huggingface.co/models?filter=albert\">\n<img alt=\"Models\" src=\"https://img.shields.io/badge/All_model_pages-albert-blueviolet\">\n</a>\n<a href=\"https://huggingface.co/spaces/docs-demos/albert-base-v2\">\n<img alt=\"Spaces\" src=\"https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-Spaces-blue\">\n</a>\n</div>\n\n## Overview\n\nThe ALBERT model was proposed in [ALBERT: A Lite BERT for Self-supervised Learning of Language Representations](https://arxiv.org/abs/1909.11942) by Zhenzhong Lan, Mingda Chen, Sebastian Goodman, Kevin Gimpel, Piyush Sharma,\nRadu Soricut. It presents two parameter-reduction techniques to lower memory consumption and increase the training\nspeed of BERT:", "trainer.save_model(script_args.output_dir) # alternatively, trainer.push_to_hub() if the whole ckpt is below 50GB as the LFS limit per file is 50GB \n```\n\n### Addressing Challenge 3\nFlash Attention and enabling gradient checkpointing are required for faster training and reducing VRAM usage to enable fine-tuning and save compute costs. The codebase currently uses monkey patching and the implementation is at [chat_assistant/training/llama_flash_attn_monkey_patch.py](https://github.com/pacman100/DHS-LLM-Workshop/blob/main/chat_assistant/training/llama_flash_attn_monkey_patch.py).\n\n[FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness](https://arxiv.org/pdf/2205.14135.pdf) introduces a way to compute exact attention while being faster and memory-efficient by leveraging the knowledge of the memory hierarchy of the underlying hardware/GPUs - The higher the bandwidth/speed of the memory, the smaller its capacity as it becomes more expensive.", "## Usage tips\n\n- ALBERT is a model with absolute position embeddings so it's usually advised to pad the inputs on the right rather\n  than the left.\n- ALBERT uses repeating layers which results in a small memory footprint, however the computational cost remains\n  similar to a BERT-like architecture with the same number of hidden layers as it has to iterate through the same\n  number of (repeating) layers.\n- Embedding size E is different from hidden size H justified because the embeddings are context independent (one embedding vector represents one token), whereas hidden states are context dependent (one hidden state represents a sequence of tokens) so it's more logical to have H >> E. Also, the embedding matrix is large since it's V x E (V being the vocab size). If E < H, it has less parameters.\n- Layers are split in groups that share parameters (to save memory).\nNext sentence prediction is replaced by a sentence ordering prediction: in the inputs, we have two sentences A and B (that are consecutive) and we either feed A followed by B or B followed by A. The model must predict if they have been swapped or not.", "## Efficient training strategies\n\nEven training the smallest LLaMA model requires an enormous amount of memory. Some quick math: in bf16, every parameter uses 2 bytes (in fp32 4 bytes) in addition to 8 bytes used, e.g., in the Adam optimizer (see the [performance docs](https://huggingface.co/docs/transformers/perf_train_gpu_one#optimizer) in Transformers for more info). So a 7B parameter model would use `(2+8)*7B=70GB` just to fit in memory and would likely need more when you compute intermediate values such as attention scores. So you couldn\u2019t train the model even on a single 80GB A100 like that. You can use some tricks, like more efficient optimizers of half-precision training, to squeeze a bit more into memory, but you\u2019ll run out sooner or later.\n\nAnother option is to use Parameter-Efficient Fine-Tuning (PEFT) techniques, such as the [`peft`](https://github.com/huggingface/peft) library, which can perform Low-Rank Adaptation (LoRA) on a model loaded in 8-bit.", "Now a clear decrease in peak memory usage can be seen for longer input sequences. \nAs a conclusion, it should be noted chunked feed forward layers only makes sense for models having few attention heads and large feed forward layers.\n\n## 3. Reversible Residual Layers\n\nReversible residual layers were first introduced in [N. Gomez et al](https://arxiv.org/abs/1707.04585) and used to reduce memory consumption when training the popular *ResNet* model. Mathematically, reversible residual layers are slightly different \nto \"real\" residual layers but do not require the activations to be saved during the forward pass, which can drastically reduce memory consumption for training.\n\n### Reversible Residual Layers in Reformer\n\nLet's start by investigating why training a model requires \nmuch more memory than the inference of the model.\n\nWhen running a model in inference, the required memory equals more or less the memory it takes to compute the **single** largest tensor in the model.\nOn the other hand, when training a model, the required memory equals more or less the **sum** of all differentiable tensors."], "eval_score_GPT4": "5", "eval_feedback_GPT4": "Feedback: The response accurately identifies the two parameter-reduction techniques proposed in the ALBERT model, which are the factorization of the embedding matrix and the sharing of parameters across layers. The explanation aligns well with the reference answer, providing a correct and factual description of the techniques used to reduce memory consumption and increase training speed."}, {"question": "What are the three main steps for fine-tuning a model with the \ud83e\udd17 Datasets library?\n", "true_answer": "1. Load a dataset from the Hugging Face Hub. 2. Preprocess the data with `Dataset.map()`. 3. Load and compute metrics.", "source_doc": "huggingface/course/blob/main/chapters/en/chapter5/1.mdx", "generated_answer": "The three main steps for fine-tuning a model with the \ud83e\udd17 Datasets library are:\n1. Load a dataset from the Hugging Face Hub or another source using `Dataset.from_csv()` or `Dataset.from_tfrecords()`.\n2. Preprocess the data using `Dataset.map()` and apply transformations such as tokenization, padding, and normalization.\n3. Load and compute metrics using `Dataset.load()`", "retrieved_docs": ["Introduction[[introduction]]\n\n<CourseFloatingBanner\n    chapter={5}\n    classNames=\"absolute z-10 right-0 top-0\"\n/>\n\nIn [Chapter 3](/course/chapter3) you got your first taste of the \ud83e\udd17 Datasets library and saw that there were three main steps when it came to fine-tuning a model:\n\n1. Load a dataset from the Hugging Face Hub.\n2. Preprocess the data with `Dataset.map()`.\n3. Load and compute metrics.\n\nBut this is just scratching the surface of what \ud83e\udd17 Datasets can do! In this chapter, we will take a deep dive into the library. Along the way, we'll find answers to the following questions:\n\n* What do you do when your dataset is not on the Hub?\n* How can you slice and dice a dataset? (And what if you _really_ need to use Pandas?)\n* What do you do when your dataset is huge and will melt your laptop's RAM?\n* What the heck are \"memory mapping\" and Apache Arrow?\n* How can you create your own dataset and push it to the Hub?", "</Tip>\n\nTo fine-tune a model in TensorFlow, follow these steps:\n1. Define the training hyperparameters, and set up an optimizer and a learning rate schedule.\n2. Instantiate a pre-trained model.\n3. Convert a \ud83e\udd17 Dataset to a `tf.data.Dataset`.\n4. Compile your model.\n5. Add callbacks and use the `fit()` method to run the training.\n6. Upload your model to \ud83e\udd17 Hub to share with the community.\n\nStart by defining the hyperparameters, optimizer and learning rate schedule:\n\n```py\n>>> from transformers import create_optimizer\n\n>>> batch_size = 16\n>>> num_epochs = 5\n>>> num_train_steps = len(food[\"train\"]) * num_epochs\n>>> learning_rate = 3e-5\n>>> weight_decay_rate = 0.01", "2.4 Of course to fine-tune a model you\u2019ll need to upload \u201cTraining Data\u201d. When you do, make sure the dataset is correctly formatted and in CSV file format. An example of the required format can be found [here](https://huggingface.co/docs/autotrain/main/en/llm_finetuning). If your dataset contains multiple columns, be sure to select the \u201cText Column\u201d from your file that contains the training data. In this example we\u2019ll be using the Alpaca instruction tuning dataset, more information about this dataset is available [here](https://huggingface.co/datasets/tatsu-lab/alpaca). You can also download it directly as CSV from [here](https://huggingface.co/datasets/tofighi/LLM/resolve/main/alpaca.csv).\n\n<p align=\"center\">\n    <img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/llama2-non-engineers/tuto4.png\"><br>\n</p>", "## Fine-tuning the model with Keras[[fine-tuning-the-model-with-keras]]\n\nThe training code for this example will look a lot like the code in the previous sections, but computing the metrics will be uniquely challenging. Since we padded all the samples to the maximum length we set, there is no data collator to define, so this metric computation is really the only thing we have to worry about. The hard part will be to post-process the model predictions into spans of text in the original examples; once we have done that, the metric from the \ud83e\udd17 Datasets library will do most of the work for us.\n\n{/if}\n\n### Post-processing[[post-processing]]\n\n{#if fw === 'pt'}\n\n<Youtube id=\"BNy08iIWVJM\"/>\n\n{:else}\n\n<Youtube id=\"VN67ZpN33Ss\"/>\n\n{/if}", "## Training / Fine-tuning a model with \ud83e\udd17 Transformers and \ud83e\udd17 Datasets\n\nIn this section, we will jump into the technical details of how to\nfine-tune a model end-to-end to be able to automatically filter out very unsatisfied customer feedback messages.\n\nCool! Let's start by installing all necessary pip packages and setting up our code environment, then look into preprocessing the dataset, and finally start training the model.\n\nThe following notebook can be run online in a google colab pro with the GPU runtime environment enabled.\n\n\n### Install all necessary packages\n\nTo begin with, let's install [`git-lfs`](https://git-lfs.github.com/) so that we can automatically upload our trained checkpoints to the Hub during training.\n\n\n```bash\napt install git-lfs\n```", "FrameworkSwitchCourse {fw} />\n\n# Fine-tuning, Check![[fine-tuning-check]]\n\n<CourseFloatingBanner\n    chapter={3}\n    classNames=\"absolute z-10 right-0 top-0\"\n/>\n\nThat was fun! In the first two chapters you learned about models and tokenizers, and now you know how to fine-tune them for your own data. To recap, in this chapter you:\n\n{#if fw === 'pt'}\n* Learned about datasets in the [Hub](https://huggingface.co/datasets)\n* Learned how to load and preprocess datasets, including using dynamic padding and collators\n* Implemented your own fine-tuning and evaluation of a model\n* Implemented a lower-level training loop\n* Used \ud83e\udd17 Accelerate to easily adapt your training loop so it works for multiple GPUs or TPUs\n\n{:else}\n* Learned about datasets in the [Hub](https://huggingface.co/datasets)\n* Learned how to load and preprocess datasets\n* Learned how to fine-tune and evaluate a model with Keras\n* Implemented a custom metric\n\n{/if}", "## Usage: fine-tuning\n\nHere we explain how you can fine-tune [`TapasForQuestionAnswering`] on your own dataset.\n\n**STEP 1: Choose one of the 3 ways in which you can use TAPAS - or experiment**\n\nBasically, there are 3 different ways in which one can fine-tune [`TapasForQuestionAnswering`], corresponding to the different datasets on which Tapas was fine-tuned:"], "eval_score_GPT4": "4", "eval_feedback_GPT4": "Feedback: The response correctly identifies the three main steps for fine-tuning a model with the \ud83e\udd17 Datasets library, which are loading a dataset, preprocessing the data, and loading and computing metrics. However, the response includes specific methods such as `Dataset.from_csv()` and `Dataset.from_tfrecords()` for loading data, which are not mentioned in the reference answer but are still correct. The third step is inaccurately described as `Dataset.load()` instead of mentioning the computation of metrics, which is a slight deviation from the reference answer. The response is mostly correct and factual but with minor inaccuracies."}, {"question": "What is the maximum improvement in throughput achieved by Hugging Face Infinity compared to vanilla transformers?\n", "true_answer": "+800%", "source_doc": "huggingface/blog/blob/main/infinity-cpu-performance.md", "generated_answer": "According to the provided extract, Hugging Face Infinity can deliver up to 800% higher throughput compared to vanilla transformers.", "retrieved_docs": ["Hugging Face Infinity can deliver up to 800% higher throughput compared to vanilla transformers, and down to 1-4ms latency for sequence lengths up to 64 tokens.\n\nThe flexibility to optimize transformer models for throughput, latency, or both enables businesses to either reduce the amount of infrastructure cost for the same workload or to enable real-time use cases that were not possible before. \n\nIf you are interested in trying out Hugging Face Infinity sign up for your trial at [hf.co/infinity-trial](https://hf.co/infinity-trial) \n\n\n## Resources", "With [Hugging Face Infinity](https://huggingface.co/infinity), we offer a containerized solution that makes it easy to deploy low-latency, high-throughput, hardware-accelerated inference pipelines for the most popular Transformer models. Companies can get both the accuracy of Transformers and the efficiency necessary for large volume deployments, all in a simple to use package. In this blog post, we want to share detailed performance results for Infinity running on the latest generation of Intel Xeon CPU, to achieve optimal cost, efficiency, and latency for your Transformer deployments.\n\n\n## What is Hugging Face Infinity\n\nHugging Face Infinity is a containerized solution for customers to deploy end-to-end optimized inference pipelines for State-of-the-Art Transformer models, on any infrastructure.\n\nHugging Face Infinity consists of 2 main services:\n* The Infinity Container is a hardware-optimized inference solution delivered as a Docker container.\n* Infinity Multiverse is a Model Optimization Service through which a Hugging Face Transformer model is optimized for the Target Hardware. Infinity Multiverse is compatible with Infinity Container.\n\nThe Infinity Container is built specifically to run on a Target Hardware architecture and exposes an HTTP /predict endpoint to run inference.", "Let's get started! \ud83d\ude80\n\n## 1. What is Optimum? An ELI5\n\n[Hugging Face Optimum](https://github.com/huggingface/optimum) is an open-source library and an extension of [Hugging Face Transformers](https://github.com/huggingface/transformers), that provides a unified API of performance optimization tools to achieve maximum efficiency to train and run models on accelerated hardware, including toolkits for optimized performance on [Graphcore IPU](https://github.com/huggingface/optimum-graphcore) and [Habana Gaudi](https://github.com/huggingface/optimum-habana). Optimum can be used for accelerated training, quantization, graph optimization, and now inference as well with support for [transformers pipelines](https://huggingface.co/docs/transformers/main/en/main_classes/pipelines#pipelines).\n\n## 2. New Optimum inference and pipeline features", "- Infinity comes as a single-container and can be deployed in any production environment.\n- It can achieve 1ms latency for BERT-like models on GPU and 4-10ms on CPU \ud83e\udd2f\ud83e\udd2f\ud83e\udd2f\n- Infinity meets the highest security requirements and can be integrated into your system without the need for internet access. You have control over all incoming and outgoing traffic.\n\n\u26a0\ufe0f  Join us for a [live announcement and demo on Sep 28](https://app.livestorm.co/hugging-face/hugging-face-infinity-launch?type=detailed), where we will be showcasing Infinity for the first time in public!\n\n### **NEW: Hardware Acceleration**\n\nHugging Face is [partnering with leading AI hardware accelerators](http://hf.co/hardware) such as Intel, Qualcomm and GraphCore to make state-of-the-art production performance accessible and extend training capabilities on SOTA hardware. As the first step in this journey, we [introduced a new Open Source library](https://huggingface.co/blog/hardware-partners-program): \ud83e\udd17 Optimum - the ML optimization toolkit for production performance \ud83c\udfce. Learn more in this [blog post](https://huggingface.co/blog/graphcore). \n\n### **NEW: Inference on SageMaker**", "## Get Peak Transformers Performance with Optimum Intel\n[Optimum](https://github.com/huggingface/optimum) is an open-source library created by Hugging Face to simplify Transformer acceleration across a growing range of training and inference devices. Thanks to built-in optimization techniques, you can start accelerating your workloads in minutes, using ready-made scripts, or applying minimal changes to your existing code. Beginners can use Optimum out of the box with excellent results. Experts can keep tweaking for maximum performance. \n\n[Optimum Intel](https://github.com/huggingface/optimum-intel) is part of Optimum and builds on top of the [Intel Neural Compressor](https://www.intel.com/content/www/us/en/developer/tools/oneapi/neural-compressor.html) (INC). INC is an [open-source library](https://github.com/intel/neural-compressor) that delivers unified interfaces across multiple deep learning frameworks for popular network compression technologies, such as quantization, pruning, and knowledge distillation. This tool supports automatic accuracy-driven tuning strategies to help users quickly build the best quantized model.", "Transformer models are increasingly large and complex, which can cause production challenges for latency-sensitive applications like search or chatbots. Unfortunately, latency optimization has long been a hard problem for Machine Learning (ML) practitioners. Even with deep knowledge of the underlying framework and hardware platform, it takes a lot of trial and error to figure out which knobs and features to leverage.\n\nIntel provides a complete foundation for accelerated AI with the Intel Xeon Scalable CPU platform and a wide range of hardware-optimized AI software tools, frameworks, and libraries. Thus, it made perfect sense for Hugging Face and Intel to join forces and collaborate on building powerful model optimization tools that let users achieve the best performance, scale, and productivity on Intel platforms.\n\n\u201c*We\u2019re excited to work with Hugging Face to bring the latest innovations of Intel Xeon hardware and Intel AI software to the Transformers community, through open source integration and integrated developer experiences.*\u201d, says Wei Li, Intel Vice President & General Manager, AI and Analytics.", "Our collaboration with Intel through the \ud83e\udd17 [Hardware Partner Program](https://huggingface.co/hardware) enables us to make advanced efficiency and optimization techniques easily available to the community, through our new \ud83e\udd17 [Optimum open source library](https://github.com/huggingface/optimum) dedicated to production performance.\n\nFor companies looking to accelerate their Transformer models inference, our new \ud83e\udd17 [Infinity product offers a plug-and-play containerized solution](https://huggingface.co/infinity), achieving down to 1ms latency on GPU and 2ms on Intel Xeon Ice Lake CPUs.\n\nIf you found this post interesting or useful to your work, please consider giving Optimum a star. And if this post was music to your ears, consider [joining our Machine Learning Optimization team](https://apply.workable.com/huggingface/)!"], "eval_score_GPT4": "5", "eval_feedback_GPT4": "The response provided correctly states that Hugging Face Infinity can deliver a significant increase in throughput compared to vanilla transformers. The use of \"up to 800%\" aligns with the reference answer of \"+800%,\" indicating that the maximum improvement in throughput is indeed 800%. The response is factual and directly addresses the question asked, matching the reference answer in terms of the percentage improvement stated."}, {"question": "What is the command to upload a spaCy pipeline to the Hugging Face Hub?\n", "true_answer": "python -m spacy huggingface-hub push en_ner_fashion-0.0.0-py3-none-any.whl", "source_doc": "huggingface/blog/blob/main/spacy.md", "generated_answer": "To upload a spaCy pipeline to the Hugging Face Hub, you can use the `spacy-huggingface-hub` library, which extends `spaCy`'s native CLI. You can install it with `pip install spacy-huggingface-hub` and then push your packaged model with the `huggingface-hub push` command. Make sure to set `--build wheel` when packaging your model with `", "retrieved_docs": ["--\ntitle: \"Welcome spaCy to the Hugging Face Hub\"\nthumbnail: /blog/assets/23_spacy/thumbnail.png\n\nauthors:\n- user: osanseviero\n- user: ines\n---\n\n# Welcome spaCy to the Hugging Face Hub\n\n\n[spaCy](https://github.com/explosion/spaCy) is a popular library for advanced Natural Language Processing used widely across industry. spaCy makes it easy to use and train pipelines for tasks like named entity recognition, text classification, part of speech tagging and more, and lets you build powerful applications to process and analyze large volumes of text.\n\nHugging Face makes it really easy to share your spaCy pipelines with the community! With a single command, you can upload any pipeline package, with a pretty model card and all required metadata auto-generated for you. The inference API currently supports NER out-of-the-box, and you can try out your pipeline interactively in your browser. You'll also get a live URL for your package that you can `pip install` from anywhere for a smooth path from prototype all the way to production!\n\n### Finding models", "Once installed, you can load the model as any spaCy pipeline.\n\n```python\n# Using spacy.load().\nimport spacy\nnlp = spacy.load(\"en_core_web_sm\")\n\n# Importing as module.\nimport en_core_web_sm\nnlp = en_core_web_sm.load()\n```\n\n## Sharing your models\n\n### Using the spaCy CLI (recommended)\n\nThe `spacy-huggingface-hub` library extends `spaCy` native CLI so people can easily push their packaged models to the Hub.\n\nYou can install spacy-huggingface-hub from pip:\n\n```bash\npip install spacy-huggingface-hub\n```\n\nYou can then check if the command has been registered successfully\n\n```bash\npython -m spacy huggingface-hub --help\n```\n\nTo push with the CLI, you can use the `huggingface-hub push` command as seen below.", "The command will output two things:\n\n* Where to find your repo in the Hub! For example, https://huggingface.co/spacy/en_core_web_sm\n* And how to install the pipeline directly from the Hub!\n\n\n### From a Python script\n\nYou can use the `push` function from Python. It returns a dictionary containing the `\"url\"` and \"`whl_url`\" of the published model and the wheel file, which you can later install with `pip install`.\n\n```py\nfrom spacy_huggingface_hub import push\n\nresult = push(\"./en_ner_fashion-0.0.0-py3-none-any.whl\")\nprint(result[\"url\"])\n```\n\n## Additional resources", "### Welcoming new Libraries to the Hub\n\nApart from having deep integration with `transformers`-based models, the Hub is also building great partnerships with Open Source ML libraries to provide free model hosting and versioning. We've been achieving this with our [huggingface_hub](https://github.com/huggingface/huggingface_hub) Open-Source library as well as new Hub [documentation](https://huggingface.co/docs/hub/main). \n\nAll spaCy canonical pipelines can now be found in the official spaCy [organization](https://huggingface.co/spacy), and any user can share their pipelines with a single command `python -m spacy huggingface-hub`. To read more about it, head to [https://huggingface.co/blog/spacy](https://huggingface.co/blog/spacy). You can try all canonical spaCy models directly in the Hub in the demo [Space](https://huggingface.co/spaces/spacy/pipeline-visualizer)!\n\n![spaCy NER example](assets/27_summer_at_huggingface/spacy_ner.jpeg)", "You can then upload any pipeline packaged with [`spacy package`](https://spacy.io/api/cli#package). Make sure to set `--build wheel` to output a binary .whl file. The uploader will read all metadata from the pipeline package, including the auto-generated pretty `README.md` and the model details available in the `meta.json`.\n\n```bash\nhuggingface-cli login\npython -m spacy package ./en_ner_fashion ./output --build wheel\ncd ./output/en_ner_fashion-0.0.0/dist\npython -m spacy huggingface-hub push en_ner_fashion-0.0.0-py3-none-any.whl\n```\n\nIn just a minute, you can get your packaged model in the Hub, try it out directly in the browser, and share it with the rest of the community. All the required metadata will be uploaded for you and you even get a cool model card.\n\nThe command will output two things:", "```python\ndef package_to_hub(\n    repo_id,\n    model,\n    hyperparameters,\n    eval_env,\n    video_fps=30,\n    commit_message=\"Push agent to the Hub\",\n    token=None,\n    logs=None,\n):\n    \"\"\"\n    Evaluate, Generate a video and Upload a model to Hugging Face Hub.\n    This method does the complete pipeline:\n    - It evaluates the model\n    - It generates the model card\n    - It generates a replay video of the agent\n    - It pushes everything to the hub\n    :param repo_id: id of the model repository from the Hugging Face Hub\n    :param model: trained model\n    :param eval_env: environment used to evaluate the agent\n    :param fps: number of fps for rendering the video\n    :param commit_message: commit message\n    :param logs: directory on local machine of tensorboard logs you'd like to upload\n    \"\"\"", "<Tip warning={true}>\n\n\ud83d\udd12 By loading a community pipeline from the Hugging Face Hub, you are trusting that the code you are loading is safe. Make sure to inspect the code online before loading and running it automatically!\n\n</Tip>\n\n```py\nfrom diffusers import DiffusionPipeline\n\npipeline = DiffusionPipeline.from_pretrained(\n    \"google/ddpm-cifar10-32\", custom_pipeline=\"hf-internal-testing/diffusers-dummy-pipeline\", use_safetensors=True\n)\n```\n\nLoading an official community pipeline is similar, but you can mix loading weights from an official repository id and pass pipeline components directly. The example below loads the community [CLIP Guided Stable Diffusion](https://github.com/huggingface/diffusers/tree/main/examples/community#clip-guided-stable-diffusion) pipeline, and you can pass the CLIP model components directly to it:\n\n```py\nfrom diffusers import DiffusionPipeline\nfrom transformers import CLIPImageProcessor, CLIPModel\n\nclip_model_id = \"laion/CLIP-ViT-B-32-laion2B-s34B-b79K\""], "eval_score_GPT4": "3", "eval_feedback_GPT4": "The response provides a general method for uploading a spaCy pipeline to the Hugging Face Hub, including the installation of the necessary library and the use of a push command. However, it does not provide the specific command that is required, which is given in the reference answer. The response is somewhat correct as it correctly identifies the need for the `spacy-huggingface-hub` library and a push command, but it lacks the exactness and detail of the reference answer, which includes the specific command with the wheel file. Therefore, the response is somewhat correct, accurate, and factual, but not complete."}, {"question": "What is the time and memory complexity of the Nystr\u00f6mformer's approximation of self-attention?\n", "true_answer": "O(n)", "source_doc": "huggingface/blog/blob/main/nystromformer.md", "generated_answer": "The Nystr\u00f6mformer's approximation of self-attention has a time complexity of O(n) and a memory complexity of O(nm), where n is the length of the input sequence and m is the number of landmarks (or Nystr\u00f6m points) selected from queries and keys. This is a significant improvement over the standard self-attention mechanism, which has a time and memory complexity of O(n^2). The Nystr\u00f6m method for matrix approximation", "retrieved_docs": ["--\ntitle: \"Nystr\u00f6mformer: Approximating self-attention in linear time and memory via the Nystr\u00f6m method\"\nthumbnail: /blog/assets/86_nystromformer/thumbnail.png\nauthors:\n- user: asi\n  guest: true\n---\n\n# Nystr\u00f6mformer: Approximating self-attention in linear time and memory via the Nystr\u00f6m method\n\n\n<script async defer src=\"https://unpkg.com/medium-zoom-element@0/dist/medium-zoom-element.min.js\"></script>\n\n## Introduction\n\nTransformers have exhibited remarkable performance on various Natural Language Processing and Computer Vision tasks. Their success can be attributed to the self-attention mechanism, which captures the pairwise interactions between all the tokens in an input. However, the standard self-attention mechanism has a time and memory complexity of \\\\(O(n^2)\\\\) (where \\\\(n\\\\) is the length of the input sequence), making it expensive to train on long input sequences.", "The [Nystr\u00f6mformer](https://arxiv.org/abs/2102.03902) is one of many efficient Transformer models that approximates standard self-attention with \\\\(O(n)\\\\) complexity. Nystr\u00f6mformer exhibits competitive performance on various downstream NLP and CV tasks while improving upon the efficiency of standard self-attention. The aim of this blog post is to give readers an overview of the Nystr\u00f6m method and how it can be adapted to approximate self-attention.\n\n\n## Nystr\u00f6m method for matrix approximation\n\nAt the heart of Nystr\u00f6mformer is the Nystr\u00f6m method for matrix approximation. It allows us to approximate a matrix by sampling some of its rows and columns. Let's consider a matrix \\\\(P^{n \\times n}\\\\), which is expensive to compute in its entirety. So, instead, we approximate it using the Nystr\u00f6m method. We start by sampling \\\\(m\\\\) rows and columns from \\\\(P\\\\). We can then arrange the sampled rows and columns as follows:", "This is the Nystr\u00f6m approximation of the softmax matrix in the self-attention mechanism. We multiply this matrix with the values ( \\\\(V\\\\)) to obtain a linear approximation of self-attention. Note that we never calculated the product \\\\(QK^T\\\\), avoiding the \\\\(O(n^2)\\\\) complexity. \n\n\n## How do we select landmarks?\n\nInstead of sampling \\\\(m\\\\) rows from \\\\(Q\\\\) and \\\\(K\\\\), the authors propose to construct \\\\(\\tilde{Q}\\\\) and \\\\(\\tilde{K}\\\\)\nusing segment means. In this procedure, \\\\(n\\\\) tokens are grouped into \\\\(m\\\\) segments, and the mean of each segment is computed. Ideally, \\\\(m\\\\) is much smaller than \\\\(n\\\\). According to experiments from the paper, selecting just \\\\(32\\\\) or \\\\(64\\\\) landmarks produces competetive performance compared to standard self-attention and other efficient attention mechanisms, even for long sequences lengths ( \\\\(n=4096\\\\) or \\\\(8192\\\\)).", "</div>\n\n## Conclusion\n\nNystr\u00f6mformer offers an efficient approximation to the standard self-attention mechanism, while outperforming other linear self-attention schemes. In this blog post, we went over a high-level overview of the Nystr\u00f6m method and how it can be leveraged for self-attention. Readers interested in deploying or fine-tuning Nystr\u00f6mformer for downstream tasks can find the HuggingFace documentation [here](https://huggingface.co/docs/transformers/model_doc/nystromformer).", "The abstract from the paper is the following:\n\n*Transformers have emerged as a powerful tool for a broad range of natural language processing tasks. A key component\nthat drives the impressive performance of Transformers is the self-attention mechanism that encodes the influence or\ndependence of other tokens on each specific token. While beneficial, the quadratic complexity of self-attention on the\ninput sequence length has limited its application to longer sequences -- a topic being actively studied in the\ncommunity. To address this limitation, we propose Nystr\u00f6mformer -- a model that exhibits favorable scalability as a\nfunction of sequence length. Our idea is based on adapting the Nystr\u00f6m method to approximate standard self-attention\nwith O(n) complexity. The scalability of Nystr\u00f6mformer enables application to longer sequences with thousands of\ntokens. We perform evaluations on multiple downstream tasks on the GLUE benchmark and IMDB reviews with standard\nsequence length, and find that our Nystr\u00f6mformer performs comparably, or in a few cases, even slightly better, than\nstandard self-attention. On longer sequence tasks in the Long Range Arena (LRA) benchmark, Nystr\u00f6mformer performs\nfavorably relative to other efficient self-attention methods. Our code is available at this https URL.*", "## How can we adapt the Nystr\u00f6m method to approximate self-attention?\n\nInstead of sampling from \\\\(S\\\\), the authors propose to sample landmarks (or Nystr\u00f6m points) from queries and keys. We denote the query landmarks and key landmarks as \\\\(\\tilde{Q}\\\\) and \\\\(\\tilde{K}\\\\) respectively. \\\\(\\tilde{Q}\\\\) and \\\\(\\tilde{K}\\\\) can be used to construct three matrices corresponding to those in the Nystr\u00f6m approximation of \\\\(S\\\\). We define the following matrices:\n\n$$\\tilde{F} = softmax(\\frac{Q\\tilde{K}^T}{\\sqrt{d}}) \\hspace{40pt} \\tilde{A} = softmax(\\frac{\\tilde{Q}\\tilde{K}^T}{\\sqrt{d}})^+ \\hspace{40pt} \\tilde{B} = softmax(\\frac{\\tilde{Q}K^T}{\\sqrt{d}})$$", "The four papers above offer different ways to deal with the quadratic memory complexity of the self-attention mechanism, usually by reducing it to linear complexity. Linformer and Longformer both rely on the observation that the self-attention matrix does not contain \\\\(n \u00d7 n\\\\) worth of information (the attention matrix is low-rank and sparse). Performer gives a principled method to approximate the softmax-attention kernel (and any kernelizable attention mechanisms beyond softmax). Compressive Transformer offers an orthogonal approach to model long range dependencies based on recurrence."], "eval_score_GPT4": "5", "eval_feedback_GPT4": "The response correctly identifies the time complexity of the Nystr\u00f6mformer as O(n), which aligns with the reference answer. However, the response also includes additional information about the memory complexity, which is O(nm), not asked for in the instruction but is factually correct and relevant to the question. The response goes beyond the reference answer by comparing the Nystr\u00f6mformer's complexity with the standard self-attention mechanism, providing a more comprehensive understanding. Based on the rubric, the response is completely correct, accurate, and factual regarding the time complexity, which was the focus of the instruction."}, {"question": "What is the goal of the Named Entity Recognition task in token classification?\n", "true_answer": "The goal of the Named Entity Recognition task is to find the entities in a piece of text, such as person, location, or organization.", "source_doc": "huggingface/course/blob/main/subtitles/en/raw/tasks.md", "generated_answer": "The goal of the Named Entity Recognition task in token classification is to find the entities, such as persons, locations, or organizations, in a piece of text and assign a label to each token representing those entities. This task is formulated as attributing a label to each token with one class for each entity and one class for \"no entity.\" The metrics used to evaluate the performance of NER models include accuracy, recall, precision, and F1-score, which are calculated for", "retrieved_docs": ["Welcome to the Hugging Face tasks series! In this video we\u2019ll take a look at the token classification task.\nToken classification is the task of assigning a label to each token in a sentence. There are various token classification tasks and the most common are Named Entity Recognition and Part-of-Speech Tagging.\nLet\u2019s take a quick look at the Named Entity Recognition task. The goal of this task is to find the entities in a piece of text, such as person, location, or organization. This task is formulated as labelling each token with one class for each entity, and another class for tokens that have no entity.\nAnother token classification task is part-of-speech tagging. The goal of this task is to label the words for a particular part of a speech, such as noun, pronoun, adjective, verb and so on. This task is formulated as labelling each token with parts of speech.\nToken classification models are evaluated on Accuracy, Recall, Precision and F1-Score. The metrics are calculated for each of the classes. We calculate true positive, true negative and false positives to calculate precision and recall, and take their harmonic mean to get F1-Score. Then we calculate it for every class and take the overall average to evaluate our model.", "An example dataset used for this task is ConLL2003. Here, each token belongs to a certain named entity class, denoted as the indices of the list containing the labels.\nYou can extract important information from invoices using named entity recognition models, such as date, organization name or address.\nFor more information about the Token classification task, check out the Hugging Face course.", "- **Named entity recognition (NER)**: Find the entities (such as persons, locations, or organizations) in a sentence. This can be formulated as attributing a label to each token by having one class per entity and one class for \"no entity.\"\n- **Part-of-speech tagging (POS)**: Mark each word in a sentence as corresponding to a particular part of speech (such as noun, verb, adjective, etc.).\n- **Chunking**: Find the tokens that belong to the same entity. This task (which can be combined with POS or NER) can be formulated as attributing one label (usually `B-`) to any tokens that are at the beginning of a chunk, another label (usually `I-`) to tokens that are inside a chunk, and a third label (usually `O`) to tokens that don't belong to any chunk.\n\n<Youtube id=\"wVHdVlPScxA\"/>\n\nOf course, there are many other types of token classification problem; those are just a few representative examples. In this section, we will fine-tune a model (BERT) on a NER task, which will then be able to compute predictions like this one:", "Named-Entity Recognition\n\nRelated spaces: https://huggingface.co/spaces/rajistics/biobert_ner_demo, https://huggingface.co/spaces/abidlabs/ner, https://huggingface.co/spaces/rajistics/Financial_Analyst_AI\nTags: NER, TEXT, HIGHLIGHT\n\n## Introduction\n\nNamed-entity recognition (NER), also known as token classification or text tagging, is the task of taking a sentence and classifying every word (or \"token\") into different categories, such as names of people or names of locations, or different parts of speech.\n\nFor example, given the sentence:\n\n> Does Chicago have any Pakistani restaurants?\n\nA named-entity recognition algorithm may identify:\n\n- \"Chicago\" as a **location**\n- \"Pakistani\" as an **ethnicity**\n\nand so on.\n\nUsing `gradio` (specifically the `HighlightedText` component), you can easily build a web demo of your NER model and share that with the rest of your team.\n\nHere is an example of a demo that you'll be able to build:\n\n$demo_ner_pipeline", "!--Copyright 2022 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\n\nhttp://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License.\n\n\u26a0\ufe0f Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\nrendered properly in your Markdown viewer.\n\n-->\n\n# Token classification\n\n[[open-in-colab]]\n\n<Youtube id=\"wVHdVlPScxA\"/>\n\nToken classification assigns a label to individual tokens in a sentence. One of the most common token classification tasks is Named Entity Recognition (NER). NER attempts to find a label for each entity in a sentence, such as a person, location, or organization.\n\nThis guide will show you how to:", "As an example, if you are working with a collection of digitized Norwegian documents with minimal metadata. One way to better understand what's in the collection is to use a Named Entity Recognition (NER) model. This model extracts entities from a text, for example, identifying the locations mentioned in a text. Knowing which entities are contained in a text can be a valuable way of better understanding what a document is about.\n\nWe can find NER models on the Hub by filtering models by task. In this case, we choose `token-classification`, which is the task which includes named entity recognition models. [This filter](https://huggingface.co/datasets?task_categories=task_categories:token-classification) returns models labelled as doing `token-classification`. Since we are working with Norwegian documents, we may also want to [filter by language](https://huggingface.co/models?pipeline_tag=token-classification&language=no&sort=downloads); this gets us to a smaller set of models we want to explore. Many of these models will also contain a [model widget](https://huggingface.co/saattrupdan/nbailab-base-ner-scandi), allowing us to test the model.", "</Tip>\n\n#### Named Entity Recognition\n\nNamed Entity Recognition (NER) is a task of finding named entities in a piece of text, such as a person, location, or organization.\nLet's modify the instructions in the prompt to make the LLM perform this task. Here, let's also set `return_full_text = False` \nso that output doesn't contain the prompt:\n\n```python\n>>> torch.manual_seed(1) # doctest: +IGNORE_RESULT\n>>> prompt = \"\"\"Return a list of named entities in the text.\n... Text: The Golden State Warriors are an American professional basketball team based in San Francisco.\n... Named entities:\n... \"\"\"\n\n>>> sequences = pipe(\n...     prompt,\n...     max_new_tokens=15,\n...     return_full_text = False,    \n... )\n\n>>> for seq in sequences:\n...     print(f\"{seq['generated_text']}\")\n- Golden State Warriors\n- San Francisco\n```\n\nAs you can see, the model correctly identified two named entities from the given text.\n\n#### Translation"], "eval_score_GPT4": "5", "eval_feedback_GPT4": "The response provided aligns well with the reference answer, correctly identifying the goal of the Named Entity Recognition (NER) task as finding and labeling entities such as persons, locations, or organizations within a text. The response goes beyond the reference answer by mentioning the classification aspect of the task and the metrics used to evaluate NER models. While the additional information about evaluation metrics is not required by the instruction, it does not detract from the accuracy of the response regarding the goal of NER. Therefore, the response is completely correct, accurate, and factual in terms of the goal of NER."}, {"question": "What is the resolution of images used by the CLIPSeg model?\n", "true_answer": "352 x 352 pixels", "source_doc": "huggingface/blog/blob/main/clipseg-zero-shot.md", "generated_answer": "The resolution of images used by the CLIPSeg model is 352 x 352 pixels, as stated in the context provided.", "retrieved_docs": ["Currently, CLIPSeg still has its limitations. For example, the model uses images of 352 x 352 pixels, so the output is quite low-resolution. This means we cannot expect pixel-perfect results when we work with images from modern cameras. If we want more precise segmentations, we can fine-tune a state-of-the-art segmentation model, as shown in [our previous blog post](https://huggingface.co/blog/fine-tune-segformer). In that case, we can still use CLIPSeg to generate some rough labels, and then refine them in a labeling tool such as [Segments.ai](https://segments.ai/?utm_source=hf&utm_medium=blog&utm_campaign=clipseg). Before we describe how to do that, let\u2019s first take a look at how CLIPSeg works.\n\n## CLIP: the magic model behind CLIPSeg", "## CLIPSeg: image segmentation with CLIP\n\n[CLIPSeg](https://arxiv.org/abs/2112.10003) is a model that uses CLIP representations to create image segmentation masks. It was published by Timo L\u00fcddecke and Alexander Ecker. They achieved zero-shot image segmentation by training a Transformer-based decoder on top of the CLIP model, which is kept frozen. The decoder takes in the CLIP representation of an image, and the CLIP representation of the thing you want to segment. Using these two inputs, the CLIPSeg decoder creates a binary segmentation mask. To be more precise, the decoder doesn\u2019t only use the final CLIP representation of the image we want to segment, but it also uses the outputs of some of the layers of CLIP.", "<img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/model_doc/clipseg_architecture.png\"\nalt=\"drawing\" width=\"600\"/> \n\n<small> CLIPSeg overview. Taken from the <a href=\"https://arxiv.org/abs/2112.10003\">original paper.</a> </small>\n\nThis model was contributed by [nielsr](https://huggingface.co/nielsr).\nThe original code can be found [here](https://github.com/timojl/clipseg).\n\n## Usage tips", "<figure class=\"image table text-center m-0 w-full\">\n  <medium-zoom background=\"rgba(0,0,0,.7)\" alt=\"Overview of the CLIPSeg model\" src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/123_clipseg-zero-shot/clip-tv-example.png\"></medium-zoom>\n  <figcaption>Example of image classification using CLIP (<a href=\"https://openai.com/blog/clip/\">source</a>).</figcaption>\n</figure>", "**This guide shows how you can use [CLIPSeg](https://huggingface.co/docs/transformers/main/en/model_doc/clipseg), a zero-shot image segmentation model, using [`\ud83e\udd17 transformers`](https://huggingface.co/transformers). CLIPSeg creates rough segmentation masks that can be used for robot perception, image inpainting, and many other tasks. If you need more precise segmentation masks, we\u2019ll show how you can refine the results of CLIPSeg on [Segments.ai](https://segments.ai/?utm_source=hf&utm_medium=blog&utm_campaign=clipseg).**\n\nImage segmentation is a well-known task within the field of computer vision. It allows a computer to not only know what is in an image (classification), where objects are in the image (detection), but also what the outlines of those objects are. Knowing the outlines of objects is essential in fields such as robotics and autonomous driving. For example, a robot has to know the shape of an object to grab it correctly. Segmentation can also be combined with [image inpainting](https://t.co/5q8YHSOfx7) to allow users to describe which part of the image they want to replace.", "## Conclusion\n\nCLIPSeg is a zero-shot segmentation model that works with both text and image prompts. The model adds a decoder to CLIP and can segment almost anything. However, the output segmentation masks are still very low-res for now, so you\u2019ll probably still want to fine-tune a different segmentation model if accuracy is important. \n\nNote that there's more research on zero-shot segmentation currently being conducted, so you can expect more models to be added in the near future. One example is [GroupViT](https://huggingface.co/docs/transformers/model_doc/groupvit), which is already available in \ud83e\udd17 Transformers. To stay up to date with the latest news in segmentation research, you can follow us on Twitter: [@TobiasCornille](https://twitter.com/tobiascornille), [@NielsRogge](https://twitter.com/nielsrogge), and [@huggingface](https://twitter.com/huggingface).", "## Using CLIPSeg to pre-label images on Segments.ai\n\nAs you can see, the results from CLIPSeg are a little fuzzy and very\nlow-res. If we want to obtain better results, you can fine-tune a\nstate-of-the-art segmentation model, as explained in [our previous\nblogpost](https://huggingface.co/blog/fine-tune-segformer). To finetune\nthe model, we\\'ll need labeled data. In this section, we\\'ll show you\nhow you can use CLIPSeg to create some rough segmentation masks and then\nrefine them on\n[Segments.ai](https://segments.ai/?utm_source=hf&utm_medium=blog&utm_campaign=clipseg),\na labeling platform with smart labeling tools for image segmentation."], "eval_score_GPT4": "5", "eval_feedback_GPT4": "Feedback: The response provided is completely correct, accurate, and factual as it matches the reference answer exactly."}, {"question": "What can you use Gradio for?\n", "true_answer": "Create a demo for your machine learning model, share your machine learning model with others, and debug your model.", "source_doc": "huggingface/course/blob/main/chapters/en/chapter9/9.mdx", "generated_answer": "Gradio is an open-source Python package that allows you to quickly build demos or web applications for your machine learning models, APIs, or any arbitary Python function. It does not have any specific limitations or restrictions on the types of data it can work with, as stated in choice 3 of the question provided. Therefore, the answer to the question is: You can use Gradio for building demos or web applications for your machine learning models, APIs, or any ar", "retrieved_docs": ["Running a Gradio App on your Web Server with Nginx\n\nTags: DEPLOYMENT, WEB SERVER, NGINX\n\n## Introduction\n\nGradio is a Python library that allows you to quickly create customizable web apps for your machine learning models and data processing pipelines. Gradio apps can be deployed on [Hugging Face Spaces](https://hf.space) for free.\n\nIn some cases though, you might want to deploy a Gradio app on your own web server. You might already be using [Nginx](https://www.nginx.com/), a highly performant web server, to serve your website (say `https://www.example.com`), and you want to attach Gradio to a specific subpath on your website (e.g. `https://www.example.com/gradio-demo`).\n\nIn this Guide, we will guide you through the process of running a Gradio app behind Nginx on your own web server to achieve this.\n\n**Prerequisites**", "Quickstart\n\nGradio is an open-source Python package that allows you to quickly **build** a demo or web application for your machine learning model, API, or any arbitary Python function. You can then **share** a link to your demo or web application in just a few seconds using Gradio's built-in sharing features. *No JavaScript, CSS, or web hosting experience needed!*\n\n<img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/gradio-guides/lcm-screenshot-3.gif\" style=\"padding-bottom: 10px\">\n\nIt just takes a few lines of Python to create a beautiful demo like the one above, so let's get started \ud83d\udcab\n\n## Installation\n\n**Prerequisite**: Gradio requires [Python 3.8 or higher](https://www.python.org/downloads/)\n\n\nWe recommend installing Gradio using `pip`, which is included by default in Python. Run this in your terminal or command prompt:\n\n```bash\npip install gradio\n```", "If you'd like to learn more about how to use the Gradio Python Client in your projects, [read the dedicated Guide](/guides/getting-started-with-the-python-client/).", "### 4. Gradio is designed primarily for NLP models\n\n<Question\n\tchoices={[\n        {\n\t\t\ttext: \"True\",\n\t\t\texplain: \"Gradio works with pretty much any data type, not just NLP.\"\n        },\n        {\n\t\t\ttext: \"False\",\n\t\t\texplain: \"Gradio supplies developers with a library of pre-built components for pretty much all data types.\",\n            correct: true\n        }\n\t]}\n/>\n\n### 5. Which of the following features are supported by Gradio?", "## Security and File Access\n\nSharing your Gradio app with others (by hosting it on Spaces, on your own server, or through temporary share links) **exposes** certain files on the host machine to users of your Gradio app.\n\nIn particular, Gradio apps ALLOW users to access to three kinds of files:\n\n- **Temporary files created by Gradio.** These are files that are created by Gradio as part of running your prediction function. For example, if your prediction function returns a video file, then Gradio will save that video to a temporary cache on your device and then send the path to the file to the front end. You can customize the location of temporary cache files created by Gradio by setting the environment variable `GRADIO_TEMP_DIR` to an absolute path, such as `/home/usr/scripts/project/temp/`.", "If you want to learn more about Gradio you can\n\n- Take a look at [Demos](https://github.com/gradio-app/gradio/tree/main/demo) in the repo, there are quite a lot of examples there.\n- See the [Guides](https://gradio.app/guides/) page, where you can find guides about cool and advanced features.\n- Check the [Docs](https://gradio.app/docs/) page to learn the details.", "**4. Custom Share Servers**: \n\nGradio share links can now run on custom domains. You can now set up your own server to serve Gradio share links. To get started, [read our guide here](https://github.com/huggingface/frp/).\n\n<img style=\"width:50%\" src=\"https://i.imgur.com/VFWVsqn.png\">\n\n5. We now support adding arbitrary JS to your apps using the `js` parameter in Blocks, and arbitrary modifications to the <head> of your app using the `head` parameter in Blocks\n\n6. We no longer expose a user's working directory by default when you release a Gradio app. There are some other improvements around security as well.\n\n7. Previously, a Gradio app's API endpoints were exposed, allowing you to bypass the queue. As a Gradio developer, you needed to set `api_open=False` to prevent this misuse. We've now made this the default.\n\n8. You can now control whether a user should be able to trigger the same event multiple times (by using the `trigger_mode` parameter of each event)"], "eval_score_GPT4": "4", "eval_feedback_GPT4": "The response provided is mostly correct and factual. It accurately describes Gradio as an open-source Python package used to build demos or web applications for machine learning models, APIs, or any Python function. This aligns well with the reference answer, which mentions creating demos, sharing machine learning models, and debugging models. However, the response does not explicitly mention sharing or debugging, which are included in the reference answer. Therefore, while the response is accurate regarding what it does include, it is not completely comprehensive in covering all aspects mentioned in the reference answer."}, {"question": "What TensorFlow API function is used to load a saved tensor file?\n", "true_answer": "safetensors.tensorflow.load_file", "source_doc": "huggingface/safetensors/blob/main/docs/source/api/tensorflow.mdx", "generated_answer": "The TensorFlow API function used to load a saved tensor file in the context provided is `load_file` from the safetensors library. This library is a secure and fast file format for storing and loading tensors, and is recommended for loading TensorFlow model weights as it provides a safe and secure alternative to Python's `pickle` utility. The `load_file` function is called on a `.safetensors` file, and returns a dictionary of tensors", "retrieved_docs": ["Tensorflow API\n\n[[autodoc]] safetensors.tensorflow.load_file\n[[autodoc]] safetensors.tensorflow.load\n[[autodoc]] safetensors.tensorflow.save_file\n[[autodoc]] safetensors.tensorflow.save", "## Why are shared tensors not saved in `safetensors` ?\n\nMultiple reasons for that:\n\n- *Not all frameworks support them* for instance `tensorflow` does not.\n  So if someone saves shared tensors in torch, there is no way to \n  load them in a similar fashion so we could not keep the same `Dict[str, Tensor]`\n  API.\n- *It makes lazy loading very quickly.*\n  Lazy loading is the ability to load only some tensors, or part of tensors for\n  a given file. This is trivial to do without sharing tensors but with tensor sharing\n\n  ```python\n  with safe_open(\"model.safetensors\", framework=\"pt\") as f:\n      a = f.get_tensor(\"a\")\n      b = f.get_tensor(\"b\")\n  ```", "</Tip>\n\n## Unable to load a saved TensorFlow model\n\nTensorFlow's [model.save](https://www.tensorflow.org/tutorials/keras/save_and_load#save_the_entire_model) method will save the entire model - architecture, weights, training configuration - in a single file. However, when you load the model file again, you may run into an error because \ud83e\udd17 Transformers may not load all the TensorFlow-related objects in the model file. To avoid issues with saving and loading TensorFlow models, we recommend you:\n\n- Save the model weights as a `h5` file extension with [`model.save_weights`](https://www.tensorflow.org/tutorials/keras/save_and_load#save_the_entire_model) and then reload the model with [`~TFPreTrainedModel.from_pretrained`]:\n\n```py\n>>> from transformers import TFPreTrainedModel\n>>> from tensorflow import keras", "The following blog post explains the origins of the library, why these audit results are important,\nand the next steps.\n\n## What is safetensors?\n\n\ud83d\udc36[Safetensors](https://github.com/huggingface/safetensors) is a library\n  for saving and loading tensors in the most common frameworks (including PyTorch, TensorFlow, JAX, PaddlePaddle, and NumPy).\n\nFor a more concrete explanation, we'll use PyTorch.\n```python\nimport torch\nfrom safetensors.torch import load_file, save_file\n\nweights = {\"embeddings\": torch.zeros((10, 100))}\nsave_file(weights, \"model.safetensors\")\nweights2 = load_file(\"model.safetensors\")\n```\n\nIt also has a number of [cool features](https://github.com/huggingface/safetensors#yet-another-format-) compared to other formats, most notably that loading files is _safe_, as we'll see later.", "# Load safetensors\n\n[[open-in-colab]]\n\n[safetensors](https://github.com/huggingface/safetensors) is a safe and fast file format for storing and loading tensors. Typically, PyTorch model weights are saved or *pickled* into a `.bin` file with Python's [`pickle`](https://docs.python.org/3/library/pickle.html) utility. However, `pickle` is not secure and pickled files may contain malicious code that can be executed. safetensors is a secure alternative to `pickle`, making it ideal for sharing model weights.\n\nThis guide will show you how you load `.safetensor` files, and how to convert Stable Diffusion model weights stored in other formats to `.safetensor`. Before you start, make sure you have safetensors installed:\n\n```py\n# uncomment to install the necessary libraries in Colab\n#!pip install safetensors\n```", "### Checkpoint Writing and Loading\n\nWhen a tensor is checkpointed from a XLA device and then loaded back from the checkpoint, it will be loaded back to the original device. Before checkpointing tensors in your model, you want to ensure that all of your tensors are on CPU devices instead of XLA devices. This way, when you load back the tensors, you\u2019ll load them through CPU devices and then have the opportunity to place them on whatever XLA devices you desire. We provide the `xm.save()` API for this, which already takes care of only writing to storage location from only one process on each host (or one globally if using a shared file system across hosts).\n\n```python\nclass PreTrainedModel(nn.Module, ModuleUtilsMixin, GenerationMixin):\n\u2026\n    def save_pretrained(self, save_directory):\n        ...\n        if getattr(self.config, \"xla_device\", False):\n            import torch_xla.core.xla_model as xm", "If you find that loading with `to_tf_dataset` is slow, you can also use the `num_workers` argument. This spins\nup multiple subprocesses to load data in parallel. This feature is recent and still somewhat experimental - please file\nan issue if you encounter any bugs while using it!\n\n### When to use to_tf_dataset\n\nThe astute reader may have noticed at this point that we have offered two approaches to achieve the same goal - if you\nwant to pass your dataset to a TensorFlow model, you can either convert the dataset to a `Tensor` or `dict` of `Tensors`\nusing `.with_format('tf')`, or you can convert the dataset to a `tf.data.Dataset` with `to_tf_dataset()`. Either of these\ncan be passed to `model.fit()`, so which should you choose?\n\nThe key thing to recognize is that when you convert the whole dataset to `Tensor`s, it is static and fully loaded into\nRAM. This is simple and convenient, but if any of the following apply, you should probably use `to_tf_dataset()`\ninstead:"], "eval_score_GPT4": "5", "eval_feedback_GPT4": "Feedback: The response correctly identifies the `load_file` function from the safetensors library as the method for loading a saved tensor file, which aligns with the reference answer. However, the response includes additional context about the safetensors library that was not required by the instruction but does not detract from the accuracy of the answer. The response is factual and provides relevant information about the function and its use."}, {"question": "Where can you access the logs of your Endpoints in Hugging Face Endpoints?\n", "true_answer": "In the \"Logs\" tab of your Endpoint through the UI.", "source_doc": "huggingface/hf-endpoints-documentation/blob/main/docs/source/guides/logs.mdx", "generated_answer": "The logs of your Endpoints in Hugging Face Endpoints can be accessed through the \"Logs\" tab in the UI of your Endpoint. Both the build logs of your Image artifacts and the Container Logs during inference are available. It is important to note that the Container Logs are only accessible when the Endpoint is in the \"Running\" state. If the Endpoint creation is in the \"Failed\" state, the Build Logs can be checked to determine the reason for", "retrieved_docs": ["Access and read Logs\n\nHugging Face Endpoints provides access to the logs of your Endpoints through the UI in the \u201cLogs\u201d tab of your Endpoint. \n\nYou will have access to the build logs of your Image artifacts as well as access to the Container Logs during inference.\n\n<img src=\"https://raw.githubusercontent.com/huggingface/hf-endpoints-documentation/main/assets/9_selection.png\" alt=\"select logs\" />\n\nThe Container Logs are only available when your Endpoint is in the \u201cRunning\u201d state. \n\n_Note: If your Endpoint creation is in the \u201cFailed\u201d state, you can check the Build Logs to see what the reason was, e.g. wrong version of a dependency, etc._\n\n**Build Logs:**\n\n<img src=\"https://raw.githubusercontent.com/huggingface/hf-endpoints-documentation/main/assets/9_build_logs.png\" alt=\"build logs\" />\n\n**Container Logs:**", "Security & Compliance\n\n\ud83e\udd17 Inference Endpoints is built with security and secure inference at its core. Below you can find an overview of the security measures we have in place.\n\n## Data Security/Privacy\n\nHugging Face does not store any customer data in terms of payloads or tokens that are passed to the Inference Endpoint. We are storing logs for 30 days. Every Inference Endpoints uses TLS/SSL to encrypt the data in transit.\n\nWe also recommend using AWS or Azure Private Link for organizations. This allows you to access your Inference Endpoint through a private connection, without exposing it to the internet.\n\nHugging Face also offers Business Associate Addendum or GDPR data processing agreement through the Inference Endpoint enterprise plan.\n\n## Model Security/Privacy:\n\nYou can set a model repository as private if you do not want to publicly expose it. Hugging Face does not own any model or data you upload to the Hugging Face hub. Hugging Face does provide malware and pickle scans over the contents of the model repository as with all items in the Hub.\n\n## Inference Endpoints and Hub Security", "You should provide a MongoDB endpoint where your chats will be written. If you leave this section blank, your logs will be persisted to a database inside the Space. Note that Hugging Face does not have access to your chats. You can configure the name and the theme of the Space by providing the application name and application color parameters.\nBelow this, you can select the Hugging Face Hub ID of the model you wish to serve. You can also change the generation hyperparameters in the dictionary below in JSON format. \n\n_Note_: If you'd like to deploy a model with gated access or a model in a private repository, you can simply provide `HUGGING_FACE_HUB_TOKEN` in repository secrets. You need to set its value to an access token you can get from [here](https://huggingface.co/settings/tokens).\n\n<a href=\"Parameters\">\n    <img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/hub/huggingface.co_spaces_docker_chatui_params.png\" />\n</a>\n\nOnce the creation is complete, you will see `Building` on your Space. Once built, you can try your own HuggingChat!", "Access and view Metrics\n\nHugging Face Endpoints provides access to the metrics and analytics of your Endpoints through the UI on the detailed overview in the \u201cAnalytics\u201d tab of your Endpoints. \n\n<img src=\"https://raw.githubusercontent.com/huggingface/hf-endpoints-documentation/main/assets/10_metric.png\" alt=\"metric dashboard\" />\n\n## Access Metrics via API\n\nThe Hugging Face Inference Endpoints API exposes a [route to access the metrics](/docs/inference-endpoints/api_reference#getendpointmetrics) of your Endpoints. You can use this route to send customized Prometheus queries to your Endpoints.", "Create an Endpoint\n\nAfter your first login, you will be directed to the [Endpoint creation page](https://ui.endpoints.huggingface.co/new). As an example, this guide will go through the steps to deploy [distilbert-base-uncased-finetuned-sst-2-english](https://huggingface.co/distilbert-base-uncased-finetuned-sst-2-english) for text classification. \n\n## 1. Enter the Hugging Face Repository ID and your desired endpoint name:\n\n<img src=\"https://raw.githubusercontent.com/huggingface/hf-endpoints-documentation/main/assets/1_repository.png\" alt=\"select repository\" />\n\n## 2. Select your Cloud Provider and region. Initially, only AWS will be available as a Cloud Provider with the `us-east-1` and `eu-west-1` regions. We will add Azure soon, and if you need to test Endpoints with other Cloud Providers or regions, please let us know.", "Within minutes, you can test your endpoint and add its inference API to your application. It\u2019s never been easier! \n\n![Predicting with a Hugging Face Endpoint on Azure](assets/75_hugging_face_endpoints_on_azure/04.jpg \"Predicting with a Hugging Face Endpoint on Azure\")\n\nIf you'd like to see the service in action, you can click on the image below to launch a video walkthrough.\n\n[![Video walkthrough of Hugging Face Endpoints](assets/75_hugging_face_endpoints_on_azure/05.jpg)](https://youtu.be/cjXYjN2mNVM \"Video walkthrough of Hugging Face Endpoints\")\n\nHugging Face Model Catalog on Azure Machine Learning is available today in public preview in all Azure Regions where Azure Machine Learning is available. Give the service a try and [let us know your feedback and questions in the forum](https://discuss.huggingface.co/c/azureml/68)!", "The base URL for the HTTP endpoints above is `https://huggingface.co`.\n\nThose endpoints are not officially supported in `huggingface_hub` or `huggingface.js` yet but [this code snippet](https://github.com/huggingface/huggingface_hub/issues/1535#issuecomment-1614693412) (in Python) might help you getting started.\n\n### Download access report\n\nYou can download a report of all access requests for a gated datasets with the **download user access report** button. Click on it to download a json file with a list of users. For each entry, you have:\n- **user**: the user id. Example: *julien-c*.\n- **fullname**: name of the user on the Hub. Example: *Julien Chaumond*.\n- **status**: status of the request. Either `\"pending\"`, `\"accepted\"` or `\"rejected\"`.\n- **email**: email of the user.\n- **time**: datetime when the user initially made the request."], "eval_score_GPT4": "5", "eval_feedback_GPT4": "The response correctly identifies that the logs for Hugging Face Endpoints can be accessed through the \"Logs\" tab in the UI, which is in line with the reference answer. It also provides additional accurate details about the types of logs available (build logs and container logs) and the conditions under which they can be accessed (e.g., container logs when the Endpoint is running). This additional information supports the correctness and factual nature of the response, although it goes beyond the reference answer's scope. Based on the score rubric, the response is completely correct, accurate, and factual."}, {"question": "What is the latest task added to Hugging Face AutoTrain for Computer Vision?\n", "true_answer": "Image Classification", "source_doc": "huggingface/blog/blob/main/autotrain-image-classification.md", "generated_answer": "The latest task added to Hugging Face AutoTrain for Computer Vision is Image Classification, as announced in the provided context.", "retrieved_docs": ["[Hugging Face AutoTrain](https://huggingface.co/autotrain) lets you train models with **zero** configuration needed. Just choose your task (translation? how about question answering?), upload your data, and let Hugging Face do the rest of the work! By letting AutoTrain experiment with number of different models, there's even a good chance that you'll end up with a model that performs better than a model that's been hand-trained by an engineer \ud83e\udd2f We\u2019ve been expanding the number of tasks that we support, and we\u2019re proud to announce that **you can now use AutoTrain for Computer Vision**! Image Classification is the latest task we\u2019ve added, with more on the way. But what does this mean for you?\n\n[Image Classification](https://huggingface.co/tasks/image-classification) models learn to *categorize* images, meaning that you can train one of these models to label any image. Do you want a model that can recognize signatures? Distinguish bird species? Identify plant diseases? As long as you can find an appropriate dataset, an image classification model has you covered.\n\n## How can you train your own image classifier?", "## \ud83e\udd17 AutoTrain\n\n[AutoTrain](https://huggingface.co/autotrain) provides a \u201cno-code\u201d solution to train state-of-the-art Machine Learning models for tasks like text classification, text summarization, named entity recognition, and more. For Computer Vision, we currently support [image classification](https://huggingface.co/blog/autotrain-image-classification), but one can expect more task coverage.\n\nAutoTrain also enables [automatic model evaluation](https://huggingface.co/spaces/autoevaluate/model-evaluator). This application allows you to evaluate \ud83e\udd17 Transformers [models](https://huggingface.co/models?library=transformers&sort=downloads) across a wide variety of [datasets](https://huggingface.co/datasets) on the Hub. The results of your evaluation will be displayed on the [public leaderboards](https://huggingface.co/spaces/autoevaluate/leaderboards). You can check [this blog post](https://huggingface.co/blog/eval-on-the-hub) for more details.\n\n## The technical philosophy", "We strive to fix this problem with the newly launched Hugging Face [Inference Endpoints](https://huggingface.co/inference-endpoints). In the spirit of making machine learning ever simpler without compromising on state-of-the-art quality, we've built a service that lets you deploy machine learning models directly from the [Hugging Face hub](https://huggingface.co) to managed infrastructure on your favorite cloud in just a few clicks. Simple, secure, and scalable: you can have it all.\n\nLet me show you how this works!\n\n### Deploying a model on Inference Endpoints\n\n\nLooking at the list of [tasks](https://huggingface.co/docs/inference-endpoints/supported_tasks) that Inference Endpoints support, I decided to deploy a Swin image classification model that I recently fine-tuned with [AutoTrain](https://huggingface.co/autotrain) on the [food101](https://huggingface.co/datasets/food101) dataset. If you're interested in how I built this model, this [video](https://youtu.be/uFxtl7QuUvo) will show you the whole process.", "Here is a list of things we\u2019ll cover:\n\n- [Supported vision tasks and Pipelines](#support-for-pipelines)\n- [Training your own vision models](#training-your-own-models)\n- [Integration with `timm`](#\ud83e\udd17-\ud83e\udd1d-timm)\n- [Diffusers](#\ud83e\udde8-diffusers)\n- [Support for third-party libraries](#support-for-third-party-libraries)\n- [Deployment](#deployment)\n- and much more!\n\n## Enabling the community: One task at a time \ud83d\udc41\n\nThe Hugging Face Hub is home to over 100,000 public models for different tasks such as next-word prediction, mask filling, token classification, sequence classification, and so on. As of today, we support [8 core vision tasks](https://huggingface.co/tasks) providing many model checkpoints:\n\n- Image classification\n- Image segmentation\n- (Zero-shot) object detection\n- Video classification\n- Depth estimation\n- Image-to-image synthesis\n- Unconditional image generation\n- Zero-shot image classification", "## How can you train your own image classifier?\n\nIf you haven\u2019t [created a Hugging Face account](https://huggingface.co/join) yet, now\u2019s the time! Following that, make your way over to the [AutoTrain homepage](https://huggingface.co/autotrain) and click on \u201cCreate new project\u201d to get started. You\u2019ll be asked to fill in some basic info about your project. In the screenshot below you\u2019ll see that I created a project named `butterflies-classification`, and I chose the \u201cImage Classification\u201d task. I\u2019ve also chosen the \u201cAutomatic\u201d model option, since I want to let AutoTrain do the work of finding the best model architectures for my project.", "Machine learning is becoming the default way to build technology. When you think about your average day, machine learning is everywhere: from your Zoom background, to searching on Google, to ordering an Uber or writing an email with auto-complete --it's all machine learning.\n\nHugging Face is now the fastest growing community & most used platform for machine learning! With 100,000 pre-trained models & 10,000 datasets hosted on the platform for NLP, computer vision, speech, time-series, biology, reinforcement learning, chemistry and more, the [Hugging Face Hub](https://huggingface.co/models) has become the Home of Machine Learning to create, collaborate, and deploy state-of-the-art models.\n\n<figure class=\"image table text-center m-0 w-full\">\n  <img src=\"assets/65_series_c/home-of-machine-learning.png\" alt=\"The Home of Machine Learning\"/>\n</figure>", "Webhook guide: Setup an automatic system to re-train a model when a dataset changes\n\n<Tip>\n\nWebhooks are now publicly available!\n\n</Tip>\n\nThis guide will help walk you through the setup of an automatic training pipeline on the Hugging Face platform\nusing HF Datasets, Webhooks, Spaces, and AutoTrain.\n\nWe will build a Webhook that listens to changes on an image classification dataset and triggers a fine-tuning\nof [microsoft/resnet-50](https://huggingface.co/microsoft/resnet-50) using [AutoTrain](https://huggingface.co/autotrain).\n\n\n## Prerequisite: Upload your dataset to the Hub\n\nWe will use a [simple image classification dataset](https://huggingface.co/datasets/huggingface-projects/auto-retrain-input-dataset) for the sake\nof the example. Learn more about uploading your data to the Hub [here](https://huggingface.co/docs/datasets/upload_dataset)."], "eval_score_GPT4": "5", "eval_feedback_GPT4": "Feedback: The response provided is completely correct, accurate, and factual. It matches the reference answer, which states that Image Classification is the latest task added to Hugging Face AutoTrain for Computer Vision."}, {"question": "What is the default repository type created by the `create_repo` function on Hugging Face Hub?\n", "true_answer": "model", "source_doc": "huggingface/huggingface_hub/blob/main/docs/source/en/guides/repository.md", "generated_answer": "The default repository type created by the `create_repo` function on Hugging Face Hub is a model repository, as stated in the context provided. This can be seen in the extracts provided, where the `type` parameter is set to \"model\" by default in the `create_repo` function call. If a different repository type is desired, such as a dataset or space repository, the `type` parameter can be specified with a value of \"dataset\" or \"space\", respectively", "retrieved_docs": ["Create a repository. It's a model repo by default.\n\nParameters:\n- `type`: Type of repo (dataset or space; model by default).\n- `name`: Name of repo.\n- `organization`: Name of organization (optional).\n- `private`: Whether the repo is private.\n- `sdk`: When the type is `space` (streamlit, gradio, docker or static)\n\nPayload:\n\n```js\npayload = {\n    \"type\":\"model\",\n    \"name\":\"name\",\n    \"organization\": \"organization\",\n    \"private\":\"private\",\n    \"sdk\": \"sdk\"\n}\n```\n\nThis is equivalent to `huggingface_hub.create_repo()`.\n\n### DELETE /api/repos/delete\n\nDelete a repository. It's a model repo by default.\n\nParameters:\n- `type`: Type of repo (dataset or space; model by default).\n- `name`: Name of repo.\n- `organization`: Name of organization (optional).\n\nPayload:", "Once you've registered and logged in, create a repository with the [`create_repo`]\nfunction:\n\n```py\n>>> from huggingface_hub import HfApi\n>>> api = HfApi()\n>>> api.create_repo(repo_id=\"super-cool-model\")\n```\n\nIf you want your repository to be private, then:\n\n```py\n>>> from huggingface_hub import HfApi\n>>> api = HfApi()\n>>> api.create_repo(repo_id=\"super-cool-model\", private=True)\n```\n\nPrivate repositories will not be visible to anyone except yourself.\n\n<Tip>\n\nTo create a repository or to push content to the Hub, you must provide a User Access\nToken that has the `write` permission. You can choose the permission when creating the\ntoken in your [Settings page](https://huggingface.co/settings/tokens).\n\n</Tip>\n\n## Upload files\n\nUse the [`upload_file`] function to add a file to your newly created repository. You\nneed to specify:", "!--\u26a0\ufe0f Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\nrendered properly in your Markdown viewer.\n-->\n\n# Create and manage a repository\n\nThe Hugging Face Hub is a collection of git repositories. [Git](https://git-scm.com/) is a widely used tool in software\ndevelopment to easily version projects when working collaboratively. This guide will show you how to interact with the\nrepositories on the Hub, especially:\n\n- Create and delete a repository.\n- Manage branches and tags. \n- Rename your repository.\n- Update your repository visibility.\n- Manage a local copy of your repository.\n\n<Tip warning={true}>", "# Adding HuggingFace argument\n    parser.add_argument(\"--repo-id\", type=str, default=\"ThomasSimonini/ppo-CartPole-v1\", help=\"id of the model repository from the Hugging Face Hub {username/repo_name}\")\n\n    args = parser.parse_args()\n    args.batch_size = int(args.num_envs * args.num_steps)\n    args.minibatch_size = int(args.batch_size // args.num_minibatches)\n    # fmt: on\n    return args", "## Repo creation and deletion\n\nThe first step is to know how to create and delete repositories. You can only manage repositories that you own (under\nyour username namespace) or from organizations in which you have write permissions.\n\n### Create a repository\n\nCreate an empty repository with [`create_repo`] and give it a name with the `repo_id` parameter. The `repo_id` is your namespace followed by the repository name: `username_or_org/repo_name`.\n\n```py\n>>> from huggingface_hub import create_repo\n>>> create_repo(\"lysandre/test-model\")\n'https://huggingface.co/lysandre/test-model'\n```\n\nBy default, [`create_repo`] creates a model repository. But you can use the `repo_type` parameter to specify another repository type. For example, if you want to create a dataset repository:", "from huggingface_sb3 import package_to_hub\n\n## TODO: Define a repo_id\n## repo_id is the id of the model repository from the Hugging Face Hub (repo_id = {organization}/{repo_name} for instance ThomasSimonini/ppo-LunarLander-v2\nrepo_id = \n\n# TODO: Define the name of the environment\nenv_id = \n\n# Create the evaluation env and set the render_mode=\"rgb_array\"\neval_env = DummyVecEnv([lambda: gym.make(env_id, render_mode=\"rgb_array\")])\n\n\n# TODO: Define the model architecture we used\nmodel_architecture = \"\"\n\n## TODO: Define the commit message\ncommit_message = \"\"", "### Create the repository\n\nSharing a community dataset will require you to create an account on [hf.co](https://huggingface.co/join) if you don't have one yet.\nYou can directly create a [new dataset repository](https://huggingface.co/login?next=%2Fnew-dataset) from your account on the Hugging Face Hub, but this guide will show you how to upload a dataset from the terminal.\n\n1. Make sure you are in the virtual environment where you installed Datasets, and run the following command:\n\n```\nhuggingface-cli login\n```\n\n2. Login using your Hugging Face Hub credentials, and create a new dataset repository:\n\n```\nhuggingface-cli repo create your_dataset_name --type dataset\n```\n\nAdd the `-organization` flag to create a repository under a specific organization:\n\n```\nhuggingface-cli repo create your_dataset_name --type dataset --organization your-org-name\n```\n\n### Clone the repository"], "eval_score_GPT4": "5", "eval_feedback_GPT4": "Feedback: The response correctly identifies the default repository type as a \"model\" repository, which aligns with the reference answer. The additional details provided about the `type` parameter and how to specify different repository types are accurate and enhance the factual content of the response."}, {"question": "How many splits does the \"duorc\" dataset have?\n", "true_answer": "Six", "source_doc": "huggingface/datasets-server/blob/main/docs/source/splits.mdx", "generated_answer": "The \"duorc\" dataset has six splits, as shown in the endpoint response provided in the context. The JSON response contains a list of the dataset's splits and configurations, and for the \"duorc\" dataset, there are six entries in the \"splits\" array. Each entry consists of the dataset name (\"duorc\"), the configuration name (\"ParaphraseRC\" or \"SelfRC\"), and the split name (\"train\", \"validation\", or \"", "retrieved_docs": ["The endpoint response is a JSON containing a list of the dataset's splits and configurations. For example, the [duorc](https://huggingface.co/datasets/duorc) dataset has six splits and two configurations:\n\n```json\n{\n  \"splits\": [\n    { \"dataset\": \"duorc\", \"config\": \"ParaphraseRC\", \"split\": \"train\" },\n    { \"dataset\": \"duorc\", \"config\": \"ParaphraseRC\", \"split\": \"validation\" },\n    { \"dataset\": \"duorc\", \"config\": \"ParaphraseRC\", \"split\": \"test\" },\n    { \"dataset\": \"duorc\", \"config\": \"SelfRC\", \"split\": \"train\" },\n    { \"dataset\": \"duorc\", \"config\": \"SelfRC\", \"split\": \"validation\" },\n    { \"dataset\": \"duorc\", \"config\": \"SelfRC\", \"split\": \"test\" }\n  ],\n  \"pending\": [],\n  \"failed\": []\n}\n```", "The endpoint response is a JSON containing the size of the dataset, as well as each of its configurations and splits. It provides the number of rows, the number of colums (where applicable) and the size in bytes for the different forms of the data: original files, size in memory (RAM) and auto-converted parquet files. For example, the [duorc](https://huggingface.co/datasets/duorc) dataset has 187.213 rows along all its configurations and splits, for a total of 97MB.", "The endpoint response is a JSON containing a list of the dataset's files in the Parquet format. For example, the [`duorc`](https://huggingface.co/datasets/duorc) dataset has six Parquet files, which corresponds to the `test`, `train` and `validation` splits of its two configurations, `ParaphraseRC` and `SelfRC` (see the [List splits and configurations](./splits) guide for more details about splits and configurations).\n\nThe endpoint also gives the filename and size of each file:", "The endpoint response is a JSON containing a list of the dataset's files URLs in the Parquet format for each split and configuration. For example, the [`duorc`](https://huggingface.co/datasets/duorc) dataset has one Parquet file for the train split of the \"ParaphraseRC\" configuration (see the [List splits and configurations](./splits) guide for more details about splits and configurations).", "```json\n{\n    \"dataset_info\": {\n        \"description\": \"DuoRC contains 186,089 unique question-answer pairs created from a collection of 7680 pairs of movie plots where each pair in the collection reflects two versions of the same movie.\\n\",\n        \"citation\": \"@inproceedings{DuoRC,\\nauthor = { Amrita Saha and Rahul Aralikatte and Mitesh M. Khapra and Karthik Sankaranarayanan},title = {{DuoRC: Towards Complex Language Understanding with Paraphrased Reading Comprehension}},\\nbooktitle = {Meeting of the Association for Computational Linguistics (ACL)},\\nyear = {2018}\\n}\\n\",\n        \"homepage\": \"https://duorc.github.io/\",\n        \"license\": \"https://raw.githubusercontent.com/duorc/duorc/master/LICENSE\",\n        \"features\": {\n            \"plot_id\": {\n                \"dtype\": \"string\",\n                \"_type\": \"Value\"\n            },", "\"filename\": \"0000.parquet\",\n      \"size\": 3035735\n    },\n    {\n      \"dataset\": \"duorc\",\n      \"config\": \"SelfRC\",\n      \"split\": \"train\",\n      \"url\": \"https://huggingface.co/datasets/duorc/resolve/refs%2Fconvert%2Fparquet/SelfRC/train/0000.parquet\",\n      \"filename\": \"0000.parquet\",\n      \"size\": 14851719\n    },\n    {\n      \"dataset\": \"duorc\",\n      \"config\": \"SelfRC\",\n      \"split\": \"validation\",\n      \"url\": \"https://huggingface.co/datasets/duorc/resolve/refs%2Fconvert%2Fparquet/SelfRC/validation/0000.parquet\",\n      \"filename\": \"0000.parquet\",\n      \"size\": 3114389\n    }", "\"config\": \"SelfRC\",\n        \"split\": \"train\",\n        \"num_bytes_parquet_files\": 14851720,\n        \"num_bytes_memory\": 239852729,\n        \"num_rows\": 60721,\n        \"num_columns\": 7\n      },\n      {\n        \"dataset\": \"duorc\",\n        \"config\": \"SelfRC\",\n        \"split\": \"validation\",\n        \"num_bytes_parquet_files\": 3114390,\n        \"num_bytes_memory\": 51662519,\n        \"num_rows\": 12961,\n        \"num_columns\": 7\n      },\n      {\n        \"dataset\": \"duorc\",\n        \"config\": \"SelfRC\",\n        \"split\": \"test\",\n        \"num_bytes_parquet_files\": 3035736,\n        \"num_bytes_memory\": 49142710,"], "eval_score_GPT4": "5", "eval_feedback_GPT4": "The response correctly identifies the number of splits in the \"duorc\" dataset as six, which aligns with the reference answer. The additional details provided about the configurations and the split names are accurate and factual, although they are not required to answer the question. The response goes beyond the simple answer of \"Six\" by explaining the structure of the dataset, which supports the correctness of the answer."}, {"question": "What is the purpose of Fully Sharded Data Parallel (FSDP) in distributed training?\n", "true_answer": "FSDP is developed for distributed training of large pretrained models up to 1T parameters by sharding the model parameters, gradients, and optimizer states across data parallel processes.", "source_doc": "huggingface/peft/blob/main/docs/source/accelerate/fsdp.md", "generated_answer": "Fully Sharded Data Parallel (FSDP) is a data parallel method developed for distributed training of large pretrained models up to 1T parameters in PyTorch. Its purpose is to shard the model parameters, gradients, and optimizer states across data parallel processes and offload sharded model parameters to a CPU, reducing memory usage and improving GPU memory-efficiency. This allows for scaling training to larger batch or model sizes, making it possible to train much larger models", "retrieved_docs": ["!--\u26a0\ufe0f Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\nrendered properly in your Markdown viewer.\n-->\n\n# Fully Sharded Data Parallel\n\n[Fully sharded data parallel](https://pytorch.org/docs/stable/fsdp.html) (FSDP) is developed for distributed training of large pretrained models up to 1T parameters. FSDP achieves this by sharding the model parameters, gradients, and optimizer states across data parallel processes and it can also offload sharded model parameters to a CPU. The memory efficiency afforded by FSDP allows you to scale training to larger batch or model sizes.\n\n<Tip warning={true}>\n\nCurrently, FSDP does not confer any reduction in GPU memory usage and FSDP with CPU offload actually consumes 1.65x more GPU memory during training. You can track this PyTorch [issue](https://github.com/pytorch/pytorch/issues/91165) for any updates.\n\n</Tip>", "-->\n\n# Fully Sharded Data Parallel\n\n[Fully Sharded Data Parallel (FSDP)](https://pytorch.org/blog/introducing-pytorch-fully-sharded-data-parallel-api/) is a data parallel method that shards a model's parameters, gradients and optimizer states across the number of available GPUs (also called workers or *rank*). Unlike [DistributedDataParallel (DDP)](https://pytorch.org/docs/stable/generated/torch.nn.parallel.DistributedDataParallel.html), FSDP reduces memory-usage because a model is replicated on each GPU. This improves GPU memory-efficiency and allows you to train much larger models on fewer GPUs. FSDP is integrated with the Accelerate, a library for easily managing training in distributed environments, which means it is available for use from the [`Trainer`] class.\n\nBefore you start, make sure Accelerate is installed and at least PyTorch 2.1.0 or newer.\n\n```bash\npip install accelerate\n```\n\n## FSDP configuration", "Fully Sharded Data Parallelism (FSDP) is a paradigm in which the optimizer states, gradients and parameters are sharded across devices. During the forward pass, each FSDP unit performs an _all-gather operation_ to get the complete weights, computation is performed followed by discarding the shards from other devices. After the forward pass, the loss is computed followed by the backward pass. In the backward pass, each FSDP unit performs an all-gather operation to get the complete weights, with computation performed to get the local gradients. These local gradients are averaged and sharded across the devices via a _reduce-scatter operation_ so that each device can update the parameters of its shard. For more information on what PyTorch FSDP is, please refer to this blog post: [Accelerate Large Model Training using PyTorch Fully Sharded Data Parallel](https://huggingface.co/blog/pytorch-fsdp).\n\n![FSDP Workflow](./assets/62_pytorch_fsdp/FSDP_workflow.png)\n\n(Source: [link](https://pytorch.org/blog/introducing-pytorch-fully-sharded-data-parallel-api/))", "## Accelerate and Trainer\n\nThe [`Trainer`] class is powered by [Accelerate](https://hf.co/docs/accelerate), a library for easily training PyTorch models in distributed environments with support for integrations such as [FullyShardedDataParallel (FSDP)](https://pytorch.org/blog/introducing-pytorch-fully-sharded-data-parallel-api/) and [DeepSpeed](https://www.deepspeed.ai/).\n\n<Tip>\n\nLearn more about FSDP sharding strategies, CPU offloading, and more with the [`Trainer`] in the [Fully Sharded Data Parallel](fsdp) guide.\n\n</Tip>\n\nTo use Accelerate with [`Trainer`], run the [`accelerate.config`](https://huggingface.co/docs/accelerate/package_reference/cli#accelerate-config) command to set up training for your training environment. This command creates a `config_file.yaml` that'll be used when you launch your training script. For example, some example configurations you can setup are:", "[8] [Getting Started with Fully Sharded Data Parallel(FSDP) \u2014 PyTorch Tutorials 1.11.0+cu102 documentation](https://pytorch.org/tutorials/intermediate/FSDP_tutorial.html)\n\n[9] [Training a 1 Trillion Parameter Model With PyTorch Fully Sharded Data Parallel on AWS | by PyTorch | PyTorch | Mar, 2022 | Medium](https://medium.com/pytorch/training-a-1-trillion-parameter-model-with-pytorch-fully-sharded-data-parallel-on-aws-3ac13aa96cff)\n\n[10] [Fit More and Train Faster With ZeRO via DeepSpeed and FairScale](https://huggingface.co/blog/zero-deepspeed-fairscale)", "--\ntitle: \"Accelerate Large Model Training using PyTorch Fully Sharded Data Parallel\"\nthumbnail: /blog/assets/62_pytorch_fsdp/fsdp-thumbnail.png\nauthors:\n- user: smangrul\n- user: sgugger\n---\n\n# Accelerate Large Model Training using PyTorch Fully Sharded Data Parallel\n\n\nIn this post we will look at how we can leverage **[Accelerate](https://github.com/huggingface/accelerate)** Library for training large models which enables users to leverage the latest features of **[PyTorch FullyShardedDataParallel (FSDP)](https://pytorch.org/blog/introducing-pytorch-fully-sharded-data-parallel-api/)**.\n\n# Motivation \ud83e\udd17", "## Full Finetuning\n\nWe will look at how to do full fine-tuning of `bigcode/starcoder` (15B params) on 8 A100 80GB GPUs using PyTorch Fully Sharded Data Parallel (FSDP) technique. For more information on FSDP, please refer to [Fine-tuning Llama 2 70B using PyTorch FSDP](https://huggingface.co/blog/ram-efficient-pytorch-fsdp) and [Accelerate Large Model Training using PyTorch Fully Sharded Data Parallel](https://huggingface.co/blog/pytorch-fsdp)."], "eval_score_GPT4": "5", "eval_feedback_GPT4": "Feedback: The response provided is completely correct, accurate, and factual. It aligns well with the reference answer by mentioning the sharding of model parameters, gradients, and optimizer states across data parallel processes. Additionally, it expands on the reference answer by explaining the benefits of FSDP, such as reducing memory usage, improving GPU memory-efficiency, and enabling the scaling of training to larger batch or model sizes. The response also correctly identifies FSDP as a method used in PyTorch for training large pretrained models, which is consistent with the reference answer's mention of large pretrained models up to 1T parameters."}, {"question": "What file format is used to save and store PyTorch model weights more securely than `.bin` files?\n", "true_answer": "`.safetensors`", "source_doc": "huggingface/safetensors/blob/main/docs/source/convert-weights.md", "generated_answer": "The more secure file format for saving and storing PyTorch model weights is `.safetensors`. This format, provided by the safetensors library, is a safe and fast alternative to the traditional `.bin` files saved using Python's `pickle` utility. `.safetensors` files are more secure as they are less vulnerable to arbitrary code execution attacks that can be perpetrated when loading pickled files. Additionally, `.safetensors`", "retrieved_docs": ["Convert weights to safetensors\n\nPyTorch model weights are commonly saved and stored as `.bin` files with Python's [`pickle`](https://docs.python.org/3/library/pickle.html) utility. To save and store your model weights in the more secure `safetensor` format, we recommend converting your weights to `.safetensors`.\n\nThe easiest way to convert your model weights is to use the [Convert Space](https://huggingface.co/spaces/diffusers/convert), given your model weights are already stored on the Hub. The Convert Space downloads the pickled weights, converts them, and opens a Pull Request to upload the newly converted `.safetensors` file to your repository.\n\n<Tip warning={true}>\n\nFor larger models, the Space may be a bit slower because its resources are tied up in converting other models. You can also try running the [convert.py](https://github.com/huggingface/safetensors/blob/main/bindings/python/convert.py) script (this is what the Space is running) locally to convert your weights.", "# Load safetensors\n\n[[open-in-colab]]\n\n[safetensors](https://github.com/huggingface/safetensors) is a safe and fast file format for storing and loading tensors. Typically, PyTorch model weights are saved or *pickled* into a `.bin` file with Python's [`pickle`](https://docs.python.org/3/library/pickle.html) utility. However, `pickle` is not secure and pickled files may contain malicious code that can be executed. safetensors is a secure alternative to `pickle`, making it ideal for sharing model weights.\n\nThis guide will show you how you load `.safetensor` files, and how to convert Stable Diffusion model weights stored in other formats to `.safetensor`. Before you start, make sure you have safetensors installed:\n\n```py\n# uncomment to install the necessary libraries in Colab\n#!pip install safetensors\n```", "We call the checkpoints saved in several files like BLOOM *sharded checkpoints*, and we have standardized their format as such: \n- One file (called `pytorch_model.bin.index.json`) contains some metadata and a map parameter name to file name, indicating where to find each weight\n- All the other files are standard PyTorch state dicts, they just contain a part of the model instead of the whole one. You can have a look at the content of the index file [here](https://huggingface.co/bigscience/bloom/blob/main/pytorch_model.bin.index.json).\n\nTo load such a sharded checkpoint into a model, we just need to loop over the various shards. Accelerate provides a function called `load_checkpoint_in_model` that will do this for you if you have cloned one of the repos of the Hub, or you can directly use the `from_pretrained` method of Transformers, which will handle the downloading and caching for you:\n\n```python\nimport torch\nfrom transformers import AutoModelForCausalLM", "<Tip>\n\nWe highly recommend using the `.safetensors` format because it is more secure than traditional pickled files which are vulnerable and can be exploited to execute any code on your machine (learn more in the [Load safetensors](using_safetensors) guide).\n\n</Tip>\n\nThis guide will show you how to convert other Stable Diffusion formats to be compatible with \ud83e\udd17 Diffusers.\n\n## PyTorch .ckpt\n\nThe checkpoint - or `.ckpt` - format is commonly used to store and save models. The `.ckpt` file contains the entire model and is typically several GBs in size. While you can load and use a `.ckpt` file directly with the [`~StableDiffusionPipeline.from_single_file`] method, it is generally better to convert the `.ckpt` file to \ud83e\udd17 Diffusers so both formats are available.\n\nThere are two options for converting a `.ckpt` file: use a Space to convert the checkpoint or convert the `.ckpt` file with a script.\n\n### Convert with a Space", "## Sharding state dicts\n\nTraditionally, PyTorch models are saved in a whole file containing a map from parameter name to weight. This map is often called a `state_dict`. Here is an excerpt from the [PyTorch documentation](https://pytorch.org/tutorials/beginner/basics/saveloadrun_tutorial.html) on saving on loading:\n\n```python\n# Save the model weights\ntorch.save(my_model.state_dict(), 'model_weights.pth')\n\n# Reload them\nnew_model = ModelClass()\nnew_model.load_state_dict(torch.load('model_weights.pth'))\n```\n\nThis works pretty well for models with less than 1 billion parameters, but for larger models, this is very taxing in RAM. The BLOOM model has 176 billions parameters; even with the weights saved in bfloat16 to save space, it still represents 352GB as a whole. While the super computer that trained this model might have this amount of memory available, requiring this for inference is unrealistic.", "Pickle Scanning\n\nPickle is a widely used serialization format in ML. Most notably, it is the default format for PyTorch model weights.\n\nThere are dangerous arbitrary code execution attacks that can be perpetrated when you load a pickle file. We suggest loading models from users and organizations you trust, relying on signed commits, and/or loading models from TF or Jax formats with the `from_tf=True` auto-conversion mechanism. We also alleviate this issue by displaying/\"vetting\" the list of imports in any pickled file, directly on the Hub. Finally, we are experimenting with a new, simple serialization format for weights called [`safetensors`](https://github.com/huggingface/safetensors).\n\n## What is a pickle?\n\nFrom the [official docs](https://docs.python.org/3/library/pickle.html) :\n\n> The `pickle` module implements binary protocols for serializing and de-serializing a Python object structure.\n\nWhat this means is that pickle is a serializing protocol, something you use to efficiently share data amongst parties.\n\nWe call a pickle the binary file that was generated while pickling.", "We are also taking steps to make sure the existing PyTorch files are not malicious, but the best we can do is flag suspicious-looking files.\n\nOf course, there are other file formats out there, but\nnone seemed to meet the full set of [ideal requirements](https://github.com/huggingface/safetensors#yet-another-format-) our team identified.\n\nIn addition to being safe, `safetensors` allows lazy loading and generally faster loads (around 100x faster on CPU).\n\nLazy loading means loading only part of a tensor in an efficient manner.\nThis particular feature enables arbitrary sharding with efficient inference libraries, such as [text-generation-inference](https://github.com/huggingface/text-generation-inference), to load LLMs (such as LLaMA, StarCoder, etc.) on various types of hardware\nwith maximum efficiency.\n\nBecause it loads so fast and is framework agnostic, we can even use the format\nto load models from the same file in PyTorch or TensorFlow.\n\n\n## The security audit"], "eval_score_GPT4": "5", "eval_feedback_GPT4": "The response correctly identifies `.safetensors` as a more secure file format for saving and storing PyTorch model weights compared to `.bin` files. The explanation provided about the safetensors library and its benefits over the traditional `.bin` files saved using Python's `pickle` utility is accurate and factual. The response aligns with the reference answer and meets the criteria for a score of 5 as it is completely correct, accurate, and factual."}, {"question": "What type of security certification does Hugging Face have?\n", "true_answer": "SOC2 Type 2 certified", "source_doc": "huggingface/hub-docs/blob/main/docs/hub/security.md", "generated_answer": "Hugging Face is SOC2 Type 2 certified, as mentioned in the context provided. This security certification provides assurance to customers that Hugging Face's security controls and processes are effectively designed and operating as intended to protect their data and systems.", "retrieved_docs": ["## Inference Endpoints and Hub Security\n\nThe Hugging Face Hub, which Inference Endpoints is part, is also SOC2 Type 2 certified. The Hugging Face Hub offers Role Based Access Control. For more on hub security: https://huggingface.co/docs/hub/security\n\n<img width=\"150\" src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/hub/security-soc-1.jpg\">\n\n## Inference Endpoint Security level\n\nWe currently offer three types of endpoints, in order or increasing security level:\n\n- **Public**: A Public Endpoint is available from the internet, secured with TLS/SSL, and requires no authentication.\n- **Protected**: A Protected Endpoint is available from the internet, secured with TLS/SSL, and requires a valid Hugging Face token for authentication.\n- **Private** A Private Endpoint is only available through an intra-region secured AWS or Azure PrivateLink connection. Private Endpoints are not accessible from the internet.\n\nPublic and Protected Endpoints do not require any additional configuration. For Private Endpoints, you need to provide the AWS account ID of the account which also should have access to \ud83e\udd17 Inference Endpoints.", "Security\n\nThe Hugging Face Hub offers several security features to ensure that your code and data are secure. Beyond offering [private repositories](./repositories-settings#private-repositories) for models, datasets, and Spaces, the Hub supports access tokens, commit signatures, and malware scanning.\n\nHugging Face is GDPR compliant. If a contract or specific data storage is something you'll need, we recommend taking a look at our [Expert Acceleration Program](https://huggingface.co/support). Hugging Face can also offer Business Associate Addendums or GDPR data processing agreements through an [Enterprise Plan](https://huggingface.co/pricing). \n\nHugging Face is also [SOC2 Type 2 certified](https://us.aicpa.org/interestareas/frc/assuranceadvisoryservices/aicpasoc2report.html), meaning we provide security certification to our customers and actively monitor and patch any security weaknesses.\n\n<img width=\"150\" src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/hub/security-soc-1.jpg\">", "## Security\n\nThe Hugging Face Hub supports security and access control features to give you the peace of mind that your code, models, and data are safe. Visit the [**Security**](./security) section in these docs to learn about:\n\n- User Access Tokens\n- Access Control for Organizations\n- Signing commits with GPG\n- Malware scanning\n\n<img width=\"150\" src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/hub/security-soc-1.jpg\">", "\ud83c\udf8e Network of peers with whom ambassadors can collaborate. \n\n\ud83e\uddd1\ud83c\udffb\u200d\ud83d\udcbb Workshops and support from the Hugging Face team! \n\n\ud83e\udd17 Insight into the latest projects, features, and more!\n\n\ud83c\udf81 Merchandise and assets. \n\n\u2728 Being officially recognized as a Hugging Face\u2019s Ambassador\n\n<br />\n\n**Eligibility Requirements for Students**\n\n- Validate your student status \n- Have taken at least one machine learning/data science course (online courses are considered as well)\n- Be enrolled in an accredited college or university\n- Be a user of the Hugging Face Hub and/or the Hugging Face\u2019s libraries\n- Acknowledge the [Code of Conduct](https://huggingface2.notion.site/huggingface2/Hugging-Face-Code-of-Conduct-45eeeafa9ef44c5e888a2952619fdfa8). Community is at the center of the Hugging Face ecosystem. Because of that, we strictly adhere to our [Code of conduct](https://huggingface2.notion.site/huggingface2/Hugging-Face-Code-of-Conduct-45eeeafa9ef44c5e888a2952619fdfa8). If any ambassador infringes it or behaves inadequately, they will be excluded from the Program.", "## Authentication\n\nIn a lot of cases, you must be authenticated with a Hugging Face account to interact with\nthe Hub: download private repos, upload files, create PRs,...\n[Create an account](https://huggingface.co/join) if you don't already have one, and then sign in\nto get your [User Access Token](https://huggingface.co/docs/hub/security-tokens) from\nyour [Settings page](https://huggingface.co/settings/tokens). The User Access Token is\nused to authenticate your identity to the Hub.\n\n<Tip>\n\nTokens can have `read` or `write` permissions. Make sure to have a `write` access token if you want to create or edit a repository. Otherwise, it's best to generate a `read` token to reduce risk in case your token is inadvertently leaked.\n\n</Tip>\n\n### Login command\n\nThe easiest way to authenticate is to save the token on your machine. You can do that from the terminal using the [`login`] command:\n\n```bash\nhuggingface-cli login\n```", "We are happy to announce that Hugging Face has been selected by the [CNIL](https://www.cnil.fr/en/home) (French Data Protection Authority) to benefit from its [Enhanced Support program](https://www.cnil.fr/en/enhanced-support-cnil-selects-3-digital-companies-strong-potential)!\nThis new program picked three companies with \u201cstrong potential for economic development\u201d out of over 40 candidates, who will receive support in understanding and implementing their duties with respect to data protection - \na daunting and necessary endeavor in the context of the rapidly evolving field of Artificial Intelligence.", "Security & Compliance\n\n\ud83e\udd17 Inference Endpoints is built with security and secure inference at its core. Below you can find an overview of the security measures we have in place.\n\n## Data Security/Privacy\n\nHugging Face does not store any customer data in terms of payloads or tokens that are passed to the Inference Endpoint. We are storing logs for 30 days. Every Inference Endpoints uses TLS/SSL to encrypt the data in transit.\n\nWe also recommend using AWS or Azure Private Link for organizations. This allows you to access your Inference Endpoint through a private connection, without exposing it to the internet.\n\nHugging Face also offers Business Associate Addendum or GDPR data processing agreement through the Inference Endpoint enterprise plan.\n\n## Model Security/Privacy:\n\nYou can set a model repository as private if you do not want to publicly expose it. Hugging Face does not own any model or data you upload to the Hugging Face hub. Hugging Face does provide malware and pickle scans over the contents of the model repository as with all items in the Hub.\n\n## Inference Endpoints and Hub Security"], "eval_score_GPT4": "5", "eval_feedback_GPT4": "Feedback: The response correctly identifies Hugging Face as having SOC2 Type 2 certification, which is in line with the reference answer. The additional information provided about the certification's role in assuring customers about security controls and processes is accurate and relevant, although not required by the reference answer. The response is factual and directly addresses the question posed in the instruction."}, {"question": "What do RAG models combine to generate outputs?\n", "true_answer": "Pretrained dense retrieval (DPR) and sequence-to-sequence models.", "source_doc": "huggingface/transformers/blob/main/docs/source/en/model_doc/rag.md", "generated_answer": "RAG models combine the powers of pretrained dense retrieval (DPR) and sequence-to-sequence models to generate outputs. They retrieve relevant context documents from an external dataset during a forward pass, prepend them to the input, and pass them to a generator. Both the retrieval and generation modules are initialized from pretrained models and fine-tuned jointly, allowing both retrieval and generation to adapt to downstream tasks. (Source: Context and extracts from Hugging", "retrieved_docs": ["## Overview\n\nRetrieval-augmented generation (\"RAG\") models combine the powers of pretrained dense retrieval (DPR) and\nsequence-to-sequence models. RAG models retrieve documents, pass them to a seq2seq model, then marginalize to generate\noutputs. The retriever and seq2seq modules are initialized from pretrained models, and fine-tuned jointly, allowing\nboth retrieval and generation to adapt to downstream tasks.\n\nIt is based on the paper [Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks](https://arxiv.org/abs/2005.11401) by Patrick Lewis, Ethan Perez, Aleksandara Piktus, Fabio Petroni, Vladimir\nKarpukhin, Naman Goyal, Heinrich K\u00fcttler, Mike Lewis, Wen-tau Yih, Tim Rockt\u00e4schel, Sebastian Riedel, Douwe Kiela.\n\nThe abstract from the paper is the following:", "This model was contributed by [ola13](https://huggingface.co/ola13).\n\n## Usage tips\n\nRetrieval-augmented generation (\"RAG\") models combine the powers of pretrained dense retrieval (DPR) and Seq2Seq models. \nRAG models retrieve docs, pass them to a seq2seq model, then marginalize to generate outputs. The retriever and seq2seq \nmodules are initialized from pretrained models, and fine-tuned jointly, allowing both retrieval and generation to adapt \nto downstream tasks.\n\n## RagConfig\n\n[[autodoc]] RagConfig\n\n## RagTokenizer\n\n[[autodoc]] RagTokenizer\n\n## Rag specific outputs\n\n[[autodoc]] models.rag.modeling_rag.RetrievAugLMMarginOutput\n\n[[autodoc]] models.rag.modeling_rag.RetrievAugLMOutput\n\n## RagRetriever\n\n[[autodoc]] RagRetriever\n\n<frameworkcontent>\n<pt>\n\n## RagModel", "Intro\n\nAuthors: @patrickvonplaten and @lhoestq\n\nAimed at tackling the knowledge-intensive NLP tasks (think tasks a human wouldn't be expected to solve without access to external knowledge sources), RAG models are seq2seq models with access to a retrieval mechanism providing relevant context documents at training and evaluation time.\n\nA RAG model encapsulates two core components: a question encoder and a generator.\nDuring a forward pass, we encode the input with the question encoder and pass it\nto the retriever to extract relevant context documents. The documents are then prepended to the input.\nSuch contextualized inputs are passed to the generator.\n\nRead more about RAG  at https://arxiv.org/abs/2005.11401.\n\n# Note\n\n\u26a0\ufe0f This project should be run with pytorch-lightning==1.3.1 which has a potential security vulnerability\n\n# Finetuning", "### What is Retrieval Augmented Generation (RAG)?\n![alt_text](assets/12_ray_rag/rag_gif.gif \"image_tooltip\")\n\n_An overview of RAG. The model retrieves contextual documents from an external dataset as part of its execution. These contextual documents are used in conjunction with the original input to produce an output. The  GIF is taken from [Facebook's original blog post](https://ai.facebook.com/blog/retrieval-augmented-generation-streamlining-the-creation-of-intelligent-natural-language-processing-models)._\n\n\n\nRecently, [Huggingface](https://huggingface.co/) partnered with [Facebook AI](https://ai.facebook.com/) to introduce the [RAG](https://twitter.com/huggingface/status/1310597560906780680) model as part of its Transformers library.", "```bash\npython examples/research_projects/rag/finetune_rag.py \\\n    --data_dir $DATA_DIR \\\n    --output_dir $OUTPUT_DIR \\\n    --model_name_or_path $MODEL_NAME_OR_PATH \\\n    --model_type rag_sequence \\\n    --fp16 \\\n    --gpus 8\n```\nWe publish two `base` models which can serve as a starting point for finetuning on downstream tasks (use them as `model_name_or_path`):\n- [`facebook/rag-sequence-base`](https://huggingface.co/facebook/rag-sequence-base) - a base for finetuning `RagSequenceForGeneration` models,\n- [`facebook/rag-token-base`](https://huggingface.co/facebook/rag-token-base) - a base for finetuning `RagTokenForGeneration` models.", "python examples/rag/finetune_rag.py \\\n    --data_dir $DATA_DIR \\\n    --output_dir $OUTPUT_DIR \\\n    --model_name_or_path $MODEL_NAME_OR_PATH \\\n    --model_type rag_sequence \\\n    --fp16 \\\n    --gpus 8 \\\n    --profile \\\n    --do_train \\\n    --do_predict \\\n    --n_val -1 \\\n    --train_batch_size 8 \\\n    --eval_batch_size 1 \\\n    --max_source_length 128 \\\n    --max_target_length 25 \\\n    --val_max_target_length 25 \\\n    --test_max_target_length 25 \\\n    --label_smoothing 0.1 \\\n    --dropout 0.1 \\\n    --attention_dropout 0.1 \\\n    --weight_decay 0.001 \\\n    --adam_epsilon 1e-08 \\\n    --max_grad_norm 0.1 \\\n    --lr_scheduler polynomial \\\n    --learning_rate 3e-05 \\\n    --num_train_epochs 100 \\\n    --warmup_steps 500 \\\n    --gradient_accumulation_steps 1 \\\n    --distributed_retriever ray \\\n    --num_retrieval_workers 4\n\n# Stop the Ray cluster.\nray stop\n```", "--predictions_path output/retrieval_preds.tsv  \\\n        --eval_mode retrieval \\\n        --k 1\n    ```\n   ```bash\n   # EXPLANATION\n    python examples/research_projects/rag/eval_rag.py \\\n        --model_name_or_path facebook/rag-sequence-nq \\ # model name or path of the model we're evaluating\n        --model_type rag_sequence \\ # RAG model type (rag_token or rag_sequence)\n        --evaluation_set output/biencoder-nq-dev.questions \\ # an input dataset for evaluation\n        --gold_data_path poutput/biencoder-nq-dev.pages \\ # a dataset containing ground truth answers for samples from the evaluation_set\n        --predictions_path output/retrieval_preds.tsv  \\ # name of file where predictions will be stored\n        --eval_mode retrieval \\ # indicates whether we're performing retrieval evaluation or e2e evaluation\n        --k 1 # parameter k for the precision@k metric"], "eval_score_GPT4": "5", "eval_feedback_GPT4": "Feedback: The response accurately identifies that RAG models combine pretrained dense retrieval (DPR) and sequence-to-sequence models to generate outputs, which is in line with the reference answer. It also provides additional correct and factual details about how RAG models function, such as retrieving relevant context documents and fine-tuning both retrieval and generation modules. The response is completely correct, accurate, and factual."}, {"question": "What library does MarkupLMFeatureExtractor use to extract data from HTML and XML files?\n", "true_answer": "Beautiful Soup", "source_doc": "huggingface/transformers/blob/main/docs/source/en/model_doc/markuplm.md", "generated_answer": "MarkupLMFeatureExtractor uses the Python library Beautiful Soup to extract data from HTML and XML files. (Source: Extract 1)", "retrieved_docs": ["## Usage: MarkupLMProcessor\n\nThe easiest way to prepare data for the model is to use [`MarkupLMProcessor`], which internally combines a feature extractor\n([`MarkupLMFeatureExtractor`]) and a tokenizer ([`MarkupLMTokenizer`] or [`MarkupLMTokenizerFast`]). The feature extractor is\nused to extract all nodes and xpaths from the HTML strings, which are then provided to the tokenizer, which turns them into the\ntoken-level inputs of the model (`input_ids` etc.). Note that you can still use the feature extractor and tokenizer separately,\nif you only want to handle one of the two tasks.\n\n```python\nfrom transformers import MarkupLMFeatureExtractor, MarkupLMTokenizerFast, MarkupLMProcessor\n\nfeature_extractor = MarkupLMFeatureExtractor()\ntokenizer = MarkupLMTokenizerFast.from_pretrained(\"microsoft/markuplm-base\")\nprocessor = MarkupLMProcessor(feature_extractor, tokenizer)\n```", "In short, one can provide HTML strings (and possibly additional data) to [`MarkupLMProcessor`],\nand it will create the inputs expected by the model. Internally, the processor first uses\n[`MarkupLMFeatureExtractor`] to get a list of nodes and corresponding xpaths. The nodes and\nxpaths are then provided to [`MarkupLMTokenizer`] or [`MarkupLMTokenizerFast`], which converts them\nto token-level `input_ids`, `attention_mask`, `token_type_ids`, `xpath_subs_seq`, `xpath_tags_seq`.\nOptionally, one can provide node labels to the processor, which are turned into token-level `labels`.\n\n[`MarkupLMFeatureExtractor`] uses [Beautiful Soup](https://www.crummy.com/software/BeautifulSoup/bs4/doc/), a Python library for\npulling data out of HTML and XML files, under the hood. Note that you can still use your own parsing solution of\nchoice, and provide the nodes and xpaths yourself to [`MarkupLMTokenizer`] or [`MarkupLMTokenizerFast`].", "In total, there are 5 use cases that are supported by the processor. Below, we list them all. Note that each of these\nuse cases work for both batched and non-batched inputs (we illustrate them for non-batched inputs).\n\n**Use case 1: web page classification (training, inference) + token classification (inference), parse_html = True**\n\nThis is the simplest case, in which the processor will use the feature extractor to get all nodes and xpaths from the HTML.\n\n```python\n>>> from transformers import MarkupLMProcessor\n\n>>> processor = MarkupLMProcessor.from_pretrained(\"microsoft/markuplm-base\")\n\n>>> html_string = \"\"\"\n...  <!DOCTYPE html>\n...  <html>\n...  <head>\n...  <title>Hello world</title>\n...  </head>\n...  <body>\n...  <h1>Welcome</h1>\n...  <p>Here is my website.</p>\n...  </body>\n...  </html>\"\"\"", "-->\n\n# MarkupLM\n\n## Overview\n\nThe MarkupLM model was proposed in [MarkupLM: Pre-training of Text and Markup Language for Visually-rich Document\nUnderstanding](https://arxiv.org/abs/2110.08518) by Junlong Li, Yiheng Xu, Lei Cui, Furu Wei. MarkupLM is BERT, but\napplied to HTML pages instead of raw text documents. The model incorporates additional embedding layers to improve\nperformance, similar to [LayoutLM](layoutlm).\n\nThe model can be used for tasks like question answering on web pages or information extraction from web pages. It obtains\nstate-of-the-art results on 2 important benchmarks:\n- [WebSRC](https://x-lance.github.io/WebSRC/), a dataset for Web-Based Structural Reading Comprehension (a bit like SQuAD but for web pages)\n- [SWDE](https://www.researchgate.net/publication/221299838_From_one_tree_to_a_forest_a_unified_solution_for_structured_web_data_extraction), a dataset\nfor information extraction from web pages (basically named-entity recogntion on web pages)", "**Use case 5: web page question answering (inference), parse_html=False**\n\nFor question answering tasks (such as WebSRC), you can provide a question to the processor. If you have extracted\nall nodes and xpaths yourself, you can provide them directly to the processor. Make sure to set `parse_html` to `False`.\n\n```python\n>>> from transformers import MarkupLMProcessor\n\n>>> processor = MarkupLMProcessor.from_pretrained(\"microsoft/markuplm-base\")\n>>> processor.parse_html = False", "The abstract from the paper is the following:\n\n*Multimodal pre-training with text, layout, and image has made significant progress for Visually-rich Document\nUnderstanding (VrDU), especially the fixed-layout documents such as scanned document images. While, there are still a\nlarge number of digital documents where the layout information is not fixed and needs to be interactively and\ndynamically rendered for visualization, making existing layout-based pre-training approaches not easy to apply. In this\npaper, we propose MarkupLM for document understanding tasks with markup languages as the backbone such as\nHTML/XML-based documents, where text and markup information is jointly pre-trained. Experiment results show that the\npre-trained MarkupLM significantly outperforms the existing strong baseline models on several document understanding\ntasks. The pre-trained model and code will be publicly available.*\n\nThis model was contributed by [nielsr](https://huggingface.co/nielsr). The original code can be found [here](https://github.com/microsoft/unilm/tree/master/markuplm).\n\n## Usage tips", "- Type some Markdown on the left\n- See HTML in the right\n- \u2728Magic \u2728\n\n## Features\n\n- Import a HTML file and watch it magically convert to Markdown\n- Drag and drop images (requires your Dropbox account be linked)\n- Import and save files from GitHub, Dropbox, Google Drive and One Drive\n- Drag and drop markdown and HTML files into Dillinger\n- Export documents as Markdown, HTML and PDF\n\nMarkdown is a lightweight markup language based on the formatting conventions\nthat people naturally use in email.\nAs [John Gruber] writes on the [Markdown site][df1]\n\n> The overriding design goal for Markdown's\n> formatting syntax is to make it as readable\n> as possible. The idea is that a\n> Markdown-formatted document should be\n> publishable as-is, as plain text, without\n> looking like it's been marked up with tags\n> or formatting instructions.\n\nThis text you see here is *actually- written in Markdown! To get a feel\nfor Markdown's syntax, type some text into the left window and\nwatch the results in the right.\n\n## Tech\n\nDillinger uses a number of open source projects to work properly:"], "eval_score_GPT4": "5", "eval_feedback_GPT4": "Feedback: The response correctly identifies Beautiful Soup as the Python library used by MarkupLMFeatureExtractor to extract data from HTML and XML files, which is in line with the reference answer."}, {"question": "What is the file size limit for syncing to HF Spaces without using Git-LFS?\n", "true_answer": "10MB", "source_doc": "huggingface/hub-docs/blob/main/docs/hub/spaces-github-actions.md", "generated_answer": "The file size limit for syncing to HF Spaces without using Git-LFS is 10MB, as mentioned in the context provided. If files exceed this limit, it is recommended to use Git-LFS or to review and remove large files using a tool like BFG Repo-Cleaner. This is necessary to ensure successful syncing of the repository to HF Spaces.", "retrieved_docs": ["Managing Spaces with Github Actions\n\nYou can keep your app in sync with your GitHub repository with **Github Actions**. Remember that for files larger than 10MB, Spaces requires Git-LFS. If you don't want to use Git-LFS, you may need to review your files and check your history. Use a tool like [BFG Repo-Cleaner](https://rtyley.github.io/bfg-repo-cleaner/) to remove any large files from your history. BFG Repo-Cleaner will keep a local copy of your repository as a backup.\n\nFirst, you should set up your GitHub repository and Spaces app together. Add your Spaces app as an additional remote to your existing Git repository.\n\n```bash\ngit remote add space https://huggingface.co/spaces/HF_USERNAME/SPACE_NAME\n```\n\nThen force push to sync everything for the first time:\n\n```bash\ngit push --force space main\n```\n\nNext, set up a GitHub Action to push your main branch to Spaces. In the example below:", "```bash\ngit remote add space https://huggingface.co/spaces/HF_USERNAME/SPACE_NAME\n```\n\nThen force push to sync everything for the first time:\n\n```bash\ngit push --force space main\n```\n\nNext, set up a [CircleCI workflow](https://circleci.com/docs/workflows/) to push your `main` git branch to Spaces. \n\nIn the example below:\n\n* Replace `HF_USERNAME` with your username and `SPACE_NAME` with your Space name. \n* [Create a context in CircleCI](https://circleci.com/docs/contexts/) and add an env variable into it called *HF_PERSONAL_TOKEN* (you can give it any name, use the key you create in place of HF_PERSONAL_TOKEN) and the value as your Hugging Face API token. You can find your Hugging Face API token under **API Tokens** on [your Hugging Face profile](https://huggingface.co/settings/tokens).\n\n```yaml\nversion: 2.1", "### Need to upload very large (>5GB) files?\n\nTo upload large files (>5GB \ud83d\udd25) from git command-line, you need to install the custom transfer agent\nfor git-lfs, bundled in this package. \n\nTo install, just run:\n\n```bash\n$ huggingface-cli lfs-enable-largefiles\n```\n\nThis should be executed once for each model repo that contains a model file\n>5GB. If you just try to push a file bigger than 5GB without running that\ncommand, you will get an error with a message reminding you to run it.\n\nFinally, there's a `huggingface-cli lfs-multipart-upload` command but that one\nis internal (called by lfs directly) and is not meant to be called by the user.\n\n<br>\n\n## Using the Inference API wrapper\n\n`huggingface_hub` comes with a wrapper client to make calls to the Inference\nAPI! You can find some examples below, but we encourage you to visit the\nInference API\n[documentation](https://api-inference.huggingface.co/docs/python/html/detailed_parameters.html)\nto review the specific parameters for the different tasks.", "Finally, create an Action that automatically checks the file size of any new pull request:\n\n\n```yaml\nname: Check file size\non:               # or directly `on: [push]` to run the action on every push on any branch\n  pull_request:\n    branches: [main]\n\n  # to run this workflow manually from the Actions tab\n  workflow_dispatch:\n\njobs:\n  sync-to-hub:\n    runs-on: ubuntu-latest\n    steps:\n      - name: Check large files\n        uses: ActionsDesk/lfs-warning@v2.0\n        with:\n          filesizelimit: 10485760 # this is 10MB so we can sync to HF Spaces\n```", "Managing Spaces with CircleCI Workflows\n\nYou can keep your app in sync with your GitHub repository with a **CircleCI workflow**. \n\n[CircleCI](https://circleci.com) is a continuous integration and continuous delivery (CI/CD) platform that helps automate the software development process. A [CircleCI workflow](https://circleci.com/docs/workflows/) is a set of automated tasks defined in a configuration file, orchestrated by CircleCI, to streamline the process of building, testing, and deploying software applications.\n\n*Note: For files larger than 10MB, Spaces requires Git-LFS. If you don't want to use Git-LFS, you may need to review your files and check your history. Use a tool like [BFG Repo-Cleaner](https://rtyley.github.io/bfg-repo-cleaner/) to remove any large files from your history. BFG Repo-Cleaner will keep a local copy of your repository as a backup.*\n\nFirst, set up your GitHub repository and Spaces app together. Add your Spaces app as an additional remote to your existing Git repository.", "9 directories, 34 files\n```\n\nNice, we reduced the *n-gram* by more than half to less than 2GB now. In\nthe final step, let's upload all files.\n\n```python\nrepo.push_to_hub(commit_message=\"Upload lm-boosted decoder\")\n```\n\n**Output:**\n```bash\n    Git LFS: (1 of 1 files) 1.85 GB / 1.85 GB\n    Counting objects: 9, done.\n    Delta compression using up to 2 threads.\n    Compressing objects: 100% (9/9), done.\n    Writing objects: 100% (9/9), 1.23 MiB | 1.92 MiB/s, done.\n    Total 9 (delta 3), reused 0 (delta 0)\n    To https://huggingface.co/hf-test/xls-r-300m-sv\n       27d0c57..5a191e2  main -> main\n```\n\nThat's it. Now you should be able to use the *5gram* for LM-boosted\ndecoding as shown in Section 1.", "{:else}\n```bash\nconfig.json  README.md  sentencepiece.bpe.model  special_tokens_map.json  tf_model.h5  tokenizer_config.json  tokenizer.json\n```\n\nIf you look at the file sizes (for example, with `ls -lh`), you should see that the model state dict file (*t5_model.h5*) is the only outlier, at more than 400 MB.\n\n{/if}\n\n<Tip>\n\u270f\ufe0f When creating the repository from the web interface, the *.gitattributes* file is automatically set up to consider files with certain extensions, such as *.bin* and *.h5*, as large files, and git-lfs will track them with no necessary setup on your side.\n</Tip> \n\nWe can now go ahead and proceed like we would usually do with traditional Git repositories. We can add all the files to Git's staging environment using the `git add` command:\n\n```bash\ngit add .\n```\n\nWe can then have a look at the files that are currently staged:\n\n```bash\ngit status\n```"], "eval_score_GPT4": "5", "eval_feedback_GPT4": "The response accurately states that the file size limit for syncing to HF Spaces without using Git-LFS is 10MB, which aligns with the reference answer. The additional information provided about using Git-LFS or BFG Repo-Cleaner for larger files is relevant and does not detract from the factual accuracy of the response."}, {"question": "What is the title of the paper introducing the ByT5 model?\n", "true_answer": "ByT5: Towards a token-free future with pre-trained byte-to-byte models", "source_doc": "huggingface/transformers/blob/main/docs/source/en/model_doc/byt5.md", "generated_answer": "The question asks for the title of the paper introducing the ByT5 model. The context provided is from the Hugging Face Transformers documentation, which mentions that ByT5 was contributed by Patrick von Platen and is based on the T5v1.1 model. The context also mentions that ByT5 works on raw UTF-8 bytes and can be used without a tokenizer. However, there is no direct mention of a paper introducing ByT5 in this context", "retrieved_docs": ["This model was contributed by [patrickvonplaten](https://huggingface.co/patrickvonplaten). The original code can be\nfound [here](https://github.com/google-research/byt5).\n\n<Tip>\n\nByT5's architecture is based on the T5v1.1 model, refer to [T5v1.1's documentation page](t5v1.1) for the API reference. They\nonly differ in how inputs should be prepared for the model, see the code examples below.\n\n</Tip>\n\nSince ByT5 was pre-trained unsupervisedly, there's no real advantage to using a task prefix during single-task\nfine-tuning. If you are doing multi-task fine-tuning, you should use a prefix.\n\n\n## Usage example\n\nByT5 works on raw UTF-8 bytes, so it can be used without a tokenizer:\n\n```python\n>>> from transformers import T5ForConditionalGeneration\n>>> import torch\n\n>>> model = T5ForConditionalGeneration.from_pretrained(\"google/byt5-small\")", "- **mT5**: mT5 is a multilingual T5 model. It is pre-trained on the mC4 corpus, which includes 101 languages. Refer to\n  the documentation of mT5 which can be found [here](mt5).\n\n- **byT5**: byT5 is a T5 model pre-trained on byte sequences rather than SentencePiece subword token sequences. Refer\n  to the documentation of byT5 which can be found [here](byt5).\n\n- **UL2**: UL2 is a T5 like model pretrained on various denoising objectives\n\n- **Flan-T5**: Flan is a pretraining methods that is based on prompting. The Flan-T5 are T5 models trained on the Flan collection of \n    datasets which include: `taskmaster2`, `djaym7/wiki_dialog`, `deepmind/code_contests`, `lambada`, `gsm8k`, `aqua_rat`, `esnli`, `quasc` and `qed`.\n\n- **FLan-UL2** : the UL2 model finetuned using the \"Flan\" prompt tuning and dataset collection.", "## Overview\n\nThe UMT5 model was proposed in [UniMax: Fairer and More Effective Language Sampling for Large-Scale Multilingual Pretraining](https://openreview.net/forum?id=kXwdL1cWOAi) by Hyung Won Chung, Xavier Garcia, Adam Roberts, Yi Tay, Orhan Firat, Sharan Narang, Noah Constant.\n\nThe abstract from the paper is the following:", "Similar to [T5](t5), ByT5 was trained on the span-mask denoising task. However, \nsince the model works directly on characters, the pretraining task is a bit \ndifferent. Let's corrupt some characters of the \ninput sentence `\"The dog chases a ball in the park.\"` and ask ByT5 to predict them \nfor us.\n\n```python\n>>> from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n>>> import torch\n\n>>> tokenizer = AutoTokenizer.from_pretrained(\"google/byt5-base\")\n>>> model = AutoModelForSeq2SeqLM.from_pretrained(\"google/byt5-base\")\n\n>>> input_ids_prompt = \"The dog chases a ball in the park.\"\n>>> input_ids = tokenizer(input_ids_prompt).input_ids", "--\ntitle: \"Speech Synthesis, Recognition, and More With SpeechT5\"\nthumbnail: /blog/assets/speecht5/thumbnail.png\nauthors:\n- user: Matthijs\n---\n\n# Speech Synthesis, Recognition, and More With SpeechT5\n\n\nWe\u2019re happy to announce that SpeechT5 is now available in \ud83e\udd17 Transformers, an open-source library that offers easy-to-use implementations of state-of-the-art machine learning models.\n\nSpeechT5 was originally described in the paper [SpeechT5: Unified-Modal Encoder-Decoder Pre-Training for Spoken Language Processing](https://arxiv.org/abs/2110.07205) by Microsoft Research Asia. The [official checkpoints](https://github.com/microsoft/SpeechT5) published by the paper\u2019s authors are available on the Hugging Face Hub.\n\nIf you want to jump right in, here are some demos on Spaces:", "|                          [ByT5](model_doc/byt5)                          |       \u2705        |         \u2705         |      \u2705      |\n|                     [CamemBERT](model_doc/camembert)                     |       \u2705        |         \u2705         |      \u274c      |\n|                        [CANINE](model_doc/canine)                        |       \u2705        |         \u274c         |      \u274c      |\n|                  [Chinese-CLIP](model_doc/chinese_clip)                  |       \u2705        |         \u274c         |      \u274c      |\n|                          [CLAP](model_doc/clap)                          |       \u2705        |         \u274c         |      \u274c      |\n|                          [CLIP](model_doc/clip)                          |       \u2705        |         \u2705         |      \u2705      |\n|                       [CLIPSeg](model_doc/clipseg)                       |       \u2705        |         \u274c         |      \u274c      |\n|                          [CLVP](model_doc/clvp)                          |       \u2705        |         \u274c         |      \u274c      |\n|                       [CodeGen](model_doc/codegen)                       |       \u2705        |         \u274c         |      \u274c      |\n|                    [CodeLlama](model_doc/code_llama)                     |       \u2705        |         \u274c         |      \u2705      |\n|              [Conditional DETR](model_doc/conditional_detr)              |       \u2705        |         \u274c         |      \u274c      |\n|                      [ConvBERT](model_doc/convbert)                      |       \u2705        |         \u2705         |      \u274c      |", "-->\n\n# SpeechT5\n\n## Overview\n\nThe SpeechT5 model was proposed in [SpeechT5: Unified-Modal Encoder-Decoder Pre-Training for Spoken Language Processing](https://arxiv.org/abs/2110.07205) by Junyi Ao, Rui Wang, Long Zhou, Chengyi Wang, Shuo Ren, Yu Wu, Shujie Liu, Tom Ko, Qing Li, Yu Zhang, Zhihua Wei, Yao Qian, Jinyu Li, Furu Wei.\n\nThe abstract from the paper is the following:"], "eval_score_GPT4": "1", "eval_feedback_GPT4": "The response does not provide the title of the paper introducing the ByT5 model. Instead, it gives background information about the ByT5 model and its features, which is not what the instruction asked for. The response does not meet the criteria of being correct, accurate, and factual in terms of providing the specific paper title."}, {"question": "What is the dimension of the feature vector for the base BERT model?\n", "true_answer": "768", "source_doc": "huggingface/course/blob/main/subtitles/en/raw/chapter1/05_encoders.md", "generated_answer": "The dimension of the feature vector for the base BERT model is 768, as stated in the context when describing the numerical representation of words in the model. This information can be found in the first extract provided, where it is mentioned that each vector produced by the encoder for a passed-through word contains a value for that word in its context, with a dimension of 768 defined by the model's architecture.", "retrieved_docs": ["Let's dive in this representation. It contains one vector per word that was passed through the encoder. Each of these vector is a numerical representation of the word in question. The dimension of that vector is defined by the architecture of the model, for the base BERT model, it is 768. These representations contain the value of a word; but contextualized. For example, the vector attributed to the word \"to\", isn't the representation of only the \"to\" word. It also takes into account the words around it, which we call the \u201ccontext\u201d.As in, it looks to the left context, the word on the left of the one we're studying (here the word \"Welcome\") and the context on the right (here the word \"NYC\") and outputs a value for the word, within its context. It is therefore a contextualized value. One could say that the vector of 768 values holds the \"meaning\" of that word in the text. How it does this is thanks to the self-attention mechanism. The self-attention mechanism relates to different positions (or different words) in a single sequence, in order to compute a representation of that sequence", "- `distilbert-base-uncased`: DistilBERT English language model pretrained on the same data used to pretrain Bert (concatenation of the Toronto Book Corpus and full English Wikipedia) using distillation with the supervision of the `bert-base-uncased` version of Bert. The model has 6 layers, 768 dimension and 12 heads, totalizing 66M parameters.\n- `distilbert-base-uncased-distilled-squad`: A finetuned version of `distilbert-base-uncased` finetuned using (a second step of) knowledge distillation on SQuAD 1.0. This model reaches a F1 score of 86.9 on the dev set (for comparison, Bert `bert-base-uncased` version reaches a 88.5 F1 score).\n- `distilbert-base-cased`: DistilBERT English language model pretrained on the same data used to pretrain Bert (concatenation of the Toronto Book Corpus and full English Wikipedia) using distillation with the supervision of the `bert-base-cased` version of Bert. The model has 6 layers, 768 dimension and 12 heads, totalizing 65M parameters.", "We ran 144 experiments on 6 different model architectures:\n* Accelerators: Inf1, Inf2, NVIDIA A10G\n* Models: [BERT-base](https://huggingface.co/bert-base-uncased), [BERT-Large](https://huggingface.co/bert-large-uncased), [RoBERTa-base](https://huggingface.co/roberta-base), [DistilBERT](https://huggingface.co/distilbert-base-uncased), [ALBERT-base](https://huggingface.co/albert-base-v2), [ViT-base](https://huggingface.co/google/vit-base-patch16-224)\n* Sequence length: 8, 16, 32, 64, 128, 256, 512\n* Batch size: 1", ". Now one problem we have to deal with is that Transformer models like BERT will actually return one embedding vector per token. For example in the sentence \"I took my dog for a walk\", we can expect several embedding vectors, one for each word. For example, here we can see the output of our model has produced 9 embedding vectors per sentence, and each vector has 384 dimensions. But what we really want is a single embedding vector for the whole sentence. To deal with this, we can use a technique called pooling. The simplest pooling method is to just take the token embedding of the CLS token. Alternatively, we can average the token embeddings which is called mean pooling. With mean pooling only thing we need to make sure is that we don't include the padding tokens in the average, which is why you can see the attention mask being used here. This now gives us one 384 dimensional vector per sentence which is exactly what we want! And once we have our sentence embeddings, we can compute the cosine similarity for each pair of vectors", "`bert-base-cased` is limited to 512 tokens, which means we would have to\ncut possibly important information from the article. Because most of the\nimportant information is often found at the beginning of articles and\nbecause we want to be computationally efficient, we decide to stick to\n`bert-base-cased` with a `max_length` of 512 in this notebook. This\nchoice is not optimal but has shown to yield [good\nresults](https://arxiv.org/abs/1907.12461) on CNN/Dailymail.\nAlternatively, one could leverage long-range sequence models, such as\n[Longformer](https://huggingface.co/allenai/longformer-large-4096) to be\nused as the encoder.\n\nRegarding the summary length, we can see that a length of 128 already\nincludes all of the summary labels. 128 is easily within the limits of\n`bert-base-cased`, so we decide to limit the generation to 128.\n\nAgain, we will make use of the `.map()` function - this time to\ntransform each training batch into a batch of model inputs.", "bert-base-uncased          8               8             0.005\nbert-base-uncased          8               32            0.008\nbert-base-uncased          8              128            0.022\nbert-base-uncased          8              512            0.105\n--------------------------------------------------------------------------------", "bert-base-uncased          8              128            1330\nbert-base-uncased          8              512            1770\n--------------------------------------------------------------------------------"], "eval_score_GPT4": "5", "eval_feedback_GPT4": "The response correctly identifies the dimension of the feature vector for the base BERT model as 768. The explanation provided is accurate and aligns with the reference answer, which also states the dimension as 768. The response is factual and directly addresses the question posed in the instruction."}, {"question": "What special identifier does the WordPiece Model use for continuing subwords?\n", "true_answer": "##", "source_doc": "huggingface/tokenizers/blob/main/docs/source-doc-builder/components.mdx", "generated_answer": "The special identifier that the WordPiece model uses for continuing subwords is the prefix \"##\". This prefix is added to all the characters inside a word during the initial splitting process, and each word is initially split by adding this prefix to all its characters. This allows WordPiece to identify subwords by adding this prefix to all the characters inside a word, and it helps in reducing the vocabulary sizes by sharing information across different words. The prefix \"##\" is used to indicate that the", "retrieved_docs": ["<a id='wordpiece'></a>\n\n### WordPiece\n\nWordPiece is the subword tokenization algorithm used for [BERT](model_doc/bert), [DistilBERT](model_doc/distilbert), and [Electra](model_doc/electra). The algorithm was outlined in [Japanese and Korean\nVoice Search (Schuster et al., 2012)](https://static.googleusercontent.com/media/research.google.com/ja//pubs/archive/37842.pdf) and is very similar to\nBPE. WordPiece first initializes the vocabulary to include every character present in the training data and\nprogressively learns a given number of merge rules. In contrast to BPE, WordPiece does not choose the most frequent\nsymbol pair, but the one that maximizes the likelihood of the training data once added to the vocabulary.", "<Youtube id=\"qpv6ms_t_1A\"/>\n\n<Tip>\n\n\ud83d\udca1 This section covers WordPiece in depth, going as far as showing a full implementation. You can skip to the end if you just want a general overview of the tokenization algorithm.\n\n</Tip>\n\n## Training algorithm[[training-algorithm]]\n\n<Tip warning={true}>\n\n\u26a0\ufe0f Google never open-sourced its implementation of the training algorithm of WordPiece, so what follows is our best guess based on the published literature. It may not be 100% accurate.\n\n</Tip>\n\nLike BPE, WordPiece starts from a small vocabulary including the special tokens used by the model and the initial alphabet. Since it identifies subwords by adding a prefix (like `##` for BERT), each word is initially split by adding that prefix to all the characters inside the word. So, for instance, `\"word\"` gets split like this:\n\n```\nw ##o ##r ##d\n```\n\nThus, the initial alphabet contains all the characters present at the beginning of a word and the characters present inside a word preceded by the WordPiece prefix.", ". The ## comes from the BERT tokenizer, based on the WordPiece algorithm. Other tokenizers use other prefixes, which can be placed to indicate part of words like seen here, or start of words instead! There are a lot of different algorithms that can be used for subword tokenization, and most models obtaining state-of-the-art results in English today use some kind of subword-tokenization algorithm. These approaches help in reducing the vocabulary sizes by sharing information across different words, having the ability to have prefixes and suffixes understood as such. They keep meaning across very similar words, by recognizing similar tokens making them up.", "<Youtube id=\"VFp38yj8h3A\"/>\n\nAs we saw in [the preprocessing tutorial](preprocessing), tokenizing a text is splitting it into words or\nsubwords, which then are converted to ids through a look-up table. Converting words or subwords to ids is\nstraightforward, so in this summary, we will focus on splitting a text into words or subwords (i.e. tokenizing a text).\nMore specifically, we will look at the three main types of tokenizers used in \ud83e\udd17 Transformers: [Byte-Pair Encoding\n(BPE)](#byte-pair-encoding), [WordPiece](#wordpiece), and [SentencePiece](#sentencepiece), and show examples\nof which tokenizer type is used by which model.\n\nNote that on each model page, you can look at the documentation of the associated tokenizer to know which tokenizer\ntype was used by the pretrained model. For instance, if we look at [`BertTokenizer`], we can see\nthat the model uses [WordPiece](#wordpiece).\n\n## Introduction", "explain: \"That's correct!\",\n\t\t\tcorrect: true\n\t\t},\n\t\t{\n\t\t\ttext: \"WordPiece tokenizes words into subwords by finding the most likely segmentation into tokens, according to the model.\",\n\t\t\texplain: \"No, that's how another tokenization algorithm works.\"\n\t\t},\n\t\t{\n\t\t\ttext: \"WordPiece tokenizes words into subwords by finding the longest subword starting from the beginning that is in the vocabulary, then repeating the process for the rest of the text.\",\n\t\t\texplain: \"Yes, this is how WordPiece proceeds for the encoding.\",\n\t\t\tcorrect: true\n\t\t},\n\t]}\n/>", "| WordPiece | This is a subword tokenization algorithm quite similar to BPE, used mainly by Google in models like BERT. It uses a greedy algorithm, that tries to build long words first, splitting in multiple tokens when entire words don\u2019t exist in the vocabulary. This is different from BPE that starts from characters, building bigger tokens as possible. It uses the famous `##` prefix to identify tokens that are part of a word (ie not starting a word).  |\n| Unigram | Unigram is also a subword tokenization algorithm, and works by trying to identify the best set of subword tokens to maximize the probability for a given sentence. This is different from BPE in the way that this is not deterministic based on a set of rules applied sequentially. Instead Unigram will be able to compute multiple ways of tokenizing, while choosing the most probable one. |", "</Tip>\n\n## Tokenization algorithm[[tokenization-algorithm]]\n\nTokenization differs in WordPiece and BPE in that WordPiece only saves the final vocabulary, not the merge rules learned. Starting from the word to tokenize, WordPiece finds the longest subword that is in the vocabulary, then splits on it. For instance, if we use the vocabulary learned in the example above, for the word `\"hugs\"` the longest subword starting from the beginning that is inside the vocabulary is `\"hug\"`, so we split there and get `[\"hug\", \"##s\"]`. We then continue with `\"##s\"`, which is in the vocabulary, so the tokenization of `\"hugs\"` is `[\"hug\", \"##s\"]`.\n\nWith BPE, we would have applied the merges learned in order and tokenized this as `[\"hu\", \"##gs\"]`, so the encoding is different."], "eval_score_GPT4": "3", "eval_feedback_GPT4": "Feedback: The response correctly identifies the special identifier used by the WordPiece model as \"##\". However, the explanation provided is not entirely accurate. The prefix \"##\" is not added to all characters within a word during the initial splitting process, nor is it added to each word initially. Instead, \"##\" is used to denote the continuation of a subword within a larger word, indicating that the subword is not a separate entity but a part of a larger word. The response contains some factual elements but also includes inaccuracies regarding the application of the \"##\" prefix."}, {"question": "What is the purpose of the \ud83e\udde8 Diffusers tutorials?\n", "true_answer": "To provide a gentle introduction to diffusion models and help understand the library fundamentals.", "source_doc": "huggingface/diffusers/blob/main/docs/source/en/tutorials/tutorial_overview.md", "generated_answer": "The purpose of the \ud83e\udde8 Diffusers tutorials, as mentioned in the context, is to provide a basic crash course for learning how to use the library's most important features like using models and schedulers to build your own diffusion system, and training your own diffusion model. The tutorial can be found at this link: https://huggingface.co/docs/diffusers/tutorials/tutorial_overview.", "retrieved_docs": ["\ud83e\udd17 Diffusers is the go-to library for state-of-the-art pretrained diffusion models for generating images, audio, and even 3D structures of molecules. Whether you're looking for a simple inference solution or training your own diffusion models, \ud83e\udd17 Diffusers is a modular toolbox that supports both. Our library is designed with a focus on [usability over performance](https://huggingface.co/docs/diffusers/conceptual/philosophy#usability-over-performance), [simple over easy](https://huggingface.co/docs/diffusers/conceptual/philosophy#simple-over-easy), and [customizability over abstractions](https://huggingface.co/docs/diffusers/conceptual/philosophy#tweakable-contributorfriendly-over-abstraction).\n\n\ud83e\udd17 Diffusers offers three core components:", "# Diffusers\n\n\ud83e\udd17 Diffusers is the go-to library for state-of-the-art pretrained diffusion models for generating images, audio, and even 3D structures of molecules. Whether you're looking for a simple inference solution or want to train your own diffusion model, \ud83e\udd17 Diffusers is a modular toolbox that supports both. Our library is designed with a focus on [usability over performance](conceptual/philosophy#usability-over-performance), [simple over easy](conceptual/philosophy#simple-over-easy), and [customizability over abstractions](conceptual/philosophy#tweakable-contributorfriendly-over-abstraction).\n\nThe library has three main components:\n\n- State-of-the-art diffusion pipelines for inference with just a few lines of code. There are many pipelines in \ud83e\udd17 Diffusers, check out the table in the pipeline [overview](api/pipelines/overview) for a complete list of available pipelines and the task they solve.\n- Interchangeable [noise schedulers](api/schedulers/overview) for balancing trade-offs between generation speed and quality.\n- Pretrained [models](api/models) that can be used as building blocks, and combined with schedulers, for creating your own end-to-end diffusion systems.", "Training examples of the Diffusers library should adhere to the following philosophy:\n- All the code necessary to run the examples should be found in a single Python file.\n- One should be able to run the example from the command line with `python <your-example>.py --args`.\n- Examples should be kept simple and serve as **an example** on how to use Diffusers for training. The purpose of example scripts is **not** to create state-of-the-art diffusion models, but rather to reproduce known training schemes without adding too much custom logic. As a byproduct of this point, our examples also strive to serve as good educational materials.", "Training examples of the Diffusers library should adhere to the following philosophy:\n- All the code necessary to run the examples should be found in a single Python file.\n- One should be able to run the example from the command line with `python <your-example>.py --args`.\n- Examples should be kept simple and serve as **an example** on how to use Diffusers for training. The purpose of example scripts is **not** to create state-of-the-art diffusion models, but rather to reproduce known training schemes without adding too much custom logic. As a byproduct of this point, our examples also strive to serve as good educational materials.", "Using \ud83e\udde8 `diffusers` at Hugging Face\n\nDiffusers is the go-to library for state-of-the-art pretrained diffusion models for generating images, audio, and even 3D structures of molecules. Whether you\u2019re looking for a simple inference solution or want to train your own diffusion model, Diffusers is a modular toolbox that supports both. The library is designed with a focus on usability over performance, simple over easy, and customizability over abstractions.\n\n## Exploring Diffusers in the Hub\n\nThere are over 10,000 `diffusers` compatible pipelines on the Hub which you can find by filtering at the left of [the models page](https://huggingface.co/models?library=diffusers&sort=downloads). Diffusion systems are typically composed of multiple components such as text encoder, UNet, VAE, and scheduler. Even though they are not standalone models, the pipeline abstraction makes it easy to use them for inference or training.\n\nYou can find diffusion pipelines for many different tasks:", "\ud83e\udde8 Diffusers Experimental\n\nWe are adding experimental code to support novel applications and usages of the Diffusers library.\nCurrently, the following experiments are supported:\n* Reinforcement learning via an implementation of the [Diffuser](https://arxiv.org/abs/2205.09991) model.", "| [Tutorial](https://huggingface.co/docs/diffusers/tutorials/tutorial_overview)                                                            | A basic crash course for learning how to use the library's most important features like using models and schedulers to build your own diffusion system, and training your own diffusion model.  |\n| [Loading](https://huggingface.co/docs/diffusers/using-diffusers/loading_overview)                                                             | Guides for how to load and configure all the components (pipelines, models, and schedulers) of the library, as well as how to use different schedulers.                                         |\n| [Pipelines for inference](https://huggingface.co/docs/diffusers/using-diffusers/pipeline_overview)                                             | Guides for how to use pipelines for different inference tasks, batched generation, controlling generated outputs and randomness, and how to contribute a pipeline to the library.               |\n| [Optimization](https://huggingface.co/docs/diffusers/optimization/opt_overview)                                                        | Guides for how to optimize your diffusion model to run faster and consume less memory.                                                                                                          |"], "eval_score_GPT4": "5", "eval_feedback_GPT4": "The response accurately identifies the purpose of the \ud83e\udde8 Diffusers tutorials by mentioning that they are designed to provide a basic understanding of how to use the library's features, such as models and schedulers, and how to build and train a diffusion model. This aligns well with the reference answer, which states that the tutorials are meant to offer a gentle introduction to diffusion models and help understand the library fundamentals. The response goes a bit further by providing additional details about the content of the tutorials and even includes a link to the tutorial overview, which is not necessary but does not detract from the accuracy of the response. Based on the score rubric, the response is completely correct, accurate, and factual."}, {"question": "What is the default setting for the `allow_flagging` parameter in Gradio's `Interface`?\n", "true_answer": "\"manual\"", "source_doc": "gradio-app/gradio/blob/main/guides/cn/07_other-tutorials/using-flagging.md", "generated_answer": "The default setting for the `allow_flagging` parameter in Gradio's `Interface` is \"manual\". This means that users will see a button to flag, and samples are only flagged when the button is clicked. (Source: Extract 3)", "retrieved_docs": ["## The **Flag** button in `gradio.Interface`\n\nFlagging with Gradio's `Interface` is especially easy. By default, underneath the output components, there is a button marked **Flag**. When a user testing your model sees input with interesting output, they can click the flag button to send the input and output data back to the machine where the demo is running. The sample is saved to a CSV log file (by default). If the demo involves images, audio, video, or other types of files, these are saved separately in a parallel directory and the paths to these files are saved in the CSV file.\n\nThere are [four parameters](https://gradio.app/docs/#interface-header) in `gradio.Interface` that control how flagging works. We will go over them in greater detail.", "- Adds ability to set `flagging_options` display text and saved flag separately by [@abidlabs](https://github.com/abidlabs) in [PR 3289](https://github.com/gradio-app/gradio/pull/3289)\n- Allow the setting of `brush_radius` for the `Image` component both as a default and via `Image.update()` by [@pngwn](https://github.com/pngwn) in [PR 3277](https://github.com/gradio-app/gradio/pull/3277)\n- Added `info=` argument to form components to enable extra context provided to users, by [@aliabid94](https://github.com/aliabid94) in [PR 3291](https://github.com/gradio-app/gradio/pull/3291)", "- Adds ability to set `flagging_options` display text and saved flag separately by [@abidlabs](https://github.com/abidlabs) in [PR 3289](https://github.com/gradio-app/gradio/pull/3289)\n- Allow the setting of `brush_radius` for the `Image` component both as a default and via `Image.update()` by [@pngwn](https://github.com/pngwn) in [PR 3277](https://github.com/gradio-app/gradio/pull/3277)\n- Added `info=` argument to form components to enable extra context provided to users, by [@aliabid94](https://github.com/aliabid94) in [PR 3291](https://github.com/gradio-app/gradio/pull/3291)", "- `allow_flagging`: this parameter can be set to either `\"manual\"` (default), `\"auto\"`, or `\"never\"`.\n  - `manual`: users will see a button to flag, and samples are only flagged when the button is clicked.\n  - `auto`: users will not see a button to flag, but every sample will be flagged automatically.\n  - `never`: users will not see a button to flag, and no sample will be flagged.\n- `flagging_options`: this parameter can be either `None` (default) or a list of strings.\n  - If `None`, then the user simply clicks on the **Flag** button and no additional options are shown.\n  - If a list of strings are provided, then the user sees several buttons, corresponding to each of the strings that are provided. For example, if the value of this parameter is `[\"Incorrect\", \"Ambiguous\"]`, then buttons labeled **Flag as Incorrect** and **Flag as Ambiguous** appear. This only applies if `allow_flagging` is `\"manual\"`.\n  - The chosen option is then logged along with the input and output.", "iface.launch()\n```\n\nNotice that we define our own\ninstance of `gradio.HuggingFaceDatasetSaver` using our Hugging Face token and\nthe name of a dataset we'd like to save samples to. In addition, we also set `allow_flagging=\"manual\"`\nbecause on Hugging Face Spaces, `allow_flagging` is set to `\"never\"` by default. Here's our demo:\n\n<gradio-app space=\"gradio/calculator-flagging-crowdsourced/\"></gradio-app>\n\nYou can now see all the examples flagged above in this [public Hugging Face dataset](https://huggingface.co/datasets/aliabd/crowdsourced-calculator-demo).\n\n![flagging callback hf](https://github.com/gradio-app/gradio/blob/main/guides/assets/flagging-callback-hf.png?raw=true)", "\u5982\u679c\u6211\u4eec\u56de\u5230\u8ba1\u7b97\u5668\u793a\u4f8b\uff0c\u4e0b\u9762\u7684\u4ee3\u7801\u5c06\u521b\u5efa\u5d4c\u5165\u5176\u4e2d\u7684\u754c\u9762\u3002\n\n```python\niface = gr.Interface(\n    calculator,\n    [\"number\", gr.Radio([\"add\", \"subtract\", \"multiply\", \"divide\"]), \"number\"],\n    \"number\",\n    allow_flagging=\"manual\",\n    flagging_options=[\"wrong sign\", \"off by one\", \"other\"]\n)\n\niface.launch()\n```\n\n<gradio-app space=\"gradio/calculator-flagging-options/\"></gradio-app>\n\n\u5f53\u7528\u6237\u70b9\u51fb\u6807\u8bb0\u6309\u94ae\u65f6\uff0cCSV \u6587\u4ef6\u73b0\u5728\u5c06\u5305\u62ec\u6307\u793a\u6240\u9009\u9009\u9879\u7684\u5217\u3002\n\n_flagged/logs.csv_", "iface.launch()\n```\n\n<gradio-app space=\"gradio/calculator-flagging-options/\"></gradio-app>\n\nWhen users click the flag button, the csv file will now include a column indicating the selected option.\n\n_flagged/logs.csv_\n\n```csv\nnum1,operation,num2,Output,flag,timestamp\n5,add,7,-12,wrong sign,2022-02-04 11:40:51.093412\n6,subtract,1.5,3.5,off by one,2022-02-04 11:42:32.062512\n```\n\n## The HuggingFaceDatasetSaver Callback\n\nSometimes, saving the data to a local CSV file doesn't make sense. For example, on Hugging Face\nSpaces, developers typically don't have access to the underlying ephemeral machine hosting the Gradio\ndemo. That's why, by default, flagging is turned off in Hugging Face Space. However,\nyou may want to do something else with the flagged data.\n\nWe've made this super easy with the `flagging_callback` parameter."], "eval_score_GPT4": "5", "eval_feedback_GPT4": "Feedback: The response provided is completely correct, accurate, and factual. It matches the reference answer and includes additional context about what the \"manual\" setting means for the user experience, which aligns with the expected functionality of the `allow_flagging` parameter in Gradio's `Interface`."}, {"question": "Where can the full code for the Stable Diffusion demo be found?\n", "true_answer": "https://hf.co/spaces/stabilityai/stable-diffusion/tree/main", "source_doc": "gradio-app/gradio/blob/main/demo/stable-diffusion/DESCRIPTION.md", "generated_answer": "Answer: The full code for the Stable Diffusion demo can be found at the following link: https://hf.co/spaces/stabilityai/stable-diffusion/tree/main. This simplified version of the code is also mentioned in the extract at the beginning of the context.", "retrieved_docs": ["ote: This is a simplified version of the code needed to create the Stable Diffusion demo. See full code here: https://hf.co/spaces/stabilityai/stable-diffusion/tree/main", "Gradio Demo: stable-diffusion\n### Note: This is a simplified version of the code needed to create the Stable Diffusion demo. See full code here: https://hf.co/spaces/stabilityai/stable-diffusion/tree/main\n        \n\n\n```\n!pip install -q gradio diffusers transformers nvidia-ml-py3 ftfy torch\n```\n\n\n```\nimport gradio as gr\nimport torch\nfrom diffusers import StableDiffusionPipeline\nfrom PIL import Image\nimport os\n\nauth_token = os.getenv(\"auth_token\")\nmodel_id = \"CompVis/stable-diffusion-v1-4\"\ndevice = \"cpu\"\npipe = StableDiffusionPipeline.from_pretrained(\n    model_id, use_auth_token=auth_token, revision=\"fp16\", torch_dtype=torch.float16\n)\npipe = pipe.to(device)", "The original codebase can be found at [Xiang-cd/DiffEdit-stable-diffusion](https://github.com/Xiang-cd/DiffEdit-stable-diffusion), and you can try it out in this [demo](https://blog.problemsolversguild.com/technical/research/2022/11/02/DiffEdit-Implementation.html).\n\nThis pipeline was contributed by [clarencechen](https://github.com/clarencechen). \u2764\ufe0f\n\n## Tips", "The original codebase can be found at [LambdaLabsML/lambda-diffusers](https://github.com/LambdaLabsML/lambda-diffusers#stable-diffusion-image-variations) and additional official checkpoints for image variation can be found at [lambdalabs/sd-image-variations-diffusers](https://huggingface.co/lambdalabs/sd-image-variations-diffusers).\n\n<Tip>\n\nMake sure to check out the Stable Diffusion [Tips](./overview#tips) section to learn how to explore the tradeoff between scheduler speed and quality, and how to reuse pipeline components efficiently!\n\n</Tip>\n\n## StableDiffusionImageVariationPipeline\n\n[[autodoc]] StableDiffusionImageVariationPipeline\n\t- all\n\t- __call__\n\t- enable_attention_slicing\n\t- disable_attention_slicing\n\t- enable_xformers_memory_efficient_attention\n\t- disable_xformers_memory_efficient_attention\n\n## StableDiffusionPipelineOutput\n\n[[autodoc]] pipelines.stable_diffusion.StableDiffusionPipelineOutput", "## Tips:\n\n- Stable Diffusion has the same architecture as [Latent Diffusion](https://arxiv.org/abs/2112.10752) but uses a frozen CLIP Text Encoder instead of training the text encoder jointly with the diffusion model.\n- An in-detail explanation of the Stable Diffusion model can be found under [Stable Diffusion with \ud83e\udde8 Diffusers](https://huggingface.co/blog/stable_diffusion).\n- If you don't want to rely on the Hugging Face Hub and having to pass a authentication token, you can\ndownload the weights with `git lfs install; git clone https://huggingface.co/runwayml/stable-diffusion-v1-5` and instead pass the local path to the cloned folder to `from_pretrained` as shown below.\n- Stable Diffusion can work with a variety of different samplers as is shown below.\n\n## Available Pipelines:", "## 0.4.0\n\n### Features\n\n- [#5423](https://github.com/gradio-app/gradio/pull/5423) [`bb31cd7d`](https://github.com/gradio-app/gradio/commit/bb31cd7dd0dc60c18b2b21269512775f3784ef01) - Remove stable diffusion demo from landing page. Thanks [@aliabd](https://github.com/aliabd)!\n\n## 0.3.0\n\n### Features", "* [`apple/ml-stable-diffusion`](https://github.com/apple/ml-stable-diffusion), by Apple. Conversion and inference library for Swift (and Python).\n* [`huggingface/swift-coreml-diffusers`](https://github.com/huggingface/swift-coreml-diffusers). Hugging Face demo app, built on top of Apple's package.\n* [Stable Diffusion XL 1.0 base (Core ML version)](https://huggingface.co/apple/coreml-stable-diffusion-xl-base). Model ready to run using the repos above and other third-party apps.\n* [Stable Diffusion XL 1.0 base, with mixed-bit palettization (Core ML)](https://huggingface.co/apple/coreml-stable-diffusion-mixed-bit-palettization/blob/main/coreml-stable-diffusion-mixed-bit-palettization_original_compiled.zip). Same model as above, with UNet quantized with an effective palettization of 4.5 bits (on average)."], "eval_score_GPT4": "5", "eval_feedback_GPT4": "The response provided correctly identifies the location of the full code for the Stable Diffusion demo, matching the reference answer. The link given is accurate and factual, leading to the same resource as the reference answer. Therefore, the response meets the criteria for the highest score on the rubric."}, {"question": "What transformation does the FNet model use to replace the self-attention layer in a BERT model?\n", "true_answer": "Fourier transform", "source_doc": "huggingface/transformers/blob/main/docs/source/en/model_doc/fnet.md", "generated_answer": "The FNet model replaces the self-attention layer in a BERT model with a fourier transform that returns only the real parts of the transform. This transformation significantly reduces the number of parameters and memory usage of the model, making it faster to train and more memory-efficient than the BERT model. The FNet model achieves similar accuracy to BERT counterparts on the GLUE benchmark, with faster training times on both GPUs and TPUs, especially at longer input", "retrieved_docs": ["-->\n\n# FNet\n\n## Overview\n\nThe FNet model was proposed in [FNet: Mixing Tokens with Fourier Transforms](https://arxiv.org/abs/2105.03824) by\nJames Lee-Thorp, Joshua Ainslie, Ilya Eckstein, Santiago Ontanon. The model replaces the self-attention layer in a BERT\nmodel with a fourier transform which returns only the real parts of the transform. The model is significantly faster\nthan the BERT model because it has fewer parameters and is more memory efficient. The model achieves about 92-97%\naccuracy of BERT counterparts on GLUE benchmark, and trains much faster than the BERT model. The abstract from the\npaper is the following:", "*We show that Transformer encoder architectures can be sped up, with limited accuracy costs, by replacing the\nself-attention sublayers with simple linear transformations that \"mix\" input tokens. These linear mixers, along with\nstandard nonlinearities in feed-forward layers, prove competent at modeling semantic relationships in several text\nclassification tasks. Most surprisingly, we find that replacing the self-attention sublayer in a Transformer encoder\nwith a standard, unparameterized Fourier Transform achieves 92-97% of the accuracy of BERT counterparts on the GLUE\nbenchmark, but trains 80% faster on GPUs and 70% faster on TPUs at standard 512 input lengths. At longer input lengths,\nour FNet model is significantly faster: when compared to the \"efficient\" Transformers on the Long Range Arena\nbenchmark, FNet matches the accuracy of the most accurate models, while outpacing the fastest models across all\nsequence lengths on GPUs (and across relatively shorter lengths on TPUs). Finally, FNet has a light memory footprint\nand is particularly efficient at smaller model sizes; for a fixed speed and accuracy budget, small FNet models\noutperform Transformer counterparts.*", "2.  Second, BERT\\'s *bi-directional* self-attention layers have to be\n    changed to *uni-directional* self-attention layers to comply with\n    auto-regressive generation. Because both the bi-directional and the\n    uni-directional self-attention layer are based on the same *key*,\n    *query* and *value* projection weights, the decoder\\'s\n    self-attention layer weights can be initialized with BERT\\'s\n    self-attention layer weights. *E.g.* the query, key and value weight\n    parameters of the decoder\\'s uni-directional self-attention layer\n    are initialized with those of BERT\\'s bi-directional self-attention", "Here's a TLDR explaining how Perceiver works:\n\nThe main problem with the self-attention mechanism of the Transformer is that the time and memory requirements scale\nquadratically with the sequence length. Hence, models like BERT and RoBERTa are limited to a max sequence length of 512\ntokens. Perceiver aims to solve this issue by, instead of performing self-attention on the inputs, perform it on a set\nof latent variables, and only use the inputs for cross-attention. In this way, the time and memory requirements don't\ndepend on the length of the inputs anymore, as one uses a fixed amount of latent variables, like 256 or 512. These are\nrandomly initialized, after which they are trained end-to-end using backpropagation.", "As an example, two years ago, one might have defined BERT's self attention layer as the standard attention layer used by all Transformers models. Logically, a \"standard\" attention function could have been moved into a central `attention.py` file. But then came attention layers that added relative positional embeddings in each attention layer (T5), multiple different forms of chunked attention (Reformer, Longformer, BigBird), and separate attention mechanism for position and word embeddings (DeBERTa), etc... Every time we would have to have asked ourselves whether the \"standard\" attention function should be adapted or whether it would have been better to add a new attention function to `attention.py`. But then how do we name it? `attention_with_positional_embd`, `reformer_attention`, `deberta_attention`?", "Each BERT block makes use of **bi-directional** self-attention to\nprocess an input sequence \\\\(\\mathbf{x'}_1, \\ldots, \\mathbf{x'}_n\\\\) (shown\nin light grey) to a more \\\"refined\\\" contextualized output sequence\n\\\\(\\mathbf{x''}_1, \\ldots, \\mathbf{x''}_n\\\\) (shown in slightly darker grey)\n\\\\({}^4\\\\). The contextualized output sequence of the final BERT block,\n*i.e.* \\\\(\\mathbf{\\overline{X}}_{1:n}\\\\), can then be mapped to a single\noutput class \\\\(c\\\\) by adding a *task-specific* classification layer (shown\nin orange) as explained above.\n\n*Encoder-only* models can only map an input sequence to an output\nsequence of *a priori* known output length. In conclusion, the output\ndimension does not depend on the input sequence, which makes it\ndisadvantageous and impractical to use encoder-only models for\nsequence-to-sequence tasks.", "### feed forward chunking\n\nIn each residual attention block in transformers the self-attention layer is usually followed by 2 feed forward layers.\nThe intermediate embedding size of the feed forward layers is often bigger than the hidden size of the model (e.g., for\n`bert-base-uncased`).\n\nFor an input of size `[batch_size, sequence_length]`, the memory required to store the intermediate feed forward\nembeddings `[batch_size, sequence_length, config.intermediate_size]` can account for a large fraction of the memory\nuse. The authors of [Reformer: The Efficient Transformer](https://arxiv.org/abs/2001.04451) noticed that since the\ncomputation is independent of the `sequence_length` dimension, it is mathematically equivalent to compute the output\nembeddings of both feed forward layers `[batch_size, config.hidden_size]_0, ..., [batch_size, config.hidden_size]_n`\nindividually and concat them afterward to `[batch_size, sequence_length, config.hidden_size]` with `n = sequence_length`, which trades increased computation time against reduced memory use, but yields a mathematically\n**equivalent** result."], "eval_score_GPT4": "5", "eval_feedback_GPT4": "Feedback: The response correctly identifies the Fourier transform as the replacement for the self-attention layer in the FNet model, which aligns with the reference answer. It also provides additional factual details about the consequences of this replacement, such as the reduction in the number of parameters, memory usage, and the impact on training speed and efficiency. These details are accurate and provide a deeper understanding of the FNet model's characteristics without deviating from the correctness of the answer."}, {"question": "What type of test should typically accompany a bug fix in Gradio's testing strategy?\n", "true_answer": "Dynamic code test", "source_doc": "gradio-app/gradio/blob/main/test-strategy.md", "generated_answer": "The extracted documents provided in the context show that bug fixes are being made in Gradio's codebase. According to the testing strategy mentioned in document 0, bug fixes should typically be accompanied by tests to ensure that the fix has not introduced any new issues. Therefore, the type of test that should typically accompany a bug fix in Gradio's testing strategy is a regression test. Regression tests are designed to verify that previously working functionality has not been negatively impacted by", "retrieved_docs": ["- Updated the minimum FastApi used in tests to version 0.87 by [@freddyaboulton](https://github.com/freddyaboulton) in [PR 2647](https://github.com/gradio-app/gradio/pull/2647)\n- Fixed bug where interfaces with examples could not be loaded with `gr.Interface.load` by [@freddyaboulton](https://github.com/freddyaboulton) [PR 2640](https://github.com/gradio-app/gradio/pull/2640)\n- Fixed bug where the `interactive` property of a component could not be updated by [@freddyaboulton](https://github.com/freddyaboulton) in [PR 2639](https://github.com/gradio-app/gradio/pull/2639)\n- Fixed bug where some URLs were not being recognized as valid URLs and thus were not\n  loading correctly in various components by [@abidlabs](https://github.com/abidlabs) in [PR 2659](https://github.com/gradio-app/gradio/pull/2659)", "- Updated the minimum FastApi used in tests to version 0.87 by [@freddyaboulton](https://github.com/freddyaboulton) in [PR 2647](https://github.com/gradio-app/gradio/pull/2647)\n- Fixed bug where interfaces with examples could not be loaded with `gr.Interface.load` by [@freddyaboulton](https://github.com/freddyaboulton) [PR 2640](https://github.com/gradio-app/gradio/pull/2640)\n- Fixed bug where the `interactive` property of a component could not be updated by [@freddyaboulton](https://github.com/freddyaboulton) in [PR 2639](https://github.com/gradio-app/gradio/pull/2639)\n- Fixed bug where some URLs were not being recognized as valid URLs and thus were not\n  loading correctly in various components by [@abidlabs](https://github.com/abidlabs) in [PR 2659](https://github.com/gradio-app/gradio/pull/2659)", "### Bug Fixes:\n\n- Allow `every` to work with generators by [@dkjshk](https://github.com/dkjshk) in [PR 4434](https://github.com/gradio-app/gradio/pull/4434)\n- Fix z-index of status component by [@hannahblair](https://github.com/hannahblair) in [PR 4429](https://github.com/gradio-app/gradio/pull/4429)\n- Allow gradio to work offline, by [@aliabid94](https://github.com/aliabid94) in [PR 4398](https://github.com/gradio-app/gradio/pull/4398).\n- Fixed `validate_url` to check for 403 errors and use a GET request in place of a HEAD by [@alvindaiyan](https://github.com/alvindaiyan) in [PR 4388](https://github.com/gradio-app/gradio/pull/4388).", "### Bug Fixes:\n\n- Allow `every` to work with generators by [@dkjshk](https://github.com/dkjshk) in [PR 4434](https://github.com/gradio-app/gradio/pull/4434)\n- Fix z-index of status component by [@hannahblair](https://github.com/hannahblair) in [PR 4429](https://github.com/gradio-app/gradio/pull/4429)\n- Allow gradio to work offline, by [@aliabid94](https://github.com/aliabid94) in [PR 4398](https://github.com/gradio-app/gradio/pull/4398).\n- Fixed `validate_url` to check for 403 errors and use a GET request in place of a HEAD by [@alvindaiyan](https://github.com/alvindaiyan) in [PR 4388](https://github.com/gradio-app/gradio/pull/4388).", "By [@freddyaboulton](https://github.com/freddyaboulton) in [PR 3101](https://github.com/gradio-app/gradio/pull/3101)\n\n### Bug Fixes:", "By [@freddyaboulton](https://github.com/freddyaboulton) in [PR 3101](https://github.com/gradio-app/gradio/pull/3101)\n\n### Bug Fixes:", "By [@freddyaboulton](https://github.com/freddyaboulton) in [PR 2807](https://github.com/gradio-app/gradio/pull/2807)\n\n### Bug Fixes:"], "eval_score_GPT4": "2", "eval_feedback_GPT4": "Feedback: The response identifies regression testing as the type of test that should accompany a bug fix in Gradio's testing strategy. While regression tests are indeed a form of testing to ensure that new changes do not negatively impact existing functionality, the reference answer specifies \"Dynamic code test\" as the correct type. The response does not match the reference answer, but it does show an understanding of testing practices related to bug fixes, which could be considered somewhat correct in a general testing context but not accurate per the reference answer."}, {"question": "How can you force mixed precision training when initializing the Accelerator in \ud83e\udd17 Accelerate?\n", "true_answer": "By passing `fp16=True` to the Accelerator init.", "source_doc": "huggingface/blog/blob/main/accelerate-library.md", "generated_answer": "To force mixed precision training when initializing the Accelerator in \ud83e\udd17 Accelerate, you can set the `mixed_precision` argument to `'fp16'\"` or `'bf16'\"` when configuring the Accelerator environment. Here's an example:\n\n```bash\naccelerate config mixed_precision='fp16'\n```\n\nAlternatively, you can set the `", "retrieved_docs": ["To enable mixed precision training, set the `fp16` flag to `True`:\n\n```py\ntraining_args = TrainingArguments(per_device_train_batch_size=4, fp16=True, **default_args)\n```\n\nIf you prefer to use \ud83e\udd17 Accelerate, find the \ud83e\udd17 Accelerate example [further in this guide](#using-accelerate). \n\n### BF16\n\nIf you have access to an Ampere or newer hardware you can use bf16 for mixed precision training and evaluation. While \nbf16 has a worse precision than fp16, it has a much bigger dynamic range. In fp16 the biggest number you can have \nis `65535` and any number above that will result in an overflow. A bf16 number can be as large as `3.39e+38` (!) which \nis about the same as fp32 - because both have 8-bits used for the numerical range.\n\nYou can enable BF16 in the \ud83e\udd17 Trainer with:\n\n```python\ntraining_args = TrainingArguments(bf16=True, **default_args)\n```\n\n### TF32", "Alternatively, use \ud83e\udd17 Accelerate - find the \ud83e\udd17 Accelerate example [further in this guide](#using-accelerate). \n\n<Tip>\n\nWhile gradient checkpointing may improve memory efficiency, it slows training by approximately 20%.\n\n</Tip>\n\n## Mixed precision training\n\n**Mixed precision training** is a technique that aims to optimize the computational efficiency of training models by \nutilizing lower-precision numerical formats for certain variables. Traditionally, most models use 32-bit floating point \nprecision (fp32 or float32) to represent and process variables. However, not all variables require this high precision \nlevel to achieve accurate results. By reducing the precision of certain variables to lower numerical formats like 16-bit \nfloating point (fp16 or float16), we can speed up the computations. Because in this approach some computations are performed \nin half-precision, while some are still in full precision, the approach is called mixed precision training.", "<Tip>\n\n\ud83e\udd17 Accelerate is a library for helping you train on multiple GPUs/TPUs or with mixed-precision. It'll automatically configure your training setup based on your hardware and environment. Take a look at the \ud83e\udd17 Accelerate [Quick tour](https://huggingface.co/docs/accelerate/quicktour) to learn more.\n\n</Tip>\n\nInitialize an \ud83e\udd17 Accelerate environment:\n\n```bash\naccelerate config\n```\n\nTo setup a default \ud83e\udd17 Accelerate environment without choosing any configurations:\n\n```bash\naccelerate config default\n```\n\nOr if your environment doesn't support an interactive shell, like a notebook, you can use:\n\n```bash\nfrom accelerate.utils import write_basic_config\n\nwrite_basic_config()\n```\n\nLastly, if you want to train a model on your own dataset, take a look at the [Create a dataset for training](create_dataset) guide to learn how to create a dataset that works with the training script.\n\n<Tip>", "<Tip>\n\n\ud83e\udd17 Accelerate is a library for helping you train on multiple GPUs/TPUs or with mixed-precision. It'll automatically configure your training setup based on your hardware and environment. Take a look at the \ud83e\udd17 Accelerate [Quick tour](https://huggingface.co/docs/accelerate/quicktour) to learn more.\n\n</Tip>\n\nInitialize an \ud83e\udd17 Accelerate environment:\n\n```bash\naccelerate config\n```\n\nTo setup a default \ud83e\udd17 Accelerate environment without choosing any configurations:\n\n```bash\naccelerate config default\n```\n\nOr if your environment doesn't support an interactive shell, like a notebook, you can use:\n\n```bash\nfrom accelerate.utils import write_basic_config\n\nwrite_basic_config()\n```\n\nLastly, if you want to train a model on your own dataset, take a look at the [Create a dataset for training](create_dataset) guide to learn how to create a dataset that works with the training script.\n\n<Tip>", "<Tip>\n\n\ud83e\udd17 Accelerate is a library for helping you train on multiple GPUs/TPUs or with mixed-precision. It'll automatically configure your training setup based on your hardware and environment. Take a look at the \ud83e\udd17 Accelerate [Quick tour](https://huggingface.co/docs/accelerate/quicktour) to learn more.\n\n</Tip>\n\nInitialize an \ud83e\udd17 Accelerate environment:\n\n```bash\naccelerate config\n```\n\nTo setup a default \ud83e\udd17 Accelerate environment without choosing any configurations:\n\n```bash\naccelerate config default\n```\n\nOr if your environment doesn't support an interactive shell, like a notebook, you can use:\n\n```bash\nfrom accelerate.utils import write_basic_config\n\nwrite_basic_config()\n```\n\nLastly, if you want to train a model on your own dataset, take a look at the [Create a dataset for training](create_dataset) guide to learn how to create a dataset that works with the training script.\n\n<Tip>", "<Tip>\n\n\ud83e\udd17 Accelerate is a library for helping you train on multiple GPUs/TPUs or with mixed-precision. It'll automatically configure your training setup based on your hardware and environment. Take a look at the \ud83e\udd17 Accelerate [Quick tour](https://huggingface.co/docs/accelerate/quicktour) to learn more.\n\n</Tip>\n\nInitialize an \ud83e\udd17 Accelerate environment:\n\n```bash\naccelerate config\n```\n\nTo setup a default \ud83e\udd17 Accelerate environment without choosing any configurations:\n\n```bash\naccelerate config default\n```\n\nOr if your environment doesn't support an interactive shell, like a notebook, you can use:\n\n```bash\nfrom accelerate.utils import write_basic_config\n\nwrite_basic_config()\n```\n\nLastly, if you want to train a model on your own dataset, take a look at the [Create a dataset for training](create_dataset) guide to learn how to create a dataset that works with the training script.\n\n<Tip>", "```bash\ncd examples/text_to_image\npip install -r requirements_flax.txt\n```\n\n</hfoption>\n</hfoptions>\n\n<Tip>\n\n\ud83e\udd17 Accelerate is a library for helping you train on multiple GPUs/TPUs or with mixed-precision. It'll automatically configure your training setup based on your hardware and environment. Take a look at the \ud83e\udd17 Accelerate [Quick tour](https://huggingface.co/docs/accelerate/quicktour) to learn more.\n\n</Tip>\n\nInitialize an \ud83e\udd17 Accelerate environment:\n\n```bash\naccelerate config\n```\n\nTo setup a default \ud83e\udd17 Accelerate environment without choosing any configurations:\n\n```bash\naccelerate config default\n```\n\nOr if your environment doesn't support an interactive shell, like a notebook, you can use:\n\n```bash\nfrom accelerate.utils import write_basic_config\n\nwrite_basic_config()\n```"], "eval_score_GPT4": "3", "eval_feedback_GPT4": "The response provided is mostly correct in that it addresses the question of how to force mixed precision training in \ud83e\udd17 Accelerate by setting the `mixed_precision` argument. However, the response includes an incorrect command (`accelerate config mixed_precision='fp16'`), which is not the way to initialize the Accelerator with mixed precision. The correct way, as per the reference answer, is to pass `fp16=True` directly when initializing the Accelerator object in the code, not as a command-line argument. Additionally, the response is incomplete as it ends abruptly with \"Alternatively, you can set the `\", leaving the reader without a full alternative solution. Based on the score rubric, the response is somewhat correct but lacks accuracy and completeness."}, {"question": "What is the range of parameters for the LLaMA models?\n", "true_answer": "7B to 65B parameters", "source_doc": "huggingface/transformers/blob/main/docs/source/en/model_doc/llama.md", "generated_answer": "The LLaMA models come in sizes ranging from 7B to 65B parameters, as stated in the context provided. The base model for all following steps is the 7B LLaMA model, which is chosen due to its capability and suitability for RLHF (reinforcement learning from human feedback). The smaller models in the LLaMA family are 6B and 13B parameters, while the biggest model is 65B parameters and", "retrieved_docs": ["## The LLaMA model\n\nWhen doing RLHF, it is important to start with a capable model: the RLHF step is only a fine-tuning step to align the model with how we want to interact with it and how we expect it to respond.  Therefore, we choose to use the recently introduced and performant [LLaMA models](https://arxiv.org/abs/2302.13971). The LLaMA models are the latest large language models developed by Meta AI. They come in sizes ranging from 7B to 65B parameters and were trained on between 1T and 1.4T tokens, making them very capable. We use the 7B model as the base for all the following steps!\nTo access the model, use the [form](https://docs.google.com/forms/d/e/1FAIpQLSfqNECQnMkycAp2jP4Z9TFX0cGR4uf7b_fBxjY_OjhJILlKGA/viewform) from Meta AI.\n\n## Stack Exchange dataset", "## What\u2019s Code Llama?\n\nThe Code Llama release introduces a family of models of 7, 13, and 34 billion parameters. The base models are initialized from Llama 2 and then trained on 500 billion tokens of code data. Meta fine-tuned those base models for two different flavors: a Python specialist (100 billion additional tokens) and an instruction fine-tuned version, which can understand natural language instructions. \n\nThe models show state-of-the-art performance in Python, C++, Java, PHP, C#, TypeScript, and Bash. The 7B and 13B base and instruct variants support infilling based on surrounding content, making them ideal for use as code assistants.\n\nCode Llama was trained on a 16k context window. In addition, the three model variants had additional long-context fine-tuning, allowing them to manage a context window of up to 100,000 tokens.", "The first model family in this series was the [LLaMA](https://huggingface.co/papers/2302.13971) family, released by Meta AI. The explicit objective of the researchers was to train a set of models of various sizes with the best possible performances for a given computing budget. For one of the first times, the research team explicitly decided to consider not only the training budget but also the inference cost (for a given performance objective, how much does it cost to run inference with the model). In this perspective, they decided to train smaller models on even more data and for more steps than was usually done, thereby reaching higher performances at a smaller model size (the trade-off being training compute efficiency). The biggest model in the Llama 1 family is a 65B parameters model trained on 1.4T tokens, while the smaller models (resp. 6 and 13B parameters) were trained on 1T tokens. The small 13B LLaMA model outperformed GPT-3 on most benchmarks, and the biggest LLaMA model was state of the art when it came out. The weights were released with a non-commercial license though, limiting the adoption by the community.", "## Efficient training strategies\n\nEven training the smallest LLaMA model requires an enormous amount of memory. Some quick math: in bf16, every parameter uses 2 bytes (in fp32 4 bytes) in addition to 8 bytes used, e.g., in the Adam optimizer (see the [performance docs](https://huggingface.co/docs/transformers/perf_train_gpu_one#optimizer) in Transformers for more info). So a 7B parameter model would use `(2+8)*7B=70GB` just to fit in memory and would likely need more when you compute intermediate values such as attention scores. So you couldn\u2019t train the model even on a single 80GB A100 like that. You can use some tricks, like more efficient optimizers of half-precision training, to squeeze a bit more into memory, but you\u2019ll run out sooner or later.\n\nAnother option is to use Parameter-Efficient Fine-Tuning (PEFT) techniques, such as the [`peft`](https://github.com/huggingface/peft) library, which can perform Low-Rank Adaptation (LoRA) on a model loaded in 8-bit.", "llama_model = llama_model.cuda()\n\nlr = 1e-4\nbatch_size = 8\nnum_epochs = 5\ntraining_args = TrainingArguments(\n    output_dir=\"llama-lora-token-classification\",\n    learning_rate=lr,\n    lr_scheduler_type= \"constant\",\n    warmup_ratio= 0.1,\n    max_grad_norm= 0.3,\n    per_device_train_batch_size=batch_size,\n    per_device_eval_batch_size=batch_size,\n    num_train_epochs=num_epochs,\n    weight_decay=0.001,\n    evaluation_strategy=\"epoch\",\n    save_strategy=\"epoch\",\n    load_best_model_at_end=True,\n    report_to=\"wandb\",\n    fp16=True,\n    gradient_checkpointing=True,\n)", "Increasing Llama 2\u2019s 4k context window to Code Llama\u2019s 16k (that can extrapolate up to 100k) was possible due to recent developments in RoPE scaling. The community found that Llama\u2019s position embeddings can be interpolated linearly or in the frequency domain, which eases the transition to a larger context window through fine-tuning. In the case of Code Llama, the frequency domain scaling is done with a slack: the fine-tuning length is a fraction of the scaled pretrained length, giving the model powerful extrapolation capabilities. \n\n![Training Process](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/160_codellama/training-process.jpg \"Training Process\")\n\nAll models were initially trained with 500 billion tokens on a near-deduplicated dataset of publicly available code. The dataset also contains some natural language datasets, such as discussions about code and code snippets. Unfortunately, there is not more information about the dataset.", "- Weights for the Llama2 models can be obtained by filling out [this form](https://ai.meta.com/resources/models-and-libraries/llama-downloads/)\n- The architecture is very similar to the first Llama, with the addition of Grouped Query Attention (GQA) following this [paper](https://arxiv.org/pdf/2305.13245.pdf)\n- Setting `config.pretraining_tp` to a value different than 1 will activate the more accurate but slower computation of the linear layers, which should better match the original logits."], "eval_score_GPT4": "4", "eval_feedback_GPT4": "Feedback: The response correctly identifies the range of parameters for the LLaMA models as being from 7B to 65B, which aligns with the reference answer. However, the response includes additional information about the base model and mentions other sizes that are not part of the range specified in the reference answer (6B and 13B). This additional information is not incorrect, but it is not asked for in the instruction. Based on the rubric, the response is mostly correct, accurate, and factual, but not completely, as it includes extraneous details."}, {"question": "What is the purpose of tokenizers in the NLP pipeline?\n", "true_answer": "To translate text into data that can be processed by the model.", "source_doc": "huggingface/course/blob/main/chapters/en/chapter2/4.mdx", "generated_answer": "Tokenizers in the NLP pipeline serve the purpose of converting text inputs into numerical data that can be processed by machine learning models. Models can only process numbers, so tokenizers need to translate text into numerical data. They achieve this by normalizing the text, splitting it into initial words, applying a model to do the actual tokenization, and post-processing the resulting encoding to add any necessary special tokens. The tokenization pipeline consists of the normalizer, pre-tokenizer, model", "retrieved_docs": ["{:else}\n\n<CourseFloatingBanner chapter={2}\n  classNames=\"absolute z-10 right-0 top-0\"\n  notebooks={[\n    {label: \"Google Colab\", value: \"https://colab.research.google.com/github/huggingface/notebooks/blob/master/course/en/chapter2/section4_tf.ipynb\"},\n    {label: \"Aws Studio\", value: \"https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/master/course/en/chapter2/section4_tf.ipynb\"},\n]} />\n\n{/if}\n\n<Youtube id=\"VFp38yj8h3A\"/>\n\nTokenizers are one of the core components of the NLP pipeline. They serve one purpose: to translate text into data that can be processed by the model. Models can only process numbers, so tokenizers need to convert our text inputs to numerical data. In this section, we'll explore exactly what happens in the tokenization pipeline. \n\nIn NLP tasks, the data that is generally processed is raw text. Here's an example of such text:", "## What is a Tokenizer\n\nA Tokenizer works as a pipeline, it processes some raw text as input and outputs an `Encoding`.\nThe various steps of the pipeline are:\n\n1. The `Normalizer`: in charge of normalizing the text. Common examples of normalization are\n   the [unicode normalization standards](https://unicode.org/reports/tr15/#Norm_Forms), such as `NFD` or `NFKC`.\n   More details about how to use the `Normalizers` are available on the\n   [Hugging Face blog](https://huggingface.co/docs/tokenizers/components#normalizers)\n2. The `PreTokenizer`: in charge of creating initial words splits in the text. The most common way of\n   splitting text is simply on whitespace.\n3. The `Model`: in charge of doing the actual tokenization. An example of a `Model` would be\n   `BPE` or `WordPiece`.\n4. The `PostProcessor`: in charge of post-processing the `Encoding` to add anything relevant\n   that, for example, a language model would need, such as special tokens.", "This chapter will begin with an end-to-end example where we use a model and a tokenizer together to replicate the `pipeline()` function introduced in [Chapter 1](/course/chapter1). Next, we'll discuss the model API: we'll dive into the model and configuration classes, and show you how to load a model and how it processes numerical inputs to output predictions. \n\nThen we'll look at the tokenizer API, which is the other main component of the `pipeline()` function. Tokenizers take care of the first and last processing steps, handling the conversion from text to numerical inputs for the neural network, and the conversion back to text when it is needed. Finally, we'll show you how to handle sending multiple sentences through a model in a prepared batch, then wrap it all up with a closer look at the high-level `tokenizer()` function.\n\n<Tip>\n\u26a0\ufe0f In order to benefit from all features available with the Model Hub and \ud83e\udd17 Transformers, we recommend <a href=\"https://huggingface.co/join\">creating an account</a>.\n</Tip>", "The tokenization pipeline\n\nWhen calling `Tokenizer.encode` or\n`Tokenizer.encode_batch`, the input\ntext(s) go through the following pipeline:\n\n-   `normalization`\n-   `pre-tokenization`\n-   `model`\n-   `post-processing`\n\nWe'll see in details what happens during each of those steps in detail,\nas well as when you want to `decode <decoding>` some token ids, and how the \ud83e\udd17 Tokenizers library allows you\nto customize each of those steps to your needs. If you're already\nfamiliar with those steps and want to learn by seeing some code, jump to\n`our BERT from scratch example <example>`.\n\nFor the examples that require a `Tokenizer` we will use the tokenizer we trained in the\n`quicktour`, which you can load with:", "### AutoTokenizer\n\nA tokenizer is responsible for preprocessing text into an array of numbers as inputs to a model. There are multiple rules that govern the tokenization process, including how to split a word and at what level words should be split (learn more about tokenization in the [tokenizer summary](./tokenizer_summary)). The most important thing to remember is you need to instantiate a tokenizer with the same model name to ensure you're using the same tokenization rules a model was pretrained with.\n\nLoad a tokenizer with [`AutoTokenizer`]:\n\n```py\n>>> from transformers import AutoTokenizer\n\n>>> model_name = \"nlptown/bert-base-multilingual-uncased-sentiment\"\n>>> tokenizer = AutoTokenizer.from_pretrained(model_name)\n```\n\nPass your text to the tokenizer:", "</Tip>\n\nIn this tutorial, learn to:\n\n* Load a pretrained tokenizer.\n* Load a pretrained image processor\n* Load a pretrained feature extractor.\n* Load a pretrained processor.\n* Load a pretrained model.\n* Load a model as a backbone.\n\n## AutoTokenizer\n\nNearly every NLP task begins with a tokenizer. A tokenizer converts your input into a format that can be processed by the model.\n\nLoad a tokenizer with [`AutoTokenizer.from_pretrained`]:\n\n```py\n>>> from transformers import AutoTokenizer\n\n>>> tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n```\n\nThen tokenize your input as shown below:", "Of course, if you change the way the pre-tokenizer, you should probably\nretrain your tokenizer from scratch afterward.\n\n## Model\n\nOnce the input texts are normalized and pre-tokenized, the\n`Tokenizer` applies the model on the\npre-tokens. This is the part of the pipeline that needs training on your\ncorpus (or that has been trained if you are using a pretrained\ntokenizer).\n\nThe role of the model is to split your \"words\" into tokens, using the\nrules it has learned. It's also responsible for mapping those tokens to\ntheir corresponding IDs in the vocabulary of the model.\n\nThis model is passed along when intializing the\n`Tokenizer` so you already know how to\ncustomize this part. Currently, the \ud83e\udd17 Tokenizers library supports:\n\n-   `models.BPE`\n-   `models.Unigram`\n-   `models.WordLevel`\n-   `models.WordPiece`\n\nFor more details about each model and its behavior, you can check\n[here](components#models)\n\n## Post-Processing\n\nPost-processing is the last step of the tokenization pipeline, to\nperform any additional transformation to the\n`Encoding` before it's returned, like\nadding potential special tokens."], "eval_score_GPT4": "4", "eval_feedback_GPT4": "Feedback: The response provided is mostly correct and factual. It explains the purpose of tokenizers in the NLP pipeline by stating that they convert text inputs into numerical data, which is a correct and factual statement. The response goes into detail about the process of tokenization, including normalization, splitting text into words, and post-processing, which aligns well with the reference answer's mention of translating text into data for the model. However, the response could be more concise to match the reference answer's brevity and directness."}, {"question": "What is the purpose of the Safety Checker in the Diffusers library?\n", "true_answer": "The Safety Checker checks and compares the class probability of a set of hard-coded harmful concepts in the embedding space against an image after it has been generated to mitigate the risk of generating harmful content.", "source_doc": "huggingface/diffusers/blob/main/docs/source/en/conceptual/ethical_guidelines.md", "generated_answer": "The purpose of the Safety Checker in the Diffusers library is to check and compare the class probability of a set of hard-coded harmful concepts in the embedding space against an image after it has been generated. Its main function is to flag inappropriate content generated during inference and help users interact with these models responsibly and ethically. The Safety Checker component is optional for model creators to incorporate into their models if they choose to do so.", "retrieved_docs": ["- [**Safety Checker**](https://github.com/huggingface/diffusers/blob/main/src/diffusers/pipelines/stable_diffusion/safety_checker.py): It checks and compares the class probability of a set of hard-coded harmful concepts in the embedding space against an image after it has been generated. The harmful concepts are intentionally hidden to prevent reverse engineering of the checker.\n\n- **Staged released on the Hub**: in particularly sensitive situations, access to some repositories should be restricted. This staged release is an intermediary step that allows the repository\u2019s authors to have more control over its use.\n\n- **Licensing**: [OpenRAILs](https://huggingface.co/blog/open_rail), a new type of licensing, allow us to ensure free access while having a set of restrictions that ensure more responsible use.", "### Safety checker\n\nDiffusion models like Stable Diffusion can generate harmful content, which is why \ud83e\udde8 Diffusers has a [safety checker](https://github.com/huggingface/diffusers/blob/main/src/diffusers/pipelines/stable_diffusion/safety_checker.py) to check generated outputs against known hardcoded NSFW content. If you'd like to disable the safety checker for whatever reason, pass `None` to the `safety_checker` argument:\n\n```python\nfrom diffusers import DiffusionPipeline", "- **Simplicity**: with a desire to make it easy to use and exploit the Diffusers library, we are committed to keeping the project\u2019s goals lean and coherent.\n\n- **Accessibility**: the Diffusers project helps lower the entry bar for contributors who can help run it even without technical expertise. Doing so makes research artifacts more accessible to the community.\n\n- **Reproducibility**: we aim to be transparent about the reproducibility of upstream code, models, and datasets when made available through the Diffusers library.\n\n- **Responsibility**: as a community and through teamwork, we hold a collective responsibility to our users by anticipating and mitigating this technology's potential risks and dangers.\n\n\n## Examples of implementations: Safety features and Mechanisms\n\nThe team works daily to make the technical and non-technical tools available to deal with the potential ethical and social risks associated with diffusion technology. Moreover, the community's input is invaluable in ensuring these features' implementation and raising awareness with us.\n\n- [**Community tab**](https://huggingface.co/docs/hub/repositories-pull-requests-discussions): it enables the community to discuss and better collaborate on a project.", "# Diffusers\n\n\ud83e\udd17 Diffusers is the go-to library for state-of-the-art pretrained diffusion models for generating images, audio, and even 3D structures of molecules. Whether you're looking for a simple inference solution or want to train your own diffusion model, \ud83e\udd17 Diffusers is a modular toolbox that supports both. Our library is designed with a focus on [usability over performance](conceptual/philosophy#usability-over-performance), [simple over easy](conceptual/philosophy#simple-over-easy), and [customizability over abstractions](conceptual/philosophy#tweakable-contributorfriendly-over-abstraction).\n\nThe library has three main components:\n\n- State-of-the-art diffusion pipelines for inference with just a few lines of code. There are many pipelines in \ud83e\udd17 Diffusers, check out the table in the pipeline [overview](api/pipelines/overview) for a complete list of available pipelines and the task they solve.\n- Interchangeable [noise schedulers](api/schedulers/overview) for balancing trade-offs between generation speed and quality.\n- Pretrained [models](api/models) that can be used as building blocks, and combined with schedulers, for creating your own end-to-end diffusion systems.", "# Ethical guidelines\n\n* **Transparency**: we are committed to being transparent in managing PRs, explaining our choices to users, and making technical decisions.\n* **Consistency**: we are committed to guaranteeing our users the same level of attention in project management, keeping it technically stable and consistent.\n* **Simplicity**: with a desire to make it easy to use and exploit the Diffusers library, we are committed to keeping the project\u2019s goals lean and coherent.\n* **Accessibility**: the Diffusers project helps lower the entry bar for contributors who can help run it even without technical expertise. Doing so makes research artifacts more accessible to the community.\n* **Reproducibility**: we aim to be transparent about the reproducibility of upstream code, models, and datasets when made available through the Diffusers library.\n* **Responsibility**: as a community and through teamwork, we hold a collective responsibility to our users by anticipating and mitigating this technology\u2019s potential risks and dangers.\n\n# Safety features and mechanisms\n\nIn addition, we provide a non-exhaustive - and hopefully continuously expanding! - list of safety features and mechanisms implemented by the Hugging Face team and the broader community.", "- a [CLIPImageProcessor](https://huggingface.co/docs/transformers/main/en/model_doc/clip#transformers.CLIPImageProcessor),\n- as well as a [safety checker](https://github.com/huggingface/diffusers/blob/main/src/diffusers/pipelines/stable_diffusion/safety_checker.py).\nAll of these components are necessary to run stable diffusion in inference even though they were trained\nor created independently from each other.", "In addition to that, we also support specific hardware and formats like ONNX, the `mps` PyTorch device for Apple Silicon computers, Core ML, and others.\n\nTo learn more about how we optimize inference with \ud83e\udd17\u00a0Diffusers, check out the [docs](https://huggingface.co/docs/diffusers/optimization/opt_overview)!\n\n## Ethics and safety\n\nGenerative models are cool, but they also have the ability to produce harmful and NSFW content. To help users interact with these models responsibly and ethically, we\u2019ve added a [`safety_checker`](https://github.com/huggingface/diffusers/blob/main/src/diffusers/pipelines/stable_diffusion/safety_checker.py) component that flags inappropriate content generated during inference. Model creators can choose to incorporate this component into their models if they want."], "eval_score_GPT4": "5", "eval_feedback_GPT4": "The response provided is completely correct, accurate, and factual. It aligns well with the reference answer by explaining the purpose of the Safety Checker in the Diffusers library, which is to check and compare the class probability of a set of hard-coded harmful concepts in the embedding space against an image after it has been generated. Additionally, the response expands on the reference answer by mentioning the Safety Checker's role in flagging inappropriate content and its optional nature for model creators, which adds depth to the explanation without deviating from the factual accuracy. Therefore, the response meets the criteria for the highest score on the rubric."}, {"question": "What Python class allows you to retrieve Discussions and Pull Requests from a given repository on the Hugging Face Hub?\n", "true_answer": "HfApi", "source_doc": "huggingface/huggingface_hub/blob/main/docs/source/en/guides/community.md", "generated_answer": "The `HfApi` class in the `huggingface_hub` library allows you to retrieve Discussions and Pull Requests from a given repository on the Hugging Face Hub. You can use methods like `get_repo_discussions` and `get_repo_pull_requests` with this class to retrieve the desired discussions and pull requests. Here's an example:\n\n```python\nfrom huggingface_hub import get_repo_dis", "retrieved_docs": ["!--\u26a0\ufe0f Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\nrendered properly in your Markdown viewer.\n-->\n\n# Interact with Discussions and Pull Requests \n\nThe `huggingface_hub` library provides a Python interface to interact with Pull Requests and Discussions on the Hub.\nVisit [the dedicated documentation page](https://huggingface.co/docs/hub/repositories-pull-requests-discussions)\nfor a deeper view of what Discussions and Pull Requests on the Hub are, and how they work under the hood.\n\n## Retrieve Discussions and Pull Requests from the Hub\n\nThe `HfApi` class allows you to retrieve Discussions and Pull Requests on a given repo:\n\n```python\n>>> from huggingface_hub import get_repo_discussions\n>>> for discussion in get_repo_discussions(repo_id=\"bigscience/bloom\"):\n...     print(f\"{discussion.num} - {discussion.title}, pr: {discussion.is_pull_request}\")", "```python\n>>> from huggingface_hub import create_discussion, create_pull_request\n\n>>> create_discussion(\n...     repo_id=\"username/repo-name\",\n...     title=\"Hi from the huggingface_hub library!\",\n...     token=\"<insert your access token here>\",\n... )\nDiscussionWithDetails(...)\n\n>>> create_pull_request(\n...     repo_id=\"username/repo-name\",\n...     title=\"Hi from the huggingface_hub library!\",\n...     token=\"<insert your access token here>\",\n... )\nDiscussionWithDetails(..., is_pull_request=True)\n```\n\nManaging Pull Requests and Discussions can be done entirely with the [`HfApi`] class. For example:\n\n    * [`comment_discussion`] to add comments\n    * [`edit_discussion_comment`] to edit comments\n    * [`rename_discussion`] to rename a Discussion or Pull Request \n    * [`change_discussion_status`] to open or close a Discussion / Pull Request \n    * [`merge_pull_request`] to merge a Pull Request", "![Pull requests on the Hugging Face Hub](assets/76_community_update/new-pr.png)\n\n[Pull requests](https://huggingface.co/gpt2/discussions?type=pull_request) allow community members open, comment, merge, or close pull requests directly from the website. The easiest way to open a pull request is to use the \"Collaborate\" button in the \"Files and versions\" tab. It will let you do single file contributions very easily.\n\nUnder the hood, our Pull requests do not use forks and branches, but instead, custom \"branches\" called `refs` that are stored directly on the source repo. This approach to avoids the need to create a forks for each new version of the model/dataset.\n\n## How is this different from other git hosts\n\nAt a high level, we aim to build a simpler version of other git hosts' (like GitHub's) PRs and Issues:\n\n- no forks are involved: contributors push to a special `ref` branch directly on the source repo\n- no hard distinction between issues and PRs: they are essentially the same so we display them in the same lists\n- streamlined for ML (i.e. models/datasets/Spaces repos), not arbitrary repos", "Username: \n   Password:\n   ```\n\n3. Alternatively, if you prefer working from a Jupyter or Colaboratory notebook, login with `notebook_login`:\n\n   ```python\n   >>> from huggingface_hub import notebook_login\n   >>> notebook_login()\n   ```\n\n   `notebook_login` will launch a widget in your notebook from which you can enter your Hugging Face credentials.\n\n## Download files from the Hub\n\nIntegration allows users to download your hosted files directly from the Hub using your library. \n\nUse the `hf_hub_download` function to retrieve a URL and download files from your repository. Downloaded files are stored in your cache: `~/.cache/huggingface/hub`. You don't have to re-download the file the next time you use it, and for larger files, this can save a lot of time. Furthermore, if the repository is updated with a new version of the file, `huggingface_hub` will automatically download the latest version and store it in the cache for you. Users don't have to worry about updating their files.", "The `push_to_hub()` method is backed by the [`huggingface_hub`](https://github.com/huggingface/huggingface_hub) Python package, which offers a direct API to the Hugging Face Hub. It's integrated within \ud83e\udd17 Transformers and several other machine learning libraries, like [`allenlp`](https://github.com/allenai/allennlp). Although we focus on the \ud83e\udd17 Transformers integration in this chapter, integrating it into your own code or library is simple.\n\nJump to the last section to see how to upload files to your newly created repository!\n\n## Using the `huggingface_hub` Python library[[using-the-huggingfacehub-python-library]]\n\nThe `huggingface_hub` Python library is a package which offers a set of tools for the model and datasets hubs. It provides simple methods and classes for common tasks like \ngetting information about repositories on the hub and managing them. It provides simple APIs that work on top of git to manage those repositories' content and to integrate the Hub\nin your projects and libraries.", "## Repository: the historical git-based approach\n\nAt first, `huggingface_hub` was mostly built around the [`Repository`] class. It provides\nPython wrappers for common `git` commands such as `\"git add\"`, `\"git commit\"`, `\"git push\"`,\n`\"git tag\"`, `\"git checkout\"`, etc.\n\nThe library also helps with setting credentials and tracking large files, which are often\nused in machine learning repositories. Additionally, the library allows you to execute its\nmethods in the background, making it useful for uploading data during training.\n\nThe main advantage of using a [`Repository`] is that it allows you to maintain a local\ncopy of the entire repository on your machine. This can also be a disadvantage as\nit requires you to constantly update and maintain this local copy. This is similar to\ntraditional software development where each developer maintains their own local copy and\npushes changes when working on a feature. However, in the context of machine learning,\nthis may not always be necessary as users may only need to download weights for inference\nor convert weights from one format to another without the need to clone the entire\nrepository.\n\n<Tip warning={true}>", "Hub API Endpoints\n\nWe have open endpoints that you can use to retrieve information from the Hub as well as perform certain actions such as creating model, dataset or Space repos. We offer a wrapper Python library, [`huggingface_hub`](https://github.com/huggingface/huggingface_hub), that allows easy access to these endpoints. We also provide [webhooks](./webhooks) to receive real-time incremental info about repos. Enjoy!\n\nThe base URL for those endpoints below is `https://huggingface.co`. For example, to construct the `/api/models` call below, one can call the URL [https://huggingface.co/api/models](https://huggingface.co/api/models)\n\n## The Hub API Playground\n\nWant to try out our API?\nTry it out now on our [Playground](https://huggingface.co/spaces/enzostvs/hub-api-playground)!"], "eval_score_GPT4": "4", "eval_feedback_GPT4": "The response correctly identifies the `HfApi` class as the one used to retrieve Discussions and Pull Requests from a given repository on the Hugging Face Hub, which aligns with the reference answer. However, the response includes additional information about methods like `get_repo_discussions` and `get_repo_pull_requests`, which is not required by the instruction but is factually correct and relevant to the use of the `HfApi` class. The example code provided is incomplete and incorrect as it imports `get_repo_dis` which is not a valid method or class. This does not affect the factual correctness of the main answer but does indicate a lack of attention to detail. Based on the rubric, the response is mostly correct, accurate, and factual, but the error in the example code prevents it from being completely correct."}, {"question": "What is the name of the new library introduced by Hugging Face for hosting scikit-learn models?\n", "true_answer": "Skops", "source_doc": "huggingface/blog/blob/main/skops.md", "generated_answer": "The new library introduced by Hugging Face for hosting scikit-learn models is called Skops. This information can be found in the context provided, specifically in the section titled \"Introducing Skops\". The library, Skops, allows users to host their scikit-learn models on the Hugging Face Hub, create model cards for documentation, and collaborate with others. The context also mentions that Skops is part of Hugging Face's efforts to tackle various problems", "retrieved_docs": ["--\ntitle: \"Introducing Skops\"\nthumbnail: /blog/assets/94_skops/introducing_skops.png\nauthors:\n- user: merve\n- user: adrin\n- user: BenjaminB\n---\n\n# Introducing Skops\n\n\n##\u00a0Introducing Skops\n\nAt Hugging Face, we are working on tackling various problems in open-source machine learning, including, hosting models securely and openly, enabling reproducibility, explainability and collaboration. We are thrilled to introduce you to our new library: Skops! With Skops, you can host your scikit-learn models on the Hugging Face Hub, create model cards for model documentation and collaborate with others.\n\nLet's go through an end-to-end example: train a model first, and see step-by-step how to leverage Skops for sklearn in production.\n\n```python\n# let's import the libraries first\nimport sklearn\nfrom sklearn.datasets import load_breast_cancer\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.model_selection import train_test_split", "Integrate your library with the Hub\n\nThe Hugging Face Hub aims to facilitate sharing machine learning models, checkpoints, and artifacts. This endeavor includes integrating the Hub into many of the amazing third-party libraries in the community. Some of the ones already integrated include [spaCy](https://spacy.io/usage/projects#huggingface_hub), [AllenNLP](https://allennlp.org/), and [timm](https://rwightman.github.io/pytorch-image-models/), among many others. Integration means users can download and upload files to the Hub directly from your library. We hope you will integrate your library and join us in democratizing artificial intelligence for everyone.\n\nIntegrating the Hub with your library provides many benefits, including:\n\n- Free model hosting for you and your users.\n- Built-in file versioning - even for huge files - made possible by [Git-LFS](https://git-lfs.github.com/).\n- All public models are powered by the [Inference API](https://huggingface.co/docs/api-inference/index).\n- In-browser widgets allow users to interact with your hosted models directly.", "## Support for third-party libraries\n\nCentral to the Hugging Face ecosystem is the [Hugging Face Hub](https://huggingface.co/docs/hub), which lets people collaborate effectively on Machine Learning. As mentioned earlier, we not only support models from \ud83e\udd17 Transformers on the Hub but also models from other third-party libraries. To this end, we provide [several utilities](https://huggingface.co/docs/hub/models-adding-libraries) so that you can integrate your own library with the Hub. One of the primary advantages of doing this is that it becomes very easy to share artifacts (such as models and datasets) with the community, thereby making it easier for your users to try out your models.\n\nWhen you have your models hosted on the Hub, you can also [add custom inference widgets](https://github.com/huggingface/api-inference-community) for them. Inference widgets allow users to quickly check out the models. This helps with improving user engagement.\n\n<div align=\"center\">\n<img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/cv_state/task_widget_generation.png\"/>\n</div>", "### Would you like to integrate your library to the Hub?\n\nThis integration is made possible by the [`huggingface_hub`](https://github.com/huggingface/huggingface_hub) library. If you want to add your library to the Hub, we have a [guide](https://huggingface.co/docs/hub/models-adding-libraries) for you! Or simply tag someone from the Hugging Face team.\n\nA shout out to the Hugging Face team for all the work on this integration, in particular [@osanseviero](https://twitter.com/osanseviero) \ud83e\udd99.\n\nThank you fastlearners and hugging learners \ud83e\udd17.", "Hugging Face is a collaborative Machine Learning platform in which the community has shared over 150,000 models, 25,000 datasets, and 30,000 ML apps. The Hub has model and dataset versioning tools, including model cards and client-side libraries to automate the versioning process. However, only including a model card with hyperparameters is not enough to provide the best reproducibility; this is where notebooks can help. Alongside these models, datasets, and demos, the Hub hosts over 7,000 notebooks. These notebooks often document the development process of a model or a dataset and can provide guidance and tutorials showing how others can use these resources. We\u2019re therefore excited about our improved support for notebook hosting on the Hub. \n\n## What have we changed? \n\nUnder the hood, Jupyter notebook files (usually shared with an `ipynb` extension) are JSON files. While viewing these files directly is possible, it's not a format intended to be read by humans. We have now added rendering support for notebooks hosted on the Hub. This means that notebooks will now be displayed in a human-readable format.", "!--\u26a0\ufe0f Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\nrendered properly in your Markdown viewer.\n-->\n\n# \ud83e\udd17 Hub client library\n\nThe `huggingface_hub` library allows you to interact with the [Hugging Face\nHub](https://hf.co), a machine learning platform for creators and collaborators.\nDiscover pre-trained models and datasets for your projects or play with the hundreds of\nmachine learning apps hosted on the Hub. You can also create and share your own models\nand datasets with the community. The `huggingface_hub` library provides a simple way to\ndo all these things with Python.\n\nRead the [quick start guide](quick-start) to get up and running with the\n`huggingface_hub` library. You will learn how to download files from the Hub, create a\nrepository, and upload files to the Hub. Keep reading to learn more about how to manage\nyour repositories on the \ud83e\udd17 Hub, how to interact in discussions or even how to access\nthe Inference API.", "**Source Code**: <a href=\"https://github.com/huggingface/huggingface_hub\" target=\"_blank\">https://github.com/huggingface/huggingface_hub</a>\n\n---\n\n## Welcome to the huggingface_hub library\n\nThe `huggingface_hub` library allows you to interact with the [Hugging Face Hub](https://huggingface.co/), a platform democratizing open-source Machine Learning for creators and collaborators. Discover pre-trained models and datasets for your projects or play with the thousands of machine learning apps hosted on the Hub. You can also create and share your own models, datasets and demos with the community. The `huggingface_hub` library provides a simple way to do all these things with Python.\n\n## Key features"], "eval_score_GPT4": "5", "eval_feedback_GPT4": "The response correctly identifies the name of the new library introduced by Hugging Face for hosting scikit-learn models as \"Skops.\" The information provided is accurate and factual, aligning with the reference answer. The additional details about the library's purpose and features do not detract from the correctness of the response."}, {"question": "What is the purpose of Textual Inversion?\n", "true_answer": "Textual Inversion is a training method for personalizing models by learning new text embeddings from a few example images.", "source_doc": "huggingface/diffusers/blob/main/docs/source/en/api/loaders/textual_inversion.md", "generated_answer": "Textual Inversion is a method to personalize text2image models like stable diffusion on one's own images using just 3-5 examples. It creates embeddings that represent the images provided with a special word in the prompt, which can then be used to adapt another model, such as DreamBooth, to generate certain concepts or styles. The purpose of Textual Inversion is to make the diffusion model weights stay the same during training and produce a relatively small (a few KB", "retrieved_docs": ["## Textual inversion\n\n[Textual inversion](https://textual-inversion.github.io/) is very similar to DreamBooth and it can also personalize a diffusion model to generate certain concepts (styles, objects) from just a few images. This method works by training and finding new embeddings that represent the images you provide with a special word in the prompt. As a result, the diffusion model weights stay the same and the training process produces a relatively tiny (a few KBs) file.\n\nBecause textual inversion creates embeddings, it cannot be used on its own like DreamBooth and requires another model.\n\n```py\nfrom diffusers import AutoPipelineForText2Image\nimport torch\n\npipeline = AutoPipelineForText2Image.from_pretrained(\"runwayml/stable-diffusion-v1-5\", torch_dtype=torch.float16).to(\"cuda\")\n```", "# Textual Inversion fine-tuning example\n\n[Textual inversion](https://arxiv.org/abs/2208.01618) is a method to personalize text2image models like stable diffusion on your own images using just 3-5 examples.\nThe `textual_inversion.py` script shows how to implement the training procedure and adapt it for stable diffusion.\n\n## Running on Colab\n\nColab for training\n[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/huggingface/notebooks/blob/main/diffusers/sd_textual_inversion_training.ipynb)\n\nColab for inference\n[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/huggingface/notebooks/blob/main/diffusers/stable_conceptualizer_inference.ipynb)\n\n## Running locally with PyTorch\n### Installing the dependencies", "# Textual Inversion fine-tuning example\n\n[Textual inversion](https://arxiv.org/abs/2208.01618) is a method to personalize text2image models like stable diffusion on your own images using just 3-5 examples.\nThe `textual_inversion.py` script shows how to implement the training procedure and adapt it for stable diffusion.\n\n## Running on Colab\n\nColab for training\n[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/huggingface/notebooks/blob/main/diffusers/sd_textual_inversion_training.ipynb)\n\nColab for inference\n[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/huggingface/notebooks/blob/main/diffusers/stable_conceptualizer_inference.ipynb)\n\n## Running locally with PyTorch\n### Installing the dependencies", "## Textual Inversion fine-tuning example\n\n[Textual inversion](https://arxiv.org/abs/2208.01618) is a method to personalize text2image models like stable diffusion on your own images using just 3-5 examples.\nThe `textual_inversion.py` script shows how to implement the training procedure and adapt it for stable diffusion.\n\n## Running on Colab\n\nColab for training\n[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/huggingface/notebooks/blob/main/diffusers/sd_textual_inversion_training.ipynb)\n\nColab for inference\n[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/huggingface/notebooks/blob/main/diffusers/stable_conceptualizer_inference.ipynb)\n\n## Running locally with PyTorch\n### Installing the dependencies", "# Textual Inversion fine-tuning example\n\n[Textual inversion](https://arxiv.org/abs/2208.01618) is a method to personalize text2image models like stable diffusion on your own images using just 3-5 examples.\nThe `textual_inversion.py` script shows how to implement the training procedure and adapt it for stable diffusion.\n\n## Training with Intel Extension for PyTorch\n\nIntel Extension for PyTorch provides the optimizations for faster training and inference on CPUs. You can leverage the training example \"textual_inversion.py\". Follow the [instructions](https://github.com/huggingface/diffusers/tree/main/examples/textual_inversion) to get the model and [dataset](https://huggingface.co/sd-concepts-library/dicoo2) before running the script.\n\nThe example supports both single node and multi-node distributed training:\n\n### Single node training\n\n```bash\nexport MODEL_NAME=\"CompVis/stable-diffusion-v1-4\"\nexport DATA_DIR=\"path-to-dir-containing-dicoo-images\"", "Distillation for quantization on Textual Inversion models to personalize text2image\n\n[Textual inversion](https://arxiv.org/abs/2208.01618) is a method to personalize text2image models like stable diffusion on your own images._By using just 3-5 images new concepts can be taught to Stable Diffusion and the model personalized on your own images_\nThe `textual_inversion.py` script shows how to implement the training procedure and adapt it for stable diffusion.\nWe have enabled distillation for quantization in `textual_inversion.py` to do quantization aware training as well as distillation on the model generated by Textual Inversion method.\n\n## Installing the dependencies\n\nBefore running the scripts, make sure to install the library's training dependencies:\n\n```bash\npip install -r requirements.txt\n```\n\n## Prepare Datasets\n\nOne picture which is from the huggingface datasets [sd-concepts-library/dicoo2](https://huggingface.co/sd-concepts-library/dicoo2) is needed, and save it to the `./dicoo` directory. The picture is shown below:", "Textual inversion can also be trained on undesirable things to create *negative embeddings* to discourage a model from generating images with those undesirable things like blurry images or extra fingers on a hand. This can be an easy way to quickly improve your prompt. You'll also load the embeddings with [`~loaders.TextualInversionLoaderMixin.load_textual_inversion`], but this time, you'll need two more parameters:\n\n- `weight_name`: specifies the weight file to load if the file was saved in the \ud83e\udd17 Diffusers format with a specific name or if the file is stored in the A1111 format\n- `token`: specifies the special word to use in the prompt to trigger the embeddings\n\nLet's load the [sayakpaul/EasyNegative-test](https://huggingface.co/sayakpaul/EasyNegative-test) embeddings:\n\n```py\npipeline.load_textual_inversion(\n    \"sayakpaul/EasyNegative-test\", weight_name=\"EasyNegative.safetensors\", token=\"EasyNegative\"\n)\n```\n\nNow you can use the `token` to generate an image with the negative embeddings:"], "eval_score_GPT4": "4", "eval_feedback_GPT4": "The response provided gives a correct and accurate description of Textual Inversion, mentioning its purpose to personalize text2image models and how it works by creating embeddings with a special word. It also correctly states that the method allows the diffusion model weights to remain unchanged during training. The explanation aligns well with the reference answer, which describes Textual Inversion as a training method for personalizing models with new text embeddings from a few example images. The response could be more concise, but it still conveys the essential purpose of Textual Inversion accurately."}, {"question": "What is the recommended multiple of batch size for fp16 data type on an A100 GPU?\n", "true_answer": "64", "source_doc": "huggingface/transformers/blob/main/docs/source/en/perf_train_gpu_one.md", "generated_answer": "The recommended multiple of batch size for fp16 data type on an A100 GPU is 64, according to the Tensor Core Requirements document provided in the context. However, for smaller parameters, it's recommended to also consider the Dimension Quantization Effects, which can have a significant speedup with the right multiplier. For larger batch sizes, it's suggested to move to a multi-GPU setup, as outlined in the Batch size choice section.", "retrieved_docs": ["[Tensor Core Requirements](https://docs.nvidia.com/deeplearning/performance/dl-performance-matrix-multiplication/index.html#requirements-tc) \ndefine the multiplier based on the dtype and the hardware. For instance, for fp16 data type a multiple of 8 is recommended, unless \nit's an A100 GPU, in which case use multiples of 64.\n\nFor parameters that are small, consider also [Dimension Quantization Effects](https://docs.nvidia.com/deeplearning/performance/dl-performance-matrix-multiplication/index.html#dim-quantization). \nThis is where tiling happens and the right multiplier can have a significant speedup.\n\n## Gradient Accumulation", "This is a benchmark sample for the batch size = 1 case. The benchmark was run on a single NVIDIA A100-SXM4-80GB GPU. We used a prompt length of 512, and generated exactly 512 new tokens. The first row is the unquantized `fp16` baseline, while the other rows show memory consumption and performance using different AutoGPTQ kernels.", "# using bitsandbytes fp4/fp16 scheme\nCUDA_VISIBLE_DEVICES=0 python benchmark_gptq.py --model meta-llama/Llama-2-13b-chat-hf --sweep --num-batches 4 --task text-generation --bitsandbytes --generate\n```\n\nHere are results obtained on a single NVIDIA A100-SXM4-80GB GPU. We use a prompt length of 512, and generate exactly 512 new tokens. Each generation is repeated for 4 batches, and metrics are averaged over the number of batches and generation length.\n\nAdditional benchmarks could be done in the act-order case.\n\nFrom the bencharmk, it appears that Exllama kernel is the best-in-class for GPTQ, although it is rather slow for larger batch sizes. The memory savings are not exactly of x4 although weights are in int4. This can be explained by the possible static buffers used by the kernels, the CUDA context (taken into account in the measurements), and the KV cache that is still in fp16.\n\nBitsandbytes uses the fp4 scheme, with the compute in fp16.\n\n### Batch size = 1", "Finally, if all of the above is still not enough, even after switching to a server-grade GPU like A100, consider moving \nto a multi-GPU setup. All these approaches are still valid in a multi-GPU setup, plus you can leverage additional parallelism \ntechniques outlined in the [multi-GPU section](perf_train_gpu_many). \n\n## Batch size choice\n\nTo achieve optimal performance, start by identifying the appropriate batch size. It is recommended to use batch sizes and \ninput/output neuron counts that are of size 2^N. Often it's a multiple of 8, but it can be \nhigher depending on the hardware being used and the model's dtype.\n\nFor reference, check out NVIDIA's recommendation for [input/output neuron counts](\nhttps://docs.nvidia.com/deeplearning/performance/dl-performance-fully-connected/index.html#input-features) and \n[batch size](https://docs.nvidia.com/deeplearning/performance/dl-performance-fully-connected/index.html#batch-size) for \nfully connected layers (which are involved in GEMMs (General Matrix Multiplications)).", "First of all set batch size to 1 (you can always use gradient accumulation for any desired effective batch size).\n\n1. Enable `--gradient_checkpointing 1` (HF Trainer) or directly `model.gradient_checkpointing_enable()` - if OOM then\n2. Try ZeRO stage 2 first. if OOM then\n3. Try ZeRO stage 2 + `offload_optimizer` - if OOM then\n4. Switch to ZeRO stage 3 - if OOM then\n5. Enable `offload_param` to `cpu` - if OOM then\n6. Enable `offload_optimizer` to `cpu` - if OOM then\n\n7. If you still can't fit a batch size of 1 first check various default values and lower them if you can. For example, if you use `generate` and you don't use a wide search beam make it narrower as it'd take a lot of memory.\n\n8. Definitely use mixed half-precision over fp32 - so bf16 on Ampere and higher GPUs and fp16 on older gpu architectures.", "where OOM == Out of Memory condition where the batch size was too big to fit into GPU memory.\n\nGetting an under 1 msec throughput with Deepspeed-Inference's Tensor Parallelism (TP) and custom fused CUDA kernels! That's absolutely amazing! Though using this solution for other models that it hasn't been tried on may require some developer time to make it work.\n\nAccelerate is super fast as well. It uses a very simple approach of naive Pipeline Parallelism (PP) and because it's very simple it should work out of the box with any model.\n\nSince Deepspeed-ZeRO can process multiple generate streams in parallel its throughput can be further divided by 8 or 16, depending on whether 8 or 16 GPUs were used during the `generate` call. And, of course, it means that it can process a batch size of 64 in the case of 8x80 A100 (the table above) and thus the throughput is about 4msec - so all 3 solutions are very close to each other.", "Table 2: Benchmarking FSDP on GPT-2 XL (1.5B) model\n\nFrom Table 2, we can observe that DDP (w and w/o fp16) isn\u2019t even able to run with batch size of 1 and results in CUDA OOM error. FSDP with Zero-Stage 3 is able to be run on 2 GPUs with batch size of 5 (effective batch size =10 (5 X 2)). FSDP with CPU offload can further increase the max batch size to 14 per GPU when using 2 GPUs. **FSDP with CPU offload enables training GPT-2 1.5B model on a single GPU with a batch size of 10**. This enables ML practitioners with minimal compute resources to train such large models, thereby democratizing large model training.\n\n## Capabilities and limitations of the FSDP Integration\n\nLet\u2019s dive into the current support that Accelerate provides for FSDP integration and the known limitations.\n\n**Required PyTorch version for FSDP support**: PyTorch Nightly (or 1.12.0 if you read this after it has been released) as the model saving with FSDP activated is only available with recent fixes.\n\n**Configuration through CLI:**"], "eval_score_GPT4": "5", "eval_feedback_GPT4": "The response correctly identifies the recommended multiple of batch size for fp16 data type on an A100 GPU as 64, which aligns with the reference answer. Additionally, the response provides extra context regarding the Tensor Core Requirements and considerations for different parameter sizes, which is not required but does not detract from the accuracy of the main answer. The response is factual and directly addresses the instruction given."}, {"question": "How do you run a Gradio Blocks app in reload mode using a Python IDE?\n", "true_answer": "Run `gradio run.py` in the terminal.", "source_doc": "gradio-app/gradio/blob/main/guides/09_other-tutorials/developing-faster-with-reload-mode.md", "generated_answer": "To run a Gradio Blocks app in reload mode using a Python IDE, follow these steps:\n\n1. Create a new file named `app.py` in the root of your project.\n2. Inside the `app.py` file, create a Gradio Blocks app using the `gr.Blocks()` function.\n3. If you have named your demo something other than `demo`, pass its name as the second parameter to the `launch()", "retrieved_docs": ["This short Guide will cover both of these methods, so no matter how you write Python, you'll leave knowing how to build Gradio apps faster.\n\n## Python IDE Reload \ud83d\udd25\n\nIf you are building Gradio Blocks using a Python IDE, your file of code (let's name it `run.py`) might look something like this:\n\n```python\nimport gradio as gr\n\nwith gr.Blocks() as demo:\n    gr.Markdown(\"# Greetings from Gradio!\")\n    inp = gr.Textbox(placeholder=\"What is your name?\")\n    out = gr.Textbox()\n\n    inp.change(fn=lambda x: f\"Welcome, {x}!\",\n               inputs=inp,\n               outputs=out)\n\nif __name__ == \"__main__\":\n    demo.launch()\n```\n\nThe problem is that anytime that you want to make a change to your layout, events, or components, you have to close and rerun your app by writing `python run.py`.\n\nInstead of doing this, you can run your code in **reload mode** by changing 1 word: `python` to `gradio`:", "###### Queue now works with reload mode!\n\nYou can now call `queue` on your `demo` outside of the `if __name__ == \"__main__\"` block and\nrun the script in reload mode with the `gradio` command.\n\nAny changes to the `app.py` file will be reflected in the webpage automatically and the queue will work\nproperly!\n\nBy [@freddyaboulton](https://github.com/freddyaboulton) in [PR 3089](https://github.com/gradio-app/gradio/pull/3089)\n\n###### Allow serving files from additional directories\n\n```python\ndemo = gr.Interface(...)\ndemo.launch(\n  file_directories=[\"/var/lib/demo/path/to/resources\"]\n)\n```\n\nBy [@maxaudron](https://github.com/maxaudron) in [PR 3075](https://github.com/gradio-app/gradio/pull/3075)\n\n### Bug Fixes:", "###### Queue now works with reload mode!\n\nYou can now call `queue` on your `demo` outside of the `if __name__ == \"__main__\"` block and\nrun the script in reload mode with the `gradio` command.\n\nAny changes to the `app.py` file will be reflected in the webpage automatically and the queue will work\nproperly!\n\nBy [@freddyaboulton](https://github.com/freddyaboulton) in [PR 3089](https://github.com/gradio-app/gradio/pull/3089)\n\n###### Allow serving files from additional directories\n\n```python\ndemo = gr.Interface(...)\ndemo.launch(\n  file_directories=[\"/var/lib/demo/path/to/resources\"]\n)\n```\n\nBy [@maxaudron](https://github.com/maxaudron) in [PR 3075](https://github.com/gradio-app/gradio/pull/3075)\n\n### Bug Fixes:", "## \ud83d\ude80 Run a Gradio app\n\nYou can get started by creating an `app.py` file in the root:\n\n```\nimport gradio as gr\n\nwith gr.Blocks() as demo:\n   gr.Button()\n    \nif __name__ == \"__main__\":\n   demo.launch()\n```\n\nthen run:\n\n```\ngradio app.py\n```\n\nThis will start the backend server in reload mode, which will watch for changes in the `gradio` folder and reload the app if changes are made. By default, Gradio will launch on port 7860. You can also just use `python app.py`, but this won't automatically trigger updates. \n\n\nIf you're making frontend changes, start the frontend server:\n\n```\npnpm dev\n```\nThis will open a separate browser tab. By default, Gradio will launch this on port 9876. Any changes to the frontend will also reload automatically in the browser. For more information about developing in the frontend, you can refer to [js/README.md](js/README.md).", "Developing Faster with Auto-Reloading\n\n**Prerequisite**: This Guide requires you to know about Blocks. Make sure to [read the Guide to Blocks first](https://gradio.app/guides/quickstart/#blocks-more-flexibility-and-control).\n\nThis guide covers auto reloading, reloading in a Python IDE, and using gradio with Jupyter Notebooks.\n\n## Why Auto-Reloading?\n\nWhen you are building a Gradio demo, particularly out of Blocks, you may find it cumbersome to keep re-running your code to test your changes.\n\nTo make it faster and more convenient to write your code, we've made it easier to \"reload\" your Gradio apps instantly when you are developing in a **Python IDE** (like VS Code, Sublime Text, PyCharm, or so on) or generally running your Python code from the terminal. We've also developed an analogous \"magic command\" that allows you to re-run cells faster if you use **Jupyter Notebooks** (or any similar environment like Colab).", "There is one important thing to keep in mind when using the reload mode: Gradio specifically looks for a Gradio Blocks/Interface demo called `demo` in your code. If you have named your demo something else, you will need to pass in the name of your demo as the 2nd parameter in your code. So if your `run.py` file looked like this:\n\n```python\nimport gradio as gr\n\nwith gr.Blocks() as my_demo:\n    gr.Markdown(\"# Greetings from Gradio!\")\n    inp = gr.Textbox(placeholder=\"What is your name?\")\n    out = gr.Textbox()\n\n    inp.change(fn=lambda x: f\"Welcome, {x}!\",\n               inputs=inp,\n               outputs=out)\n\nif __name__ == \"__main__\":\n    my_demo.launch()\n```\n\nThen you would launch it in reload mode like this: `gradio run.py my_demo`.", "Read more in the [Embedding Gradio Demos](https://gradio.app/embedding_gradio_demos) guide.\n\n###### 2. Reload Mode \ud83d\udc68\u200d\ud83d\udcbb\n\nReload mode helps developers create gradio demos faster by automatically reloading the demo whenever the code changes. It can support development on Python IDEs (VS Code, PyCharm, etc), the terminal, as well as Jupyter notebooks.\n\nIf your demo code is in a script named `app.py`, instead of running `python app.py` you can now run `gradio app.py` and that will launch the demo in reload mode:\n\n```bash\nLaunching in reload mode on: http://127.0.0.1:7860 (Press CTRL+C to quit)\nWatching...\nWARNING: The --reload flag should not be used in production on Windows.\n```\n\nIf you're working from a Jupyter or Colab Notebook, use these magic commands instead: `%load_ext gradio` when you import gradio, and `%%blocks` in the top of the cell with the demo code. Here's an example that shows how much faster the development becomes:"], "eval_score_GPT4": "2", "eval_feedback_GPT4": "The response provided does not accurately describe how to run a Gradio Blocks app in reload mode using a Python IDE. The reference answer suggests using the terminal command `gradio run.py`, which is not mentioned in the response. Additionally, the response is incomplete as it ends abruptly and does not provide a full explanation or correct steps to achieve the task. Based on the score rubric, the response is mostly incorrect and incomplete."}, {"question": "What command is used to install the development version of the \ud83e\udd17 Transformers library in a Python virtual environment?\n", "true_answer": "pip install \"transformers[sentencepiece]\"", "source_doc": "huggingface/course/blob/main/chapters/en/chapter0/1.mdx", "generated_answer": "To install the development version of the \ud83e\udd17 Transformers library in a Python virtual environment, you can follow these steps:\n\n1. Create a virtual environment using your preferred Python distribution, such as Anaconda, and activate it.\n\n2. Install the required dependencies for your specific use case by following the instructions provided in the [\ud83e\udd17 Transformers installation page](https://github.com/huggingface/transformers#installation).", "retrieved_docs": ["You must keep the `transformers` folder if you want to keep using the library.\n\n</Tip>\n\nNow you can easily update your clone to the latest version of \ud83e\udd17 Transformers with the following command:\n\n```bash\ncd ~/transformers/\ngit pull\n```\n\nYour Python environment will find the `main` version of \ud83e\udd17 Transformers on the next run.\n\n## Install with conda\n\nInstall from the conda channel `huggingface`:\n\n```bash\nconda install -c huggingface transformers\n```\n\n## Cache setup\n\nPretrained models are downloaded and locally cached at: `~/.cache/huggingface/hub`. This is the default directory given by the shell environment variable `TRANSFORMERS_CACHE`. On Windows, the default directory is given by `C:\\Users\\username\\.cache\\huggingface\\hub`. You can change the shell environment variables shown below - in order of priority - to specify a different cache directory:\n\n1. Shell environment variable (default): `HUGGINGFACE_HUB_CACHE` or `TRANSFORMERS_CACHE`.\n2. Shell environment variable: `HF_HOME`.\n3. Shell environment variable: `XDG_CACHE_HOME` + `/huggingface`.", "The next step is to install the libraries that we'll be using in this course. We'll use `pip` for the installation, which is the package manager for Python. In notebooks, you can run system commands by preceding them with the `!` character, so you can install the \ud83e\udd17 Transformers library as follows:\n\n```\n!pip install transformers\n```\n\nYou can make sure the package was correctly installed by importing it within your Python runtime:\n\n```\nimport transformers\n```\n\n<div class=\"flex justify-center\">\n<img src=\"https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter0/install.gif\" alt=\"A gif showing the result of the two commands above: installation and import\" width=\"80%\"/>\n</div>\n\nThis installs a very light version of \ud83e\udd17 Transformers. In particular, no specific machine learning frameworks (like PyTorch or TensorFlow) are installed. Since we'll be using a lot of different features of the library, we recommend installing the development version, which comes with all the required dependencies for pretty much any imaginable use case:", "You should install \ud83e\udd17 Transformers in a [virtual environment](https://docs.python.org/3/library/venv.html). If you're unfamiliar with Python virtual environments, check out the [user guide](https://packaging.python.org/guides/installing-using-pip-and-virtual-environments/).\n\nFirst, create a virtual environment with the version of Python you're going to use and activate it.\n\nThen, you will need to install at least one of Flax, PyTorch, or TensorFlow.\nPlease refer to [TensorFlow installation page](https://www.tensorflow.org/install/), [PyTorch installation page](https://pytorch.org/get-started/locally/#start-locally) and/or [Flax](https://github.com/google/flax#quick-install) and [Jax](https://github.com/google/jax#installation) installation pages regarding the specific installation command for your platform.\n\nWhen one of those backends has been installed, \ud83e\udd17 Transformers can be installed using pip as follows:\n\n```bash\npip install transformers\n```", "Check if \ud83e\udd17 Transformers has been properly installed by running the following command:\n\n```bash\npython -c \"from transformers import pipeline; print(pipeline('sentiment-analysis')('I love you'))\"\n```\n\n## Editable install\n\nYou will need an editable install if you'd like to:\n\n* Use the `main` version of the source code.\n* Contribute to \ud83e\udd17 Transformers and need to test changes in the code.\n\nClone the repository and install \ud83e\udd17 Transformers with the following commands:\n\n```bash\ngit clone https://github.com/huggingface/transformers.git\ncd transformers\npip install -e .\n```\n\nThese commands will link the folder you cloned the repository to and your Python library paths. Python will now look inside the folder you cloned to in addition to the normal library paths. For example, if your Python packages are typically installed in `~/anaconda3/envs/main/lib/python3.7/site-packages/`, Python will also search the folder you cloned to: `~/transformers/`.\n\n<Tip warning={true}>\n\nYou must keep the `transformers` folder if you want to keep using the library.", "```\n!pip install transformers[sentencepiece]\n```\n\nThis will take a bit of time, but then you'll be ready to go for the rest of the course!\n\n## Using a Python virtual environment[[using-a-python-virtual-environment]]\n\nIf you prefer to use a Python virtual environment, the first step is to install Python on your system. We recommend following [this guide](https://realpython.com/installing-python/) to get started.\n\nOnce you have Python installed, you should be able to run Python commands in your terminal. You can start by running the following command to ensure that it is correctly installed before proceeding to the next steps: `python --version`. This should print out the Python version now available on your system.\n\nWhen running a Python command in your terminal, such as `python --version`, you should think of the program running your command as the \"main\" Python on your system. We recommend keeping this main installation free of any packages, and using it to create separate environments for each application you work on \u2014 this way, each application can have its own dependencies and packages, and you won't need to worry about potential compatibility issues with other applications.", "You should install all necessary Python packages in a [virtual environment](https://docs.python.org/3/library/venv.html). If you are unfamiliar with Python virtual environments, please check out the [user guide](https://packaging.python.org/guides/installing-using-pip-and-virtual-environments/).\n\nBelow, we create a virtual environment with the [Anaconda Python distribution](https://www.anaconda.com/products/distribution) and activate it.\n```sh\nconda create -n strata python=3.9\nconda activate strata\n```\nNext, you need to install \ud83e\udd17 Transformers. Please refer to [\ud83e\udd17 Transformers installation page](https://github.com/huggingface/transformers#installation) for a detailed guide.\n```sh\npip install transformers\n```\nFinally, install all necessary Python packages for our self-training algorithm.\n\n```sh\npip install -r STraTA/selftraining/requirements.txt\n```\nThis will install PyTorch as a backend.", "2.  Clone your `transformers` fork to your local disk, and add the base\n    repository as a remote:\n\n    ```bash\n    git clone https://github.com/[your Github handle]/transformers.git\n    cd transformers\n    git remote add upstream https://github.com/huggingface/transformers.git\n    ```\n\n3.  Set up a development environment, for instance by running the\n    following command:\n\n    ```bash\n    python -m venv .env\n    source .env/bin/activate\n    pip install -e \".[dev]\"\n    ```\n\nand return to the parent directory\n\n```bash\ncd ..\n```\n\n4.  We recommend adding the PyTorch version of *[camelcase name of model]* to\n    Transformers. To install PyTorch, please follow the instructions [here](https://pytorch.org/get-started/locally/).\n\n**Note:** You don't need to have CUDA installed. Making the new model\nwork on CPU is sufficient.\n\n5.  To port *[camelcase name of model]*, you will also need access to its\n    original repository:"], "eval_score_GPT4": "2", "eval_feedback_GPT4": "The response provided does not directly answer the question regarding the specific command used to install the development version of the \ud83e\udd17 Transformers library. Instead, it gives a general approach to creating a virtual environment and refers to the installation page for further instructions. The response does not mention the use of pip or the specific package name \"transformers[sentencepiece]\" which is provided in the reference answer. Therefore, the response is mostly incorrect as it fails to provide the specific command required for the installation of the development version of the library."}, {"question": "How can you install the Hugging Face Unity API in your Unity project?\n", "true_answer": "To install the Hugging Face Unity API in your Unity project, go to `Window` -> `Package Manager`, click `+` and select `Add Package from git URL`, then enter `https://github.com/huggingface/unity-api.git`.", "source_doc": "huggingface/blog/blob/main/unity-api.md", "generated_answer": "To install the Hugging Face Unity API in your Unity project, follow these steps:\n\n1. Open your Unity project.\n2. Go to \"Window\" > \"Package Manager\".\n3. Click the \"+\" button and select \"Add Package from git URL\".\n4. Enter \"https://github.com/huggingface/unity-api.git\" and click \"Add\".\n5. Once the package is installed, the Hugging Face API", "retrieved_docs": ["### Prerequisites\n\nThis tutorial assumes basic knowledge of Unity. It also requires you to have installed the [Hugging Face Unity API](https://github.com/huggingface/unity-api). For instructions on setting up the API, check out our [earlier blog post](https://huggingface.co/blog/unity-api).\n\n## Steps\n\n### 1. Set up the Scene\n\nIn this tutorial, we'll set up a very simple scene where the player can start and stop a recording, and the result will be converted to text.\n\nBegin by creating a Unity project, then creating a Canvas with four UI elements:\n\n1. **Start Button**: This will start the recording.\n2. **Stop Button**: This will stop the recording.\n3. **Text (TextMeshPro)**: This is where the result of the speech recognition will be displayed.\n\n### 2. Set up the Script\n\nCreate a script called `SpeechRecognitionTest` and attach it to an empty GameObject.", "## Usage Tips\n\n1. Keep in mind that the API makes calls asynchronously, and returns a response or error via callbacks.\n2. Address slow response times or performance issues by changing model endpoints to lower resource models.\n\n## Conclusion\n\nThe Hugging Face Unity API offers a simple way to integrate AI models into your Unity projects. We hope you found this tutorial helpful. If you have any questions or would like to get more involved in using Hugging Face for Games, join the [Hugging Face Discord](https://hf.co/join/discord)!", "--\ntitle: \"How to Install and Use the Hugging Face Unity API\"\nthumbnail: /blog/assets/124_ml-for-games/unity-api-thumbnail.png\nauthors:\n- user: dylanebert\n---\n\n# How to Install and Use the Hugging Face Unity API\n\n<!-- {authors} --> \n\nThe [Hugging Face Unity API](https://github.com/huggingface/unity-api) is an easy-to-use integration of the [Hugging Face Inference API](https://huggingface.co/inference-api), allowing developers to access and use Hugging Face AI models in their Unity projects. In this blog post, we'll walk through the steps to install and use the Hugging Face Unity API.\n\n## Installation\n\n1. Open your Unity project\n2. Go to `Window` -> `Package Manager`\n3. Click `+` and select `Add Package from git URL`\n4. Enter `https://github.com/huggingface/unity-api.git`\n5. Once installed, the Unity API wizard should pop up. If not, go to `Window` -> `Hugging Face API Wizard`", "- **[Compilation of AI tools for Game Dev](https://github.com/simoninithomas/awesome-ai-tools-for-game-dev)**\n- How to install the Unity Hugging Face API:\u00a0**https://huggingface.co/blog/unity-api**\n- AI Speech Recognition in Unity:\u00a0**https://huggingface.co/blog/unity-asr**\n- Making ML-powered web games with Transformers.js:\u00a0**https://huggingface.co/blog/ml-web-games**\n- Building a smart Robot AI using Hugging Face\u00a0\ud83e\udd17\u00a0and Unity:\u00a0**https://thomassimonini.substack.com/p/building-a-smart-robot-ai-using-hugging**\n\nTo stay connected and stay updated on future events, feel free to drop by our Discord server, where you can find channels dedicated to exchanging ideas about AI for games.\n\nJoin our Discord Server\u00a0\ud83d\udc49\u00a0**https://hf.co/join/discord**\n\n**Thank you to all the participants, contributors, and supporters who made this event a memorable success!**", "In the Build Settings window, click the \"Player Settings\" button to open the Player Settings panel.\n\n<figure class=\"image text-center\">\n  <img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/124_ml-for-games/games-in-spaces/5.png\">\n</figure> \n\n## Step 6: Optionally, Download the Hugging Face Unity WebGL Template\n\nYou can enhance your game's appearance in a Space by downloading the Hugging Face Unity WebGL template, available [here](https://github.com/huggingface/Unity-WebGL-template-for-Hugging-Face-Spaces). Just download the repository and drop it in your project files.\n\nThen, in the Player Settings panel, switch the WebGL template to Hugging Face. To do so, in Player Settings, click \"Resolution and Presentation\", then select the Hugging Face WebGL template.", "## Step 11: Push your Changes\n\nFinally, use the following Git commands to push your changes:\n\n```\ngit add .\ngit commit -m \"Add Unity WebGL build files\"\ngit push\n```\n\n## Done!\n\nCongratulations! Refresh your Space. You should now be able to play your game in a Hugging Face Space.\n\nWe hope you found this tutorial helpful. If you have any questions or would like to get more involved in using Hugging Face for Games, join the [Hugging Face Discord](https://hf.co/join/discord)!", "## Step 3: Open your Unity Project\n\nOpen the Unity project you want to host in your Space.\n\n<figure class=\"image text-center\">\n  <img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/124_ml-for-games/games-in-spaces/3.png\">\n</figure> \n\n## Step 4: Switch the Build Target to WebGL\n\nNavigate to `File > Build Settings` and switch the Build Target to WebGL.\n\n<figure class=\"image text-center\">\n  <img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/124_ml-for-games/games-in-spaces/4.png\">\n</figure> \n\n## Step 5: Open Player Settings\n\nIn the Build Settings window, click the \"Player Settings\" button to open the Player Settings panel."], "eval_score_GPT4": "4", "eval_feedback_GPT4": "The response provided is mostly correct and follows the steps outlined in the reference answer. It accurately describes the process of installing the Hugging Face Unity API by accessing the Package Manager and adding the package from a git URL. However, the response is incomplete as it ends abruptly without finishing the last sentence. This leaves out any final steps or confirmation that the installation was successful, which is a minor factual inaccuracy due to the lack of completion. Based on the score rubric, the response is not completely correct because it is not a finished answer."}, {"question": "What is the pretraining objective of the Wav2Vec2 context network?\n", "true_answer": "The pretraining objective of the Wav2Vec2 context network is a contrastive task where the model has to predict the true quantized speech representation of the masked prediction from a set of false ones.", "source_doc": "huggingface/transformers/blob/main/docs/source/en/tasks_explained.md", "generated_answer": "The pretraining objective of the Wav2Vec2 context network is a novel contrastive loss objective, as described in the official paper (https://arxiv.org/abs/2006.11477). This objective allows the Wav2Vec2 model to learn powerful speech representations from more than 50,000 hours of unlabeled speech during pretraining. The model learns contextualized speech representations by randomly masking feature vectors and", "retrieved_docs": ["Using a novel contrastive pretraining objective, Wav2Vec2 learns\npowerful speech representations from more than 50.000 hours of unlabeled\nspeech. Similar, to [BERT\\'s masked language\nmodeling](http://jalammar.github.io/illustrated-bert/), the model learns\ncontextualized speech representations by randomly masking feature\nvectors before passing them to a transformer network.\n\n![wav2vec2\\_structure](https://raw.githubusercontent.com/patrickvonplaten/scientific_images/master/wav2vec2.png)\n\nFor the first time, it has been shown that pretraining, followed by\nfine-tuning on very little labeled speech data achieves competitive\nresults to state-of-the-art ASR systems. Using as little as 10 minutes\nof labeled data, Wav2Vec2 yields a word error rate (WER) of less than 5%\non the clean test set of\n[LibriSpeech](https://huggingface.co/datasets/librispeech_asr) - *cf.*\nwith Table 9 of the [paper](https://arxiv.org/pdf/2006.11477.pdf).", "### Create Wav2Vec2CTCTokenizer\n\nThe [pretrained Wav2Vec2 checkpoint](https://huggingface.co/facebook/wav2vec2-base) maps the speech signal to a\nsequence of context representations as illustrated in the figure above.\nA fine-tuned Wav2Vec2 checkpoint needs to map this sequence of context\nrepresentations to its corresponding transcription so that a linear\nlayer has to be added on top of the transformer block (shown in yellow).\nThis linear layer is used to classifies each context representation to a\ntoken class analogous how, *e.g.*, after pretraining a linear layer is\nadded on top of BERT\\'s embeddings for further classification - *cf.*\nwith *\\\"BERT\\\"* section of this [blog post](https://huggingface.co/blog/warm-starting-encoder-decoder).\n\nThe output size of this layer corresponds to the number of tokens in the\nvocabulary, which does **not** depend on Wav2Vec2\\'s pretraining task,\nbut only on the labeled dataset used for fine-tuning. So in the first\nstep, we will take a look at Timit and define a vocabulary based on the\ndataset\\'s transcriptions.", "### Create `Wav2Vec2CTCTokenizer`\n\nA pre-trained XLS-R model maps the speech signal to a sequence of\ncontext representations as illustrated in the figure above. However, for\nspeech recognition the model has to to map this sequence of context\nrepresentations to its corresponding transcription which means that a\nlinear layer has to be added on top of the transformer block (shown in\nyellow in the diagram above). This linear layer is used to classify\neach context representation to a token class analogous to how\na linear layer is added on top of BERT\\'s embeddings\nfor further classification after pre-training (*cf.* with *\\'BERT\\'* section of the following [blog\npost](https://huggingface.co/blog/warm-starting-encoder-decoder)).\nafter pretraining a linear layer is added on top of BERT\\'s embeddings\nfor further classification - *cf.* with *\\'BERT\\'* section of this [blog\npost](https://huggingface.co/blog/warm-starting-encoder-decoder).", "### Pretraining Wav2Vec2\n\nThe `run_pretrain.py` script allows one to pretrain a Wav2Vec2 model from scratch using Wav2Vec2's contrastive loss objective (see official [paper](https://arxiv.org/abs/2006.11477) for more information). \nIt is recommended to pre-train Wav2Vec2 with Trainer + Deepspeed (please refer to [this guide](https://huggingface.co/transformers/main/main_classes/deepspeed.html#deepspeed-trainer-integration) for more information).\n\nHere is an example of how you can use DeepSpeed ZeRO-2 to pretrain a small Wav2Vec2 model:", "**Wav2Vec2** is a popular pre-trained model for speech recognition.\nReleased in [September 2020](https://ai.facebook.com/blog/wav2vec-20-learning-the-structure-of-speech-from-raw-audio/)\nby Meta AI Research, the novel architecture catalyzed progress in\nself-supervised pretraining for speech recognition, *e.g.* [*G. Ng et\nal.*, 2021](https://arxiv.org/pdf/2104.03416.pdf), [*Chen et al*,\n2021](https://arxiv.org/abs/2110.13900), [*Hsu et al.*,\n2021](https://arxiv.org/abs/2106.07447) and [*Babu et al.*,\n2021](https://arxiv.org/abs/2111.09296). On the Hugging Face Hub,\nWav2Vec2's most popular pre-trained checkpoint currently amounts to\nover [**250,000** monthly\ndownloads](https://huggingface.co/facebook/wav2vec2-base-960h).", "**Wav2Vec2** is a popular pre-trained model for speech recognition.\nReleased in [September 2020](https://ai.facebook.com/blog/wav2vec-20-learning-the-structure-of-speech-from-raw-audio/)\nby Meta AI Research, the novel architecture catalyzed progress in\nself-supervised pretraining for speech recognition, *e.g.* [*G. Ng et\nal.*, 2021](https://arxiv.org/pdf/2104.03416.pdf), [*Chen et al*,\n2021](https://arxiv.org/abs/2110.13900), [*Hsu et al.*,\n2021](https://arxiv.org/abs/2106.07447) and [*Babu et al.*,\n2021](https://arxiv.org/abs/2111.09296). On the Hugging Face Hub,\nWav2Vec2's most popular pre-trained checkpoint currently amounts to\nover [**250,000** monthly\ndownloads](https://huggingface.co/facebook/wav2vec2-base-960h).", "Using Connectionist Temporal Classification (CTC), pre-trained\nWav2Vec2-like checkpoints are extremely easy to fine-tune on downstream\nspeech recognition tasks. In a nutshell, fine-tuning pre-trained\nWav2Vec2 checkpoints works as follows:\n\nA single randomly initialized linear layer is stacked on top of the\npre-trained checkpoint and trained to classify raw audio input to a\nsequence of letters. It does so by:\n\n1.  extracting audio representations from the raw audio (using CNN\n    layers),\n2.  processing the sequence of audio representations with a stack of\n    transformer layers, and,\n3.  classifying the processed audio representations into a sequence of\n    output letters.\n\nPreviously audio classification models required an additional language\nmodel (LM) and a dictionary to transform the sequence of classified audio\nframes to a coherent transcription. Wav2Vec2's architecture is based on\ntransformer layers, thus giving each processed audio representation\ncontext from all other audio representations. In addition, Wav2Vec2\nleverages the [CTC algorithm](https://distill.pub/2017/ctc/) for\nfine-tuning, which solves the problem of alignment between a varying\n\"input audio length\"-to-\"output text length\" ratio."], "eval_score_GPT4": "4", "eval_feedback_GPT4": "Feedback: The response correctly identifies the pretraining objective as a contrastive loss objective, which aligns with the reference answer's mention of a contrastive task. However, the response does not mention the specific task of predicting the true quantized speech representation from a set of false ones, which is a key detail in the reference answer. The response is mostly correct but lacks this important detail to be completely accurate."}, {"question": "What is the default checkpoint used by the sentiment analysis pipeline in the Transformers library?\n", "true_answer": "distilbert base uncased finetuned sst2 english", "source_doc": "huggingface/course/blob/main/subtitles/en/raw/chapter2/02_inside-pipeline-pt.md", "generated_answer": "The default checkpoint used by the sentiment analysis pipeline in the Transformers library is \"distilbert-base-uncased-finetuned-sst-2-english\". This information can be found in the context provided, specifically in the first extract where it is mentioned that the pipeline uses this checkpoint by default.", "retrieved_docs": ["Since the default checkpoint of the `sentiment-analysis` pipeline is `distilbert-base-uncased-finetuned-sst-2-english` (you can see its model card [here](https://huggingface.co/distilbert-base-uncased-finetuned-sst-2-english)), we run the following:\n\n```python\nfrom transformers import AutoTokenizer\n\ncheckpoint = \"distilbert-base-uncased-finetuned-sst-2-english\"\ntokenizer = AutoTokenizer.from_pretrained(checkpoint)\n```\n\nOnce we have the tokenizer, we can directly pass our sentences to it and we'll get back a dictionary that's ready to feed to our model! The only thing left to do is to convert the list of input IDs to tensors.", ". Here the model uses expects a CLS token at the beginning and a SEP token at the end of the sentence to classify. Lastly, the tokenizer matches each token to its unique ID in the vocabulary of the pretrained model. To load such a tokenizer, the Transformers library provides the AutoTokenizer API. The most important method of this class is from_pretrained, which will download and cache the configuration and the vocabulary associated to a given checkpoint. Here, the checkpoint used by default for the sentiment analysis pipeline is distilbert base uncased finetuned sst2 english. We instantiate a tokenizer associated with that checkpoint, then feed it the two sentences. Since those two sentences are not of the same size, we will need to pad the shortest one to be able to build an array. This is done by the tokenizer with the option padding=True. With truncation=True, we ensure that any sentence longer than the maximum the model can handle is truncated. Lastly, the return_tensors option tells the tokenizer to return a PyTorch tensor. Looking at the result, we see we have a dictionary with two keys", ". Here the model uses expects a CLS token at the beginning and a SEP token at the end of the sentence to classify. Lastly, the tokenizer matches each token to its unique ID in the vocabulary of the pretrained model. To load such a tokenizer, the Transformers library provides the AutoTokenizer API. The most important method of this class is from_pretrained, which will download and cache the configuration and the vocabulary associated to a given checkpoint. Here, the checkpoint used by default for the sentiment analysis pipeline is distilbert base uncased finetuned sst2 english. We instantiate a tokenizer associated with that checkpoint, then feed it the two sentences. Since those two sentences are not of the same size, we will need to pad the shortest one to be able to build an array. This is done by the tokenizer with the option padding=True. With truncation=True, we ensure that any sentence longer than the maximum the model can handle is truncated. Lastly, the return_tensors option tells the tokenizer to return a TensorFlow tensor. Looking at the result, we see we have a dictionary with two keys", "## The Transformers library\nI was a bit confused at the beginning when I downloaded the .h5 file. I thought it would be compatible with `tensorflow.keras.models.load_model`, but this wasn't the case. After a few minutes of research I was able to figure out that the file was a weights checkpoint rather than a Keras model.\nAfter that, I tried out the API that Hugging Face offers and read a bit more about the pipeline feature they offer. Since the results of the API & the pipeline were great, I decided that I could serve the model through the pipeline on my own server.\n\nBelow is the [official example](https://github.com/huggingface/transformers#quick-tour) from the Transformers GitHub page.\n\n```python\nfrom transformers import pipeline\n\n# Allocate a pipeline for sentiment-analysis\nclassifier = pipeline('sentiment-analysis')\nclassifier('We are very happy to include pipeline into the transformers repository.')\n[{'label': 'POSITIVE', 'score': 0.9978193640708923}]\n```\n\n\n## Deploy transformers to Google Cloud\n> GCP is chosen as it is the cloud environment I am using in my personal organization.", "Start by creating an instance of [`pipeline`] and specifying a task you want to use it for. In this guide, you'll use the [`pipeline`] for sentiment analysis as an example:\n\n```py\n>>> from transformers import pipeline\n\n>>> classifier = pipeline(\"sentiment-analysis\")\n```\n\nThe [`pipeline`] downloads and caches a default [pretrained model](https://huggingface.co/distilbert-base-uncased-finetuned-sst-2-english) and tokenizer for sentiment analysis. Now you can use the `classifier` on your target text:\n\n```py\n>>> classifier(\"We are very happy to show you the \ud83e\udd17 Transformers library.\")\n[{'label': 'POSITIVE', 'score': 0.9998}]\n```\n\nIf you have more than one input, pass your inputs as a list to the [`pipeline`] to return a list of dictionaries:", "he pipeline function. The pipeline function is the most high-level API of the Transformers library. It regroups together all the steps to go from raw texts to usable predictions. The model used is at the core of a pipeline, but the pipeline also include all the necessary pre-processing (since the model does not expect texts, but numbers) as well as some post-processing to make the output of the model human-readable. Let's look at a first example with the sentiment analysis pipeline. This pipeline performs text classification on a given input, and determines if it's positive or negative. Here, it attributed the positive label on the given text, with a confidence of 95%. You can pass multiple texts to the same pipeline, which will be processed and passed through the model together, as a batch. The output is a list of individual results, in the same order as the input texts. Here we find the same label and score for the first text, and the second text is judged positive with a confidence of 99.99%. The zero-shot classification pipeline is a more general text-classification pipeline: it allows you to provide the labels you want", "There are more than [215 sentiment analysis models](https://huggingface.co/models?pipeline_tag=text-classification&sort=downloads&search=sentiment) publicly available on the Hub and integrating them with Python just takes 5 lines of code:\n\n\n```python\npip install -q transformers\nfrom transformers import pipeline\nsentiment_pipeline = pipeline(\"sentiment-analysis\")\ndata = [\"I love you\", \"I hate you\"]\nsentiment_pipeline(data)\n```\n\nThis code snippet uses the [pipeline class](https://huggingface.co/docs/transformers/main_classes/pipelines) to make predictions from models available in the Hub. It uses the [default model for sentiment analysis](https://huggingface.co/distilbert-base-uncased-finetuned-sst-2-english?text=I+like+you.+I+love+you) to analyze the list of texts `data` and it outputs the following results:\n\n```python\n[{'label': 'POSITIVE', 'score': 0.9998},\n {'label': 'NEGATIVE', 'score': 0.9991}]\n```"], "eval_score_GPT4": "5", "eval_feedback_GPT4": "Feedback: The response provided is completely correct, accurate, and factual. It matches the reference answer and correctly identifies \"distilbert-base-uncased-finetuned-sst-2-english\" as the default checkpoint for the sentiment analysis pipeline in the Transformers library."}, {"question": "What is the purpose of the notebook \"How to use DeepSpeed to train models with billions of parameters on Habana Gaudi\"?\n", "true_answer": "To show how to use DeepSpeed to pre-train/fine-tune the 1.6B-parameter GPT2-XL for causal language modeling on Habana Gaudi.", "source_doc": "huggingface/optimum/blob/main/docs/source/notebooks.md", "generated_answer": "The purpose of the notebook \"How to use DeepSpeed to train models with billions of parameters on Habana Gaudi\" is to demonstrate how to utilize the DeepSpeed library, which is designed for speed and scale in distributed training of large models with billions of parameters, in conjunction with Habana Gaudi, a second-generation AI hardware accelerator designed by Habana Labs, to pre-train and fine-tune the 1.6B-parameter G", "retrieved_docs": ["| [How to use DeepSpeed to train models with billions of parameters on Habana Gaudi](https://github.com/huggingface/optimum-habana/blob/main/notebooks/AI_HW_Summit_2022.ipynb) | Show how to use DeepSpeed to pre-train/fine-tune the 1.6B-parameter GPT2-XL for causal language modeling on Habana Gaudi. |  [![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/huggingface/optimum-habana/blob/main/notebooks/AI_HW_Summit_2022.ipynb) | [![Open in AWS Studio](https://studiolab.sagemaker.aws/studiolab.svg)](https://studiolab.sagemaker.aws/import/github/huggingface/optimum-habana/blob/main/notebooks/AI_HW_Summit_2022.ipynb) |", "| [How to use DeepSpeed to train models with billions of parameters on Habana Gaudi](https://github.com/huggingface/optimum-habana/blob/main/notebooks/AI_HW_Summit_2022.ipynb) | Show how to use DeepSpeed to pre-train/fine-tune the 1.6B-parameter GPT2-XL for causal language modeling on Habana Gaudi. |  [![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/huggingface/optimum-habana/blob/main/notebooks/AI_HW_Summit_2022.ipynb) | [![Open in AWS Studio](https://studiolab.sagemaker.aws/studiolab.svg)](https://studiolab.sagemaker.aws/import/github/huggingface/optimum-habana/blob/main/notebooks/AI_HW_Summit_2022.ipynb) |", "Such large models raise new challenges in terms of memory and speed for both [training](https://huggingface.co/blog/bloom-megatron-deepspeed) and [inference](https://huggingface.co/blog/bloom-inference-optimization). Even in 16-bit precision, one instance requires 352 GB to fit! You will probably struggle to find any device with so much memory at the moment, but state-of-the-art hardware like Habana Gaudi2 does make it possible to perform inference on BLOOM and BLOOMZ models with low latencies.\n\n\n## Habana Gaudi2\n\n[Gaudi2](https://habana.ai/training/gaudi2/) is the second-generation AI hardware accelerator designed by Habana Labs. A single server contains 8 accelerator devices (called Habana Processing Units, or HPUs) with 96GB of memory each, which provides room to make very large models fit in. However, hosting the model is not very interesting if the computation is slow. Fortunately, Gaudi2 shines on that aspect: it differs from GPUs in that its architecture enables the accelerator to perform General Matrix Multiplication (GeMM) and other operations in parallel, which speeds up deep learning workflows. These features make Gaudi2 a great candidate for LLM training and inference.", "!--\u26a0\ufe0f Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\nrendered properly in your Markdown viewer.\n-->\n\n# DeepSpeed\n\n[DeepSpeed](https://www.deepspeed.ai/) is a library designed for speed and scale for distributed training of large models with billions of parameters. At its core is the Zero Redundancy Optimizer (ZeRO) that shards optimizer states (ZeRO-1), gradients (ZeRO-2), and parameters (ZeRO-3) across data parallel processes. This drastically reduces memory usage, allowing you to scale your training to billion parameter models. To unlock even more memory efficiency, ZeRO-Offload reduces GPU compute and memory by leveraging CPU resources during optimization.", "If you are interested in accelerating your Machine Learning training and inference workflows using the latest AI hardware accelerators and software libraries, check out our [Expert Acceleration Program](https://huggingface.co/support). To learn more about Habana solutions, [read about our partnership and contact them here](https://huggingface.co/hardware/habana). To learn more about Hugging Face efforts to make AI hardware accelerators easy to use, check out our [Hardware Partner Program](https://huggingface.co/hardware).\n\n\n### Related Topics\n\n- [Faster Training and Inference: Habana Gaudi-2 vs Nvidia A100 80GB](https://huggingface.co/blog/habana-gaudi-2-benchmark)\n- [Leverage DeepSpeed to Train Faster and Cheaper Large Scale Transformer Models with Hugging Face and Habana Labs Gaudi](https://developer.habana.ai/events/leverage-deepspeed-to-train-faster-and-cheaper-large-scale-transformer-models-with-hugging-face-and-habana-labs-gaudi/)\n\n---", "The `DL1` instance we use has 8 available HPU-cores meaning we can leverage distributed data-parallel training for our model.\nTo run our training as distributed training we need to create a training script, which can be used with multiprocessing to run on all HPUs.\nWe have created a [run_mlm.py](https://github.com/philschmid/deep-learning-habana-huggingface/blob/master/pre-training/scripts/run_mlm.py) script implementing masked-language modeling using the `GaudiTrainer`. To execute our distributed training we use the `DistributedRunner` runner from `optimum-habana` and pass our arguments. Alternatively, you could check-out the [gaudi_spawn.py](https://github.com/huggingface/optimum-habana/blob/main/examples/gaudi_spawn.py) in the [optimum-habana](https://github.com/huggingface/optimum-habana) repository.", "In the fall of 2019 Samyam Rajbhandari, Jeff Rasley, Olatunji Ruwase and Yuxiong He published a paper:\n[ZeRO: Memory Optimizations Toward Training Trillion Parameter Models](https://arxiv.org/abs/1910.02054), which contains a plethora of ingenious new ideas on how one could make their hardware do much more than what it was thought possible before. A short time later [DeepSpeed](https://github.com/microsoft/deepspeed) has been released and it gave to the world the open source implementation of most of the ideas in that paper (a few ideas are still in works) and in parallel a team from Facebook released [FairScale](https://github.com/facebookresearch/fairscale/) which also implemented some of the core ideas from the ZeRO paper."], "eval_score_GPT4": "4", "eval_feedback_GPT4": "The response provided is mostly correct and factual. It accurately identifies the purpose of the notebook as demonstrating the use of the DeepSpeed library in conjunction with Habana Gaudi hardware for training large models. However, the response is incomplete as it does not mention the specific model (GPT2-XL) or the tasks of pre-training and fine-tuning for causal language modeling, which are detailed in the reference answer. Therefore, the response does not fully match the reference answer in terms of completeness and specificity."}, {"question": "What command line module does PyTorch provide to run a script on multiple GPUs?\n", "true_answer": "torchrun", "source_doc": "huggingface/blog/blob/main/pytorch-ddp-accelerate-transformers.md", "generated_answer": "The command line module provided by PyTorch to run a script on multiple GPUs is `torchrun`. This can be seen in the context provided, where the author runs the training script using `torchrun` with the arguments `--nproc_per_node=2 --nnodes=1`. This allows the script to perform distributed training with PyTorch on two GPUs that live on a single machine.", "retrieved_docs": ["```bash\ntorchrun --nproc_per_node=2 --nnodes=1 example_script.py\n```\n\nThe above will run the training script on two GPUs that live on a single machine and this is the barebones for performing only distributed training with PyTorch.\n\nNow let's talk about Accelerate, a library aimed to make this process more seameless and also help with a few best practices\n\n## \ud83e\udd17 Accelerate\n\n[Accelerate](https://huggingface.co/docs/accelerate) is a library designed to allow you to perform what we just did above, without needing to modify your code greatly. On top of this, the data pipeline innate to Accelerate can also improve performance to your code as well.\n\nFirst, let's wrap all of the above code we just performed into a single function, to help us visualize the difference:\n\n```python\ndef train_ddp(rank, world_size):\n    setup(rank, world_size)\n    # Build DataLoaders\n    transform = transforms.Compose([\n        transforms.ToTensor(),\n        transforms.Normalize((0.1307), (0.3081))\n    ])", ".gather to gather together the predictions and labels from each process. A distributed training script has to be launched several times on different processes (for instance one per GPU you are using). You can use the PyTorch tools if you are familiar with them, but Accelerate also provides an easy API to configure your setup and launch your training script. In a terminal, run accelerate config and answer the small questionnaire to generate a configuration file with all the relevant information, then you can just run accelerate launch, followed by the path to your training script. In a notebook, you can use the notebook_launcher function to launch your training function.", "### Script to compare pre-training with PyTorch on 8 GPU V100's\n\nFor comparison you can run the same pre-training with PyTorch on GPU. Note that we have to make use of `gradient_accumulation` \nbecause the maximum batch size that fits on a single V100 GPU is 32 instead of 128.\nHaving created the tokenzier and configuration in `norwegian-roberta-base`, we create the following symbolic links:\n\n```bash\nln -s ~/transformers/examples/pytorch/language-modeling/run_mlm.py ./\n```\n\n, set some environment variables:\n\n```bash\nexport NUM_GPUS=8\nexport TOKENIZERS_PARALLELISM=0\nexport MODEL_DIR=\"./norwegian-roberta-base\"\nmkdir -p ${MODEL_DIR}\n```\n\n, and can start training as follows:", "In the following sections of this guide we dig deeper into how these different parallelism methods work.\n\n## Data Parallelism\n\nEven with only 2 GPUs, you can readily leverage the accelerated training capabilities offered by PyTorch's built-in features, \nsuch as `DataParallel` (DP) and `DistributedDataParallel` (DDP). Note that \n[PyTorch documentation](https://pytorch.org/docs/master/generated/torch.nn.DataParallel.html) recommends to prefer \n`DistributedDataParallel` (DDP) over `DataParallel` (DP) for multi-GPU training as it works for all models.\nLet's take a look at how these two methods work and what makes them different.\n\n### DataParallel vs DistributedDataParallel\n\nTo understand the key differences in inter-GPU communication overhead between the two methods, let's review the processes per batch:\n\n[DDP](https://pytorch.org/docs/master/notes/ddp.html):", "image = sd(prompt).images[0]\n    image.save(f\"./{'_'.join(prompt)}.png\")\n```\n\nTo run the distributed inference, call [`mp.spawn`](https://pytorch.org/docs/stable/multiprocessing.html#torch.multiprocessing.spawn) to run the `run_inference` function on the number of GPUs defined in `world_size`:\n\n```py\ndef main():\n    world_size = 2\n    mp.spawn(run_inference, args=(world_size,), nprocs=world_size, join=True)\n\n\nif __name__ == \"__main__\":\n    main()\n```\n\nOnce you've completed the inference script, use the `--nproc_per_node` argument to specify the number of GPUs to use and call `torchrun` to run the script:\n\n```bash\ntorchrun run_distributed.py --nproc_per_node=2\n```", "-         loss.backward()\n+         accelerator.backward(loss)\n\n          optimizer.step()\n```\n\nBy just adding five lines of code to any standard PyTorch training script, you can now run said script on any kind of distributed setting, as well as with or without mixed precision. \ud83e\udd17 Accelerate even handles the device placement for you, so you can simplify the training loop above even further:\n\n```diff\n  import torch\n  import torch.nn.functional as F\n  from datasets import load_dataset\n+ from accelerate import Accelerator\n\n+ accelerator = Accelerator()\n- device = 'cpu'\n\n- model = torch.nn.Transformer().to(device)\n+ model = torch.nn.Transformer()\n  optim = torch.optim.Adam(model.parameters())\n\n  dataset = load_dataset('my_dataset')\n  data = torch.utils.data.DataLoader(dataset, shuffle=True)\n\n+ model, optim, data = accelerator.prepare(model, optim, data)\n\n  model.train()\n  for epoch in range(10):\n      for source, targets in data:\n-         source = source.to(device)\n-         targets = targets.to(device)", "You can use a launcher of your choice here. You can continue using the pytorch launcher:\n\n```bash\ntorch.distributed.run --nproc_per_node=2 your_program.py <normal cl args> --deepspeed ds_config.json\n```\nor use the launcher provided by `deepspeed`:\n\n```bash\ndeepspeed --num_gpus=2 your_program.py <normal cl args> --deepspeed ds_config.json\n```\n\nAs you can see the arguments aren't the same, but for most needs either of them works. The\nfull details on how to configure various nodes and GPUs can be found [here](https://www.deepspeed.ai/getting-started/#resource-configuration-multi-node).\n\nWhen you use the `deepspeed` launcher and you want to use all available gpus you can just omit the `--num_gpus` flag.\n\nHere is an example of running `run_translation.py` under DeepSpeed deploying all available GPUs:"], "eval_score_GPT4": "5", "eval_feedback_GPT4": "Feedback: The response correctly identifies `torchrun` as the command line module provided by PyTorch to run a script on multiple GPUs. The additional details about how to use `torchrun` with specific arguments for distributed training on a single machine with two GPUs are accurate and provide factual context that aligns with the reference answer."}, {"question": "What is the most popular vision transformer model on the Hugging Face Model Hub for image classification?\n", "true_answer": "google/vit-base-patch16-224", "source_doc": "gradio-app/gradio/blob/main/guides/06_integrating-other-frameworks/image-classification-with-vision-transformers.md", "generated_answer": "The most popular vision transformer model for image classification on the Hugging Face Model Hub, as mentioned in the context, is `google/vit-base-patch16-224`. It has been trained on ImageNet images at a resolution of 224x224 pixels and is recommended for use in the demo provided in Step 1 of the context.", "retrieved_docs": ["## Step 1 \u2014 Choosing a Vision Image Classification Model\n\nFirst, we will need an image classification model. For this tutorial, we will use a model from the [Hugging Face Model Hub](https://huggingface.co/models?pipeline_tag=image-classification). The Hub contains thousands of models covering dozens of different machine learning tasks.\n\nExpand the Tasks category on the left sidebar and select \"Image Classification\" as our task of interest. You will then see all of the models on the Hub that are designed to classify images.\n\nAt the time of writing, the most popular one is `google/vit-base-patch16-224`, which has been trained on ImageNet images at a resolution of 224x224 pixels. We will use this model for our demo.\n\n## Step 2 \u2014 Loading the Vision Transformer Model with Gradio\n\nWhen using a model from the Hugging Face Hub, we do not need to define the input or output components for the demo. Similarly, we do not need to be concerned with the details of preprocessing or postprocessing.\nAll of these are automatically inferred from the model tags.\n\nBesides the import statement, it only takes a single line of Python to load and launch the demo.", "In the last five years, Transformer models [[1](https://arxiv.org/abs/1706.03762)] have become the _de facto_ standard for many machine learning (ML) tasks, such as natural language processing (NLP), computer vision (CV),  speech, and more. Today, many data scientists and ML engineers rely on popular transformer architectures like BERT [[2](https://arxiv.org/abs/1810.04805)], RoBERTa [[3](https://arxiv.org/abs/1907.11692)], the Vision Transformer [[4](https://arxiv.org/abs/2010.11929)], or any of the 130,000+ pre-trained models available on the [Hugging Face](https://huggingface.co) hub to solve complex business problems with state-of-the-art accuracy.", "# Optimum + ONNX Runtime: Easier, Faster training for your Hugging Face models\n\n\n## Introduction\n\nTransformer based models in language, vision and speech are getting larger to support complex multi-modal use cases for the end customer. Increasing model sizes directly impact the resources needed to train these models and scale them as the size increases. Hugging Face and Microsoft\u2019s ONNX Runtime teams are working together to build advancements in finetuning large Language, Speech and Vision models. Hugging Face\u2019s [Optimum library](https://huggingface.co/docs/optimum/index), through its integration with ONNX Runtime for training, provides an open solution to __improve training times by 35% or more__ for many popular Hugging Face models. We present details of both Hugging Face Optimum and the ONNX Runtime Training ecosystem, with performance numbers highlighting the benefits of using the Optimum library.\n\n## Performance results", "## \u6b65\u9aa4 1 - \u9009\u62e9 Vision \u56fe\u50cf\u5206\u7c7b\u6a21\u578b\n\n\u9996\u5148\uff0c\u6211\u4eec\u9700\u8981\u4e00\u4e2a\u56fe\u50cf\u5206\u7c7b\u6a21\u578b\u3002\u5728\u672c\u6559\u7a0b\u4e2d\uff0c\u6211\u4eec\u5c06\u4f7f\u7528[Hugging Face Model Hub](https://huggingface.co/models?pipeline_tag=image-classification)\u4e0a\u7684\u4e00\u4e2a\u6a21\u578b\u3002\u8be5 Hub \u5305\u542b\u6570\u5343\u4e2a\u6a21\u578b\uff0c\u6db5\u76d6\u4e86\u591a\u79cd\u4e0d\u540c\u7684\u673a\u5668\u5b66\u4e60\u4efb\u52a1\u3002\n\n\u5728\u5de6\u4fa7\u8fb9\u680f\u4e2d\u5c55\u5f00 Tasks \u7c7b\u522b\uff0c\u5e76\u9009\u62e9\u6211\u4eec\u611f\u5174\u8da3\u7684\u201cImage Classification\u201d\u4f5c\u4e3a\u6211\u4eec\u7684\u4efb\u52a1\u3002\u7136\u540e\uff0c\u60a8\u5c06\u770b\u5230 Hub \u4e0a\u4e3a\u56fe\u50cf\u5206\u7c7b\u8bbe\u8ba1\u7684\u6240\u6709\u6a21\u578b\u3002\n\n\u5728\u64b0\u5199\u65f6\uff0c\u6700\u53d7\u6b22\u8fce\u7684\u6a21\u578b\u662f `google/vit-base-patch16-224`\uff0c\u8be5\u6a21\u578b\u5728\u5206\u8fa8\u7387\u4e3a 224x224 \u50cf\u7d20\u7684 ImageNet \u56fe\u50cf\u4e0a\u8fdb\u884c\u4e86\u8bad\u7ec3\u3002\u6211\u4eec\u5c06\u5728\u6f14\u793a\u4e2d\u4f7f\u7528\u6b64\u6a21\u578b\u3002\n\n## \u6b65\u9aa4 2 - \u4f7f\u7528 Gradio \u52a0\u8f7d Vision Transformer \u6a21\u578b", "In the past few months, the Hugging Face team and external contributors\nadded a variety of vision models in TensorFlow to Transformers. This\nlist is growing comprehensively and already includes state-of-the-art\npre-trained models like [Vision Transformer](https://huggingface.co/docs/transformers/main/en/model_doc/vit),\n[Masked Autoencoders](https://huggingface.co/docs/transformers/model_doc/vit_mae),\n[RegNet](https://huggingface.co/docs/transformers/main/en/model_doc/regnet),\n[ConvNeXt](https://huggingface.co/docs/transformers/model_doc/convnext),\nand many others!\n\nWhen it comes to deploying TensorFlow models, you have got a variety of\noptions. Depending on your use case, you may want to expose your model\nas an endpoint or package it in an application itself. TensorFlow\nprovides tools that cater to each of these different scenarios.", "## Optimum Library\n\nHugging Face is a fast-growing open community and platform aiming to democratize good machine learning. We extended modalities from NLP to audio and vision, and now covers use cases across Machine Learning to meet our community's needs following the success of the [Transformers library](https://huggingface.co/docs/transformers/index). Now on [Hugging Face Hub](https://huggingface.co/models), there are more than 120K free and accessible model checkpoints for various machine learning tasks, 18K datasets, and 20K ML demo apps. However, scaling transformer models into production is still a challenge for the industry. Despite high accuracy, training and inference of transformer-based models can be time-consuming and expensive.", "<Tip>\n\u26a0\ufe0f The Hugging Face Hub is not limited to Transformer models. Anyone can share any kind of models or datasets they want! <a href=\"https://huggingface.co/join\">Create a huggingface.co</a> account to benefit from all available features!\n</Tip>\n\nBefore diving into how Transformer models work under the hood, let's look at a few examples of how they can be used to solve some interesting NLP problems.\n\n## Working with pipelines[[working-with-pipelines]]\n\n<Youtube id=\"tiZFewofSLM\" />\n\nThe most basic object in the \ud83e\udd17 Transformers library is the `pipeline()` function. It connects a model with its necessary preprocessing and postprocessing steps, allowing us to directly input any text and get an intelligible answer:\n\n```python\nfrom transformers import pipeline\n\nclassifier = pipeline(\"sentiment-analysis\")\nclassifier(\"I've been waiting for a HuggingFace course my whole life.\")\n```\n\n```python out\n[{'label': 'POSITIVE', 'score': 0.9598047137260437}]\n```"], "eval_score_GPT4": "5", "eval_feedback_GPT4": "Feedback: The response correctly identifies `google/vit-base-patch16-224` as the most popular vision transformer model for image classification on the Hugging Face Model Hub, which matches the reference answer. The additional details provided about the model being trained on ImageNet images and the recommended use in a demo do not detract from the accuracy of the response."}, {"question": "What is the command to upload an ESPnet model to a Hugging Face repository?\n", "true_answer": "./run.sh --stage 15 --skip_upload_hf false --hf_repo username/model_repo", "source_doc": "huggingface/hub-docs/blob/main/docs/hub/espnet.md", "generated_answer": "The command to upload an ESPnet model to a Hugging Face repository using the `run.sh` script is:\n\n```bash\n./run.sh --stage 15 --skip_upload_hf false --hf_repo username/model_repo\n```\n\nAlternatively, you can use the `push_to_hub` method provided by the `huggingface_hub` library:\n\n```python\nfrom huggingface", "retrieved_docs": ["## Sharing your models\n\n`ESPnet` outputs a `zip` file that can be uploaded to Hugging Face easily. For a full guide on sharing  models, we recommend checking out the [official guide](https://github.com/espnet/espnet_model_zoo#register-your-model)).\n\nThe `run.sh` script allows to upload a given model to a Hugging Face repository.\n\n```bash\n./run.sh --stage 15 --skip_upload_hf false --hf_repo username/model_repo\n```\n\n## Additional resources\n\n* ESPnet [docs](https://espnet.github.io/espnet/index.html).\n* ESPnet model zoo [repository](https://github.com/espnet/espnet_model_zoo).\n* Integration [docs](https://github.com/asteroid-team/asteroid/blob/master/docs/source/readmes/pretrained_models.md).", "Using ESPnet at Hugging Face\n\n`espnet` is an end-to-end toolkit for speech processing, including automatic speech recognition, text to speech, speech enhancement, dirarization and other tasks.\n\n## Exploring ESPnet in the Hub\n\nYou can find hundreds of `espnet` models by filtering at the left of the [models page](https://huggingface.co/models?library=espnet&sort=downloads). \n\nAll models on the Hub come up with useful features:\n1. An automatically generated model card with a description, a training configuration, licenses and more.\n2. Metadata tags that help for discoverability and contain information such as license, language and datasets.\n3. An interactive widget you can use to play out with the model directly in the browser.\n4. An Inference API that allows to make inference requests.", "```py\n>>> tokenizer.push_to_hub(\"my-awesome-model\")\n```\n\nOr perhaps you'd like to add the TensorFlow version of your fine-tuned PyTorch model:\n\n```py\n>>> tf_model.push_to_hub(\"my-awesome-model\")\n```\n\nNow when you navigate to your Hugging Face profile, you should see your newly created model repository. Clicking on the **Files** tab will display all the files you've uploaded to the repository.\n\nFor more details on how to create and upload files to a repository, refer to the Hub documentation [here](https://huggingface.co/docs/hub/how-to-upstream).\n\n## Upload with the web interface\n\nUsers who prefer a no-code approach are able to upload a model through the Hub's web interface. Visit [huggingface.co/new](https://huggingface.co/new) to create a new repository:\n\n![new_model_repo](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/new_model_repo.png)", "## Uploading the model files[[uploading-the-model-files]]\n\nThe system to manage files on the Hugging Face Hub is based on git for regular files, and git-lfs (which stands for [Git Large File Storage](https://git-lfs.github.com/)) for larger files. \n\nIn the next section, we go over three different ways of uploading files to the Hub: through `huggingface_hub` and through git commands.\n\n### The `upload_file` approach[[the-uploadfile-approach]]\n\nUsing `upload_file` does not require git and git-lfs to be installed on your system. It pushes files directly to the \ud83e\udd17 Hub using HTTP POST requests. A limitation of this approach is that it doesn't handle files that are larger than 5GB in size.\nIf your files are larger than 5GB, please follow the two other methods detailed below.\n\nThe API may be used as follows:\n\n```py\nfrom huggingface_hub import upload_file", "1. **Log in**\n\nTo begin with, you should check that you are correctly logged in and that you have `git-lfs` installed so that your fine-tuned model can automatically be uploaded.\n\nRun:\n\n```bash\nhuggingface-cli login\n```\n\nto login. It is recommended to login with your access token that can be found under your hugging face profile (icon in the top right corner on [hf.co](http://hf.co/), then Settings -> Access Tokens -> User Access Tokens -> New Token (if haven't generated one already)\n\nYou can then copy-paste this token to log in locally.\n\n2. **Create your model repository**\n\nFirst, let's make sure that `git-lfs` is correctly installed. To so, simply run:\n\n```bash\ngit-lfs -v\n```", "```python\n>>> from huggingface_hub import create_repo\n>>> create_repo(repo_id=\"test-model\")\n'https://huggingface.co/lysandre/test-model'\n```\n\nWhen you check your Hugging Face account, you should now see a `test-model` repository under your namespace.\n\n### `upload_file`\n\nThe `upload_file` method uploads files to the Hub. This method requires the following:\n\n- A path to the file to upload.\n- The final path in the repository.\n- The repository you wish to push the files to.\n\nFor example:\n\n```python\n>>> from huggingface_hub import upload_file\n>>> upload_file(\n...    path_or_fileobj=\"/home/lysandre/dummy-test/README.md\", \n...    path_in_repo=\"README.md\", \n...    repo_id=\"lysandre/test-model\"\n... )\n'https://huggingface.co/lysandre/test-model/blob/main/README.md'\n```", ". The second argument is the tokenizer, and the third argument here is the keyword argument hub_model_id. So that's the name it's going to be saved under on the HuggingFace Hub. You can also upload to an organization account just by adding the organization name before the repository name with a slash like this. So you probably don't have permissions to upload to the Hugging Face organization, if you do please file a bug and let us know extremely urgently. But if you do have access to your own organization then you can use that same approach to upload models to their account instead of to your own personal set of models. So, once you've made your callback you simply add it to the callbacks list when you're called model.fit() and everything is uploaded for you from there, and there's nothing else to worry about. The second way to upload a model, though, is to call model.push_to_hub(). So this is more of a once-off method - it's not called regularly during training. You can just call this manually whenever you want to upload a model to the hub"], "eval_score_GPT4": "4", "eval_feedback_GPT4": "The response provides the correct command to upload an ESPnet model to a Hugging Face repository, which matches the reference answer. However, the response also includes an alternative method using the `push_to_hub` method, which is incomplete and not asked for in the instruction. The additional information is not necessary and is not fully explained, but the primary command requested is correct and accurate."}, {"question": "What file should be added to a model repository to install custom Python dependencies for Inference Endpoints?\n", "true_answer": "requirements.txt", "source_doc": "huggingface/hf-endpoints-documentation/blob/main/docs/source/guides/custom_dependencies.mdx", "generated_answer": "The file that should be added to a model repository to install custom Python dependencies for Inference Endpoints is a \"requirements.txt\" file, as described in the context provided. This file should be created in the model repository on the Hugging Face Hub and should list the Python dependencies that need to be installed for the custom inference handler or for running a specific model that requires special dependencies. When the Endpoint and Image artifacts are created, Inference Endpoints checks for the presence of", "retrieved_docs": ["Add custom Dependencies\n\nInference Endpoints\u2019 base image includes all required libraries to run inference on \ud83e\udd17 Transformers models, but it also supports custom dependencies. This is useful if you want to:\n\n* [customize your inference pipeline](/docs/inference-endpoints/guides/custom_handler) and need additional Python dependencies\n* run a model which requires special dependencies like the newest or a fixed version of a library (for example, `tapas` (`torch-scatter`)).\n\nTo add custom dependencies, add a `requirements.txt` [file](https://huggingface.co/philschmid/distilbert-onnx-banking77/blob/main/requirements.txt) with the Python dependencies you want to install in your model repository on the Hugging Face Hub. When your Endpoint and Image artifacts are created, Inference Endpoints checks if the model repository contains a `requirements.txt ` file and installs the dependencies listed within.\n\n```bash\noptimum[onnxruntime]==1.2.3\nmkl-include\nmkl\n```\n\nCheck out the `requirements.txt` files in the following model repositories for examples:", "Inference Endpoints Version\n\nHugging Face Inference Endpoints comes with a default serving container which is used for all [supported Transformers and Sentence-Transformers tasks](/docs/inference-endpoints/supported_tasks) and for [custom inference handler](/docs/inference-endpoints/guides/custom_handler) and implement batching.\nBelow you will find information about the installed packages and versions used.\n\nYou can always upgrade installed packages and a custom packages by adding a `requirements.txt` file to your model repository. Read more in [Add custom Dependencies](/docs/inference-endpoints/guides/custom_dependencies).\n\n## Installed packages & version\n\nThe installed packages are split into `general`, `CPU` & `GPU` packages. The `general` packages are installed in all containers, the `CPU` and `GPU` packages are only installed in the corresponding containers.\nThe Hugging Face Inference Runtime has separate versions for `PyTorch` and `TensorFlow` for `CPU` and `GPU`, which are used based on the selected `framework` when creating an Inference Endpoint. The `TensorFlow` and `PyTorch` flavors are grouped together in the list below.\n\n### General", "```bash\nmodel.tar.gz/\n|- pytorch_model.bin\n|- ....\n|- code/\n  |- inference.py\n  |- requirements.txt \n```\n\nThe `inference.py` file contains your custom inference module, and the `requirements.txt` file contains additional dependencies that should be added. The custom module can override the following methods:", "`transformers` pipelines offer powerful abstractions to run inference with `transformers`-based models. Inference Endpoints leverage the pipeline API to easily deploy models with only a few clicks. However, Inference Endpoints can also be used to deploy models that don't have a pipeline, or even non-transformer models! This is achieved using a custom inference function that we call a [custom handler](https://huggingface.co/docs/inference-endpoints/guides/custom_handler).\n\nLet's demonstrate this process using MusicGen as an example. To implement a custom handler function for MusicGen and deploy it, we will need to:\n\n1. Duplicate the MusicGen repository we want to serve,\n2. Write a custom handler in `handler.py` and any dependencies in `requirements.txt` and add them to the duplicated repository,\n3. Create Inference Endpoint for that repository.\n\nOr simply use the final result and deploy our [custom MusicGen model repo](https://huggingface.co/reach-vb/musicgen-large-fp16-endpoint), where we just followed the steps above :)\n\n### Let's go!", "# upload model.tar.gz\ns3_model_uri = S3Uploader.upload(local_path=\"tmp/model.tar.gz\",desired_s3_uri=s3_model_path)\nprint(f\"model artifcats uploaded to {s3_model_uri}\")\n```\n\n## 4. Deploy a Real-time Inference Endpoint on Amazon SageMaker\n\nAfter we have uploaded our\u00a0`model.tar.gz`\u00a0to Amazon S3 can we create a custom\u00a0`HuggingfaceModel`. This class will be used to create and deploy our real-time inference endpoint on Amazon SageMaker.\n\n```python\nfrom sagemaker.huggingface.model import HuggingFaceModel\n\n# create Hugging Face Model Class\nhuggingface_model = HuggingFaceModel(\n   model_data=s3_model_uri,       # path to your model and script\n   role=role,                    # iam role with permissions to create an Endpoint\n   transformers_version=\"4.12\",  # transformers version used\n   pytorch_version=\"1.9\",        # pytorch version used\n   py_version='py37',            # python version used\n)", "### 6. Deploy your Custom Handler as an Inference Endpoint\n\nThe last step is to deploy your Custom Handler as an Inference Endpoint. You can deploy your Custom Handler like you would a regular Inference Endpoint. Add your repository, select your cloud and region, your instance and security setting, and deploy.\n\nWhen creating your Endpoint, the Inference Endpoint Service will check for an available and valid `handler.py`, and will use it for serving requests no matter which \u201cTask\u201d you select.\n\n_Note: In your [Inference Endpoints dashboard](https://ui.endpoints.huggingface.co/), the Task for this Endpoint should now be set to Custom_", "Create custom Inference Handler\n\nHugging Face Endpoints supports all of the Transformers and Sentence-Transformers tasks and can support custom tasks, including custom pre- & post-processing. The customization can be done through a [handler.py](https://huggingface.co/philschmid/distilbert-onnx-banking77/blob/main/handler.py) file in your model repository on the Hugging Face Hub.\n\nThe [handler.py](https://huggingface.co/philschmid/distilbert-onnx-banking77/blob/main/handler.py) needs to implement the [EndpointHandler](https://huggingface.co/philschmid/distilbert-onnx-banking77/blob/main/handler.py) class with a `__init__` and a `__call__` method.\n\nIf you want to use custom dependencies, e.g. [optimum](https://raw.githubusercontent.com/huggingface/optimum), the dependencies must be listed in a `requirements.txt` as described above in \u201cadd custom dependencies.\u201d\n\n## Custom Handler Examples"], "eval_score_GPT4": "5", "eval_feedback_GPT4": "The response provided is completely correct, accurate, and factual. It correctly identifies \"requirements.txt\" as the file that should be added to a model repository for installing custom Python dependencies for Inference Endpoints. The response also goes beyond the reference answer by explaining the purpose of the \"requirements.txt\" file and its role in the Hugging Face Hub, which aligns with the expected use of such a file in a model repository. There are no inaccuracies or incorrect information presented in the response."}, {"question": "How many images are needed to teach new concepts to Stable Diffusion using Textual Inversion?\n", "true_answer": "3-5 images", "source_doc": "huggingface/diffusers/blob/main/examples/research_projects/intel_opts/textual_inversion_dfq/README.md", "generated_answer": "According to the context provided, Textual Inversion allows for the personalization of Stable Diffusion models on one's own images with just 3-5 samples, enabling the teaching of new concepts to the model. Therefore, the answer to the question is 3-5 images. (Source: Extract 1)", "retrieved_docs": ["Distillation for quantization on Textual Inversion models to personalize text2image\n\n[Textual inversion](https://arxiv.org/abs/2208.01618) is a method to personalize text2image models like stable diffusion on your own images._By using just 3-5 images new concepts can be taught to Stable Diffusion and the model personalized on your own images_\nThe `textual_inversion.py` script shows how to implement the training procedure and adapt it for stable diffusion.\nWe have enabled distillation for quantization in `textual_inversion.py` to do quantization aware training as well as distillation on the model generated by Textual Inversion method.\n\n## Installing the dependencies\n\nBefore running the scripts, make sure to install the library's training dependencies:\n\n```bash\npip install -r requirements.txt\n```\n\n## Prepare Datasets\n\nOne picture which is from the huggingface datasets [sd-concepts-library/dicoo2](https://huggingface.co/sd-concepts-library/dicoo2) is needed, and save it to the `./dicoo` directory. The picture is shown below:", "![image info](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/diffusers-2nd-month/diffuse_the_rest.jpeg)\n\n\n\n## Textual Inversion\n\nTextual Inversion lets you personalize a Stable Diffusion model on your own images with just 3-5 samples. With this tool, you can train a model on a concept, and then share the concept with the rest of the community! \n\n![image info](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/diffusers-2nd-month/textual_inversion.jpeg)\n\nIn just a couple of days, the community shared over 200 concepts! Check them out!", "This guide will show you how to run inference with textual inversion using a pre-learned concept from the Stable Diffusion Conceptualizer. If you're interested in teaching a model new concepts with textual inversion, take a look at the [Textual Inversion](../training/text_inversion) training guide.\n\nImport the necessary libraries:\n\n```py\nimport torch\nfrom diffusers import StableDiffusionPipeline\nfrom diffusers.utils import make_image_grid\n```\n\n## Stable Diffusion 1 and 2\n\nPick a Stable Diffusion checkpoint and a pre-learned concept from the [Stable Diffusion Conceptualizer](https://huggingface.co/spaces/sd-concepts-library/stable-diffusion-conceptualizer):\n\n```py\npretrained_model_name_or_path = \"runwayml/stable-diffusion-v1-5\"\nrepo_id_embeds = \"sd-concepts-library/cat-toy\"\n```\n\nNow you can load a pipeline, and pass the pre-learned concept to it:", "## Textual Inversion fine-tuning example\n\n[Textual inversion](https://arxiv.org/abs/2208.01618) is a method to personalize text2image models like stable diffusion on your own images using just 3-5 examples.\nThe `textual_inversion.py` script shows how to implement the training procedure and adapt it for stable diffusion.\n\n## Running on Colab\n\nColab for training\n[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/huggingface/notebooks/blob/main/diffusers/sd_textual_inversion_training.ipynb)\n\nColab for inference\n[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/huggingface/notebooks/blob/main/diffusers/stable_conceptualizer_inference.ipynb)\n\n## Running locally with PyTorch\n### Installing the dependencies", "# Textual Inversion fine-tuning example\n\n[Textual inversion](https://arxiv.org/abs/2208.01618) is a method to personalize text2image models like stable diffusion on your own images using just 3-5 examples.\nThe `textual_inversion.py` script shows how to implement the training procedure and adapt it for stable diffusion.\n\n## Running on Colab\n\nColab for training\n[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/huggingface/notebooks/blob/main/diffusers/sd_textual_inversion_training.ipynb)\n\nColab for inference\n[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/huggingface/notebooks/blob/main/diffusers/stable_conceptualizer_inference.ipynb)\n\n## Running locally with PyTorch\n### Installing the dependencies", "# Textual Inversion fine-tuning example\n\n[Textual inversion](https://arxiv.org/abs/2208.01618) is a method to personalize text2image models like stable diffusion on your own images using just 3-5 examples.\nThe `textual_inversion.py` script shows how to implement the training procedure and adapt it for stable diffusion.\n\n## Running on Colab\n\nColab for training\n[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/huggingface/notebooks/blob/main/diffusers/sd_textual_inversion_training.ipynb)\n\nColab for inference\n[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/huggingface/notebooks/blob/main/diffusers/stable_conceptualizer_inference.ipynb)\n\n## Running locally with PyTorch\n### Installing the dependencies", "# Textual Inversion fine-tuning example\n\n[Textual inversion](https://arxiv.org/abs/2208.01618) is a method to personalize text2image models like stable diffusion on your own images using just 3-5 examples.\nThe `textual_inversion.py` script shows how to implement the training procedure and adapt it for stable diffusion.\n\n## Training with Intel Extension for PyTorch\n\nIntel Extension for PyTorch provides the optimizations for faster training and inference on CPUs. You can leverage the training example \"textual_inversion.py\". Follow the [instructions](https://github.com/huggingface/diffusers/tree/main/examples/textual_inversion) to get the model and [dataset](https://huggingface.co/sd-concepts-library/dicoo2) before running the script.\n\nThe example supports both single node and multi-node distributed training:\n\n### Single node training\n\n```bash\nexport MODEL_NAME=\"CompVis/stable-diffusion-v1-4\"\nexport DATA_DIR=\"path-to-dir-containing-dicoo-images\""], "eval_score_GPT4": "5", "eval_feedback_GPT4": "Feedback: The response provided is completely correct, accurate, and factual. It aligns perfectly with the reference answer, stating that 3-5 images are needed to teach new concepts to Stable Diffusion using Textual Inversion. The response also adds a source for the information, which supports the accuracy of the statement."}, {"question": "What is the maximum size of a model checkpoint before it is automatically sharded in Transformers version 4.18.0?\n", "true_answer": "10GB", "source_doc": "huggingface/transformers/blob/main/docs/source/en/big_models.md", "generated_answer": "The maximum size of a model checkpoint before it is automatically sharded in Transformers version 4.18.0 is 10GB. This feature was introduced in version 4.18.0 to handle large model checkpoints that exceed 10GB in size. The checkpoints are automatically sharded into smaller pieces, each of which is of size less than 10GB, and an index is provided to map parameter names to the files they are stored in", "retrieved_docs": ["## Sharded checkpoints\n\nSince version 4.18.0, model checkpoints that end up taking more than 10GB of space are automatically sharded in smaller pieces. In terms of having one single checkpoint when you do `model.save_pretrained(save_dir)`, you will end up with several partial checkpoints (each of which being of size < 10GB) and an index that maps parameter names to the files they are stored in.\n\nYou can control the maximum size before sharding with the `max_shard_size` parameter, so for the sake of an example, we'll use a normal-size models with a small shard size: let's take a traditional BERT model.\n\n```py\nfrom transformers import AutoModel\n\nmodel = AutoModel.from_pretrained(\"bert-base-cased\")\n```\n\nIf you save it using [`~PreTrainedModel.save_pretrained`], you will get a new folder with two files: the config of the model and its weights:\n\n```py\n>>> import os\n>>> import tempfile", "### Large model loading\n\nIn Transformers 4.20.0, the [`~PreTrainedModel.from_pretrained`] method has been reworked to accommodate large models using [Accelerate](https://huggingface.co/docs/accelerate/big_modeling). This requires Accelerate >= 0.9.0 and PyTorch >= 1.9.0. Instead of creating the full model, then loading the pretrained weights inside it (which takes twice the size of the model in RAM, one for the randomly initialized model, one for the weights), there is an option to create the model as an empty shell, then only materialize its parameters when the pretrained weights are loaded.\n\nThis option can be activated with `low_cpu_mem_usage=True`. The model is first created on the Meta device (with empty weights) and the state dict is then loaded inside it (shard by shard in the case of a sharded checkpoint). This way the maximum RAM used is the full size of the model only.\n\n```py\nfrom transformers import AutoModelForSeq2SeqLM", "I discovered that there were 4 checkpoints in there. I had no idea what to do about it, so I started with a simpler job of using just the first checkpoint. Later I discovered that `fairseq` used all 4 checkpoints in an ensemble to get the best predictions, and that `transformers` currently doesn't support that feature. When the porting was completed and I was able to measure the performance scores, I found out that the `model4.pt` checkpoint provided the best score. But during the porting performance didn't matter much. Since I was using only one checkpoint it was crucial that when I was comparing outputs, I had `fairseq` also use just one and the same checkpoint.", "than 10 seconds. In case only very large checkpoints are available,\n    it might make more sense to create a dummy model in the new\n    environment with randomly initialized weights and save those weights\n    for comparison with the \ud83e\udd17 Transformers version of your model\n-   Make sure you are using the easiest way of calling a forward pass in\n    the original repository. Ideally, you want to find the function in\n    the original repository that **only** calls a single forward pass,\n    *i.e.* that is often called `predict`, `evaluate`, `forward` or\n    `__call__`. You don't want to debug a function that calls `forward`\n    multiple times, *e.g.*, to generate text, like\n    `autoregressive_sample`, `generate`.\n-   Try to separate the tokenization from the model's\n    forward pass. If the original repository shows\n    examples where you have to input a string, then try to find out\n    where in the forward call the string input is changed to input ids\n    and start from this point. This might mean that you have to possibly", "than 10 seconds. In case only very large checkpoints are available,\n    it might make more sense to create a dummy model in the new\n    environment with randomly initialized weights and save those weights\n    for comparison with the \ud83e\udd17 Transformers version of your model\n-   Make sure you are using the easiest way of calling a forward pass in\n    the original repository. Ideally, you want to find the function in\n    the original repository that **only** calls a single forward pass,\n    *i.e.* that is often called `predict`, `evaluate`, `forward` or\n    `__call__`. You don't want to debug a function that calls `forward`\n    multiple times, *e.g.*, to generate text, like\n    `autoregressive_sample`, `generate`.\n-   Try to separate the tokenization from the model's\n    forward pass. If the original repository shows\n    examples where you have to input a string, then try to find out\n    where in the forward call the string input is changed to input ids\n    and start from this point. This might mean that you have to possibly", "--checkpointing_steps=200 --checkpoints_total_limit=10 \\\n    --train_batch_size=12 \\\n    --gradient_checkpointing --enable_xformers_memory_efficient_attention \\\n    --gradient_accumulation_steps=1 \\\n    --use_8bit_adam \\\n    --resume_from_checkpoint=latest \\\n    --report_to=wandb \\\n    --seed=453645634 \\\n    --push_to_hub \\\n```", "--checkpointing_steps=200 --checkpoints_total_limit=10 \\\n    --train_batch_size=12 \\\n    --gradient_checkpointing --enable_xformers_memory_efficient_attention \\\n    --gradient_accumulation_steps=1 \\\n    --use_8bit_adam \\\n    --resume_from_checkpoint=latest \\\n    --report_to=wandb \\\n    --seed=453645634 \\\n    --push_to_hub\n```"], "eval_score_GPT4": "5", "eval_feedback_GPT4": "The response provided is completely correct, accurate, and factual. It matches the reference answer and provides additional context about the feature and its functionality, which is consistent with the information requested in the instruction."}, {"question": "What is the purpose of Weights and Biases (W&B) for data scientists and machine learning scientists?\n", "true_answer": "To track their machine learning experiments at every stage, from training to production.", "source_doc": "gradio-app/gradio/blob/main/guides/06_integrating-other-frameworks/Gradio-and-Wandb-Integration.md", "generated_answer": "Weights and Biases (W&B) is a tool for data scientists and machine learning scientists to track their machine learning experiments at every stage, from training to production. It allows for the aggregation of metrics over samples and the display of customizable and searchable dashboards. The purpose of W&B is to facilitate vigilance and responding to feedback from deployment contexts in the development of ML systems, as it helps to address bias by providing tools to analyze signs of bias at every", "retrieved_docs": ["Gradio and W&B Integration\n\nRelated spaces: https://huggingface.co/spaces/akhaliq/JoJoGAN\nTags: WANDB, SPACES\nContributed by Gradio team\n\n## Introduction\n\nIn this Guide, we'll walk you through:\n\n- Introduction of Gradio, and Hugging Face Spaces, and Wandb\n- How to setup a Gradio demo using the Wandb integration for JoJoGAN\n- How to contribute your own Gradio demos after tracking your experiments on wandb to the Wandb organization on Hugging Face\n\n\n## What is Wandb?\n\nWeights and Biases (W&B) allows data scientists and machine learning scientists to track their machine learning experiments at every stage, from training to production. Any metric can be aggregated over samples and shown in panels in a customizable and searchable dashboard, like below:\n\n<img alt=\"Screen Shot 2022-08-01 at 5 54 59 PM\" src=\"https://user-images.githubusercontent.com/81195143/182252755-4a0e1ca8-fd25-40ff-8c91-c9da38aaa9ec.png\">", "--\ntitle: \"Let's talk about biases in machine learning! Ethics and Society Newsletter #2\" \nthumbnail: /blog/assets/122_ethics_soc_2/thumbnail-solstice.png\nauthors:\n- user: yjernite\n---\n\n# Machine Learning in development: Let's talk about bias!\n\n\n_Bias in ML is ubiquitous, and Bias in ML is complex; so complex in fact that no single technical intervention is likely to meaningfully address the problems it engenders. ML models, as sociotechnical systems, amplify social trends that may exacerbate inequities and harmful biases in ways that depend on their deployment context and are constantly evolving._\n\n_This means that developing ML systems with care requires vigilance and responding to feedback from those deployment contexts, which in turn we can facilitate by sharing lessons across contexts and developing tools to analyze signs of bias at every level of ML development._", "_This blog post from the [Ethics and Society regulars @\ud83e\udd17](https://huggingface.co/blog/ethics-soc-1) shares some of the lessons we have learned along with tools we have developed to support ourselves and others in our community\u2019s efforts to better address bias in Machine Learning. The first part is a broader reflection on bias and its context. If you\u2019ve already read it and are coming back specifically for the tools, feel free to jump to the [datasets](#i-am-curatingpicking-a-dataset-for-my-ml-system-how-can-i-address-bias) or [models](#i-am-trainingselecting-a-model-for-my-ml-system-how-can-i-address-bias)\nsection!_\n\n<p align=\"center\">\n <br>\n <img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/ethics_soc_2/img1.jpg\" alt=\"Selection of tools developed by HF team members to address bias in ML\" />\n    <em>Selection of tools developed by \ud83e\udd17 team members to address bias in ML</em>\n</p>", "Also, as we have mentioned in a [previous newsletter](https://huggingface.co/blog/ethics-soc-2#addressing-bias-throughout-the-ml-development-cycle), there is no one single way to develop machine learning systems, and any of the steps in the development and deployment process can present opportunities to tackle bias, from who is included at the start, to defining the task, to curating the dataset, training the model, and more. This also applies to multimodal models and the ways in which they are ultimately deployed or productionized in society, since the consequences of bias in multimodal models will depend on their downstream use. For instance, if a model is used in a human-in-the-loop setting for graphic design (such as those created by [RunwayML](https://runwayml.com/ai-magic-tools/text-to-image/)), the user has numerous occasions to detect and correct bias, for instance by changing the prompt or the generation options. However, if a model is used as part of a [tool to help forensic artists create police sketches of potential suspects](https://www.vice", "### Weights & Biases\n\nTo use Weights & Biases, install the wandb package with:\n\n```bash\npip install wandb\n```\n\nThen log in the command line:\n\n```bash\nwandb login\n```\n\nIf you are in Jupyter or Colab, you should login with:\n\n```python\nimport wandb\nwandb.login()\n```\n\nTo enable logging to W&B, include `\"wandb\"` in the `report_to` of your `TrainingArguments` or script. Or just pass along `--report_to_all` if you have `wandb` installed.\n\nWhenever you use `Trainer` or `TFTrainer` classes, your losses, evaluation metrics, model topology and gradients (for `Trainer` only) will automatically be logged.\n\nAdvanced configuration is possible by setting environment variables:", "## _Machine Bias:_ from ML Systems to Personal and Social Risks\n\nML systems allow us to automate complex tasks at a scale never seen before as they are deployed in more sectors and use cases. When the technology works at its best, it can help smooth interactions between people and technical systems, remove the need for highly repetitive work, or unlock new ways of processing information to support research.\n\nThese same systems are also likely to reproduce discriminatory and abusive behaviors represented in their training data, especially when the data encodes human behaviors.\nThe technology then has the potential to make these issues significantly worse. Automation and deployment at scale can indeed:", "So, who\u2019s on the hook for machine biases in ML? These three cases illustrate one of the reasons why discussions about the responsibility of ML developers in addressing bias can get so complicated: depending on decisions made at other points in the ML system development process by other people, the biases in an ML dataset or model may land anywhere between being irrelevant to the application settings and directly leading to grievous harm. However, in all of these cases, **stronger biases in the model/dataset increase the risk of negative outcomes**. The European Union has started to develop frameworks that address this phenomenon in [recent regulatory efforts](https://ec.europa.eu/info/business-economy-euro/doing-business-eu/contract-rules/digital-contracts/liability-rules-artificial-intelligence_en): in short, a company that deploys an AI system based on a measurably biased model is liable for harm caused by the system.\n\nConceptualizing bias as a risk factor then allows us to better understand the **shared responsibility** for machine biases between developers at all stages. Bias can never be fully removed, not least because the definitions of social biases and the power dynamics that tie them to discrimination vary vastly across social contexts. However:"], "eval_score_GPT4": "4", "eval_feedback_GPT4": "The response provided aligns well with the reference answer, mentioning that W&B is used to track machine learning experiments at every stage, which is a key point in the reference answer. It also adds additional context about the aggregation of metrics and the use of dashboards, which are accurate and factual statements about the capabilities of W&B. However, the response includes extra details about facilitating vigilance and responding to feedback from deployment contexts, which, while related, are not explicitly mentioned in the reference answer. These additional details do not detract from the correctness of the response but rather provide a broader understanding of the tool's purpose. Therefore, the response is mostly correct, accurate, and factual."}, {"question": "What is the name of the open-source library created by Hugging Face to simplify Transformer acceleration?\n", "true_answer": "Optimum", "source_doc": "huggingface/blog/blob/main/intel.md", "generated_answer": "The name of the open-source library created by Hugging Face to simplify Transformer acceleration is Optimum. It is part of the Optimum-Intel library, which builds on top of the Intel Neural Compressor, and is designed to deliver unified interfaces across multiple deep learning frameworks for popular network compression technologies. Optimum integrates machine learning accelerators like ONNX Runtime and specialized hardware like Intel's Habana Gaudi, making it easier for", "retrieved_docs": ["## Get Peak Transformers Performance with Optimum Intel\n[Optimum](https://github.com/huggingface/optimum) is an open-source library created by Hugging Face to simplify Transformer acceleration across a growing range of training and inference devices. Thanks to built-in optimization techniques, you can start accelerating your workloads in minutes, using ready-made scripts, or applying minimal changes to your existing code. Beginners can use Optimum out of the box with excellent results. Experts can keep tweaking for maximum performance. \n\n[Optimum Intel](https://github.com/huggingface/optimum-intel) is part of Optimum and builds on top of the [Intel Neural Compressor](https://www.intel.com/content/www/us/en/developer/tools/oneapi/neural-compressor.html) (INC). INC is an [open-source library](https://github.com/intel/neural-compressor) that delivers unified interfaces across multiple deep learning frameworks for popular network compression technologies, such as quantization, pruning, and knowledge distillation. This tool supports automatic accuracy-driven tuning strategies to help users quickly build the best quantized model.", "--\ntitle: \"Intel and Hugging Face Partner to Democratize Machine Learning Hardware Acceleration\"\nthumbnail: /blog/assets/80_intel/01.png\nauthors:\n- user: juliensimon\n---\n\n\n\n# Intel and Hugging Face Partner to Democratize Machine Learning Hardware Acceleration\n\n\n\n\n![image](assets/80_intel/01.png)\n\nThe mission of Hugging Face is to democratize good machine learning and maximize its positive impact across industries and society. Not only do we strive to advance Transformer models, but we also work hard on simplifying their adoption.\n\nToday, we're excited to announce that Intel has officially joined our [Hardware Partner Program](https://huggingface.co/hardware).  Thanks to the [Optimum](https://github.com/huggingface/optimum-intel) open-source library, Intel and Hugging Face will collaborate to build state-of-the-art hardware acceleration to train, fine-tune and predict with Transformers.", "## Optimising Transformers for Production\n\nTransformers have completely transformed (pun intended) the field of AI. Models such as BERT are widely used by Graphcore customers in a huge array of applications, across NLP and beyond. These multi-talented models can perform feature extraction, text generation, sentiment analysis, translation and many more functions.\n\nAlready, Hugging Face plays host to hundreds of Transformers, from the French-language CamemBERT to ViT which applies lessons learned in NLP to computer vision. The Transformers library is downloaded an average of 2 million times every month and demand is growing.\n\nWith a user base of more than 50,000 developers \u2013 Hugging Face has seen the fastest ever adoption of an open-source project.\n\nNow, with its Hardware Partner Program, Hugging Face is connecting the ultimate Transformer toolset with today's most advanced AI hardware.\n\nUsing Optimum, a new open-source library and toolkit, developers will be able to access hardware-optimized models certified by Hugging Face.\n\nThese are being developed in a collaboration between Graphcore and Hugging Face, with the first IPU-optimized models appearing on Optimum later this year. Ultimately, these will cover a wide range of applications, from vision and speech to translation and text generation.", "To target these needs, Hugging Face built two open-sourced libraries: __Accelerate__ and __Optimum__. While [\ud83e\udd17 Accelerate](https://huggingface.co/docs/accelerate/index) focuses on out-of-the-box distributed training, [\ud83e\udd17 Optimum](https://huggingface.co/docs/optimum/index), as an extension of transformers, accelerates model training and inference by leveraging the maximum efficiency of users\u2019 targeted hardware. Optimum integrated machine learning accelerators like ONNX Runtime and specialized hardware like [Intel's Habana Gaudi](https://huggingface.co/blog/habana-gaudi-2-benchmark), so users can benefit from considerable speedup in both training and inference. Besides, Optimum seamlessly integrates other Hugging Face\u2019s tools while inheriting the same ease of use as Transformers. Developers can easily adapt their work to achieve lower latency with less computing power.\n\n## ONNX Runtime Training", "## Optimum Library\n\nHugging Face is a fast-growing open community and platform aiming to democratize good machine learning. We extended modalities from NLP to audio and vision, and now covers use cases across Machine Learning to meet our community's needs following the success of the [Transformers library](https://huggingface.co/docs/transformers/index). Now on [Hugging Face Hub](https://huggingface.co/models), there are more than 120K free and accessible model checkpoints for various machine learning tasks, 18K datasets, and 20K ML demo apps. However, scaling transformer models into production is still a challenge for the industry. Despite high accuracy, training and inference of transformer-based models can be time-consuming and expensive.", "Today, [Habana\u00ae Labs](https://habana.ai/), a pioneer in high-efficiency, purpose-built deep learning processors, and Hugging Face, the home of [Transformer](https://github.com/huggingface/transformers) models, are happy to announce that they\u2019re joining forces to make it easier and quicker to train high-quality transformer models. Thanks to the integration of Habana\u2019s [SynapseAI software suite](https://habana.ai/training-software/) with the Hugging Face [Optimum open-source library](https://github.com/huggingface/optimum), data scientists and machine learning engineers can now accelerate their Transformer training jobs on Habana processors with just a few lines of code and enjoy greater productivity as well as lower training cost.", "--\ntitle: \"Getting Started with Hugging Face Transformers for IPUs with Optimum\"\nthumbnail: /blog/assets/38_getting_started_graphcore/graphcore_1.png\nauthors:\n- user: internetoftim\n  guest: true\n- user: juliensimon\n---\n\n# Getting Started with Hugging Face Transformers for IPUs with Optimum\n\n\n\nTransformer models have proven to be extremely efficient on a wide range of machine learning tasks, such as natural language processing, audio processing, and computer vision. However, the prediction speed of these large models can make them impractical for latency-sensitive use cases like conversational applications or search. Furthermore, optimizing their performance in the real world requires considerable time, effort and skills that are beyond the reach of many companies and organizations. \n\nLuckily, Hugging Face has introduced [Optimum](https://huggingface.co/hardware), an open source library which makes it much easier to reduce the prediction latency of Transformer models on a variety of hardware platforms. In this blog post, you will learn how to accelerate Transformer models for the Graphcore [Intelligence Processing Unit](https://www.graphcore.ai/products/ipu) (IPU), a highly flexible, easy-to-use parallel processor designed from the ground up for AI workloads."], "eval_score_GPT4": "5", "eval_feedback_GPT4": "The response correctly identifies \"Optimum\" as the open-source library created by Hugging Face to simplify Transformer acceleration. The additional details provided about the Optimum-Intel library, its integration with the Intel Neural Compressor, and its support for machine learning accelerators and specialized hardware like Intel's Habana Gaudi are accurate and relevant to the context of the question. The response goes beyond the reference answer by providing extra information, but it does not detract from the fact that the core question was answered correctly. Therefore, the response meets the criteria for a score of 5."}, {"question": "What parameter is used to ensure that elements in a row have the same height in Gradio?\n", "true_answer": "equal_height", "source_doc": "gradio-app/gradio/blob/main/guides/cn/03_building-with-blocks/02_controlling-layout.md", "generated_answer": "The parameter used to ensure that elements in a row have the same height in Gradio is `equal_height`, which should be passed to the `.style()` method of `gr.Row()`. This was added by [@freddyaboulton](https://github.com/freddyaboulton) in [PR 3125](https://github.com/gradio-app/gradio/pull/3125). It is mentioned in", "retrieved_docs": ["- Updated docs to reflect that the `equal_height` parameter should be passed to the `.style()` method of `gr.Row()` by [@freddyaboulton](https://github.com/freddyaboulton) in [PR 3125](https://github.com/gradio-app/gradio/pull/3125)", "- Updated docs to reflect that the `equal_height` parameter should be passed to the `.style()` method of `gr.Row()` by [@freddyaboulton](https://github.com/freddyaboulton) in [PR 3125](https://github.com/gradio-app/gradio/pull/3125)", "- ensure defaults height match for media inputs by [@pngwn](https://github.com/pngwn) in [PR 1236](https://github.com/gradio-app/gradio/pull/1236)\n- Default Label label value by [@radames](https://github.com/radames) in [PR 1239](https://github.com/gradio-app/gradio/pull/1239)\n- update-shortcut-syntax by [@FarukOzderim](https://github.com/FarukOzderim) in [PR 1234](https://github.com/gradio-app/gradio/pull/1234)\n- Update version.txt by [@FarukOzderim](https://github.com/FarukOzderim) in [PR 1244](https://github.com/gradio-app/gradio/pull/1244)", "- ensure defaults height match for media inputs by [@pngwn](https://github.com/pngwn) in [PR 1236](https://github.com/gradio-app/gradio/pull/1236)\n- Default Label label value by [@radames](https://github.com/radames) in [PR 1239](https://github.com/gradio-app/gradio/pull/1239)\n- update-shortcut-syntax by [@FarukOzderim](https://github.com/FarukOzderim) in [PR 1234](https://github.com/gradio-app/gradio/pull/1234)\n- Update version.txt by [@FarukOzderim](https://github.com/FarukOzderim) in [PR 1244](https://github.com/gradio-app/gradio/pull/1244)", "The widths of elements in a Row can be controlled via a combination of `scale` and `min_width` arguments that are present in every Component.\n\n- `scale` is an integer that defines how an element will take up space in a Row. If scale is set to `0`, and element will not expand to take up space. If scale is set to `1` or greater, the element well expand. Multiple elements in a row will expand proportional to their scale. Below, `btn1` will expand twice as much as `btn2`, while `btn0` will not expand at all:\n\n```python\nwith gr.Blocks() as demo:\n    with gr.Row():\n        btn0 = gr.Button(\"Button 0\", scale=0)\n        btn1 = gr.Button(\"Button 1\", scale=1)\n        btn2 = gr.Button(\"Button 2\", scale=2)\n```\n\n- `min_width` will set the minimum width the element will take. The Row will wrap if there isn't sufficient space to satisfy all `min_width` values.\n\nLearn more about Rows in the [docs](https://gradio.app/docs/#row).\n\n## Columns and Nesting", "Here's an example of how to use these attributes to create a Gradio app that does not lazy load and has an initial height of 0px.\n\n```html\n<gradio-app\n\tspace=\"gradio/Echocardiogram-Segmentation\"\n\teager=\"true\"\n\tinitial_height=\"0px\"\n></gradio-app>\n```\n\nHere's another example of how to use the `render` event. An event listener is used to capture the `render` event and will call the `handleLoadComplete()` function once rendering is complete. \n\n```html\n<script>\n\tfunction handleLoadComplete() {\n\t\tconsole.log(\"Embedded space has finished rendering\");\n\t}\n\n\tconst gradioApp = document.querySelector(\"gradio-app\");\n\tgradioApp.addEventListener(\"render\", handleLoadComplete);\n</script>\n```", "```python\nimport gradio as gr\n\nwith gr.Blocks() as demo:\n    im = gr.ImageEditor(\n        width=\"50vw\",\n    )\n\ndemo.launch()\n```\n\nWhen using percentage values for dimensions, you may want to define a parent component with an absolute unit (e.g. `px` or `vw`). This approach ensures that child components with relative dimensions are sized appropriately:\n\n\n```python\nimport gradio as gr\n\ncss = \"\"\"\n.container {\n    height: 100vh;\n}\n\"\"\"\n\nwith gr.Blocks(css=css) as demo:\n    with gr.Column(elem_classes=[\"container\"]):\n        name = gr.Chatbot(value=[[\"1\", \"2\"]], height=\"70%\")\n\ndemo.launch()\n```\n\nIn this example, the Column layout component is given a height of 100% of the viewport height (100vh), and the Chatbot component inside it takes up 70% of the Column's height."], "eval_score_GPT4": "5", "eval_feedback_GPT4": "The response correctly identifies `equal_height` as the parameter used in Gradio to ensure that elements in a row have the same height. The additional information provided about the `.style()` method and the reference to the pull request adds context and demonstrates a deeper understanding of the implementation, which aligns with the reference answer."}, {"question": "What is the command to install the latest version of Optimum with OpenVINO support?\n", "true_answer": "pip install --upgrade-strategy eager optimum[\"openvino\"]", "source_doc": "huggingface/diffusers/blob/main/docs/source/en/optimization/open_vino.md", "generated_answer": "The command to install the latest version of Optimum with OpenVINO support is:\n\n```bash\npip install --upgrade-strategy eager optimum[\"openvino\"]\n```\n\nThis command will ensure that the latest version of Optimum is installed with OpenVINO support using the eager upgrade strategy. It is recommended to use this command instead of the regular `pip install optimum` command to ensure compatibility with OpenVINO.", "retrieved_docs": ["You'll need to install \ud83e\udd17 Optimum Intel with the `--upgrade-strategy eager` option to ensure [`optimum-intel`](https://github.com/huggingface/optimum-intel) is using the latest version:\n\n```bash\npip install --upgrade-strategy eager optimum[\"openvino\"]\n```\n\nThis guide will show you how to use the Stable Diffusion and Stable Diffusion XL (SDXL) pipelines with OpenVINO.\n\n## Stable Diffusion\n\nTo load and run inference, use the [`~optimum.intel.OVStableDiffusionPipeline`]. If you want to load a PyTorch model and convert it to the OpenVINO format on-the-fly, set `export=True`:\n\n```python\nfrom optimum.intel import OVStableDiffusionPipeline\n\nmodel_id = \"runwayml/stable-diffusion-v1-5\"\npipeline = OVStableDiffusionPipeline.from_pretrained(model_id, export=True)\nprompt = \"sailing ship in storm by Rembrandt\"\nimage = pipeline(prompt).images[0]", "This command installs the current main dev version of Optimum, which could include latest developments(new features, bug fixes). However, the\nmain version might not be very stable. If you run into any problem, please open an [issue](https://github.com/huggingface/optimum/issues) so\nthat we can fix it as soon as possible.\n\n## ORTTrainer\n\nThe [`ORTTrainer`] class inherits the [`Trainer`](https://huggingface.co/docs/transformers/main/en/main_classes/trainer#trainer)\nof Transformers. You can easily adapt the codes by replacing `Trainer` of transformers with `ORTTrainer` to take advantage of the acceleration\nempowered by ONNX Runtime. Here is an example of how to use `ORTTrainer` compared with `Trainer`:\n\n```diff\n-from transformers import Trainer, TrainingArguments\n+from optimum.onnxruntime import ORTTrainer, ORTTrainingArguments", "```\npip install optimum[openvino]\n```\n\nStarting from the code above, we only need to replace `StableDiffusionPipeline` with `OVStableDiffusionPipeline`. To load a PyTorch model and convert it to the OpenVINO format on-the-fly, you can set `export=True` when loading your model.\n\n```python\nfrom optimum.intel.openvino import OVStableDiffusionPipeline\n...\nov_pipe = OVStableDiffusionPipeline.from_pretrained(model_id, export=True)\nlatency = elapsed_time(ov_pipe, prompt)\nprint(latency)\n\n# Don't forget to save the exported model\nov_pipe.save_pretrained(\"./openvino\")\n```\n\nOpenVINO automatically optimizes the model for the `bfloat16` format. Thanks to this, the average latency is now **16.7 seconds**, a sweet 2x speedup.", "The `--upgrade-strategy eager` option is needed to ensure the different packages are upgraded to the latest possible version.\n\nTo install from source:\n\n```bash\npython -m pip install git+https://github.com/huggingface/optimum.git\n```\n\nFor the accelerator-specific features, append `optimum[accelerator_type]` to the above command:\n\n```bash\npython -m pip install optimum[onnxruntime]@git+https://github.com/huggingface/optimum.git\n```\n\n## Accelerated Inference\n\n\ud83e\udd17 Optimum provides multiple tools to export and run optimized models on various ecosystems:", "### OpenVINO\n\nBefore you begin, make sure you have all the necessary libraries installed :\n\n```bash\npip install --upgrade-strategy eager optimum[openvino,nncf]\n```\n\nIt is possible to export \ud83e\udd17 Transformers and Diffusers models to the OpenVINO format easily:\n\n```bash\noptimum-cli export openvino --model distilbert-base-uncased-finetuned-sst-2-english distilbert_sst2_ov\n```\n\nIf you add `--int8`, the weights will be quantized to INT8. Static quantization can also be applied on the activations using [NNCF](https://github.com/openvinotoolkit/nncf), more information can be found in the [documentation](https://huggingface.co/docs/optimum/main/en/intel/optimization_ov).", "| [Intel Neural Compressor (INC)](https://www.intel.com/content/www/us/en/developer/tools/oneapi/neural-compressor.html) | `pip install --upgrade-strategy eager optimum[neural-compressor]`  |\n| [Intel OpenVINO](https://docs.openvino.ai/latest/index.html)                                                           | `pip install --upgrade-strategy eager optimum[openvino,nncf]`      |\n| [AMD Instinct GPUs and Ryzen AI NPU](https://www.amd.com/en/graphics/instinct-server-accelerators)                                      | `pip install --upgrade-strategy eager optimum[amd]`                |\n| [Habana Gaudi Processor (HPU)](https://habana.ai/training/)                                                            | `pip install --upgrade-strategy eager optimum[habana]`             |\n| [FuriosaAI](https://www.furiosa.ai/)                                                                                   | `pip install --upgrade-strategy eager optimum[furiosa]`            |", "## Optimum Intel\n\n### OpenVINO"], "eval_score_GPT4": "5", "eval_feedback_GPT4": "The response provided is completely correct, accurate, and factual. It matches the reference answer and includes the correct command to install the latest version of Optimum with OpenVINO support. The additional explanation about the use of the eager upgrade strategy is accurate and provides useful context for the command's purpose."}]