{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZyAtD0L5KR3u"
      },
      "source": [
        "# Object detection fine tuning on a custom dataset, deployment in Spaces and Gradio API communication"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BGwQ0mW3Idu0"
      },
      "source": [
        "_Authored by: [Sergio Paniego](https://github.com/sergiopaniego)_\n",
        "\n",
        "In this cookbook, we will be fine tuning an object detection model (DETR) using a custom dataset. After that, we will deploy it as a Gradio Space in HF and we will see how we can leverage the Gradio API to directly communicate with the deployed Spaces.\n",
        "\n",
        "![DETR architecture](https://github.com/facebookresearch/detr/raw/main/.github/DETR.png)\n",
        "\n",
        "\n",
        "\n",
        "[Link to DETR HF docs]\n",
        "\n",
        "[More relevant references]\n",
        "\n",
        "* https://huggingface.co/docs/transformers/tasks/object_detection\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v9O_tFkDIdu1"
      },
      "source": [
        "## Install dependencies\n",
        "\n",
        "Let's install the libraries needed!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_SgVfhL7H_AU"
      },
      "outputs": [],
      "source": [
        "!pip install -U -q datasets transformers[torch] timm wandb torchmetrics"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U1hsQ0xsIdu3"
      },
      "source": [
        "## Load dataset\n",
        "\n",
        "[Image of the dataset]\n",
        "\n",
        "The dataset that we will use is [Fashionpedia](https://huggingface.co/datasets/detection-datasets/fashionpedia).\n",
        "\n",
        "This dataset comes from the paper [Fashionpedia: Ontology, Segmentation, and an Attribute Localization Dataset](https://arxiv.org/abs/2004.12276). The authors describe it in the following terms:\n",
        "\n",
        "````\n",
        "Fashionpedia is a dataset which consists of two parts: (1) an ontology built by fashion experts containing 27 main apparel categories, 19 apparel parts, 294 fine-grained attributes and their relationships; (2) a dataset with 48k everyday and celebrity event fashion images annotated with segmentation masks and their associated per-mask fine-grained attributes, built upon the Fashionpedia ontology.\n",
        "````\n",
        "\n",
        "It contains:\n",
        "\n",
        "* 46781 images\n",
        "* 342182 bounding-boxes\n",
        "\n",
        "It is available in the HF Dataset: https://huggingface.co/datasets/detection-datasets/fashionpedia"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1KOv_4u2IJCG"
      },
      "outputs": [],
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "dataset = load_dataset('detection-datasets/fashionpedia')\n",
        "dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dU6FY0vfIfLK"
      },
      "outputs": [],
      "source": [
        "dataset[\"train\"][0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qoY4vo17Idu4"
      },
      "source": [
        "## Get splits of the dataset for training and testing\n",
        "\n",
        "The dataset contains two data splits: train and test. We will use the first one for traing and the second one for validation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EOnVkURVIg5n"
      },
      "outputs": [],
      "source": [
        "train_dataset = dataset['train']\n",
        "test_dataset = dataset['val']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sP7ycLFdIdu5"
      },
      "source": [
        "**Optional**\n",
        "\n",
        "In the following two cells, we take a 1% sample from the original dataset for each split. This is a convetion done so the training is faster since the dataset contains a lot of examples.\n",
        "For the best results, we would recommend skipping these two cells."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1Lm7ue-LIdu5"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "sample_size = int(0.01 * len(train_dataset))\n",
        "\n",
        "train_dataset = train_dataset.shuffle(seed=42).select(range(sample_size))\n",
        "\n",
        "print(f\"Original size: {len(train_dataset)}\")\n",
        "print(f\"Sample size: {len(train_dataset)}\")\n",
        "'''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4nXv9Qj2Idu5"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "sample_size = int(0.01 * len(test_dataset))\n",
        "\n",
        "test_dataset = test_dataset.shuffle(seed=42).select(range(sample_size))\n",
        "\n",
        "print(f\"Original size: {len(test_dataset)}\")\n",
        "print(f\"Sample size: {len(test_dataset)}\")\n",
        "'''"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2WvRElLzIdu6"
      },
      "source": [
        "## Visualize one example from the dataset with its objects\n",
        "\n",
        "Now that we've loaded the dataset, let's visualize some examples."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZtZy-WWTIdu6"
      },
      "source": [
        "### Generate id2label and label2id\n",
        "\n",
        "These variables contain the mapping between the ids and the actual labels for the objects."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9xIaIpnUIdu6"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from PIL import Image, ImageDraw\n",
        "\n",
        "\n",
        "id2label = {\n",
        "    0: 'shirt, blouse', 1: 'top, t-shirt, sweatshirt', 2: 'sweater', 3: 'cardigan',\n",
        "    4: 'jacket', 5: 'vest', 6: 'pants', 7: 'shorts', 8: 'skirt', 9: 'coat',\n",
        "    10: 'dress', 11: 'jumpsuit', 12: 'cape', 13: 'glasses', 14: 'hat',\n",
        "    15: 'headband, head covering, hair accessory', 16: 'tie', 17: 'glove',\n",
        "    18: 'watch', 19: 'belt', 20: 'leg warmer', 21: 'tights, stockings',\n",
        "    22: 'sock', 23: 'shoe', 24: 'bag, wallet', 25: 'scarf', 26: 'umbrella',\n",
        "    27: 'hood', 28: 'collar', 29: 'lapel', 30: 'epaulette', 31: 'sleeve',\n",
        "    32: 'pocket', 33: 'neckline', 34: 'buckle', 35: 'zipper', 36: 'applique',\n",
        "    37: 'bead', 38: 'bow', 39: 'flower', 40: 'fringe', 41: 'ribbon',\n",
        "    42: 'rivet', 43: 'ruffle', 44: 'sequin', 45: 'tassel'\n",
        "}\n",
        "\n",
        "\n",
        "label2id = {v: k for k, v in id2label.items()}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qyy0_K0hIdu6"
      },
      "source": [
        "### Let's draw one image!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-n2NS2v4Iiwx"
      },
      "outputs": [],
      "source": [
        "def draw_image_from_idx(dataset, idx):\n",
        "    sample = dataset[idx]\n",
        "    image = sample[\"image\"]\n",
        "    annotations = sample[\"objects\"]\n",
        "    draw = ImageDraw.Draw(image)\n",
        "    width, height = sample[\"width\"], sample[\"height\"]\n",
        "\n",
        "    print(annotations)\n",
        "\n",
        "    for i in range(len(annotations[\"bbox_id\"])):\n",
        "        box = annotations[\"bbox\"][i]\n",
        "        x1, y1, x2, y2 = tuple(box)\n",
        "\n",
        "        # Normalize coordinates if necessary\n",
        "        if max(box) <= 1.0:\n",
        "            x1, y1 = int(x1 * width), int(y1 * height)\n",
        "            x2, y2 = int(x2 * width), int(y2 * height)\n",
        "        else:\n",
        "            x1, y1 = int(x1), int(y1)\n",
        "            x2, y2 = int(x2), int(y2)\n",
        "\n",
        "        draw.rectangle((x1, y1, x2, y2), outline=\"red\", width=3)\n",
        "        draw.text((x1, y1), id2label[annotations[\"category\"][i]], fill=\"green\")\n",
        "\n",
        "    return image\n",
        "\n",
        "\n",
        "draw_image_from_idx(dataset=train_dataset, idx=10) # You can test changing this id"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wnoCO2P0Idu7"
      },
      "source": [
        "### Let's visualize some more images"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C08PvxFeIdu7"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def plot_images(dataset, indices):\n",
        "    \"\"\"\n",
        "    Plot images and their annotations.\n",
        "    \"\"\"\n",
        "    num_cols = 3\n",
        "    num_rows = int(np.ceil(len(indices) / num_cols))\n",
        "    fig, axes = plt.subplots(num_rows, num_cols, figsize=(15, 10))\n",
        "\n",
        "    for i, idx in enumerate(indices):\n",
        "        row = i // num_cols\n",
        "        col = i % num_cols\n",
        "\n",
        "        image = draw_image_from_idx(dataset, idx)\n",
        "\n",
        "        axes[row, col].imshow(image)\n",
        "        axes[row, col].axis(\"off\")\n",
        "\n",
        "    for j in range(i + 1, num_rows * num_cols):\n",
        "        fig.delaxes(axes.flatten()[j])\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "plot_images(train_dataset, range(9))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NjH8v7iGIdu7"
      },
      "source": [
        "## Filter invalid bboxes\n",
        "\n",
        "To start with the preprocessing of the dataset, we will be filtering some invalid bboxes that it contains.\n",
        "After reviewing the dataset, we found that some bboxs didn't have a valid structure, so we decide to discart them."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n618Z-1rIdu7"
      },
      "outputs": [],
      "source": [
        "from datasets import Dataset\n",
        "\n",
        "def filter_invalid_bboxes(example):\n",
        "    valid_bboxes = []\n",
        "    valid_bbox_ids = []\n",
        "    valid_categories = []\n",
        "    valid_areas = []\n",
        "\n",
        "    for i, bbox in enumerate(example['objects']['bbox']):\n",
        "        x_min, y_min, x_max, y_max = bbox[:4]\n",
        "        if x_min < x_max and y_min < y_max:\n",
        "            valid_bboxes.append(bbox)\n",
        "            valid_bbox_ids.append(example['objects']['bbox_id'][i])\n",
        "            valid_categories.append(example['objects']['category'][i])\n",
        "            valid_areas.append(example['objects']['area'][i])\n",
        "        else:\n",
        "            print(f\"Image with invalid bbox: {example['image_id']} Invalid bbox detected and discarded: {bbox} - bbox_id: {example['objects']['bbox_id'][i]} - category: {example['objects']['category'][i]}\")\n",
        "\n",
        "\n",
        "    example['objects']['bbox'] = valid_bboxes\n",
        "    example['objects']['bbox_id'] = valid_bbox_ids\n",
        "    example['objects']['category'] = valid_categories\n",
        "    example['objects']['area'] = valid_areas\n",
        "\n",
        "    return example\n",
        "\n",
        "train_dataset = train_dataset.map(filter_invalid_bboxes)\n",
        "test_dataset = test_dataset.map(filter_invalid_bboxes)\n",
        "\n",
        "print(train_dataset)\n",
        "print(test_dataset)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KDHO2u9xIdu8"
      },
      "source": [
        "## Visualize each class ocurrences\n",
        "\n",
        "Let's understand further the dataset contain. In this case, we will be plotting each class ocurrences to get a better understanding of biases."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3wm0reYdIdu8"
      },
      "outputs": [],
      "source": [
        "print(train_dataset)\n",
        "\n",
        "id_list = []\n",
        "category_examples = {}\n",
        "for example in train_dataset:\n",
        "  id_list += example['objects']['bbox_id']\n",
        "  for category in example['objects']['category']:\n",
        "    if id2label[category] not in category_examples:\n",
        "      category_examples[id2label[category]] = 1\n",
        "    else:\n",
        "      category_examples[id2label[category]] += 1\n",
        "\n",
        "id_list.sort()\n",
        "print(id_list)\n",
        "print(len(id_list))\n",
        "print(category_examples)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q_UJY1y6Idu8"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Separate the keys and values\n",
        "categories = list(category_examples.keys())\n",
        "values = list(category_examples.values())\n",
        "\n",
        "# Create the bar chart\n",
        "plt.bar(categories, values, color='skyblue')\n",
        "\n",
        "# Add titles and labels\n",
        "plt.xlabel('Categories')\n",
        "plt.ylabel('Number of Occurrences')\n",
        "plt.title('Number of Occurrences by Category')\n",
        "plt.xticks(rotation=90)\n",
        "\n",
        "# Display the chart\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YwNCNc_RIdu8"
      },
      "source": [
        "We can see that some classes are overrepresented like shoe or sleeve."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PIZgUIguIdu8"
      },
      "source": [
        "## Add data augmentation to the dataset\n",
        "\n",
        "Data augmentation is key for performance in this type of problems. In this case, we leverage albumentations capabilities for our needs.\n",
        "\n",
        "[Albumentations image]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZsfZDNotIdu8"
      },
      "outputs": [],
      "source": [
        "import albumentations as A\n",
        "\n",
        "\n",
        "train_transform = A.Compose(\n",
        "    [\n",
        "        A.LongestMaxSize(500),\n",
        "        A.PadIfNeeded(500, 500, border_mode=0, value=(0, 0, 0)),\n",
        "\n",
        "        A.HorizontalFlip(p=0.5),\n",
        "        A.RandomBrightnessContrast(p=0.5),\n",
        "        A.HueSaturationValue(p=0.5),\n",
        "        A.Rotate(limit=10, p=0.5),\n",
        "        A.RandomScale(scale_limit=0.2, p=0.5),\n",
        "        A.GaussianBlur(p=0.5),\n",
        "        A.GaussNoise(p=0.5),\n",
        "    ],\n",
        "    bbox_params=A.BboxParams(\n",
        "        format=\"pascal_voc\",\n",
        "        label_fields=[\"category\"]\n",
        "    ),\n",
        ")\n",
        "\n",
        "val_transform = A.Compose(\n",
        "    [\n",
        "        A.LongestMaxSize(500),\n",
        "        A.PadIfNeeded(500, 500, border_mode=0, value=(0, 0, 0)),\n",
        "    ],\n",
        "    bbox_params=A.BboxParams(\n",
        "        format=\"pascal_voc\",\n",
        "        label_fields=[\"category\"]\n",
        "    ),\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N3CU3rm_Idu8"
      },
      "source": [
        "## Init image processor from model checkpoint\n",
        "\n",
        "We instantiate the image processor from the pretrained checkpoint. In this case, we will be using facebook/detr-resnet-50-dc5 model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "41uphcQNIdu9"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoImageProcessor\n",
        "\n",
        "checkpoint = \"facebook/detr-resnet-50-dc5\"\n",
        "image_processor = AutoImageProcessor.from_pretrained(checkpoint)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pRQduIZeIdu9"
      },
      "source": [
        "### We add some methods to process the dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t_47tZuTIdu9"
      },
      "outputs": [],
      "source": [
        "def formatted_anns(image_id, category, area, bbox):\n",
        "    annotations = []\n",
        "    for i in range(0, len(category)):\n",
        "        new_ann = {\n",
        "            \"image_id\": image_id,\n",
        "            \"category_id\": category[i],\n",
        "            \"isCrowd\": 0,\n",
        "            \"area\": area[i],\n",
        "            \"bbox\": list(bbox[i]),\n",
        "        }\n",
        "        annotations.append(new_ann)\n",
        "\n",
        "    return annotations\n",
        "\n",
        "def convert_voc_to_coco(bbox):\n",
        "    xmin, ymin, xmax, ymax = bbox\n",
        "    width = xmax - xmin\n",
        "    height = ymax - ymin\n",
        "    return [xmin, ymin, width, height]\n",
        "\n",
        "def transform_aug_ann(examples, transform):\n",
        "    image_ids = examples[\"image_id\"]\n",
        "    images, bboxes, area, categories = [], [], [], []\n",
        "    for image, objects in zip(examples[\"image\"], examples[\"objects\"]):\n",
        "        image = np.array(image.convert(\"RGB\"))[:, :, ::-1]\n",
        "        out = transform(image=image, bboxes=objects[\"bbox\"], category=objects[\"category\"])\n",
        "\n",
        "        area.append(objects[\"area\"])\n",
        "        images.append(out[\"image\"])\n",
        "\n",
        "        # Convert to COCO format\n",
        "        converted_bboxes = [convert_voc_to_coco(bbox) for bbox in out[\"bboxes\"]]\n",
        "        bboxes.append(converted_bboxes)\n",
        "\n",
        "        categories.append(out[\"category\"])\n",
        "\n",
        "    targets = [\n",
        "        {\"image_id\": id_, \"annotations\": formatted_anns(id_, cat_, ar_, box_)}\n",
        "        for id_, cat_, ar_, box_ in zip(image_ids, categories, area, bboxes)\n",
        "    ]\n",
        "\n",
        "    return image_processor(images=images, annotations=targets, return_tensors=\"pt\")\n",
        "\n",
        "def transform_train(examples):\n",
        "    return transform_aug_ann(examples, transform=train_transform)\n",
        "\n",
        "def transform_val(examples):\n",
        "    return transform_aug_ann(examples, transform=val_transform)\n",
        "\n",
        "\n",
        "train_dataset_transformed = train_dataset.with_transform(transform_train)\n",
        "test_dataset_transformed = test_dataset.with_transform(transform_val)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qYGuwym5Idu9"
      },
      "outputs": [],
      "source": [
        "def collate_fn(batch):\n",
        "    pixel_values = [item[\"pixel_values\"] for item in batch]\n",
        "    encoding = image_processor.pad(pixel_values, return_tensors=\"pt\")\n",
        "    labels = [item[\"labels\"] for item in batch]\n",
        "\n",
        "    batch = {}\n",
        "    batch[\"pixel_values\"] = encoding[\"pixel_values\"]  # Do not move to GPU here\n",
        "    batch[\"pixel_mask\"] = encoding[\"pixel_mask\"]      # Do not move to GPU here\n",
        "    batch[\"labels\"] = labels\n",
        "\n",
        "    return batch"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_2oxaSrhIdu9"
      },
      "source": [
        "## Plot augmented examples\n",
        "\n",
        "We are close to training our model! But first, let's visualize some samples after the augmentation is done, so we can doble check that the augmentations are suitable for the training procedure."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v6AAA6rKIdu9"
      },
      "outputs": [],
      "source": [
        "# Updated draw function to accept an optional transform\n",
        "def draw_augmented_image_from_idx(dataset, idx, transform=None):\n",
        "    sample = dataset[idx]\n",
        "    image = sample[\"image\"]\n",
        "    annotations = sample[\"objects\"]\n",
        "\n",
        "    # Convert image to RGB and NumPy array\n",
        "    image = np.array(image.convert(\"RGB\"))[:, :, ::-1]\n",
        "\n",
        "    if transform:\n",
        "        augmented = transform(image=image, bboxes=annotations[\"bbox\"], category=annotations[\"category\"])\n",
        "        image = augmented[\"image\"]\n",
        "        annotations[\"bbox\"] = augmented[\"bboxes\"]\n",
        "        annotations[\"category\"] = augmented[\"category\"]\n",
        "\n",
        "    image = Image.fromarray(image[:, :, ::-1])  # Convert back to PIL Image\n",
        "    draw = ImageDraw.Draw(image)\n",
        "    width, height = sample[\"width\"], sample[\"height\"]\n",
        "\n",
        "    for i in range(len(annotations[\"bbox_id\"])):\n",
        "        box = annotations[\"bbox\"][i]\n",
        "        x1, y1, x2, y2 = tuple(box)\n",
        "\n",
        "        # Normalize coordinates if necessary\n",
        "        if max(box) <= 1.0:\n",
        "            x1, y1 = int(x1 * width), int(y1 * height)\n",
        "            x2, y2 = int(x2 * width), int(y2 * height)\n",
        "        else:\n",
        "            x1, y1 = int(x1), int(y1)\n",
        "            x2, y2 = int(x2), int(y2)\n",
        "\n",
        "        draw.rectangle((x1, y1, x2, y2), outline=\"red\", width=3)\n",
        "        draw.text((x1, y1), id2label[annotations[\"category\"][i]], fill=\"green\")\n",
        "\n",
        "    return image\n",
        "\n",
        "# Updated plot function to include augmentation\n",
        "def plot_augmented_images(dataset, indices, transform=None):\n",
        "    \"\"\"\n",
        "    Plot images and their annotations with optional augmentation.\n",
        "    \"\"\"\n",
        "    num_rows = len(indices) // 3\n",
        "    num_cols = 3\n",
        "    fig, axes = plt.subplots(num_rows, num_cols, figsize=(15, 10))\n",
        "\n",
        "    for i, idx in enumerate(indices):\n",
        "        row = i // num_cols\n",
        "        col = i % num_cols\n",
        "\n",
        "        # Draw augmented image\n",
        "        image = draw_augmented_image_from_idx(dataset, idx, transform=transform)\n",
        "\n",
        "        # Display image on the corresponding subplot\n",
        "        axes[row, col].imshow(image)\n",
        "        axes[row, col].axis(\"off\")\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# Now use the function to plot augmented images\n",
        "plot_augmented_images(train_dataset, range(9), transform=train_transform)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8Q9n-fiyIdu-"
      },
      "source": [
        "## Init model from checkpoint\n",
        "\n",
        "We init the model from the same checkpoint as the image processor. We load an already pretrained model that we wil fine tune for this particular dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eYzey_kqIdu-"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoModelForObjectDetection\n",
        "\n",
        "model = AutoModelForObjectDetection.from_pretrained(\n",
        "    checkpoint,\n",
        "    id2label=id2label,\n",
        "    label2id=label2id,\n",
        "    ignore_mismatched_sizes=True,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X_Z-VujxIdu-"
      },
      "outputs": [],
      "source": [
        "output_dir = \"detr-resnet-50-fashionpedia-finetuned-test\" # change this"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m7OOFCE0Idu-"
      },
      "source": [
        "## Connect to HF Hub to upload fine tuned model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L_s22FzZIdu-"
      },
      "outputs": [],
      "source": [
        "from huggingface_hub import notebook_login\n",
        "\n",
        "notebook_login()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HWgKlB1MIdvD"
      },
      "source": [
        "## Set training arguments, connect to W&B and train! <img src=\"https://wandb.ai/logo.png\" alt=\"W&B logo\" width=\"3%\">\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZBSR0TDaIdvD"
      },
      "outputs": [],
      "source": [
        "from transformers import TrainingArguments\n",
        "from transformers import Trainer\n",
        "\n",
        "import torch\n",
        "\n",
        "# Define the training arguments\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=output_dir,\n",
        "    per_device_train_batch_size=4,\n",
        "    per_device_eval_batch_size=4,\n",
        "    num_train_epochs=3,\n",
        "    max_steps=10000,\n",
        "    fp16=True,\n",
        "    save_steps=10,\n",
        "    logging_steps=1,\n",
        "    learning_rate=1e-5,\n",
        "    weight_decay=1e-4,\n",
        "    save_total_limit=2,\n",
        "    remove_unused_columns=False,\n",
        "    evaluation_strategy=\"steps\",\n",
        "    eval_steps=50,\n",
        "    eval_strategy = \"steps\",\n",
        "    report_to=\"wandb\",\n",
        "    push_to_hub=True,\n",
        "    batch_eval_metrics=True\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EpnBnJCiIdvD"
      },
      "source": [
        "### Connect to W&B to track the training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "svMSvS0MIdvD"
      },
      "outputs": [],
      "source": [
        "import wandb\n",
        "\n",
        "wandb.init(\n",
        "    project=\"detr-resnet-50-fashionpedia-finetuned-test\", # change this\n",
        "    name=\"detr-resnet-50-fashionpedia-finetuned-test\", # change this\n",
        "    config=training_args\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SzrzoUm0IdvE"
      },
      "source": [
        "### Let's train the model!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9e_B3B-eIdvE"
      },
      "outputs": [],
      "source": [
        "from torchmetrics.detection.mean_ap import MeanAveragePrecision\n",
        "from torch.nn.functional import softmax\n",
        "\n",
        "def denormalize_boxes(boxes, width, height):\n",
        "    boxes = boxes.clone()\n",
        "    boxes[:, 0] *= width  # xmin\n",
        "    boxes[:, 1] *= height  # ymin\n",
        "    boxes[:, 2] *= width  # xmax\n",
        "    boxes[:, 3] *= height  # ymax\n",
        "    return boxes\n",
        "\n",
        "batch_metrics = []\n",
        "def compute_metrics(eval_pred, compute_result):\n",
        "    global batch_metrics\n",
        "\n",
        "    (loss_dict, scores, pred_boxes, last_hidden_state, encoder_last_hidden_state), labels = eval_pred\n",
        "\n",
        "    image_sizes = []\n",
        "    target = []\n",
        "    for label in labels:\n",
        "\n",
        "        image_sizes.append(label['orig_size'])\n",
        "        width, height = label['orig_size']\n",
        "        denormalized_boxes = denormalize_boxes(label[\"boxes\"], width, height)\n",
        "        target.append(\n",
        "            {\n",
        "                \"boxes\": denormalized_boxes,\n",
        "                \"labels\": label[\"class_labels\"],\n",
        "            }\n",
        "        )\n",
        "    predictions = []\n",
        "    for score, box, target_sizes in zip(scores, pred_boxes, image_sizes):\n",
        "        # Extract the bounding boxes, labels, and scores from the model's output\n",
        "        pred_scores = score[:, :-1]  # Exclude the no-object class\n",
        "        pred_scores = softmax(pred_scores, dim=-1)\n",
        "        width, height = target_sizes\n",
        "        pred_boxes = denormalize_boxes(box, width, height)\n",
        "        pred_labels = torch.argmax(pred_scores, dim=-1)\n",
        "\n",
        "        # Get the scores corresponding to the predicted labels\n",
        "        pred_scores_for_labels = torch.gather(pred_scores, 1, pred_labels.unsqueeze(-1)).squeeze(-1)\n",
        "        predictions.append(\n",
        "            {\n",
        "                \"boxes\": pred_boxes,\n",
        "                \"scores\": pred_scores_for_labels,\n",
        "                \"labels\": pred_labels,\n",
        "            }\n",
        "        )\n",
        "\n",
        "\n",
        "    metric = MeanAveragePrecision(box_format='xywh', class_metrics=True)\n",
        "\n",
        "    if not compute_result:\n",
        "        # Accumulate batch-level metrics\n",
        "        batch_metrics.append({\"preds\": predictions, \"target\": target})\n",
        "        return {}\n",
        "    else:\n",
        "        # Compute final aggregated metrics\n",
        "        # Aggregate batch-level metrics (this should be done based on your metric library's needs)\n",
        "        all_preds = []\n",
        "        all_targets = []\n",
        "        for batch in batch_metrics:\n",
        "            all_preds.extend(batch[\"preds\"])\n",
        "            all_targets.extend(batch[\"target\"])\n",
        "\n",
        "        # Update metric with all accumulated predictions and targets\n",
        "        metric.update(preds=all_preds, target=all_targets)\n",
        "        metrics = metric.compute()\n",
        "\n",
        "        # Convert and format metrics as needed\n",
        "        classes = metrics.pop(\"classes\")\n",
        "        map_per_class = metrics.pop(\"map_per_class\")\n",
        "        mar_100_per_class = metrics.pop(\"mar_100_per_class\")\n",
        "\n",
        "        for class_id, class_map, class_mar in zip(classes, map_per_class, mar_100_per_class):\n",
        "            class_name = id2label[class_id.item()] if id2label is not None else class_id.item()\n",
        "            metrics[f\"map_{class_name}\"] = class_map\n",
        "            metrics[f\"mar_100_{class_name}\"] = class_mar\n",
        "\n",
        "        # Round metrics for cleaner output\n",
        "        metrics = {k: round(v.item(), 4) for k, v in metrics.items()}\n",
        "\n",
        "        # Clear batch metrics for next evaluation\n",
        "        batch_metrics = []\n",
        "\n",
        "        return metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_vU1bToFIdvE"
      },
      "outputs": [],
      "source": [
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    data_collator=collate_fn,\n",
        "    train_dataset=train_dataset_transformed,\n",
        "    eval_dataset=test_dataset_transformed,\n",
        "    tokenizer=image_processor,\n",
        "    compute_metrics=compute_metrics  # Add this line to compute custom metrics\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.train()"
      ],
      "metadata": {
        "id": "ju6t5RlmProw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5A8xscaCIdvE"
      },
      "outputs": [],
      "source": [
        "trainer.push_to_hub()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6u4cOuY6IdvE"
      },
      "source": [
        "## Test how the model behaves on a test image\n",
        "\n",
        "Now that the model is trained, we can check its capabilities easily since it is already available as a HF model.\n",
        "As you can see in the following cell, making a prediction for a new image is straight forward.\n",
        "\n",
        "_The image.jpg is an image uploaded to Google Colab, so you can upload any new image for testing_"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n3r_KEs4IdvF"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "from transformers import pipeline\n",
        "import numpy as np\n",
        "from PIL import Image, ImageDraw\n",
        "\n",
        "url = \"https://images.pexels.com/photos/27980131/pexels-photo-27980131/free-photo-of-mar-moda-hombre-pareja.jpeg?auto=compress&cs=tinysrgb&w=1260&h=750&dpr=2\"\n",
        "\n",
        "image = Image.open(requests.get(url, stream=True).raw)\n",
        "\n",
        "obj_detector = pipeline(\n",
        "    \"object-detection\", model=\"sergiopaniego/fashionpedia-finetuned_albumentations_coco\" # Change with your model name\n",
        ")\n",
        "\n",
        "\n",
        "results = obj_detector(image)\n",
        "print(results)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WKiAfwjZIdvF"
      },
      "source": [
        "### Now, we show the results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KUWNekChIdvF"
      },
      "outputs": [],
      "source": [
        "from PIL import Image, ImageDraw\n",
        "import numpy as np\n",
        "\n",
        "def plot_results(image, results, threshold=0.5):\n",
        "    image = Image.fromarray(np.uint8(image))\n",
        "    draw = ImageDraw.Draw(image)\n",
        "    width, height = image.size\n",
        "\n",
        "    for result in results:\n",
        "        score = result['score']\n",
        "        label = result['label']\n",
        "        box = list(result['box'].values())\n",
        "\n",
        "        if score > threshold:\n",
        "            x1, y1, x2, y2 = tuple(box)\n",
        "\n",
        "            # Normalize coordinates if necessary\n",
        "            if max(box) <= 1.0:\n",
        "                x1, y1 = int(x1 * width), int(y1 * height)\n",
        "                x2, y2 = int(x2 * width), int(y2 * height)\n",
        "            else:\n",
        "                x1, y1 = int(x1), int(y1)\n",
        "                x2, y2 = int(x2), int(y2)\n",
        "\n",
        "            draw.rectangle((x1, y1, x2, y2), outline=\"red\", width=3)\n",
        "            draw.text((x1 + 5, y1 - 10), label, fill=\"white\")\n",
        "            draw.text((x1 + 5, y1 + 10), f'{score:.2f}', fill='green' if score > 0.7 else 'red')\n",
        "\n",
        "    return image"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9XqeW9JGIdvF"
      },
      "outputs": [],
      "source": [
        "plot_results(image, results)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gEJJz0pzIdvF"
      },
      "source": [
        "## Evaluation of the model in the test set\n",
        "\n",
        "After training and visualizating the results for a test image, we generate the metrics for the whole test dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e5QrWZNKIdvF"
      },
      "outputs": [],
      "source": [
        "outputs = trainer.predict(test_dataset_transformed)\n",
        "print(outputs.metrics)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0XCZwxC3IdvG"
      },
      "source": [
        "## Evaluate model over test set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d_CZt5p-IdvG"
      },
      "outputs": [],
      "source": [
        "metrics = trainer.evaluate(test_dataset_transformed)\n",
        "print(metrics)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RtYNiyWlIdvG"
      },
      "source": [
        "## Deploy the model in a HF Space  <img src=\"https://seeklogo.com/images/G/gradio-icon-logo-908AE1836C-seeklogo.com.png\" alt=\"Gradio logo\" width=\"5%\">\n",
        "\n",
        "<img src=\"https://huggingface.co/front/thumbnails/spaces.png\" alt=\"HF Spaces logo\" width=\"20%\">\n",
        "\n",
        "Now we have our model available in the HF models. HF offers free Spaces for small applications so we can generate a new application where we can upload a test image via web and test the capabilities of the model.\n",
        "\n",
        "I've created an example application here: https://huggingface.co/spaces/sergiopaniego/DETR_object_detection_fashionpedia-finetuned"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0skRh84UIdvG"
      },
      "source": [
        "### Create a new Space"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uAKXctNRIdvG"
      },
      "source": [
        "### Create the application with the following code\n",
        "\n",
        "You can copy paste this code to a new app.py file."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZDBrkCHMIdvG"
      },
      "outputs": [],
      "source": [
        "# app.py\n",
        "\n",
        "import gradio as gr\n",
        "import spaces\n",
        "import torch\n",
        "\n",
        "from PIL import Image\n",
        "import requests\n",
        "from transformers import DetrImageProcessor\n",
        "from transformers import DetrForObjectDetection\n",
        "import matplotlib.pyplot as plt\n",
        "import io\n",
        "\n",
        "\n",
        "processor = DetrImageProcessor.from_pretrained(\"sergiopaniego/fashionpedia-finetuned_albumentations_coco\") # Change with your model\n",
        "model = DetrForObjectDetection.from_pretrained(\"sergiopaniego/fashionpedia-finetuned_albumentations_coco\") # Change with your model\n",
        "\n",
        "\n",
        "COLORS = [[0.000, 0.447, 0.741], [0.850, 0.325, 0.098], [0.929, 0.694, 0.125],\n",
        "          [0.494, 0.184, 0.556], [0.466, 0.674, 0.188], [0.301, 0.745, 0.933]]\n",
        "\n",
        "def get_output_figure(pil_img, scores, labels, boxes):\n",
        "    plt.figure(figsize=(16, 10))\n",
        "    plt.imshow(pil_img)\n",
        "    ax = plt.gca()\n",
        "    colors = COLORS * 100\n",
        "    for score, label, (xmin, ymin, xmax, ymax), c in zip(scores.tolist(), labels.tolist(), boxes.tolist(), colors):\n",
        "        ax.add_patch(plt.Rectangle((xmin, ymin), xmax - xmin, ymax - ymin, fill=False, color=c, linewidth=3))\n",
        "        text = f'{model.config.id2label[label]}: {score:0.2f}'\n",
        "        ax.text(xmin, ymin, text, fontsize=15,\n",
        "                bbox=dict(facecolor='yellow', alpha=0.5))\n",
        "    plt.axis('off')\n",
        "\n",
        "    return plt.gcf()\n",
        "\n",
        "@spaces.GPU\n",
        "def detect(image):\n",
        "    encoding = processor(image, return_tensors='pt')\n",
        "    print(encoding.keys())\n",
        "\n",
        "    with torch.no_grad():\n",
        "        outputs = model(**encoding)\n",
        "\n",
        "    width, height = image.size\n",
        "    postprocessed_outputs = processor.post_process_object_detection(outputs, target_sizes=[(height, width)], threshold=0.5)\n",
        "    results = postprocessed_outputs[0]\n",
        "\n",
        "\n",
        "    output_figure = get_output_figure(image, results['scores'], results['labels'], results['boxes'])\n",
        "\n",
        "    buf = io.BytesIO()\n",
        "    output_figure.savefig(buf, bbox_inches='tight')\n",
        "    buf.seek(0)\n",
        "    output_pil_img = Image.open(buf)\n",
        "\n",
        "    return output_pil_img\n",
        "\n",
        "with gr.Blocks() as demo:\n",
        "    gr.Markdown(\"# Object detection with DETR fine tuned on detection-datasets/fashionpedia\")\n",
        "    gr.Markdown(\n",
        "        \"\"\"\n",
        "        This application uses a fine tuned DETR (DEtection TRansformers) to detect objects on images.\n",
        "        This version was trained using detection-datasets/fashionpedia dataset.\n",
        "        You can load an image and see the predictions for the objects detected.\n",
        "        \"\"\"\n",
        "    )\n",
        "\n",
        "    gr.Interface(\n",
        "        fn=detect,\n",
        "        inputs=gr.Image(label=\"Input image\", type=\"pil\"),\n",
        "        outputs=[\n",
        "            gr.Image(label=\"Output prediction\", type=\"pil\")\n",
        "        ]\n",
        "    )#.launch()\n",
        "\n",
        "demo.launch(show_error=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H8HEDMcpIdvH"
      },
      "source": [
        "### Remember setting the requirements.txt!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D5Kuro2BIdvH"
      },
      "outputs": [],
      "source": [
        "# requirements.txt\n",
        "\n",
        "transformers\n",
        "timm\n",
        "torch"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vmNNK0XrIdvH"
      },
      "source": [
        "## Access the Space as an API\n",
        "\n",
        "Some interesting thing about these spaces is that they provide an API that you can call from outside, which can be used to generate a new application.\n",
        "We will see how easy it is to call the application as an API and to obtain the results. You could call the Space from a JS application, Python app... imagine the possibilities!\n",
        "\n",
        "You can find more info in: https://huggingface.co/learn/cookbook/enterprise_cookbook_gradio"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UBHTAPx1IdvH"
      },
      "outputs": [],
      "source": [
        "!pip install gradio_client"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JxfqXnkrIdvH"
      },
      "outputs": [],
      "source": [
        "from gradio_client import Client, handle_file\n",
        "\n",
        "url = \"https://images.pexels.com/photos/27980131/pexels-photo-27980131/free-photo-of-mar-moda-hombre-pareja.jpeg?auto=compress&cs=tinysrgb&w=1260&h=750&dpr=2\"\n",
        "\n",
        "image = Image.open(requests.get(url, stream=True).raw)\n",
        "\n",
        "client = Client(\"sergiopaniego/DETR_object_detection_fashionpedia-finetuned\") # change this with your Space\n",
        "result = client.predict(\n",
        "\t\timage=handle_file(\"https://images.pexels.com/photos/27980131/pexels-photo-27980131/free-photo-of-mar-moda-hombre-pareja.jpeg?auto=compress&cs=tinysrgb&w=1260&h=750&dpr=2\"),\n",
        "\t\tapi_name=\"/predict\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M5uLLCP8IdvH"
      },
      "outputs": [],
      "source": [
        "from PIL import Image\n",
        "\n",
        "img = Image.open(result).convert('RGB')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xh6knD7PIdvH"
      },
      "outputs": [],
      "source": [
        "from IPython.display import display\n",
        "display(img)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}