# Enterprise Hub Cookbook

The Enterprise Hub Cookbook is designed for power users and enterprises who want to go beyond the standard free features of the Hugging Face Hub and integrate machine learning deeper into their production workflows. 

The cookbook guides you through a selection of recipes (Jupyter Notebooks) with copy-pastable code to help you get started with advanced features of the Hub. For a quick overview of the Enterprise Hub, you can also watch [this 15 minute video](https://www.youtube.com/watch?v=CPQGBn-yXJQ). 


## 1. Interactive Development in HF Spaces [coming soon]
With JupyterLab Spaces you can spin up your personal Jupyter Notebook like in Google Colab, only with a wider selection of reliable CPUs and GPUs (e.g. H100 or 4xA10G) which you can select and switch on the fly. With Spaces `dev mode` you can also use this cloud hardware from your local IDE (e.g. VS Code). Read this recipe to learn how to spin up a GPU and connect to it via your local IDE.

See also our [JupyterLab Spaces docs](https://huggingface.co/docs/hub/spaces-sdks-docker-jupyter) and our `dev mode` beta feature [docs](https://huggingface.co/dev-mode-explorers).


## 2. Inference API (Serverless) [coming soon]
With our serverless Inference API, you can test a range of open source models with simple API calls (e.g. generative LLMs, efficient embedding models, or image generators). The serverless Inference API is rate limited and mostly intended for initial testing or low-volume use. Read this recipe to learn how to query the serverless Inference API.

See also our [serverless API docs](https://huggingface.co/docs/api-inference/index).


## 3. Inference Endpoints (dedicated) [coming soon]
With our dedicated Inference Endpoints, you can easily deploy any model and on a wide range of hardware, essentially creating your personal production-ready API in a few clicks. Read this recipe to learn how to create and configure your own dedicated Endpoint.

See also our [Endpoint docs](https://huggingface.co/docs/inference-endpoints/index). 


## 4. Data Annotation with Argilla Spaces [coming soon]
Whether you're zero-shot testing an LLM or you're training your own model - creating good test or train data is maybe the highest value investment you can make at the beginning of your machine learning journey. Argilla is a free open-source data annotation tool that enables you to create high quality data for text, image or audio tasks. Read this recipe to learn how to create a data annotation workflow (alone or in a larger team) in your browser.

See also the [Argilla docs](https://docs.argilla.io/en/latest/) and the [docs](https://huggingface.co/docs/hub/spaces-sdks-docker-argilla) for the HF Argilla Spaces integration.


## 5. AutoTrain Spaces [coming soon]
With our AutoTrain Spaces you can train your own machine learning models in a simple interface without any code. Read this recipe to learn how to fine-tune your own LLM in an AutoTrain Space on the Hub on a wide range of GPUs. 

See also our [AutoTrain docs](https://huggingface.co/docs/autotrain/index).


## 6. Creating Private Demos with Spaces and Gradio [coming soon]

Visual demos speak more than 1000 words. Demos are particularly important if you want to convince stakeholders of a machine learning MVP. Read this recipe to learn how to create a private machine learning demo on Spaces with Gradio.

See also our [Spaces docs](https://huggingface.co/docs/hub/spaces-overview) and the [Gradio Spaces docs](https://huggingface.co/docs/hub/spaces-sdks-gradio).


## 7. Advanced Collaboration on the Hub [coming soon]

As your team and use-cases grow, managing datasets, models, and team members becomes more complex. Read this recipe to learn about advanced collaboration features such as private datasets for specific resource groups, git-based versioning and YAML tags in model cards. 

See also our [Hub docs](https://huggingface.co/docs/hub/index) and our [Hub python library](https://huggingface.co/docs/huggingface_hub/index).

