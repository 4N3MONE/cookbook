{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RAG Evaluation\n",
    "_Authored by: [Aymeric Roucher](https://huggingface.co/m-ric)_\n",
    "\n",
    "This notebook demonstrates different prompting techniques to get the most out of your LLM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.2.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.0\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install python-dotenv -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv(override=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LLM-as-a-judge"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Small integer scales, + additive versions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Good simple test = introduce a certain nuumber of random errors in sentences?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Or just use this dataset: BAAI/JudgeLM-100K, filter by the same reviewer_id, check if you can get multiple reviewers to agree on the same question ids\n",
    "# Judge has LLM genrated scores. here we want human preference directly. Let's use pairwise comparisons.\n",
    "from datasets import load_dataset\n",
    "\n",
    "ratings = load_dataset(\"lmsys/mt_bench_human_judgments\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Constrained JSON outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "RELEVANT_CONTEXT = \"\"\"\n",
    "Document:\n",
    "In `transformers`, we simply set the parameter `num_return_sequences` to\n",
    "the number of highest scoring beams that should be returned. Make sure\n",
    "though that `num_return_sequences <= num_beams`!\n",
    "\n",
    "\n",
    "\n",
    "``` python\n",
    "# set return_num_sequences > 1\n",
    "beam_outputs = model.generate(\n",
    "    **model_inputs,\n",
    "    max_new_tokens=40,\n",
    "    num_beams=5,\n",
    "    no_repeat_ngram_size=2,\n",
    "    num_return_sequences=5,\n",
    "    early_stopping=True\n",
    ")\n",
    "\n",
    "# now we have 3 output sequences\n",
    "print(\"Output:\\n\" + 100 * '-')\n",
    "for i, beam_output in enumerate(beam_outputs):\n",
    "  print(\"{}: {}\".format(i, tokenizer.decode(beam_output, skip_special_tokens=True)))\n",
    "```\n",
    "\n",
    "Document:\n",
    "\n",
    "The weather is really nice in Paris today.\n",
    "To define a stop sequence in Transformers, you should pass the stop_sequence argument in your pipeline or model.\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "RAG_PROMPT_TEMPLATE_BASE = \"\"\"\n",
    "Answer the user query based on the source documents.\n",
    "\n",
    "Here are the source documents: {context}\n",
    "\n",
    "Here is the user question: {user_query}.\"\"\"\n",
    "\n",
    "RAG_PROMPT_TEMPLATE_JSON = (\n",
    "    RAG_PROMPT_TEMPLATE_BASE\n",
    "    + \"\"\"\n",
    "You should provide your answer as a JSON file, and also provide all relevant short snippets from the documents on which you directly based your answer.\n",
    "These snippets should be very short, a few words at most, not whole sentences!\n",
    "\n",
    "Preface your answer with 'Answer:\\n', as follows:\n",
    "\n",
    "Answer:\n",
    "{{\n",
    "  'answer': your_answer,\n",
    "  'source_snippets': ['snippet_1', 'snippet_2', ...]\n",
    "}}\n",
    "\n",
    "Now begin!\n",
    "\"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "USER_QUERY = \"How can I define a stop sequence in Transformers?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import InferenceClient\n",
    "import json\n",
    "\n",
    "client = InferenceClient(model=\"HuggingFaceH4/zephyr-7b-beta\")\n",
    "\n",
    "\n",
    "def call_llm(query: str, stop=None) -> str:\n",
    "    return client.post(json={\"inputs\": query, \"parameters\": {\"stop\": stop}})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "\n",
    "llm = ChatOpenAI(model=\"gpt-4-0125-preview\")\n",
    "\n",
    "\n",
    "def call_llm(query: str, stop=None) -> str:\n",
    "    return llm.invoke(query).content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "answer = call_llm(\n",
    "    RAG_PROMPT_TEMPLATE_JSON.format(context=RELEVANT_CONTEXT, user_query=USER_QUERY)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'answer': 'To define a stop sequence in Transformers, you should pass the stop_sequence argument in your pipeline or model.', 'source_snippets': ['pass the stop_sequence argument']}\n"
     ]
    }
   ],
   "source": [
    "from ast import literal_eval\n",
    "\n",
    "\n",
    "def parse_answer(answer):\n",
    "    answer = answer.replace(\"Answer:\\n\", \"\")  # remove prefix\n",
    "    answer = literal_eval(answer)\n",
    "    if not isinstance(\n",
    "        answer[\"source_snippets\"], list\n",
    "    ):  # parse snippets if they are not already\n",
    "        answer[\"source_snippets\"] = (\n",
    "            answer[\"source_snippets\"].strip()[1:-1].replace(\"'\", \"\").split(\", \")\n",
    "        )\n",
    "    return answer\n",
    "\n",
    "\n",
    "parsed_answer = parse_answer(answer)\n",
    "print(parsed_answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To define a stop sequence in Transformers, you should pass the stop_sequence argument in your pipeline or model.\n",
      "\n",
      "\n",
      " ========== Source documents ==========\n",
      "\n",
      "Document:\n",
      "In `transformers`, we simply set the parameter `num_return_sequences` to\n",
      "the number of highest scoring beams that should be returned. Make sure\n",
      "though that `num_return_sequences <= num_beams`!\n",
      "\n",
      "\n",
      "\n",
      "``` python\n",
      "# set return_num_sequences > 1\n",
      "beam_outputs = model.generate(\n",
      "    **model_inputs,\n",
      "    max_new_tokens=40,\n",
      "    num_beams=5,\n",
      "    no_repeat_ngram_size=2,\n",
      "    num_return_sequences=5,\n",
      "    early_stopping=True\n",
      ")\n",
      "\n",
      "# now we have 3 output sequences\n",
      "print(\"Output:\n",
      "\" + 100 * '-')\n",
      "for i, beam_output in enumerate(beam_outputs):\n",
      "  print(\"{}: {}\".format(i, tokenizer.decode(beam_output, skip_special_tokens=True)))\n",
      "```\n",
      "\n",
      "Document:\n",
      "\n",
      "The weather is really nice in Paris today.\n",
      "To define a stop sequence in Transformers, you should \u001b[1;31mpass the stop_sequence argument\u001b[0m in your pipeline or model.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def turn_red(s):\n",
    "    return \"\\x1b[1;31m\" + s + \"\\x1b[0m\"\n",
    "\n",
    "\n",
    "def print_results(answer, source_text, highlight_snippets):\n",
    "    print(answer)\n",
    "    print(\"\\n\\n\", \"=\" * 10 + \" Source documents \" + \"=\" * 10)\n",
    "    for snippet in highlight_snippets:\n",
    "        source_text = source_text.replace(snippet, turn_red(snippet))\n",
    "    print(source_text)\n",
    "\n",
    "\n",
    "print_results(\n",
    "    parsed_answer[\"answer\"], RELEVANT_CONTEXT, parsed_answer[\"source_snippets\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Demo of \"do not ask too much at once\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cookbook2",
   "language": "python",
   "name": "cookbook2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
