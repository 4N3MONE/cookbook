{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RAG Evaluation\n",
    "\n",
    "This notebook demonstrates how you can evaluate your RAG (Retrieval Augmented Generation), by building a synthetic evaluation dataset and using LLM-as-a-judge to compute the accuracy of your system.\n",
    "\n",
    "\n",
    "### What is RAG?\n",
    "\n",
    "When using a Large Language Model (LLM) to answer questions about a particular field of knowledge, two issues may appear:\n",
    "- the LLM not having knowledge of domain-specific information due to it not being available in its training data\n",
    "- the risk of hallucination: when asked to give an answer on an unknown field, the LLM may hallucinate a credible answer.\n",
    "\n",
    "These two problems can be tempered by fine-tuning the LLM on the said field of knowledge\n",
    "However, fine-tuning can be costly, and, when done repeatedly (e.g. to address changes in data), leads to \"model shift\" _(this is when the model's behavior changes in ways that are not desirable)_.\n",
    "\n",
    "RAG (Retrieval Augmented Generation) does not require model fine-tuning. Instead, RAG works by enhancing an LLM with access to additional context retrieved from relevant data, so that it can generate a better-informed response.\n",
    "\n",
    "RAG systems consists of two subsystems:\n",
    "\n",
    "1. A Retriever system, functioning like an internal search engine, identifies the most relevant documents from the knowledge base to respond to the query.\n",
    "2. A Reader Language Model (LLM) reads these retrieved documents to produce an answer to the user’s query.\n",
    "\n",
    "Here is the workflow, with paths for system tuning noted in blue:\n",
    "\n",
    "<img src=\"https://huggingface.co/datasets/huggingface/cookbook-images/resolve/main/RAG_workflow.png\" height=\"700\">\n",
    "\n",
    "\n",
    "> 💡 As you can see, there are many steps to tune in this architecture: tuning the system properly will yield significant performance gains.\n",
    "\n",
    "RAG systems have plentiful options for improvement: you can tune your system along the way to improve results beyond a baseline model.\n",
    "\n",
    "But this is useless if you cannot monitor the impact of your changes on the system's performance!\n",
    "\n",
    "\n",
    "### Evaluating RAG performance\n",
    "\n",
    "Since there are so many options to tune with a big impact on parameters, benchmarking the RAG system is crucial.\n",
    "\n",
    "For our evaluation pipeline, we will need:\n",
    "- an evaluation dataset\n",
    "- an evaluator to compute the accuracy of our system.\n",
    "\n",
    "➡️ It turns out, we can use LLMs to help us all along the way!\n",
    "- the evaluation dataset will be synthetically generated by an LLM 🤖, and questions will be filtered out by other LLMs 🤖\n",
    "- the evaluation will then be run on this synthetic dataset by a LLM-as-a-judge agent 🤖.\n",
    "\n",
    "__Let's dig into it and start building our evaluation pipeline!__ First, we install the required model dependancies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip available: \u001b[0m\u001b[31;49m22.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.3.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install -q torch transformers transformers langchain sentence-transformers faiss-gpu openpyxl openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "%reload_ext dotenv\n",
    "%dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "import pandas as pd\n",
    "from typing import Optional, List, Tuple\n",
    "from langchain_core.language_models import BaseChatModel\n",
    "from datasets import Dataset\n",
    "\n",
    "pd.set_option(\"display.max_colwidth\", None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load your knowledge base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datasets\n",
    "\n",
    "ds = datasets.load_dataset(\"m-ric/huggingface_doc\", split=\"train\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Build a synthetic dataset for evaluation\n",
    "We first build a synthetic dataset of questions and associated contexts.\n",
    "\n",
    "The idea is to randomly get elements from our knowledge base, and ask a LLM to generate questions based on these documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "LOAD_PREGENERATED_EVALUATION_DATASET = True\n",
    "\n",
    "if LOAD_PREGENERATED_EVALUATION_DATASET:\n",
    "    eval_dataset = datasets.load_dataset(\"m-ric/huggingface_doc_qa_eval\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare source documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not LOAD_PREGENERATED_EVALUATION_DATASET:\n",
    "    from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "    from langchain.docstore.document import Document as LangchainDocument\n",
    "\n",
    "    langchain_docs = [\n",
    "        LangchainDocument(page_content=doc[\"text\"], metadata={\"source\": doc[\"source\"]})\n",
    "        for doc in tqdm(ds)\n",
    "    ]\n",
    "\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=2000,\n",
    "        chunk_overlap=200,\n",
    "        add_start_index=True,\n",
    "        separators=[\"\\n\\n\", \"\\n\", \".\", \" \", \"\"],\n",
    "    )\n",
    "\n",
    "    docs_processed = []\n",
    "    for doc in langchain_docs:\n",
    "        docs_processed += text_splitter.split_documents([doc])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup agents for question generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not LOAD_PREGENERATED_EVALUATION_DATASET:\n",
    "    from langchain.prompts import ChatPromptTemplate\n",
    "    from langchain.chat_models import ChatOpenAI\n",
    "\n",
    "    QA_generation_prompt = \"\"\"\n",
    "    Your task is to write a factoid question and an answer given a context.\n",
    "    Your factoid question should be answerable with a specific, concise piece of factual information from the context.\n",
    "    Your factoid question should be formulated in the same style as questions users could ask in a search engine.\n",
    "    This means that your factoid question MUST NOT mention something like \"according to the passage\" or \"context\".\n",
    "\n",
    "    Provide your answer as follows:\n",
    "\n",
    "    Output:::\n",
    "    Factoid question: (your factoid question)\n",
    "    Answer: (your answer to the factoid question)\n",
    "\n",
    "    Now here is the context.\n",
    "\n",
    "    Context: {context}\\n\n",
    "    Output:::\"\"\"\n",
    "\n",
    "    chat_model = ChatOpenAI(model=\"gpt-4-1106-preview\")\n",
    "    QA_generation_prompt = ChatPromptTemplate.from_template(QA_generation_prompt)\n",
    "    QA_generation_agent = QA_generation_prompt | chat_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "if not LOAD_PREGENERATED_EVALUATION_DATASET:\n",
    "    N_GENERATIONS = 30\n",
    "\n",
    "    print(f\"Generating {N_GENERATIONS} QA couples...\")\n",
    "    outputs = []\n",
    "    for context in tqdm(random.sample(langchain_docs, N_GENERATIONS)):\n",
    "        # Generate QA couple\n",
    "        output_QA_couple = QA_generation_agent.invoke({\"context\": context.page_content}).content\n",
    "        try:\n",
    "            question = output_QA_couple.split(\"Factoid question: \")[1].split(\"Answer: \")[0]\n",
    "            answer = output_QA_couple.split(\"Answer: \")[1]\n",
    "            outputs.append(\n",
    "                {\n",
    "                    \"context\": context.page_content,\n",
    "                    \"question\": question,\n",
    "                    \"answer\": answer,\n",
    "                    \"source_doc\": context.metadata[\"source\"],\n",
    "                }\n",
    "            )\n",
    "        except:\n",
    "            continue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup critique agents\n",
    "\n",
    "The questions generated by the previous agent can have many flaws:\n",
    "- unrelated to the knowledge base\n",
    "- irrelevant, e.g. `\"What is the date when transformers 4.29.1 was released?\"`\n",
    "- tailored for the particular setting where the question was generated, but undecipherable by itself, like `\"What is the name of the function used in this guide?\"`\n",
    "\n",
    "To eliminate each of these flaws, we build critique agents that will rate each question on each of these possible flaws. Whenever the score is too low, we eliminate the question from our eval dataset.\n",
    "\n",
    "💡 __When asking the agents to output a score, we first ask them to produce its rationale. This will help us verify scores, but most importantly, asking it to first output rationale gives the model more tokens to think and elaborate an answer before summarizing it into a single score token.__\n",
    "\n",
    "We now build and run these critique agents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not LOAD_PREGENERATED_EVALUATION_DATASET:\n",
    "    question_relatedness_critique_prompt = \"\"\"\n",
    "    You will be given a context and a question.\n",
    "    Your task is to provide a 'total rating' scoring how well one can answer the given question unambiguously with the given context.\n",
    "    Give your answer on a scale of 1 to 5, where 1 means that the question is not answerable at all given the context, and 5 means that the question is clearly and unambiguously answerable with the context.\n",
    "\n",
    "    Provide your answer as follows:\n",
    "\n",
    "    Answer:::\n",
    "    Evaluation: (your rationale for the rating)\n",
    "    Total rating: (your rating)\n",
    "\n",
    "    Now here are the question and context.\n",
    "\n",
    "    Question: {question}\\n\n",
    "    Context: {context}\\n\n",
    "    Answer::: \"\"\"\n",
    "\n",
    "    question_relevance_critique_prompt = \"\"\"\n",
    "    You will be given a question.\n",
    "    Your task is to provide a 'total rating' representing how useful this question can be to machine learning developers building NLP applications with the Hugging Face ecosystem.\n",
    "    Give your answer on a scale of 1 to 5, where 1 means that the question is not useful at all, and 5 means that the question is extremely useful.\n",
    "\n",
    "    Rating must be minimum, i.e. 1, if the questions refers to a particular setting, like 'in the context' or 'in the document'.\n",
    "\n",
    "    Provide your answer as follows:\n",
    "\n",
    "    Answer:::\n",
    "    Evaluation: (your rationale for the rating)\n",
    "    Total rating: (your rating)\n",
    "\n",
    "    Now here is the question.\n",
    "\n",
    "    Question: {question}\\n\n",
    "    Answer::: \"\"\"\n",
    "\n",
    "    question_standalone_critique_prompt = \"\"\"\n",
    "    You will be given a question.\n",
    "    Your task is to provide a 'total rating' representing how context-independant this question is.\n",
    "    Give your answer on a scale of 1 to 5, where 1 means that the question only makes sense in a specific context, and 5 means that the question makes sense by itself.\n",
    "    For instance, if the question refers to a particular setting, like 'in the context' or 'in the document', the rating must be 1.\n",
    "    The questions can contain obscure technical nouns or acronyms like Gradio, Hub, Hugging Face or Space and still be a 5: it must simply be clear to an operator with access to documentation what the question is about.\n",
    "\n",
    "    Provide your answer as follows:\n",
    "\n",
    "    Answer:::\n",
    "    Evaluation: (your rationale for the rating)\n",
    "    Total rating: (your rating)\n",
    "\n",
    "    Now here is the question.\n",
    "\n",
    "    Question: {question}\\n\n",
    "    Answer::: \"\"\"\n",
    "\n",
    "    question_relatedness_critique_prompt = ChatPromptTemplate.from_template(\n",
    "        question_relatedness_critique_prompt\n",
    "    )\n",
    "    question_relatedness_critique_agent = question_relatedness_critique_prompt | chat_model\n",
    "\n",
    "    question_relevance_critique_prompt = ChatPromptTemplate.from_template(\n",
    "        question_relevance_critique_prompt\n",
    "    )\n",
    "    question_relevance_critique_agent = question_relevance_critique_prompt | chat_model\n",
    "\n",
    "    question_standalone_critique_prompt = ChatPromptTemplate.from_template(\n",
    "        question_standalone_critique_prompt\n",
    "    )\n",
    "    question_standalone_critique_agent = question_standalone_critique_prompt | chat_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not LOAD_PREGENERATED_EVALUATION_DATASET:\n",
    "    print(\"Generating critique for each QA couple...\")\n",
    "    for output in tqdm(outputs):\n",
    "        # Critique the generated QA couple\n",
    "        question_relatedness_evaluation = question_relatedness_critique_agent.invoke(\n",
    "            {\"context\": output[\"context\"], \"question\": output[\"question\"]}\n",
    "        ).content\n",
    "        question_relevance_evaluation = question_relevance_critique_agent.invoke(\n",
    "            {\"question\": output[\"question\"]}\n",
    "        ).content\n",
    "        question_standalone_evaluation = question_standalone_critique_agent.invoke(\n",
    "            {\"question\": output[\"question\"]}\n",
    "        ).content\n",
    "\n",
    "        try:\n",
    "            relatedness_score = int(question_relatedness_evaluation.split(\"Total rating: \")[1][0])\n",
    "            relatedness_eval = question_relatedness_evaluation.split(\"Total rating: \")[0].split(\n",
    "                \"Evaluation: \"\n",
    "            )[1]\n",
    "            relevance_score = int(question_relevance_evaluation.split(\"Total rating: \")[1][0])\n",
    "            relevance_eval = question_relevance_evaluation.split(\"Total rating: \")[0].split(\n",
    "                \"Evaluation: \"\n",
    "            )[1]\n",
    "            standalone_score = int(question_standalone_evaluation.split(\"Total rating: \")[1][0])\n",
    "            standalone_eval = question_standalone_evaluation.split(\"Total rating: \")[0].split(\n",
    "                \"Evaluation: \"\n",
    "            )[1]\n",
    "            output.update(\n",
    "                {\n",
    "                    \"relatedness_score\": relatedness_score,\n",
    "                    \"relatedness_eval\": relatedness_eval,\n",
    "                    \"relevance_score\": relevance_score,\n",
    "                    \"relevance_eval\": relevance_eval,\n",
    "                    \"standalone_score\": standalone_score,\n",
    "                    \"standalone_eval\": standalone_eval,\n",
    "                }\n",
    "            )\n",
    "        except:\n",
    "            continue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Filter down questions on the chosen criteria:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not LOAD_PREGENERATED_EVALUATION_DATASET:\n",
    "    import pandas as pd\n",
    "\n",
    "    pd.set_option(\"display.max_colwidth\", None)\n",
    "\n",
    "    generated_questions = pd.DataFrame.from_dict(outputs)\n",
    "    display(generated_questions.head())\n",
    "    generated_questions = generated_questions.loc[\n",
    "        (generated_questions[\"relatedness_score\"] >= 4)\n",
    "        & (generated_questions[\"relevance_score\"] >= 3)\n",
    "        & (generated_questions[\"standalone_score\"] >= 4)\n",
    "    ]\n",
    "    generated_questions.to_excel(\"qa_eval.xlsx\")\n",
    "    display(generated_questions.head())\n",
    "\n",
    "    eval_dataset = datasets.Dataset.from_pandas(\n",
    "        generated_questions, split=\"train\", preserve_index=False\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the synthetic evaluation dataset is complete! We can evaluate different RAG systems on this evaluation dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Build our RAG System"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building the vector database: preprocessing\n",
    "\n",
    "- In this part, __we split the documents from our knowledge base into smaller chunks__ which will be the snippets on which the reader LLM will base its answer.\n",
    "- The goal is to have semantically relevant snippets: not too small to be sufficient for supporting an answer, and not too large too avoid diluting each idea.\n",
    "\n",
    "> We use Langchain's `RecursiveCharacterTextSplitter`, which makes efficient use of code language detection to make better splits the lenght function we use is not the count of characters, but the count of tokens.\n",
    "\n",
    "💡 To help you tune this step, [this space](https://huggingface.co/spaces/A-Roucher/chunk_visualizer) lets you visualize how different chunking options affect the chunks you get."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.docstore.document import Document as LangchainDocument\n",
    "\n",
    "RAW_KNOWLEDGE_BASE = [\n",
    "    LangchainDocument(page_content=doc[\"text\"], metadata={\"source\": doc[\"source\"]})\n",
    "    for doc in tqdm(ds)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "\n",
    "def split_documents(\n",
    "    chunk_size: int,\n",
    "    knowledge_base: List[LangchainDocument],\n",
    "    tokenizer_name: str,\n",
    ") -> List[LangchainDocument]:\n",
    "    \"\"\"\n",
    "    Split documents into chunks of size `chunk_size` characters and return a list of documents.\n",
    "    \"\"\"\n",
    "    text_splitter = RecursiveCharacterTextSplitter.from_huggingface_tokenizer(\n",
    "        AutoTokenizer.from_pretrained(tokenizer_name),\n",
    "        chunk_size=chunk_size,\n",
    "        chunk_overlap=int(chunk_size / 10),\n",
    "        add_start_index=True,\n",
    "        strip_whitespace=True,\n",
    "        separators=[\"\\n\\n\", \"\\n\", \".\", \" \", \"\"],\n",
    "    )\n",
    "\n",
    "    docs_processed = []\n",
    "    for doc in knowledge_base:\n",
    "        docs_processed += text_splitter.split_documents([doc])\n",
    "\n",
    "    # Remove duplicates\n",
    "    unique_texts = {}\n",
    "    docs_processed_unique = []\n",
    "    for doc in docs_processed:\n",
    "        if doc.page_content not in unique_texts:\n",
    "            unique_texts[doc.page_content] = True\n",
    "            docs_processed_unique.append(doc)\n",
    "\n",
    "    return docs_processed_unique"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retriever - embeddings 🗂️\n",
    "The __retriever acts like an internal search engine__: given the user query, it returns the most relevant documents from your knowledge base.\n",
    "\n",
    "> For the knowledge base, we use Langchain vector databases since __it offers a convenient [FAISS](https://github.com/facebookresearch/faiss) index and allows us to keep document metadata throughout the processing__.\n",
    "\n",
    "🛠️ __Options included:__\n",
    "\n",
    "- Tune the chunking method:\n",
    "    - Size of the chunks\n",
    "    - Method: split on different separators, use [semantic chunking](https://python.langchain.com/docs/modules/data_connection/document_transformers/semantic-chunker)...\n",
    "- Change the embedding model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.vectorstores import FAISS\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "from langchain_community.vectorstores.utils import DistanceStrategy\n",
    "import os\n",
    "\n",
    "\n",
    "def load_embeddings(\n",
    "    langchain_docs: List[LangchainDocument],\n",
    "    chunk_size: int,\n",
    "    embedding_model_name: Optional[str] = \"thenlper/gte-small\",\n",
    ") -> FAISS:\n",
    "    \"\"\"\n",
    "    Creates a FAISS index from the given embedding model and documents. Loads the index directly if it already exists.\n",
    "\n",
    "    Args:\n",
    "        langchain_docs: list of documents\n",
    "        chunk_size: size of the chunks to split the documents into\n",
    "        embedding_model_name: name of the embedding model to use\n",
    "\n",
    "    Returns:\n",
    "        FAISS index\n",
    "    \"\"\"\n",
    "    # load embedding_model\n",
    "    embedding_model = HuggingFaceEmbeddings(\n",
    "        model_name=embedding_model_name,\n",
    "        multi_process=True,\n",
    "        model_kwargs={\"device\": \"cuda\"},\n",
    "        encode_kwargs={\"normalize_embeddings\": True},  # set True to compute cosine similarity\n",
    "    )\n",
    "\n",
    "    # Check if embeddings already exist on disk\n",
    "    index_name = f\"index_chunk:{chunk_size}_embeddings:{embedding_model_name.replace('/', '~')}\"\n",
    "    index_folder_path = f\"./data/indexes/{index_name}/\"\n",
    "    if os.path.isdir(index_folder_path):\n",
    "        return FAISS.load_local(\n",
    "            index_folder_path,\n",
    "            embedding_model,\n",
    "            distance_strategy=DistanceStrategy.COSINE,\n",
    "        )\n",
    "\n",
    "    else:\n",
    "        print(\"Index not found, generating it...\")\n",
    "        docs_processed = split_documents(\n",
    "            chunk_size,\n",
    "            langchain_docs,\n",
    "            embedding_model_name,\n",
    "        )\n",
    "        knowledge_index = FAISS.from_documents(\n",
    "            docs_processed, embedding_model, distance_strategy=DistanceStrategy.COSINE\n",
    "        )\n",
    "        knowledge_index.save_local(index_folder_path)\n",
    "        return knowledge_index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reader - LLM 💬\n",
    "\n",
    "In this part, the __LLM Reader reads the retrieved documents to formulate its answer.__\n",
    "\n",
    "🛠️ Here we tried the following options to improve results:\n",
    "- Switch reranking on/off\n",
    "- Change the reader model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "RAG_PROMPT_TEMPLATE = \"\"\"\n",
    "<|system|>\n",
    "Using the information contained in the context, \n",
    "give a comprehensive answer to the question.\n",
    "Respond only to the question asked, response should be concise and relevant to the question.\n",
    "Provide the number of the source document when relevant.\n",
    "If the answer cannot be deduced from the context, do not give an answer.</s>\n",
    "<|user|>\n",
    "Context:\n",
    "{context}\n",
    "---\n",
    "Now here is the question you need to answer.\n",
    "\n",
    "Question: {question}\n",
    "</s>\n",
    "<|assistant|>\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import os\n",
    "\n",
    "API_URL = \"https://ytjpei7t003tedav.us-east-1.aws.endpoints.huggingface.cloud\"\n",
    "\n",
    "headers = {\n",
    "    \"Authorization\": f\"Bearer {os.getenv('HUGGINGFACEHUB_API_TOKEN')}\",\n",
    "    \"Content-Type\": \"application/json\",\n",
    "}\n",
    "\n",
    "\n",
    "def get_llm_answer(question):\n",
    "    payload = {\n",
    "        \"inputs\": question,\n",
    "        \"max_new_tokens\": 3000,\n",
    "    }\n",
    "    response = requests.post(API_URL, headers=headers, json=payload)\n",
    "    return response.json()[0][\"generated_text\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\nParis.\\n\\nNo, that\\'s the most romantic city in the world. The capital is actually Paris, but I\\'m getting ahead of myself.\\n\\nThe capital of France is actually a city called Paris. Paris is located in the north of France, and it is the largest city in France. Paris is also known as the \"City of Love\" because of its romantic atmosphere and beautiful architecture.\\n\\nParis is home to many famous landmarks, such as'"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_llm_answer(\"What is the capital of France?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ragatouille import RAGPretrainedModel\n",
    "\n",
    "\n",
    "def answer_with_rag(\n",
    "    question: str,\n",
    "    call_llm,\n",
    "    knowledge_index: FAISS,\n",
    "    reranker: Optional[RAGPretrainedModel] = None,\n",
    "    num_retrieved_docs: int = 30,\n",
    "    num_docs_final: int = 7,\n",
    ") -> Tuple[str, List[LangchainDocument]]:\n",
    "    # Gather documents with retriever\n",
    "    relevant_docs = knowledge_index.similarity_search(query=question, k=num_retrieved_docs)\n",
    "    relevant_docs = [doc.page_content for doc in relevant_docs]  # keep only the text\n",
    "\n",
    "    # Optionally rerank results\n",
    "    if reranker:\n",
    "        relevant_docs = reranker.rerank(question, relevant_docs, k=num_docs_final)\n",
    "        relevant_docs = [doc[\"content\"] for doc in relevant_docs]\n",
    "\n",
    "    relevant_docs = relevant_docs[:num_docs_final]\n",
    "\n",
    "    # Build the final prompt\n",
    "    context = \"\\nExtracted documents:\\n\"\n",
    "    context += \"\".join([f\"Document {str(i)}:::\\n\" + doc for i, doc in enumerate(relevant_docs)])\n",
    "\n",
    "    final_prompt = RAG_PROMPT_TEMPLATE.format(question=question, context=context)\n",
    "\n",
    "    # Redact an answer\n",
    "    answer = call_llm(final_prompt)\n",
    "\n",
    "    return answer, relevant_docs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Benchmarking the RAG system\n",
    "\n",
    "The RAG system and the evaluation datasets are now ready. The last step is to judge the RAG system's output on this evlauation dataset.\n",
    "\n",
    "To this end, __we setup a judge agent__. ⚖️🤖\n",
    "\n",
    "Out of [the different RAG evaluation metrics](https://docs.ragas.io/en/latest/concepts/metrics/index.html), we choose to focus only on faithfulness since it the best end-to-end metric of our system's performance.\n",
    "\n",
    "> We use GPT4 as a judge for its good performance, but you could try with other models such as `kaist-ai/prometheus-13b-v1.0` or `BAAI/JudgeLM-33B-v1.0`.\n",
    "\n",
    "💡 In the evaluation prompt, we give a detailed description each metric on the scale 1-5, as is done in [Prometheus's prompt template](https://huggingface.co/kaist-ai/prometheus-13b-v1.0): this helps the model ground its metric precisely. If instead you give the judge LLM a vague scale to work with, the outputs will not be consistent enough inbetween examples.\n",
    "\n",
    "💡 Again, prompting the LLM to output rationale before giving its final score gives it more tokens to help it formalize and elaborate a judgement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from ragatouille import RAGPretrainedModel\n",
    "\n",
    "\n",
    "def run_rag_tests(\n",
    "    eval_dataset: Dataset,\n",
    "    llm: BaseChatModel,\n",
    "    knowledge_index: FAISS,\n",
    "    output_file: str,\n",
    "    reranker: Optional[RAGPretrainedModel] = None,\n",
    "    verbose: Optional[bool] = True,\n",
    "):\n",
    "    try:  # load previous generations if they exist\n",
    "        with open(output_file, \"r\") as f:\n",
    "            outputs = json.load(f)\n",
    "    except:\n",
    "        outputs = []\n",
    "\n",
    "    for example in tqdm(eval_dataset[\"train\"]):\n",
    "        question = example[\"question\"]\n",
    "        if question in [output[\"question\"] for output in outputs]:\n",
    "            continue\n",
    "\n",
    "        answer, relevant_docs = answer_with_rag(question, llm, knowledge_index, reranker=reranker)\n",
    "        if verbose:\n",
    "            print(\"=======================================================\")\n",
    "            print(f\"Question: {question}\")\n",
    "            print(f\"Answer: {answer}\")\n",
    "            print(f'True answer: {example[\"answer\"]}')\n",
    "        outputs.append(\n",
    "            {\n",
    "                \"question\": question,\n",
    "                \"true_answer\": example[\"answer\"],\n",
    "                \"source_doc\": example[\"source_doc\"],\n",
    "                \"generated_answer\": answer,\n",
    "                \"retrieved_docs\": [doc for doc in relevant_docs],\n",
    "            }\n",
    "        )\n",
    "\n",
    "        with open(output_file, \"w\") as f:\n",
    "            json.dump(outputs, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "EVALUATION_PROMPT = \"\"\"###Task Description:\n",
    "An instruction (might include an Input inside it), a response to evaluate, a reference answer that gets a score of 5, and a score rubric representing a evaluation criteria are given.\n",
    "1. Write a detailed feedback that assess the quality of the response strictly based on the given score rubric, not evaluating in general.\n",
    "2. After writing a feedback, write a score that is an integer between 1 and 5. You should refer to the score rubric.\n",
    "3. The output format should look as follows: \\\"Feedback: {{write a feedback for criteria}} [RESULT] {{an integer number between 1 and 5}}\\\"\n",
    "4. Please do not generate any other opening, closing, and explanations. Be sure to include [RESULT] in your output.\n",
    "\n",
    "###The instruction to evaluate:\n",
    "{instruction}\n",
    "\n",
    "###Response to evaluate:\n",
    "{response}\n",
    "\n",
    "###Reference Answer (Score 5):\n",
    "{reference_answer}\n",
    "\n",
    "###Score Rubrics:\n",
    "[Is the response correct, accurate, and factual based on the reference answer?]\n",
    "Score 1: The response is completely incorrect, inaccurate, and/or not factual.\n",
    "Score 2: The response is mostly incorrect, inaccurate, and/or not factual.\n",
    "Score 3: The response is somewhat correct, accurate, and/or factual.\n",
    "Score 4: The response is mostly correct, accurate, and factual.\n",
    "Score 5: The response is completely correct, accurate, and factual.\n",
    "\n",
    "###Feedback:\"\"\"\n",
    "\n",
    "from langchain.prompts.chat import (\n",
    "    ChatPromptTemplate,\n",
    "    HumanMessagePromptTemplate,\n",
    ")\n",
    "from langchain.schema import SystemMessage\n",
    "\n",
    "\n",
    "evaluation_prompt_template = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        SystemMessage(content=\"You are a fair evaluator language model.\"),\n",
    "        HumanMessagePromptTemplate.from_template(EVALUATION_PROMPT),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "\n",
    "eval_chat_model = ChatOpenAI(model=\"gpt-4-1106-preview\", temperature=0)\n",
    "evaluator_name = \"GPT4\"\n",
    "\n",
    "\n",
    "def evaluate_answers(\n",
    "    answer_path: str,\n",
    "    eval_chat_model: BaseChatModel,\n",
    "    evaluator_name: str,\n",
    "    evaluation_prompt_template: ChatPromptTemplate,\n",
    ") -> None:\n",
    "    try:  # load previous generations if they exist\n",
    "        with open(answer_path, \"r\") as f:\n",
    "            answers = json.load(f)\n",
    "    except:\n",
    "        answers = []\n",
    "\n",
    "    for experiment in tqdm(answers):\n",
    "        if f\"eval_score_{evaluator_name}\" in experiment:\n",
    "            continue\n",
    "\n",
    "        eval_prompt = evaluation_prompt_template.format_messages(\n",
    "            instruction=experiment[\"question\"],\n",
    "            response=experiment[\"generated_answer\"],\n",
    "            reference_answer=experiment[\"true_answer\"],\n",
    "        )\n",
    "        eval_result = eval_chat_model.invoke(eval_prompt)\n",
    "        feedback, score = [item.strip() for item in eval_result.content.split(\"[RESULT]\")]\n",
    "        experiment[f\"eval_score_{evaluator_name}\"] = score\n",
    "        experiment[f\"eval_feedback_{evaluator_name}\"] = feedback\n",
    "\n",
    "        with open(answer_path, \"w\") as f:\n",
    "            json.dump(answers, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RAG_SETTINGS = {}\n",
    "for chunk_size in [200, 300, 400, 512]:\n",
    "    for embeddings in [\"BAAI/bge-base-en-v1.5\"]:\n",
    "        for rerank in [False, True]:\n",
    "            settings_name = f\"chunk:{chunk_size}_embeddings:{embeddings.replace('/', '~')}_rerank:{rerank}_reader:zephyr-7b\"\n",
    "\n",
    "            print(f\"Running evaluation for {settings_name}:\")\n",
    "            output_file_name = f\"./output/rag_{settings_name}.json\"\n",
    "\n",
    "            print(\"Loading knowledge base embeddings...\")\n",
    "            knowledge_index = load_embeddings(\n",
    "                RAW_KNOWLEDGE_BASE,\n",
    "                chunk_size=chunk_size,\n",
    "                embedding_model_name=embeddings,\n",
    "            )\n",
    "\n",
    "            print(\"Running RAG...\")\n",
    "            reranker = (\n",
    "                RAGPretrainedModel.from_pretrained(\"colbert-ir/colbertv2.0\") if rerank else None\n",
    "            )\n",
    "            run_rag_tests(\n",
    "                eval_dataset=eval_dataset,\n",
    "                llm=get_llm_answer,\n",
    "                knowledge_index=knowledge_index,\n",
    "                output_file=output_file_name,\n",
    "                reranker=reranker,\n",
    "                verbose=False,\n",
    "            )\n",
    "\n",
    "            print(\"Running evaluation...\")\n",
    "            evaluate_answers(\n",
    "                output_file_name,\n",
    "                eval_chat_model,\n",
    "                evaluator_name,\n",
    "                evaluation_prompt_template,\n",
    "            )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "\n",
    "scores = datasets.load_dataset(\"m-ric/rag_scores_cookbook\", split=\"train\")\n",
    "scores = pd.Series(scores[\"normalized_score\"], index=scores[\"settings\"])\n",
    "\n",
    "fig = px.bar(\n",
    "    scores,\n",
    "    color=scores,m\n",
    "    labels={\n",
    "        \"value\": \"Accuracy\",\n",
    "        \"settings\": \"Configuration\",\n",
    "    },\n",
    "    color_continuous_scale=\"bluered\",\n",
    ")\n",
    "fig.update_layout(\n",
    "    width=900,\n",
    "    height=600,\n",
    "    barmode=\"group\",\n",
    "    yaxis_range=[0, 100],\n",
    "    title=\"<b>Accuracy of different RAG configurations</b>\",\n",
    "    xaxis_title=\"RAG settings\",\n",
    ")\n",
    "fig.layout.yaxis.ticksuffix = \"%\"\n",
    "fig.update_coloraxes(showscale=False)\n",
    "fig.update_traces(texttemplate=\"%{y:.1f}\", textposition=\"outside\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://huggingface.co/datasets/huggingface/cookbook-images/resolve/main/RAG_settings_accuracy.png\" height=\"500\">\n",
    "\n",
    "As you can see, each of these changes improved performance more or less. In particular, tuning the chunk size is both easy and very impactful.\n",
    "\n",
    "But this is our case, your results could be very different: now that you have a robust evaluation pipeline, you can set on to explore other options!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
